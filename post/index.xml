<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on @johtaniの日記 3rd</title>
    <link>https://blog.johtani.info/post/</link>
    <description>Recent content in Posts on @johtaniの日記 3rd</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 31 Dec 2020 22:48:46 +0900</lastBuildDate><atom:link href="https://blog.johtani.info/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PCもDIYしました</title>
      <link>https://blog.johtani.info/blog/2020/12/31/diy-pc/</link>
      <pubDate>Thu, 31 Dec 2020 22:48:46 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/12/31/diy-pc/</guid>
      <description>振り返りブログにも書きましたが、デスクトップPCを自作(DIY)しました。 組み立てたに使ったパーツのアフィリンクを貼っておきます。 10月くら</description>
      <content:encoded><p>振り返りブログにも書きましたが、デスクトップPCを自作(DIY)しました。
組み立てたに使ったパーツのアフィリンクを貼っておきます。</p>
<p>10月くらいにPC組もうと思って、組み上がったのは11月半ばでした。
最初はもうちょっと小さいPCにしようと思ってたのに気づいたらATXになってました。
パーツの選定の基準は以下のツイートです。影響されやすいなぁ。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">タワーってほど大きくなくていいと思いますが、NZXT H510 ぐらいの大きさは欲しいですね。<a href="https://t.co/s1c1fOnPyI">https://t.co/s1c1fOnPyI</a><br>単純に、スペースがないと組みにくいのと冷えにくい、買ってきたものがやっぱりこれだと入らない、というリスクがかなり減るので。</p>&mdash; さ↑ぼてん↓（さぼ福）🌵 (@cactusman) <a href="https://twitter.com/cactusman/status/1314462879790198784?ref_src=twsrc%5Etfw">October 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">今年3台作りました。ノートPC使えない体になります。 <a href="https://t.co/8zDmeRwNkS">pic.twitter.com/8zDmeRwNkS</a></p>&mdash; miyake (@kazuyukimiyake) <a href="https://twitter.com/kazuyukimiyake/status/1314568016177160197?ref_src=twsrc%5Etfw">October 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>白いケースいいなぁという感じで、ケースとCPUクーラーが決まったんで、ググって<a href="https://www.youtube.com/watch?v=XUZqsHW2EEg">こちらのHAKKAさんのYouTube</a>見ながら妄想してました。
パーツがなかなか揃わなくて。
そして、PCが光るのどうなの?と思ってたのに、気づいたら光ってました(なぜ?)。</p>
<p>グラボがない状態(CPU内蔵GPU)で12月頭までは動作してました。</p>
<p>購入自体は<a href="https://shop.tsukumo.co.jp/">TSUKUMO</a>、<a href="https://www.biccamera.com/">ビックカメラオンライン</a>、<a href="https://www.ark-pc.co.jp/">パソコンSHOPアーク</a>、<a href="https://www.e-trend.co.jp/">イートレンドオンラインショップ</a>でオンラインで購入しました。</p>
<h3 id="pcケース">PCケース</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07TD9S3HZ/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07TD9S3HZ&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07TD9S3HZ/?tag=johtani-22">
      Amazon | NZXT H510i White CA-H510I-W1 CS7946 | NZXT | PCバッグ・ケース・スリーブ 
      </a>
    </p>
  </div>
</div>
<h3 id="電源">電源</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07FZRW4H6/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07FZRW4H6&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07FZRW4H6/?tag=johtani-22">
      Amazon | NZXT E850 デジタル電源ユニット 80 Plus Gold 認証 [ 定格 850W 出力 ] NP-1PM-E850A-JP NP-1PM-E850A-JP SP947 | NZXT | パソコン・周辺機器 通販
      </a>
    </p>
  </div>
</div>
<h3 id="マザーボード">マザーボード</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07THJ8JSD/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07THJ8JSD&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07THJ8JSD/?tag=johtani-22">
      Amazon | ASRock AMD Ryzen 3000シリーズ CPU(Soket AM4)対応 X570チップセット搭載 ATX マザーボード X570 Steel Legend | ASROCK 
      </a>
    </p>
  </div>
</div>
<h3 id="cpu">CPU</h3>
<p><a href="https://pc.watch.impress.co.jp/docs/topic/review/1268025.html">AMD Ryzen 4750G</a></p>
<h3 id="cpuクーラー">CPUクーラー</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B082DYSQVF/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B082DYSQVF&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B082DYSQVF/?tag=johtani-22">
      Amazon | NZXT KRAKEN Z63 簡易水冷CPUクーラー 液晶モニタ搭載 RGB対応 280mm RL-KRZ63-01 FN1441 | NZXT | CPUファン 通販
      </a>
    </p>
  </div>
</div>
<h3 id="メモリ">メモリ</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B0861QJV67/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B0861QJV67&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B0861QJV67/?tag=johtani-22">
      Amazon | G.SKILL 128GB（4 x 32GB）Trident Z NeoシリーズDDR4 SDRAM 3200MHz PC4-25600デスクトップメモリモデルF4-3200C16Q-128GTZN | G.Skill | メモリ 通販
      </a>
    </p>
  </div>
</div>
<h3 id="ssd">SSD</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07ZPRMLJP/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07ZPRMLJP&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07ZPRMLJP/?tag=johtani-22">
      Amazon | Seagate FireCuda 520 M.2 1TB PCIe Gen4x4 内蔵SSD M.2 2280 3D TLC ZP1000GM3A002 | SEAGATE 
      </a>
    </p>
  </div>
</div>
<h3 id="グラボ">グラボ</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B08KXPPFKY/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B08KXPPFKY&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B08KXPPFKY/?tag=johtani-22">
      Amazon | GIGABYTE NVIDIA GeForce RTX3080搭載 グラフィックボード GDDR6X 10GB ホワイトモデル【国内正規代理店品】GV-N3080VISION OC-10GD | 日本ギガバイト
      </a>
    </p>
  </div>
</div>
<h3 id="拡張カード">拡張カード</h3>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07Z4XKQ4J/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07Z4XKQ4J&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07Z4XKQ4J/?tag=johtani-22">
      Amazon | ASRock 拡張インターフェースボード Thunderbolt 3 AIC R2.0 | ASROCK | インターフェースカード 通販
      </a>
    </p>
  </div>
</div>
<p>拡張カードは実は差し込んだら最初うまく動かなくてサポートに連絡しました。同じ構成の検証機を用意して解決方法を見つけてもらいました。しかも数日で。サイコーのエクスペリエンスでした!</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">日本語でサポートしてもらえてしかも数日で解決してもらった。ASRockのサポートすばらしかったです！ <a href="https://t.co/8FasCInIqt">https://t.co/8FasCInIqt</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1336190706289790977?ref_src=twsrc%5Etfw">December 8, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h3 id="その他">その他</h3>
<p>ケースのフロントパネルのUSB-Cポートを使えるようにするために。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B086MRDQFY/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B086MRDQFY&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B086MRDQFY/?tag=johtani-22">
      Amazon | Cablecc USB 3.1フロントパネルソケットType-E-マザーボード用USB 3.0 20ピンヘッダーオス拡張アダプター | cablecc | PCアクセサリ・サプライ 通販
      </a>
    </p>
  </div>
</div>
<p>最近のPCパーツはUSBでモニタリングできるみたいで、マザボのUSBポートが足りなかったので拡張しました。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B01LXKQLZW/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B01LXKQLZW&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B01LXKQLZW/?tag=johtani-22">
      Amazon | マザーボードのUSB 9ピン 増設 内部用4ポートUSB2.0 HUB | ノーブランド品 | USBハブ 通販
      </a>
    </p>
  </div>
</div>
<p>白いケースだし、「映え」のために白いケーブルを。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B06XGHCX2R/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B06XGHCX2R&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B06XGHCX2R/?tag=johtani-22">
      Amazon | Novonest 電源専用延長スリーブ 補助電源モジュラーケーブル 長さ300mm 6本1セット スリーブガイド24点セット付き 【SC304】 | ｎｏｖｏｎｅｓｔ | 電源ユニット 通販
      </a>
    </p>
  </div>
</div>
<h3 id="組み立て中">組み立て中</h3>
<p>手元にあったCPUでBiosアップデート(<a href="https://www.asrock.com/mb/AMD/X570%20Steel%20Legend/index.jp.asp#BIOS">Ryzen 5000シリーズにしたかったから。まだできてないけど</a>)。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">応急処置でBiosアップデートしてる <a href="https://t.co/mXf2B5kwqB">pic.twitter.com/mXf2B5kwqB</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1325465218504257546?ref_src=twsrc%5Etfw">November 8, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">さーて、頑張るぞー <a href="https://t.co/HmlGqREjmP">pic.twitter.com/HmlGqREjmP</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1327062393772412928?ref_src=twsrc%5Etfw">November 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">さて、仮起動実験ですよと <a href="https://t.co/5HP3cbs6SH">pic.twitter.com/5HP3cbs6SH</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1327075463718572033?ref_src=twsrc%5Etfw">November 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">きたきたー <a href="https://t.co/4sL3ymecEo">pic.twitter.com/4sL3ymecEo</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1327076477662158848?ref_src=twsrc%5Etfw">November 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">コネクター刺しまくらないと <a href="https://t.co/Soye3lS4M9">pic.twitter.com/Soye3lS4M9</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1327102892247924737?ref_src=twsrc%5Etfw">November 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h3 id="完成はこんなかんじ">完成はこんなかんじ</h3>
<p>初期構成</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">組み上がったよー <a href="https://t.co/sBwpvLsZh3">pic.twitter.com/sBwpvLsZh3</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1327603468912136198?ref_src=twsrc%5Etfw">November 14, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>グラボデプロイ</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">設置完了してこんな感じ。なんかベンチマーク走らせるかなぁ。何がいいだろ <a href="https://t.co/aZufAE9fJA">pic.twitter.com/aZufAE9fJA</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1334667407265165313?ref_src=twsrc%5Etfw">December 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>グラボのおまけも付けてみた。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">まぁ、せっかくついてきたので取り付けてみました（グラボ下の光ってるパネル）。 <a href="https://t.co/068IxaV1ps">pic.twitter.com/068IxaV1ps</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1334865685374062594?ref_src=twsrc%5Etfw">December 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>とまぁ、こんな感じでした。
メモリは多分使い切れないだろうなぁと思いながら、やってみたかったという感じです。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2020）</title>
      <link>https://blog.johtani.info/blog/2020/12/31/looking-back-2020/</link>
      <pubDate>Thu, 31 Dec 2020 18:13:24 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/12/31/looking-back-2020/</guid>
      <description>今年も振り返りブログ書いてます。カタンの航海者版を購入して家族とやってましたが、負けてしまいました。 振り返り（2019年に書いた抱負から） ま</description>
      <content:encoded><p>今年も振り返りブログ書いてます。カタンの航海者版を購入して家族とやってましたが、負けてしまいました。</p>
<h2 id="振り返り2019年に書いた抱負から">振り返り（2019年に書いた抱負から）</h2>
<p>まずは去年の抱負を元に書きますが、3月以降はコロナウィルスの影響でいろいろと生活が変わってしまいましたね。</p>
<h3 id="職につく">職につく</h3>
<p>厳密には達成してないことになるのかな？</p>
<p>1月末に<a href="https://blog.johtani.info/blog/2020/01/31/start-freelance/">フリーランスはじめました(仮)</a>という記事を書きましたが、
そのまま、現時点では個人事業主でフリーランスとして4社ほどお手伝いさせていただいています。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">お手伝いしてるお客さんの事例が出てた。直接的に類似画像検索の部分を手伝ってるわけではないんですが、Azure Cognitive Searchの調査とかやってます。 / 「ハッカソン」で社外の知見を積極的に活用! 富士フイルムソフトウエアが実践するサービス開発の理想形 - <a href="https://t.co/yDIwYmYDBM">https://t.co/yDIwYmYDBM</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1273809408875622400?ref_src=twsrc%5Etfw">June 19, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">名前出てた。ZOZOさんの手伝いさせていただいております。 / ZOZOTOWNにおける検索速度改善までの道のり - ZOZO Technologies TECH BLOG - <a href="https://t.co/OqaFMYe0TY">https://t.co/OqaFMYe0TY</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1296269326962417669?ref_src=twsrc%5Etfw">August 20, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>こんな感じです。
コロナウィルスの影響もあり、完全リモートで仕事をさせていただいています。
感謝しかないです。</p>
<h4 id="ブログプラットフォームの移行">ブログプラットフォームの移行</h4>
<p>昨年の振り返りの時点で目を付けていたHugoへの乗り換えを行いました。
乗り換えについてもまとめてあります。</p>
<ul>
<li><a href="https://blog.johtani.info/blog/2020/01/16/moving-to-hugo/">OctopressからHugoへ移行中(まだ途中)</a></li>
<li><a href="https://blog.johtani.info/blog/2020/01/22/intro-hugo-and-theme/">ブログ移行日記(その1) - Hugoとテーマ</a></li>
<li><a href="https://blog.johtani.info/blog/2020/01/23/convert-md-from-octopress-to-hugo/">ブログ移行日記(その2) - Markdownファイルの変換</a></li>
<li><a href="https://blog.johtani.info/blog/2020/01/24/setting-hugo/">ブログ移行日記(その3) - Hugoの設定と微調整(テーマに合わせた)</a></li>
<li><a href="https://blog.johtani.info/blog/2020/01/28/troduce-algolia/">ブログ移行日記(その4) - 検索機能(Algolia)の導入</a></li>
<li><a href="https://blog.johtani.info/blog/2020/02/21/import-jugem-posts/">ブログ移行日記(その5) - Jugemのブログを移行</a></li>
</ul>
<p>無事移行できて、特に困ってない感じです。</p>
<h4 id="プログラミング">プログラミング</h4>
<p>昨年よりは書いた気がしています。
まぁ、書きなぐって放置してるのもありますが。。。</p>
<p>細々としたツールがいくつか。</p>
<ul>
<li><a href="https://github.com/johtani/from-octopress-to-hugo">from-octopress-to-hugo</a></li>
<li><a href="https://github.com/johtani/bulk2es-rs">bulk2es-rs</a></li>
<li><a href="https://github.com/johtani/azure-search-analyze-client">azure-search-analyze-client</a></li>
<li><a href="https://github.com/johtani/wiki-extractor-rs">wiki-extractor-rs</a></li>
<li><a href="https://github.com/johtani/kuromoji-graphviz-output">kuromoji-graphviz-output</a></li>
<li><a href="https://github.com/johtani/kuromoji-cli">kuromoji-cli</a></li>
</ul>
<p>あとは、Linderaという形態素解析も手伝ってみました。
ちょっと後半は忙しくなり、コーディングが減ってるんで、また復活させないと。</p>
<h4 id="rust再入門">Rust再入門</h4>
<p>Tantivyは結局触れてないけど、<a href="https://github.com/lindera-morphology/">Lindera</a>を手伝えました。あとは、ちょっとしたツール(bulk2es-rs)なども作りました。
知り合いとやっていたRust the bookを読むのも一通り終わりました。
Linderaまわりのリファクタとかを年明けにやろうかな?</p>
<h4 id="検索の勉強">検索の勉強</h4>
<p>仕事になりました。というか、しました。
ブッチャー本も買ったんですが、手を付けられてないので、年明けから心機一転、よみ始めようと思います。</p>
<h4 id="検索技術勉強会の継続">検索技術勉強会の継続</h4>
<p>残念ながらだめでした。
年初にスピーカーの方に連絡は入れて履いたのですが、コロナウイルスの影響で
今年はオフラインの勉強会は無理だろうとい事で延期したままです。
オンラインの勉強会もいくつか開催されていましたが、検索技術勉強会は残念ながらモチベーションもわかずのまま今年も終わってしまいました。
単に話を聴くセミナーのようなものよりは、Podcastのような形を取るのがいいのかもなぁとは思っているのですが。。。
だれか、サクッと話したい人いたらやってみませんか?</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<p>さて、ここからは今年の出来事を。</p>
<ul>
<li>自宅環境の改善</li>
<li>Windowsへの移行</li>
<li>キーボードDIY</li>
<li>フリーランス</li>
<li>ブログ書いたなぁ</li>
</ul>
<p>3月以降はずーっと自宅で仕事してました。
ということで、自宅の環境がどんどんアップデートされていった1年でした。
最終バージョンは年内に間に合わなくて、年明けに書きますが。。。
途中経過はこんな感じでした。今は更に変わっていますが。</p>
<ul>
<li><a href="https://blog.johtani.info/blog/2020/03/26/working-facility/">3月バージョン</a></li>
<li><a href="https://blog.johtani.info/blog/2020/09/08/update-working-facility/">9月バージョン</a></li>
</ul>
<p>12月バージョンでも書きますが、ずっと自宅で作業だったのもあり、自作PCをやりたくなったので、久々にWindowsに帰ってきました。Ubuntuもチョット検討しましたが、Windowsに今は落ち着いています。自作PCはこんな感じ。
自作PCブログも後で書きますね。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">設置完了してこんな感じ。なんかベンチマーク走らせるかなぁ。何がいいだろ <a href="https://t.co/aZufAE9fJA">pic.twitter.com/aZufAE9fJA</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1334667407265165313?ref_src=twsrc%5Etfw">December 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<ul>
<li><a href="https://blog.johtani.info/blog/2020/10/15/restart-windows/">Windowsをお試し中</a></li>
<li><a href="https://blog.johtani.info/blog/2020/10/18/restart-windows-no1/">Windowsへの移行（その１）</a></li>
</ul>
<p>AutoHotKeyで色々と弄っているので、素のWindowsは触れる気がしないけど。。。</p>
<p>自作つながりで、キーボードもDIYしました。</p>
<ul>
<li><a href="https://blog.johtani.info/blog/2020/10/11/build-dozen0/">Dozen0を作成した</a></li>
<li><a href="https://blog.johtani.info/blog/2020/11/05/build-corne-light-v2/">Corne Light v2を作成した</a></li>
<li><a href="https://blog.johtani.info/blog/2020/12/03/build_corne_choc/">Corne Chocolateを組み立てた</a></li>
<li><a href="https://kochikeyboard.stores.jp/items/5fcb8040df5159606b8c5d74">Sofle Keyboard</a>も組み立てた(けど、ビルドブログはまだ)</li>
</ul>
<p>分離キーボードには昨年から興味があったのですが、手を付けていませんでした。
今年は自宅でずっと作業でしたから、これを機会に作ってみました。
既製品(<a href="https://www.zsa.io/moonlander/">Moonlander</a>)も気になってはいたのですが、DIYキーボードはプラモデル感覚で作れそうだし面白そうだなと。
今年一番ハマったものかもしれないです。
知り合いが始めた<a href="https://kochikeyboard.stores.jp/">Kochi Keyboard</a>で最初に発売されたのがCorne Light v2だったので、これが初めての分割キーボードです。
40%キーボードからいきなり入ったので最初はかなり大変でしたが、なれると入力が早くなる気がします。ホームポジションからほぼ動かないのはいいですね。最初は数字列がないのでアホか???と思っていましたが。
今は、Sofle(60%キーボード)がメインになっています。40%から60%に移行すると
数字のキーが遠いなと思ってしまいますねw</p>
<p>どうなるものかと思いつつ、フリーランスを続けています。
最初に仕事をいただいた富士フイルムソフトウェアさんには非常にお世話になっています(今もお手伝いさせてもらっています、ありがたい)。来年も精進していきます!</p>
<p>今年はブログを書きました、最後はちょっと息切れしてる感じになってるけど。。。
もっとカジュアルに書くようにしないとなぁ。
年明けにまだ書いてないブログを書いていかないと。「自作PCブログ」「Sofleのビルドブログ」「2020年買ってよかったブログ」とか。</p>
<h2 id="来年の抱負">来年の抱負</h2>
<p>ということで、来年の抱負です。</p>
<ul>
<li>フリーランス継続?</li>
<li>プログラミング</li>
<li>検索の勉強</li>
<li>検索のポッドキャスト?</li>
</ul>
<p>とりあえず今年は乗り切れました。来年も食べていくためにもフリーランスを継続できるように、お手伝いさせて頂いてる各社に
継続してもらえるように精進していきます。色々と勉強させて頂いてるのでほんとうにありがたいです。</p>
<p>プログラミングは今年の後半はスローダウンしてしまったのでがんばらないと。
検証用のコードやちょっとしたツールを書くほうが多いので、もう少し継続してメンテナンスするようなものも書きたいなぁと。</p>
<p>検索の勉強はまずはブッチャー本を買ってしまったので、ここからでしょうか。
値段も厚みもすごいので、コツコツ元を取れるようにがんばります。。。</p>
<p>来年もまだまだ自宅で仕事が続きそうなので、勉強会というよりもポッドキャストみたいなものをやるのがいいのかもなと。
2020年初にやらせていただいた、「<a href="https://blog.johtani.info/blog/2020/02/10/explore-search-system/">検索システム探訪</a>」をポッドキャスト風にやるのもありなのかもなぁ。
興味ある人いたら連絡ください。公開しない雑談とかもありだと思ってるので。</p>
<p>今年は色々ありましたが、無事に乗り切れました。
気軽にランチなどで遊びに行けない状況ですが、雑談相手になっていいなと思う方、連絡お待ちしています。</p>
<p>さて、来年はどんなキーボード作るのかなぁ(え?また作るの??)。
今後もよろしくおねがいしまーす!</p>
</content:encoded>
    </item>
    
    <item>
      <title>Index Template V2</title>
      <link>https://blog.johtani.info/blog/2020/12/17/index_template_v2/</link>
      <pubDate>Thu, 17 Dec 2020 23:55:55 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/12/17/index_template_v2/</guid>
      <description>Elastic stack (Elasticsearch) Advent Calendar 2020の18日目の記事になります。 本日の勉強会でLTをしましたが、しゃべり足りなかったんで。 Elasticsearch 7.8でこっそりとリリースされたI</description>
      <content:encoded><p><a href="https://qiita.com/advent-calendar/2020/elasticsearch">Elastic stack (Elasticsearch) Advent Calendar 2020</a>の18日目の記事になります。</p>
<p><a href="https://noti.st/johtani/aa1gAo/index-template-v2">本日の勉強会でLT</a>をしましたが、しゃべり足りなかったんで。
Elasticsearch 7.8でこっそりとリリースされたIndex Template V2について調べたのでどんなものかをまとめてみます。
リリースブログには出てきてない（<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.8/release-highlights.html#add-composable-index-templates">7.8のEsのページのWhat&rsquo;s newには出てた</a>）ので気づいてない人も多いのではないでしょうか？</p>
<h2 id="index-templateとは">Index Templateとは？</h2>
<p>まずはIndex Templateがどんなものかを説明しましょう。</p>
<p>Indexの設定やマッピングはデフォルト値以外を設定したい場合に、毎回&quot;mappings&quot;や&quot;settings&quot;の設定を指定してIndexを作成するのは手間がかかります。
そこで便利な機能として提供されているのがIndex Templateです。このIndex TemplateはCluster Stateに保管されます。</p>
<p>Index Templateを利用するときの流れは以下の通りです。</p>
<ol>
<li>Index Templateの作成</li>
<li>Indexの作成</li>
</ol>
<p>例えば、3ノード構成のクラスターでインデックスを作成するときに常に&quot;number_of_shards: 3&quot;を設定したいとします。
Index Templateは次のような感じになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">PUT</span> <span style="color:#a6e22e">_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">blog_num_shards</span>
{
  <span style="color:#e6db74">&#34;index_patterns&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;blog_*&#34;</span>,
  <span style="color:#e6db74">&#34;settings&#34;</span><span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;index&#34;</span><span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;number_of_shards&#34;</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">3</span>
    }
  }
}
</code></pre></div><p>または日本語のシンプルな形態素解析のアナライザーの設定を&quot;jp_&ldquo;という名前でるようできるようにしたい場合には次のような感じになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">PUT</span> <span style="color:#a6e22e">_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_simple_kuromoji</span>
{
  <span style="color:#e6db74">&#34;index_patterns&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;jp_&#34;</span>,
  <span style="color:#e6db74">&#34;settings&#34;</span><span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;index&#34;</span><span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;analysis&#34;</span><span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;simple_jp&#34;</span><span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;type&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;custom&#34;</span>,
          <span style="color:#e6db74">&#34;tokenizer&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;kuromoji&#34;</span>
        }
      }
    }
  }
}
</code></pre></div><p>インデックステンプレートの登録が終われば、インデックスを作成するタイミングでIndex Templateが適用されます。
例えば、<code>blog_2020</code>というインデックスを作成すると、<code>number_of_shards: 3</code>のインデックスが作成されます（デフォルトは<code>1</code>）。
<code>jp_blog_2020</code>というインデックスを作成すると、<code>simple_jp</code>というAnalyzerが設定されています。
このように、インデックス名を意識するだけで設定が適用されていくのが利点です。</p>
<p>ちなみに、Index TemplateはIndex作成時に適用されるだけなので、Index Templateを変更してもこれまでのインデックスへは影響はありません。</p>
<h2 id="これまでのindex-templateの問題点">これまでのIndex Templateの問題点</h2>
<p>と、Index Templateが便利なのはわかりましたが、ではなぜ今回V2がリリースされたのでしょうか？
先ほどの例を見るとわかりますが、これまでのIndex Templateは部品化が難しいのが問題でした。
Index Templateはそれぞれがインデックスの作成時に適用されます。
が、Index TemplateにIndex Templateを組み込むことはできません。
例えば、先ほどサンプルとして作成した<code>jp_simple_kuromoji</code>のIndex Templateは<code>jp_</code>で始まるインデックスにしか適用できません。</p>
<p>では、<code>blog_</code>で始まるインデックスにもkuromojiのシンプルなアナライザーを使いたくなった場合はどうなるでしょう？
残念ながら、<code>jp_simple_kuromoji</code>と同じ設定を<code>blog_num_shards</code>のテンプレートに追加するか、<code>jp_simple_kuromoji</code>の<code>index_patterns</code>の部分だけを書き換えた新しいテンプレートを用意するか、<code>jp_simple_kuromoji</code>の<code>index_patterns</code>に<code>jp_</code>を追加する方法です。
いずれにしてもIndex Templateの継承（複数のテンプレートを1つのインデックスに紐づける）が必要となります。
この時、複数のIndex Templateが適用されるため、適用する順番が出てきます。
この順番をIndex Templateの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-templates-v1.html#put-index-template-v1-api-query-param"><code>order</code>パラメータ</a>で指定できます。</p>
<p>ただ、これも問題のもととなっていました。
複数あるIndex Templateのどれがどの順番で適用されるのか？それはインデックスを作成時にようやくわかります。
使いたい側（インデックス）が、使いたいもの（テンプレート）を指定するのではなく、その逆（使いたいものに使いたい側の情報を設定しなくてはならない（<code>index_patterns</code>や<code>order</code>））になっているのでわかりにくくなっていました。</p>
<p>ということで解決策としてリリースされたのがIndex Template V2です（ちなみに名前にV2とは言ってるわけではなく、現在のIndex Templateの機能がlegacy index templateという名前になり、Deprecatedになっています（まだログには出ない））。</p>
<h2 id="index-template-v2">Index Template V2</h2>
<p>7.8のWhat&rsquo;s newドキュメントではComposable Index Templateと紹介されています。
大きく3つのエンドポイントが提供されます。</p>
<ul>
<li>Component Template 用API
<ul>
<li>テンプレートのコンポーネントという単位で管理するためのAPI。</li>
<li>登録更新、削除、取得が可能</li>
</ul>
</li>
<li>Index Template 用API
<ul>
<li>Index Templateを管理するためのAPI</li>
<li>登録更新、削除、取得が可能</li>
</ul>
</li>
<li>Simulate index template API(Experimental)
<ul>
<li>Index Template</li>
</ul>
</li>
</ul>
<p>Legacy Index Templateとの大きな違いは、</p>
<ul>
<li>テンプレートをコンポーネントにできる</li>
<li>1つのインデックスに適用されれるテンプレートは<strong>1つ</strong>だけ</li>
</ul>
<p>という点です。これまでとは挙動が異なるので注意が必要です。</p>
<p>新しいIndex Templateを利用する際の流れは次のようになります。</p>
<ol>
<li>Component Templateの作成</li>
<li>Index Templateの作成</li>
<li>作成したIndex Templateの挙動を確認</li>
<li>実際にIndex名を指定してIndex Templateの挙動を確認</li>
</ol>
<p>という形です。では、それぞれの機能を見ていきましょう。</p>
<h3 id="coponent-template-api">Coponent Template API</h3>
<p>テンプレートコンポーネントを管理するためのAPIです。
具体的なAPIごとに説明していきます。</p>
<h4 id="put-_component_template">PUT _component_template</h4>
<p>コンポーネントを登録、変更するためのAPIです。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-component-template.html">公式ドキュメントはこちらです。</a>
先ほどのkuromojiのAnalzyerをコンポーネントとして登録してみましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">PUT</span> <span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_simple_kuromoji</span>
{
  <span style="color:#e6db74">&#34;template&#34;</span><span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;settings&#34;</span><span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;index&#34;</span><span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;analysis&#34;</span><span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;analyzer&#34;</span><span style="color:#f92672">:</span> {
            <span style="color:#e6db74">&#34;simple_jp&#34;</span><span style="color:#f92672">:</span> {
            <span style="color:#e6db74">&#34;type&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;custom&#34;</span>,
            <span style="color:#e6db74">&#34;tokenizer&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;kuromoji_tokenizer&#34;</span>
            }
          }
        }
      }
    }
  },
  <span style="color:#e6db74">&#34;version&#34;</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">1</span>,
  <span style="color:#e6db74">&#34;_meta&#34;</span><span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;description&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;simpleなKuromoji analyzer&#34;</span>
  }
}
</code></pre></div><p>先ほどと特に違いはありません。<code>template</code>という階層が1段増え、<code>index_patterns</code>がなくなりました。
<code>template</code>部分はIndexに設定する<code>settings</code>、<code>mappings</code>、<code>aliases</code>が指定可能です。
ちなみに、&ldquo;template&quot;の中身をそのままIndex作成時に使用した場合にエラーにならない設定である必要があります。
ここは注意が必要です。例えば、component Aでkuromojiのアナライザーの設定をし、component Bでそのアナライザーを使用するフィールドのmappingだけを記述した場合、component Bでエラーが発生します。上記のようなシチュエーションでは、Index Templateで利用するフィールドを定義し、component Aを利用する宣言をすれば問題ありません。</p>
<p>そのほかに使えるパラメータで便利なものを紹介しておきます。</p>
<ul>
<li>
<p>クエリパラメータ</p>
<ul>
<li><code>create</code>: URLに指定するパラメータ。<code>true</code>を指定することで、既に存在する場合にエラーを返してくれる。更新も同じAPIなので間違わないようにするために利用すると便利。デフォルトは<code>false</code></li>
</ul>
</li>
<li>
<p>リクエストボディ</p>
<ul>
<li><code>_meta</code>: テンプレートにメタ情報を付与できる。<code>_meta</code>の中は自由に記述可能。コメントなどを書いておくとあとでわかりやすい</li>
</ul>
</li>
</ul>
<h4 id="get-_component_template">GET _component_template</h4>
<p>コンポーネントを取得するためのAPIです。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/getting-component-templates.html">公式ドキュメントはこちらです。</a>
一覧での取得とリストでの取得が可能です。</p>
<p>一覧取得サンプル</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">GET</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span>
<span style="color:#a6e22e">GET</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/*</span>
</code></pre></div><p>これ以外に、IDを指定して取得することも可能です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">GET</span> <span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_simple_kuromoji</span>
</code></pre></div><p><code>*</code>を利用して複数取得も可能です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">GET</span> <span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp</span><span style="color:#f92672">*</span>
<span style="color:#a6e22e">GET</span> <span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">j</span><span style="color:#f92672">*</span><span style="color:#a6e22e">_</span><span style="color:#f92672">*</span>
</code></pre></div><h4 id="delete-_component_template">DELETE _component_template</h4>
<p>コンポーネントを削除するためのAPIです。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-delete-component-template.html">公式ドキュメントはこちらです。</a>
IDを指定してコンポーネントを削除できます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">DELETE</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_simple_kuromoji</span>
</code></pre></div><p>ID指定以外に<code>*</code>を利用することも可能ですが気を付けて使用しましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">DELETE</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_</span><span style="color:#f92672">*</span>
<span style="color:#a6e22e">DELETE</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/*</span>
</code></pre></div><p>ちなみにIndex Templateで利用されているコンポーネントを削除しようとした場合はエラー（ステータスコード400）が返ってきます。
<code>*</code>などを使用して削除しようとした場合は、ひとつでも利用されているものが含まれている場合は削除は実行されずにエラーだけが返ってくるようになっています。</p>
<h3 id="index-template-api">Index Template API</h3>
<p>Index Templateを管理するためのAPIです。
Component Template APIで定義したコンポーネントを利用してテンプレートを作成できます。コンポーネントを利用しないで単体で完結したテンプレートも作成可能です。
具体的なAPIごとに説明していきます。</p>
<h4 id="put-_index_template">PUT _index_template</h4>
<p>Index Templateを登録・更新するためのAPIです。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-put-template.html">公式ドキュメントはこちらです。</a></p>
<p>これまでのLegacy Index TemplateのAPIのエンドポイント(<code>_template</code>)ではなく、(<code>_index_template</code>)というエンドポイントになっていることにまず注意してください。
先ほど作成した<code>jp_simple_kuromoji</code>コンポーネントと、追加で作成した<code>3_shards</code>というコンポーネントを利用して
<code>blog_</code>で始まるインデックスに適用できるIndex Templateを作成してみましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">PUT</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">blog_template</span>
{
  <span style="color:#e6db74">&#34;index_patterns&#34;</span><span style="color:#f92672">:</span> [<span style="color:#e6db74">&#34;blog_*&#34;</span>],
  <span style="color:#e6db74">&#34;template&#34;</span><span style="color:#f92672">:</span> {  
    <span style="color:#e6db74">&#34;mappings&#34;</span><span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;properties&#34;</span><span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;hoge&#34;</span><span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;type&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;text&#34;</span>,
          <span style="color:#e6db74">&#34;analyzer&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;simple_jp&#34;</span>
        }
      }
    }
  },
  <span style="color:#e6db74">&#34;priority&#34;</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">100</span>,
  <span style="color:#e6db74">&#34;composed_of&#34;</span><span style="color:#f92672">:</span> [<span style="color:#e6db74">&#34;jp_simple_kuromoji&#34;</span>, <span style="color:#e6db74">&#34;3_shards&#34;</span>]
}
</code></pre></div><p><code>composed_of</code>というパラメータに利用したいコンポーネントを配列で記述していきます。
Index Templateが適用される時に、ここに記述されている順番でコンポーネントが適用されていきます。
ですので、最後に書いてあるテンプレートが一番強いことになります。
これは例えば、同じ設定値<code>number_of_shards</code>を2つのコンポーネントが設定している場合に、最後に設定した値がインデックスに採用されるという意味です
(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-put-template.html#multiple-component-templates">公式ドキュメントにサンプルが掲載されています</a>)。
なお、存在しないコンポーネントを<code>composed_of</code>に指定した場合は400エラーが返ってきます。同じコンポーネントを複数重複して指定した場合は特にエラーにはなりませんでした。
<code>composed_of</code>を指定しなければ、単体で完結したテンプレートを定義可能です。</p>
<p>次に重要な設定は<code>priority</code>です。これまでのテンプレート機能と異なる点で説明しましたが、</p>
<blockquote>
<p>1つのインデックスに適用されれるテンプレートは<strong>1つ</strong>だけ
となっています。
この1つを決めるための値が<code>priority</code>になります。
インデックス名によっては<code>index_pattern</code>の定義によって、複数のIndex Templateにマッチします。例えば<code>blog_*</code>と<code>*_2020</code>のIndex Templateがあった場合に<code>blog_2020</code>というインデックスを作った場合です。
この時、Elasticsearchは<code>priority</code>の大きい値を持ったIndex Template<strong>だけ</strong>を適用します。
<code>blog_*</code>のIndex Templateの<code>property</code>が<code>10</code>、<code>*_2020</code>のIndex Templateの<code>property</code>が<code>100</code>だった場合、<code>*_2020</code>のテンプレートが適用されます。Legacy Index Templateでは<code>order</code>という似たパラメータがありましたが、こちらは適用する<strong>順序</strong>を決定するためのものでした。挙動が違うので注意しましょう。</p>
</blockquote>
<p>ちなみに、<code>blog_*</code>と<code>*_2020</code>の<code>property</code>がどちらも<code>10</code>というIndex TemplateをPUTしようとした場合、2番目にIndex TemplateをPUTするタイミングでエラーが返ってきます。</p>
<p>そのほかに使えるパラメータで便利なものはPUT _component_templateと同様に<code>create</code>と<code>_meta</code>です。</p>
<h4 id="get-_index_template">GET _index_template</h4>
<p>Index Templateを取得するためのAPIです。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-get-template.html">公式ドキュメントはこちらです。</a>
一覧での取得とリストでの取得が可能です。</p>
<p>一覧取得サンプル</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">GET</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_index_template</span>
<span style="color:#a6e22e">GET</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/*</span>
</code></pre></div><p>これ以外に、IDを指定して取得することも可能です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">GET</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">blog_template</span>
</code></pre></div><p><code>*</code>を利用して複数取得も可能です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">GET</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">blog_</span><span style="color:#f92672">*</span>
<span style="color:#a6e22e">GET</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">b</span><span style="color:#f92672">*</span><span style="color:#a6e22e">_</span><span style="color:#f92672">*</span>
</code></pre></div><h4 id="delete-_index_template">DELETE _index_template</h4>
<p>Index Templateを削除するためのAPIです。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-delete-template.html">公式ドキュメントはこちらです。</a></p>
<p>IDを指定してコンポーネントを削除できます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">DELETE</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_simple_kuromoji</span>
</code></pre></div><p>ID指定以外に<code>*</code>を利用することも可能ですが気を付けて使用しましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">DELETE</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">jp_</span><span style="color:#f92672">*</span>
<span style="color:#a6e22e">DELETE</span> <span style="color:#f92672">/</span><span style="color:#a6e22e">_component_template</span><span style="color:#f92672">/*</span>
</code></pre></div><p>最後のサンプルを実行すると、Elasticのツールなどが登録したIndex Template以外はすべて削除されてしまうので本当に気を付けましょう。</p>
<h3 id="simulalte-api">Simulalte API</h3>
<p>さて、コンポーネントとテンプレートを作成したので確認をしましょう。
Legacy Index Templateでは確認するためには実際にインデックスを作成するしかありませんでしたが、V2ではSimulate APIが用意されています。
このSimulate APIには2つのAPIがあります。</p>
<ul>
<li>POST /_index_template/_simulate/&lt;テンプレート名&gt;</li>
<li>POST /_index_template/_simulate_index/&lt;インデックス名&gt;</li>
</ul>
<p>Index Templateの確認のためのAPIとインデックス名を指定したときに出来上がるIndexの設定を確認するためのAPIです。
Indexを実際に作成しなくても確認できるのは便利ですね。</p>
<p>例えば先ほど作成した<code>blog_template</code>を試してみましょう。</p>
<h4 id="_index_template_simulate-api">_index_template/_simulate API</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">POST</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">_simulate</span><span style="color:#f92672">/</span><span style="color:#a6e22e">blog_template</span>
</code></pre></div><p>レスポンスはこんな感じです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js">{
  <span style="color:#e6db74">&#34;template&#34;</span> <span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;settings&#34;</span> <span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;index&#34;</span> <span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;analysis&#34;</span> <span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;analyzer&#34;</span> <span style="color:#f92672">:</span> {
            <span style="color:#e6db74">&#34;simple_jp&#34;</span> <span style="color:#f92672">:</span> {
              <span style="color:#e6db74">&#34;filter&#34;</span> <span style="color:#f92672">:</span> [
                <span style="color:#e6db74">&#34;kuromoji_readingform&#34;</span>
              ],
              <span style="color:#e6db74">&#34;type&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;custom&#34;</span>,
              <span style="color:#e6db74">&#34;tokenizer&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;kuromoji_tokenizer&#34;</span>
            }
          }
        },
        <span style="color:#e6db74">&#34;number_of_shards&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;3&#34;</span>
      }
    },
    <span style="color:#e6db74">&#34;mappings&#34;</span> <span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;properties&#34;</span> <span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;hoge&#34;</span> <span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;type&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;text&#34;</span>,
          <span style="color:#e6db74">&#34;analyzer&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;simple_jp&#34;</span>
        }
      }
    },
    <span style="color:#e6db74">&#34;aliases&#34;</span> <span style="color:#f92672">:</span> { }
  },
  <span style="color:#e6db74">&#34;overlapping&#34;</span> <span style="color:#f92672">:</span> [
    {
      <span style="color:#e6db74">&#34;name&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;blog_template2&#34;</span>,
      <span style="color:#e6db74">&#34;index_patterns&#34;</span> <span style="color:#f92672">:</span> [
        <span style="color:#e6db74">&#34;blog_*&#34;</span>
      ]
    }
  ]
}
</code></pre></div><p><code>template</code>にコンポーネントがマージされた結果が出力されます。
<code>overlapping</code>は<code>index_patterns</code>がかぶる可能性があるIndex Templateの情報が出力されます。
サンプル用に<code>blog_template2</code>という、同じ<code>index_patterns</code>で<code>priority</code>が低いものを登録してあるためです。
<code>index_patterns</code>が完全に同じではなくとも、重複する可能性があるものはここに出力されます。
例えば、<code>index_patterns</code>が<code>b*</code>という別のIndex Templateを作成してからSimulate APIを実行すると、<code>overlapping</code>にそのIndex Templateも出力されます。</p>
<p>Simulate Index Template APIのもう一つの機能は登録前のIndex Templateの確認です。
リクエストのURLのテンプレート名をなくし、<code>PUT _index_template</code>と同じJSONをリクエストボディとして送信した場合、コンポーネントをマージしたテンプレートがどんなものかを確認できます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">POST</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">_simulate</span>
{
  <span style="color:#e6db74">&#34;index_patterns&#34;</span><span style="color:#f92672">:</span> [<span style="color:#e6db74">&#34;blog_*&#34;</span>],
  <span style="color:#e6db74">&#34;template&#34;</span><span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;mappings&#34;</span><span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;properties&#34;</span><span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;hoge&#34;</span><span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;type&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;text&#34;</span>,
          <span style="color:#e6db74">&#34;analyzer&#34;</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;simple_jp&#34;</span>
        }
      }
    }
  },
  <span style="color:#e6db74">&#34;priority&#34;</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">10</span>,
  <span style="color:#e6db74">&#34;composed_of&#34;</span><span style="color:#f92672">:</span> [<span style="color:#e6db74">&#34;jp_simple_kuromoji&#34;</span>]
}
</code></pre></div><p>このリクエストを送信すると、コンポーネントがマージされた結果が返ってきます。<code>index_patterns</code>がかぶるものがある場合は<code>overlapping</code>も一緒に返却されます。</p>
<h4 id="_index_template_simulate_index-api">_index_template/_simulate_index API</h4>
<p>今度はインデックス名を指定するSimulate APIを試してみましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">POST</span> <span style="color:#a6e22e">_index_template</span><span style="color:#f92672">/</span><span style="color:#a6e22e">_simulate_index</span><span style="color:#f92672">/</span><span style="color:#a6e22e">blog_2021</span>
</code></pre></div><p>これだけです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js">{
  <span style="color:#e6db74">&#34;template&#34;</span> <span style="color:#f92672">:</span> {
    <span style="color:#e6db74">&#34;settings&#34;</span> <span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;index&#34;</span> <span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;analysis&#34;</span> <span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;analyzer&#34;</span> <span style="color:#f92672">:</span> {
            <span style="color:#e6db74">&#34;simple_jp&#34;</span> <span style="color:#f92672">:</span> {
              <span style="color:#e6db74">&#34;filter&#34;</span> <span style="color:#f92672">:</span> [
                <span style="color:#e6db74">&#34;kuromoji_readingform&#34;</span>
              ],
              <span style="color:#e6db74">&#34;type&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;custom&#34;</span>,
              <span style="color:#e6db74">&#34;tokenizer&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;kuromoji_tokenizer&#34;</span>
            }
          }
        },
        <span style="color:#e6db74">&#34;number_of_shards&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;3&#34;</span>
      }
    },
    <span style="color:#e6db74">&#34;mappings&#34;</span> <span style="color:#f92672">:</span> {
      <span style="color:#e6db74">&#34;properties&#34;</span> <span style="color:#f92672">:</span> {
        <span style="color:#e6db74">&#34;hoge&#34;</span> <span style="color:#f92672">:</span> {
          <span style="color:#e6db74">&#34;type&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;text&#34;</span>,
          <span style="color:#e6db74">&#34;analyzer&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;simple_jp&#34;</span>
        }
      }
    },
    <span style="color:#e6db74">&#34;aliases&#34;</span> <span style="color:#f92672">:</span> { }
  },
  <span style="color:#e6db74">&#34;overlapping&#34;</span> <span style="color:#f92672">:</span> [
    {
      <span style="color:#e6db74">&#34;name&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;fuga&#34;</span>,
      <span style="color:#e6db74">&#34;index_patterns&#34;</span> <span style="color:#f92672">:</span> [
        <span style="color:#e6db74">&#34;b*&#34;</span>
      ]
    },
    {
      <span style="color:#e6db74">&#34;name&#34;</span> <span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;2021&#34;</span>,
      <span style="color:#e6db74">&#34;index_patterns&#34;</span> <span style="color:#f92672">:</span> [
        <span style="color:#e6db74">&#34;*_2021&#34;</span>
      ]
    }
  ]
}
</code></pre></div><p>例で作成した<code>blog_template</code>が適用されていますが、それ以外に<code>index_patterns</code>に合致したが<code>priority</code>が低くて適用されなかったものが<code>overlapping</code>に出力されています。
実際に適用されたテンプレートの名前も別途出力してくれるとわかりやすいかもしれないですね。
(<code>*_2021</code>というインデックスパターンのテンプレートを試しに作ってみたのですが、これはバグがありそうです。<code>2021</code>に合致するインデックス名をSimulateしたら<code>overlapping</code>に合致しないものがたくさん出てきました。バグ報告しとくか)。</p>
<h3 id="kibana対応">Kibana対応</h3>
<p>ここまで、Index Template V2のAPIの説明でしたが、Kibanaでの対応についても調べてみました。</p>
<h4 id="index-managementstack-management機能elasticライセンスが必要だが無償の機能">Index Management（Stack Management機能。Elasticライセンスが必要だが無償の機能）</h4>
<p>7.9からKibanaの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/index-mgmt.html">Index管理画面</a>でComposable Templateが利用できるようになっています（<a href="https://github.com/elastic/kibana/pull/70220">GitHub Issue</a>）。
画面のスクショを一通り貼っておきます。残念ながら、それぞれのJSONの編集部分では補完などはサポートされていないようでした。
頑張って自分でsettingsやmappingsのJSONを記述していく感じになります。JSONとして正しいかどうかはチェックしてくれます。
Index Templateのウィザードでは、ボタンでコンポーネントを追加したり削除したり、順序を入れ替えたりといった作業が可能になっています。
また、プレビュー表示が可能なので、<code>composed_of</code>で選択したものが今どのように適用されているか？といったのも確認できるようになっていました。
結構便利に管理できそうです。ちなみにスクショは7.10の画面になります。</p>
<ul>
<li>Component Template周り
<ul>
<li>一覧表示とウィザード</li>
</ul>
</li>
</ul>
<p>
<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/list_component_template.png" />
    </div>
    <a href="/images/entries/20201218/list_component_template.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/component_template_wizard1.png" />
    </div>
    <a href="/images/entries/20201218/component_template_wizard1.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/component_template_wizard2.png" />
    </div>
    <a href="/images/entries/20201218/component_template_wizard2.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/component_template_wizard3.png" />
    </div>
    <a href="/images/entries/20201218/component_template_wizard3.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/component_template_wizard4.png" />
    </div>
    <a href="/images/entries/20201218/component_template_wizard4.png" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<ul>
<li>Index Template周り
<ul>
<li>一覧表示とウィザード</li>
</ul>
</li>
</ul>
<p>

<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/list_index_template.png" />
    </div>
    <a href="/images/entries/20201218/list_index_template.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/index_template_wizard1.png" />
    </div>
    <a href="/images/entries/20201218/index_template_wizard1.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/index_template_wizard2.png" />
    </div>
    <a href="/images/entries/20201218/index_template_wizard2.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/index_template_wizard3.png" />
    </div>
    <a href="/images/entries/20201218/index_template_wizard3.png" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300px">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201218/index_template_wizard4.png" />
    </div>
    <a href="/images/entries/20201218/index_template_wizard4.png" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<h4 id="consoledev-toolsossで利用可能">Console(Dev Tools。OSSで利用可能)</h4>
<p>リクエストを実行は可能ですが、自動補完機能は一部のみ対応しているようです(<a href="https://github.com/elastic/kibana/issues/75967">GitHub Issue</a>)。</p>
<p>対応済みの機能</p>
<ul>
<li><code>DELETE _component_template</code> (ただし、ここまで。存在するコンポーネント名は補完されない)</li>
</ul>
<p>上記以外はまだ未対応のようです。
プルリクエストチャンスかも？</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://github.com/elastic/elasticsearch/issues/53101">Composable Templates · Issue #53101 · elastic/elasticsearch</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.8/release-highlights.html#add-composable-index-templates">What’s new in 7.8 | Elasticsearch Reference ［7.8］ | Elastic</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/index-mgmt.html">Index management | Elasticsearch Reference ［7.10］ | Elastic</a></li>
<li><a href="https://github.com/elastic/kibana/pull/70220">［Composable template］ Create / Edit wizard by sebelga · Pull Request #70220 · elastic/kibana</a></li>
<li><a href="https://github.com/elastic/kibana/issues/75967">［Console］ Support suggesting index templates v2 · Issue #75967 · elastic/kibana</a></li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>ちょっと長くなってしまいましたが、新しいIndex Templateについての紹介でした。
これまでと違い、複数のテンプレートが適用されない点があるのでそこは注意が必要そうです。
コンポーネントをうまく使えば、管理が簡易化はされそうですね。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Corne Chocolateを組み立てた #DIYキーボード</title>
      <link>https://blog.johtani.info/blog/2020/12/03/build_corne_choc/</link>
      <pubDate>Thu, 03 Dec 2020 23:13:44 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/12/03/build_corne_choc/</guid>
      <description>はじめまして、pyspa。 ということで、pyspa Advent Calendar 2020の4日目(大谷コンビの2号)の投稿になります。 コンビそろってキーボード記事です</description>
      <content:encoded><p>はじめまして、pyspa。
ということで、<a href="https://adventar.org/calendars/5310">pyspa Advent Calendar 2020</a>の4日目(大谷コンビの2号)の投稿になります。
コンビそろってキーボード記事ですね。</p>
<p>今年はDIYキーボードにはまった（はめられた？）年でした。
もともと分割キーボードには興味があり、自宅で作業するのが基本となったのもあり手を出した次第です。
まだまだ使いこなすところまでは来てないかもしれないですが、組み立てたり、問題点のキリ分けしたり、試行錯誤するのは楽しいなと。</p>
<p>今回はCorne Chocolateというキーボードを組み立てたのでそちらのビルドログ（かつ今後のための教訓）になります。
LEDを使ったキーボードがどんなものなのか＋薄いキーボードも気になるなということで取り組んでみました。
今回もいくつか失敗をしつつ、リカバリーして動くものができました。</p>
<h2 id="パーツ材料">パーツ（材料）</h2>
<p>基本のパーツは今回も<a href="https://kochikeyboard.stores.jp/">Kochi Keyboard</a>さんから購入しました。</p>
<ul>
<li>キット : <a href="https://kochikeyboard.stores.jp/items/5f65ea9afbe5b52cac9ba2c0">Corne Chocolate（ベースキット）</a></li>
<li>キースイッチ : <a href="https://kochikeyboard.stores.jp/items/5f9e6aaef0b1085ce18b76ff">Kailh Choc v1 Blue (25gf リニア) 5個</a></li>
<li>キーキャップ : <a href="https://yushakobo.jp/shop/pg1350cap-doubleshot/">Kailhロープロ刻印キーキャップ</a></li>
</ul>
<p>ケーブル類は家に転がっていたケーブルを利用しました。
キースイッチセットもあるのですが、軽いキータッチが好みなので、一番軽いKailh ChocのBlueを試してみたくキースイッチを個別に選択しました。
キーキャップは<a href="https://yushakobo.jp/shop/shiro/">Shiro</a>を作るときに調達していたものを利用した形です。</p>
<p>今回も<a href="https://github.com/foostan/crkbd/blob/master/corne-chocolate/doc/buildguide_jp.md">公式のビルドガイド</a>にざっと目を通してから着手しました。</p>
<h2 id="準備">準備</h2>
<p>ビルドログにもあるようにPCBがリバーシルブなので、作業中に迷子にならないようにこんな感じで表にマスキングテープで目印をつけておきました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201204/marking.jpg" />
    </div>
    <a href="/images/entries/20201204/marking.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>今回から耐熱ワーキングマットで作業をすることにしました。机の天板に傷つくの嫌だし。
気兼ねなくはんだできるのでおすすめです。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr"><a href="https://t.co/0qntPD8qIa">https://t.co/0qntPD8qIa</a><br>2ヶ月ぐらいにここで買いました！</p>&mdash; 🤓k.bigwheel🤓 SREエンジニア@Speee株式会社 (@k_bigwheel) <a href="https://twitter.com/k_bigwheel/status/1326522825017057280?ref_src=twsrc%5Etfw">November 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="ダイオードoledソケットコンスルーtrrsソケット">ダイオード、OLEDソケット、コンスルー、TRRSソケット、</h2>
<p>こちらは前回のCorneとほぼ同様だったのでビルドガイド通りに進めます（写真撮り損ねました。。。）
前回との違いはダイオードの形です。
表面実装するタイプのダイオードなので向きに気を付けつつ、つけていきます。</p>
<p><a href="https://github.com/foostan/crkbd/blob/master/corne-chocolate/doc/buildguide_jp.md#%E3%83%80%E3%82%A4%E3%82%AA%E3%83%BC%E3%83%89">ビルドガイドに向きやダイオードのつけ方</a>が紹介されています。
拡大鏡とピンセット必須ですね。
前回同様に、ここまででいったんqmk_toolsでファームウェアをインストールして、ピンセットを使いながらキーの認識とOLEDの動作確認を行いました。
今使っているファームウェアがあるのでそれを入れて問題ないかを確認しました。</p>
<h2 id="led実装">LED実装</h2>
<p>今回のメインイベントです（そして苦杯をなめたイベントでもあります）。
<a href="https://github.com/foostan/crkbd/blob/master/corne-chocolate/doc/buildguide_jp.md#led%E3%82%AA%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3">ビルドガイドにも記載</a>がありますが、信号が流れる順番があるようなので1番から取り付けていきます。
また、ビルドガイドにLEDに関する注意事項もいくつか掲載されています。必ず読みましょう。</p>
<p>この1～6までのLEDがすごく大変でした。。。</p>
<h3 id="表面実装のledunderglow-ledで四苦八苦">表面実装のLED（Underglow LED）で四苦八苦</h3>
<p>いくつかのブログを参考にしながら作業を進めました。</p>
<ul>
<li><a href="https://marksard.github.io/2018/08/04/make-the-crkbd/">コルネキーボードを作りました ～LED取り付けに四苦八苦記～ | キオクノロンダリング</a>
<ul>
<li>SK6812miniの仕様などについて書いてあります。デバッグするのにすごく役に立ちました。</li>
</ul>
</li>
</ul>
<ul>
<li><a href="https://nok0714.hatenablog.com/entry/2019/03/02/194138">Corne Chocolateビルドログ - nokの雑記</a>
<ul>
<li>手書きでどんな感じでやればいいのかを解説してくれています。最終的にはこの方法が一番だったのかも？</li>
</ul>
</li>
<li><a href="https://salicylic-acid3.hatenablog.com/entry/2018/11/26/%E8%87%AA%E4%BD%9C%E3%82%AD%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%AD%E3%83%83%E3%83%88%E3%80%8CCorne_Cherry%E3%80%8D%E3%81%AE%E3%83%AC%E3%83%93%E3%83%A5%E3%83%BC#%E7%B5%84%E7%AB%8B%E9%9B%A3%E6%98%93%E5%BA%A6">自作キーボードキット「Corne Cherry」のレビュー - 自作キーボード温泉街の歩き方</a>
<ul>
<li>あとで出てきますが、リカバリ方法の参考になりました。</li>
</ul>
</li>
</ul>
<p>皆さん苦労されてますね、そして私も苦労しました。。。</p>
<p>試してみた方法、考察は次の通りです。</p>
<ul>
<li>LEDチェック用のファームウェアをインストールして1つつけては動作確認
<ul>
<li><a href="https://qiita.com/yshr04hrk/items/c050a80ab5d2ff869db9#led%E3%83%86%E3%82%B9%E3%83%88%E7%94%A8%E3%83%95%E3%82%A1%E3%83%BC%E3%83%A0%E3%82%A6%E3%82%A7%E3%82%A2%E3%82%92%E7%84%BC%E3%81%84%E3%81%A6%E5%8B%95%E4%BD%9C%E7%A2%BA%E8%AA%8D">HelixのLEDテスト用ファームウェアをインストールしました</a>。</li>
</ul>
</li>
<li><a href="https://amzn.to/39Cyc07">白光 1C型こて先</a>
<ul>
<li>失敗した後にリカバリするために購入しました。</li>
<li>LEDのランドなどが2Cよりも面積が小さいので、そのサイズに合わせたこて先のほうがよかったようです。</li>
</ul>
</li>
<li>はんだの温度（温調できるはんだごて必須）
<ul>
<li>最初は250℃でやっていましたが、なかなかはんだが溶けません。そのせいで焦りも出てきます。</li>
<li>最終的には270度で作業しましたが、温度のせいでLEDが壊れたのはなかった気がします。1Cのこて先だったので無事だったのかもしれません。</li>
</ul>
</li>
<li>フラックスなし
<ul>
<li>あると楽だったのかも？残念ながら試してないです。</li>
</ul>
</li>
<li>予備はんだ手法（だめっぽかった）
<ul>
<li>基盤のランド4か所に予備はんだをし、はんだを温めつつLEDを乗せる方法</li>
<li>LEDの裏にも予備はんだ</li>
<li>どちらも試してみましたが、温めている個所以外のはんだと高さの違いが出てしまい、LEDが浮いてしまいます。4か所を同時に温めることはできないので、すこしずつ調整しているうちにLEDを物理的に壊してしまうことがありました。。。</li>
</ul>
</li>
</ul>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ちょっとコツが分かったかも？ <a href="https://t.co/ruwrmEWCAe">pic.twitter.com/ruwrmEWCAe</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1329446835505786880?ref_src=twsrc%5Etfw">November 19, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>わかった気になっていますが結局失敗しました。。。</p>
<p>結局最後までこれというコツはわかってない気がします。結局3つか4つのLEDがお亡くなりになりました。
うまくいかなかった原因は、予備はんだで傾きなどができ、それを修正していくうちに基盤やLEDにダメージを与えてしまったのだと思います。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201204/broken_led_and_pcb.jpg" />
    </div>
    <a href="/images/entries/20201204/broken_led_and_pcb.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>よく見ると基盤が一部剥がれかけてるのがわかるかなぁと。</p>
<h3 id="リカバリー">リカバリー</h3>
<p>右手側の表面実装の4番と5番が失敗しました。</p>
<ul>
<li>4番目（以下のツイート右側画像の上）
<ul>
<li>こちらは、1度付けたLEDをはがすときにはんだを取り除くのが不十分な状態でLEDをはがしたために、基板のランドごとはがれてしまいました。</li>
<li>なのでここはLEDはつかないです。。。</li>
</ul>
</li>
<li>5番目(以下のツイート右側画像の真ん中あたりの黄色い線がつながっているLED)
<ul>
<li>LED自体はつけてありますが、青色しか発光しなくなっています。</li>
</ul>
</li>
</ul>
<p>4番目が完全に死んでしまったので、無事な3番目のLEDのDINに流れている信号を
5番目のLEDのDINにも流れるようにするために10芯コードでショートカットさせました。
（リカバリ方法は先ほど紹介したブログが非常に参考になりました、先人の知恵ありがたし）。
黒い線も売っていたのですが、自戒も込めて目立つ色にしてみました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">勉強させていただきました、、、 <a href="https://t.co/Zp16fJKziM">pic.twitter.com/Zp16fJKziM</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1330507767845711874?ref_src=twsrc%5Etfw">November 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>ちなみに、左側のショートカットのコードはつけた後に1か所LEDの向きが違うことに気づき必要なくなっています
（上記の基盤が傷ついている画像をよく見ると上下が逆になってるのがわかる人にはわかるかも）。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">片方は導線なくて良くなった。表から見たら向きが違うのに気がついたわ、、、 <a href="https://t.co/7TSDMPPxlX">pic.twitter.com/7TSDMPPxlX</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1330511865827319809?ref_src=twsrc%5Etfw">November 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>最終的に2か所おかしいLEDにはなりましたが、幸いにもアンダーグローです。
Kochi Keyboardさんで購入したキットのボトムプレートはFR4なのでほぼ見えません！（負け惜しみ）</p>
<h2 id="ソケット">ソケット</h2>
<p>さて、気を取り直してソケットをつけていきます。
Dozen0にて経験済みなのでそれほど手間はかかりませんでした。
LEDの失敗の時に1Cのこて先を購入していたため、ソケットの横の隙間からこて先が差し込めたのが便利でした（LEDでダメージを受けていたのもあり写真撮り忘れ）。</p>
<h2 id="完成">完成</h2>
<p>ということで完成です。デフォルトで赤く光ってます。キーキャップがLEDを透過してくれるタイプだったのがこれまたよかったですね。
作る前は光らなくてもなんて思ってたのに。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201204/red_light.jpg" />
    </div>
    <a href="/images/entries/20201204/red_light.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="キーマップv2">キーマップ(v2)</h2>
<p>Corne Lightで作業を数週間ほどして、いくつか入力しにくい部分があったのでマッピングを少しだけ変えました。
相変わらず日本語キーボードベースですが、数字のレイヤーにいくつかの記号を使えるように割り当てました。
コーディングをするときに、ライブラリのバージョン（例：7.10.0とか）やIPアドレスを入力していてレイヤー切り替えのためのキーを押したり話したりするのは効率が悪すぎたためです。
数字との組み合わせでよく使いそうな以下のキーを数字のレイヤーに移動しました。</p>
<ul>
<li>セミコロン（JP_SCLN）</li>
<li>コロン（KC_QUOT）</li>
<li>アンダースコア（JP_UNDS）</li>
<li>コンマ（KC_COMM）</li>
<li>ピリオド（KC_DOT）</li>
<li>スラッシュ（KC_SLSH）</li>
</ul>
<pre><code>#include &quot;keymap_jp.h&quot;

const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = {
  [0] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
       KC_TAB,    KC_Q,    KC_W,    KC_E,    KC_R,    KC_T,                         KC_Y,    KC_U,    KC_I,    KC_O,   KC_P,  JP_MINS,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LCTL,    KC_A,    KC_S,    KC_D,    KC_F,    KC_G,                         KC_H,    KC_J,    KC_K,    KC_L, JP_SCLN, KC_QUOT,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LSFT,    KC_Z,    KC_X,    KC_C,    KC_V,    KC_B,                         KC_N,    KC_M, KC_COMM,  KC_DOT, KC_SLSH, KC_RSFT,
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI,   MO(1),  KC_SPC,     KC_ENT,   MO(2), KC_RALT
                                      //`--------------------------'  `--------------------------'

  ),

  [1] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
       KC_ESC,    KC_1,    KC_2,    KC_3,    KC_4,    KC_5,                         KC_6,    KC_7,    KC_8,    KC_9,    KC_0, KC_BSPC,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LCTL, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      KC_LEFT, KC_DOWN,   KC_UP,KC_RIGHT, JP_SCLN, KC_QUOT,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LSFT, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      JP_UNDS, XXXXXXX, KC_COMM,  KC_DOT, KC_SLSH, XXXXXXX,
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI, _______,  KC_SPC,     KC_ENT,   MO(3), KC_RALT\
                                      //`--------------------------'  `--------------------------'
  ),

  [2] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
       KC_TAB, JP_EXLM, JP_DQUO, JP_HASH,  JP_DLR, JP_PERC,                      JP_AMPR, JP_QUOT, JP_LPRN, JP_RPRN, JP_CIRC, KC_BSPC,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LCTL, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      JP_MINS,  JP_EQL, JP_LBRC, JP_RBRC,  JP_YEN,   JP_AT,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LSFT, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      JP_UNDS, JP_PLUS, JP_LCBR, JP_RCBR, JP_PIPE, JP_TILD,
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI,   MO(3),  KC_SPC,     KC_ENT, _______, KC_RALT
                                      //`--------------------------'  `--------------------------'
  ),

  [3] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
        RESET, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,\
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      RGB_TOG, RGB_HUI, RGB_SAI, RGB_VAI, RGB_SPI, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,\
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      RGB_MOD, RGB_HUD, RGB_SAD, RGB_VAD, RGB_SPD, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,\
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI, _______,  KC_SPC,     KC_ENT, _______, KC_RALT\
                                      //`--------------------------'  `--------------------------'
  )
};
</code></pre><h2 id="ledの色が変えられる気づくの遅い">LEDの色が変えられる！（気づくの遅い）</h2>
<p>あと、キーマップを変更しているときに4つ目のレイヤーにRGBなどのボタンがあるのに気付いて押してみたら、LEDのパターンや色を変えることができるのに気付きました。
（遅すぎでは。。。）</p>
<p>キーキャップが白いのもありこんな感じの色にしてみています。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ユニコーンガンダムっぽくなった <a href="https://t.co/vHxZkMrYyP">pic.twitter.com/vHxZkMrYyP</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1332490582288076800?ref_src=twsrc%5Etfw">November 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="まとめ">まとめ</h2>
<p>ということで、失敗も多々ありましたがキーボードとしてはちゃんと動くできたものができたので安心しました。
今後、状況が落ち着いてきて自宅以外で仕事をするときにはCorne ChocolateをPCと一緒に持ち歩くと思います。
薄くて邪魔にならなくてよさそうです。</p>
<p>今回もはんだっしゅ太郎大活躍でした。本当に買ってよかった。
LEDがつかないときはちょっと落ち込みましたが、これのおかげで立ち直れたのもありますし。
あとは、試行錯誤しつつLEDとかの理解ができたのも楽しかったです。
キーマップの変更時にLEDの変更などができるのに気づいたのはちょっと遅すぎたので、qmkの仕組みや割り当てられるキーにどんなものがあるのかをもう少し研究したいなと思います。</p>
<p>さて、次はどんなことを試すかなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Corne Light v2を作成した #DIYキーボード</title>
      <link>https://blog.johtani.info/blog/2020/11/05/build-corne-light-v2/</link>
      <pubDate>Thu, 05 Nov 2020 00:25:27 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/11/05/build-corne-light-v2/</guid>
      <description>前回のブログのまとめに書いたように、セパレートタイプのキーボードであるCorne Light v2を作成したのでそのビルドログです。DIYキーボードとし</description>
      <content:encoded><p><a href="https://blog.johtani.info/blog/2020/10/11/build-dozen0/#%E3%81%BE%E3%81%A8%E3%82%81">前回のブログのまとめ</a>に書いたように、セパレートタイプのキーボードであるCorne Light v2を作成したのでそのビルドログです。DIYキーボードとしては2つ目になります。</p>
<p>まとめで書いていたお店が開店したので、購入してみました。
購入したのはこちらの<a href="https://kochikeyboard.stores.jp/">KOCHI KEYBOARDさん</a>です。
ついこの間オープンしたばっかりのお店です！</p>
<h2 id="なんで作ったの">なんで作ったの？</h2>
<p>スプリット型（セパレートタイプ？どっちが正しいんだろう？）のキーボードに興味があって、KOCHI KEYBOARDさんで買えたのがこれだったというのが大きな理由です。こういうのってタイミングだと思うので。
あとは、組み立てていくのがプラモデルみたいで面白いというのもあります。
基本的にここのところ自宅で仕事をしているので、ノートPCのキーボードにこだわる必要もないなというのも理由ですね。今後も当面は自宅で仕事になると思いますので。</p>
<h2 id="corne-light-v2">Corne Light v2</h2>
<p>foostanさんが設計された（って言い方であってるのかな？）、3x6のサイズに親指の3キーが配置された分離型のキーボードの一種です。
シリーズ？の名前としては<a href="https://github.com/foostan/crkbd">Corne Keyboard</a>と呼ばれています。
（そういえば、由来は何だろう？自作キーボードがどうやって設計されていくのかという流れがまとめられた<a href="https://fstn.hateblo.jp/entry/2018/12/20/070000">作者の方のブログ</a>がおもしろいです。）
また、基板の設計などがGitHub上でMITライセンスで公開されています。
オープンソースなハードウェアというのも面白いです。</p>
<h2 id="ビルドログ">ビルドログ</h2>
<p><a href="%22https://github.com/foostan/crkbd/blob/master/corne-light/doc/v2/buildguide_low_edition_jp.md%22">作者の方がビルドガイド</a>を公開してくれています。ですので、こちらに沿って作業をしていきます。
ちなみに、私は今回Gateronのクリア軸を選択してみました。さらさらと入力できるのが好きなので。
サイレント軸にも興味はあるのですが、今回は実際にスプリット型のキーボードを早く触ってみたいというのが勝ちました。キーキャップについては後述します。
ツイートに都度、写真をアップしていたので組み立ては画像でお楽しみください（時々取り忘れてるけどｗ）。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">うむ <a href="https://t.co/svW9WdzaZv">pic.twitter.com/svW9WdzaZv</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1318807312342032384?ref_src=twsrc%5Etfw">October 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">刺して曲げる <a href="https://t.co/M0jfQJg2k8">pic.twitter.com/M0jfQJg2k8</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1319288975740162049?ref_src=twsrc%5Etfw">October 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">両方できたと。ハンダまでやって今日はおしまいにすっか <a href="https://t.co/HgsSrxqpcg">pic.twitter.com/HgsSrxqpcg</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1319291510261596162?ref_src=twsrc%5Etfw">October 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">写真取り忘れたけど、ProMicroつけた <a href="https://t.co/RS4z0lqsUZ">pic.twitter.com/RS4z0lqsUZ</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1319310929826246656?ref_src=twsrc%5Etfw">October 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">OLEDもついたと。今日はここまで <a href="https://t.co/p6Z8MMP0Xz">pic.twitter.com/p6Z8MMP0Xz</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1319311317006577664?ref_src=twsrc%5Etfw">October 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>ここまでは順調です。</p>
<h3 id="oledがつかない">OLEDがつかない？</h3>
<p>ビルドガイドに従って、この後キースイッチをつけちゃうと問題の切り分けが難しくなるということで、
ここまでの段階でQMK Toolboxを使ってProMicroにファームウェアを書き込んで、それぞれのキーのソケットの取り付け部をピンセットでショートさせつつ、キーが入力されるかどうかを見ていきます。
<a href="https://salicylic-acid3.hatenablog.com/entry/qmk-toolbox">QMK Toolboxについてはサリチル酸さんのブログ</a>がわかりやすいので参考にさせていただきました（わかりやすい記事をありがとうございます）。
で、ビルドガイドにあるように動いてるかを確認しようとしたのですが。。。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">さて、ファームウェア書き込んだけど、OLEDがつかないな。どっかつながってないのか？</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1319981722914623489?ref_src=twsrc%5Etfw">October 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>ビルドガイドのようにはいかず。。。
USBでPCとつないだ状態で、<a href="https://config.qmk.fm/#/test">QMK Configurator</a>をサイトで開いて、キーボード入力テストを開くと、ショートさせたキーがPC側で認識されているかのテストができます。
これで、一通りキーは認識できていそうだというのはピンセットでショートさせながら確認しました。
ただ、OLEDには何も表示されないんです。。。</p>
<p>前回Dozen0を組み立てた時に、ソケットのはんだ付けが甘かったというのもあったので、
OLEDやソケットのはんだを温めなおしたりしてみて、何度か確認してみたものの特に進展がなしです。
（ちなみにうまくいかないのもあって、<a href="https://twitter.com/johtani/status/1319996547577229313">ぼけたツイート</a>したりもしてます。）</p>
<p>まぁ、OLEDはビルドガイドを見てもオプション扱いなので、それよりも触ってみたい衝動に駆られて、
キースイッチをはんだ付けしていきます。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ちなみに昨晩、OLEDはとりあえず置いといてって感じでキースイッチつけてた。キーキャップはまだない <a href="https://t.co/SzLmzNg60Y">pic.twitter.com/SzLmzNg60Y</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1320558068212510720?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>それも終わって、問題点の切り分けに何かできないかな？と思ってやったのがOLED単体でArduinoと接続してみて動くかどうかです。
<a href="https://qiita.com/takahiro_itazuri/items/d7bc7331b196921b5880">Qiitaにちょうどいい感じの記事</a>を見つけたのでこれまた参考にさせていただきました(4本のジャンパー線を一人で持ちながら確認するの大変だったｗ)。</p>
<h4 id="はんだでの修正にしっぱい">はんだでの修正にしっぱい</h4>
<p>で、自分の中での結論として、「OLEDソケットのはんだ付けが怪しい」となりました。
そこで、まずは外してみようかと思ったのが間違いでした。
はんだごてとはんだ吸い取り線で何とか外せるだろうと思っちゃったんですよ。
OLEDピンソケットは足が4本あって、とりあえずとれるところまではんだ吸い取り線で吸い取ってみましたが、
さすがに基盤の穴に流れ込んだはんだまでは吸い取れず、頑張って温めながらソケットを抜こうとがんばって、
ラジオペンチでピンソケットを引っ張りながら引き抜きました。かろうじて引き抜きはできたのですが、ソケットは足が折れてしまいました。</p>
<h4 id="作者登場oled問題の解決">作者登場（OLED問題の解決）</h4>
<p>そんなところにCorneの作者の方がツイートを拾ってくれたみたいで以下のような返信を頂きました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">OLEDが点かないのはファームのせいかもしれません(最近以前のファームで動かない新しいタイプのOLEDモジュールが出回るようになりました)。お手数ですが、<a href="https://t.co/vfSBfIeeJX">https://t.co/vfSBfIeeJX</a> のブランチのものを試して頂けますか？(ただいまPRレビュー中でまだマージされていない状態です)</p>&mdash; 🥖 (@foostan) <a href="https://twitter.com/foostan/status/1320410393328513024?ref_src=twsrc%5Etfw">October 25, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>暗闇に光明とはこのことです。
教えていただいたブランチを手元でビルドしてファームウェアを書き込むと、</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">すごい、出ました！右側のディスプレイにロゴが。ありがとうございます（左側のOLEDはソケットのはんだ付け直し失敗したのでもうちょっと先になりますが）！</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1320549750156869632?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>なんと、ファームウェア書き込んだ瞬間にOLEDが点くじゃないですか。
感謝感激ってやつです。お礼を言うのに便乗して<a href="https://twitter.com/foostan/status/1320555771009011712">OLEDの問題の切り分けの方法についても聞いてしまいました</a>。</p>
<h4 id="oledリベンジ">OLEDリベンジ</h4>
<p>片側は無事だったのですが、もう一方は修復が困難になったので救世主を発注します。
自作キーボードの作成に便利なものリストとして、いくつかのブログに上がっていたのですが、必要ないだろうと見送っていたツールです。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B01FEV2BPG/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B01FEV2BPG&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B01FEV2BPG/?tag=johtani-22">
      Amazon | サンハヤト はんだシュッ太郎NEO 45Wタイプ HSK-300 | ハンダゴテパーツ
      </a>
    </p>
  </div>
</div>
<p>本当にすごく使いやすかったです。
OLEDのピンソケットが修復不可能だったのですが、ProMicro用に付属して余っているピンヘッダがちょうどいい長さでした。
なので、これを4本分切り取り、OLEDモジュールにつけてしまったOLEDヘッダピンを抜き取って、代わりに切り取ったピンヘッダをつかって、OLEDと基盤を直接はんだ付けすれば修復できそうだと判断しました。</p>
<p>はんだシュッ太郎君を使ってOLEDについてるヘッダピンをまずは除去。
そのあとはこんな感じで繋げました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">見にくいかもだけど <a href="https://t.co/YAHoxjgppN">pic.twitter.com/YAHoxjgppN</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1324399704512192512?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>取り外しにくくはなったけど、無事両方のOLEDが点くのも確認できました。やったー。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ヤッター、両方のOLEDがついたよー <a href="https://t.co/6KLDWvnaEC">pic.twitter.com/6KLDWvnaEC</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1321475997141659648?ref_src=twsrc%5Etfw">October 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h3 id="キースイッチがご機嫌斜め">キースイッチがご機嫌斜め</h3>
<p>キーキャップは別途、AliExpressで発注をかけたのですが、ここまで来たら待ちきれないですよね？
ということで、手元にあった上海問屋のキーボードのキーキャップが同じMXキースイッチ用のものだったので移植しました。キーキャップ付けてみないとわかならいものですね、1つだけキースイッチがうまくはまっておらず、斜めについているのがこの時点で判明しました（ボトムプレート付けた後だったので、プレート外してから、はんだで修正）。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ちなみにキーキャップはめてる途中で、一箇所キースイッチが斜めにはんだ付けされちゃってるのを発見して慌てて直しました。 <a href="https://t.co/qofHp3o4Pc">pic.twitter.com/qofHp3o4Pc</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1322083647332085760?ref_src=twsrc%5Etfw">October 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>無事使えるようになりました。</p>
<h3 id="アンスコが打てないescどこ">アンスコが打てない＆Escどこ？</h3>
<p>キーキャップ付けたので仮運用ということで、まずはデフォルトのキーマップをもとにどんな感じで入力できるのかを試してみました。
ここにも罠がｗ
ブラウザで<a href="https://config.qmk.fm/#/crkbd/rev1/legacy/LAYOUT_split_3x6_3">QMK Configurator</a>を開いてデフォルトのキーマップを確認しながら試し打ちをしていたのですが、どうも思ったのと違う動きをしているキーが。</p>
<p>まずBASEレイヤー（レイヤー0）の右側のシフトと、LOWERレイヤーやRAISEレイヤーのEscです。
入力しているとどうも、右のシフトがEscで、レイヤーを切り替えてもTabのままだなと。
しょうがないので、OLEDが出なくてもいいのでQMK Toolboxでダウンロードしてきたものを利用したら想定通りなのにと。
きちんとqmk_firmwareの構成を理解しないままやってたつけでしたね。
結論としては、OLED用に教えてもらったブランチではキーマップが書き換わっていたようでした。
<code>make crkbd:defaut</code>でビルドした時に利用されるkeymap.cがキーマップの定義が書いてあるファイルです。
qmk/qmk_firmwareのリポジトリにあるkeymap.cとfoostanさんにもらったブランチでは差分があったみたいでした。</p>
<p>おかげで、qmk_firmwareのkeymap.cの仕組みがわかったし結果オーライです。
問題があって調べるとどんな作りになってるかとかちゃんと確認できますしね。
手順通りにやってるだけで問題が起きないと、どんな仕組みになってるのかがわからないので人に聞きまくるしかできなくなっちゃいますしね。</p>
<p>これでEsc問題は解決したのですが、アンスコがどうしても入力できません。
自作キーボード以外はすべて日本語配列のキーボードを使用しているのもあり、
日本語配列のキーボードだとどうもキーのマッピングが異なるようだと。</p>
<p>ググって参考にしたのはこの辺でした。</p>
<ul>
<li><a href="https://skyhigh-works.hatenablog.com/entry/2018/11/14/033242">【QMK】JPキーコードでキーマップを定義する - 天高工房</a></li>
<li><a href="https://salicylic-acid3.hatenablog.com/entry/2018/11/26/%E8%87%AA%E4%BD%9C%E3%82%AD%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%AD%E3%83%83%E3%83%88%E3%80%8CCorne_Cherry%E3%80%8D%E3%81%AE%E3%83%AC%E3%83%93%E3%83%A5%E3%83%BC#USB%E3%81%AE%E5%8F%B3%E5%87%BA%E3%81%97">自作キーボードキット「Corne Cherry」のレビュー - 自作キーボード温泉街の歩き方</a></li>
</ul>
<p>そのほかにもVIAのファームなども試したのですが、どれもうまくいかず。
色々ググってみて最終的な解決策はkeymap.cでkeymap_jp.hというファイルが読み込まれていないので、日本語用の設定とかを読み込んでみたつもりがうまく反映されていないという感じでした。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">JISにする場合、keymap_jp.hっていうヘッダファイル読み込んだほうがいいです</p>&mdash; Yoshi Yamaguchi ⌨ Keyboard builder (@ymotongpoo) <a href="https://twitter.com/ymotongpoo/status/1323456582152171522?ref_src=twsrc%5Etfw">November 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>今の時点ではこんなキーマップにしてあります。
今後も日本語配列のキーボードをベースに考えていくつもりです。</p>
<pre><code>
#include QMK_KEYBOARD_H
#include &quot;keymap_jp.h&quot;

const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = {
  [0] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
       KC_TAB,    KC_Q,    KC_W,    KC_E,    KC_R,    KC_T,                         KC_Y,    KC_U,    KC_I,    KC_O,   KC_P,  JP_MINS,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LCTL,    KC_A,    KC_S,    KC_D,    KC_F,    KC_G,                         KC_H,    KC_J,    KC_K,    KC_L, JP_SCLN, KC_QUOT,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LSFT,    KC_Z,    KC_X,    KC_C,    KC_V,    KC_B,                         KC_N,    KC_M, KC_COMM,  KC_DOT, KC_SLSH, KC_RSFT,
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI,   MO(1),  KC_SPC,     KC_ENT,   MO(2), KC_RALT
                                      //`--------------------------'  `--------------------------'

  ),

  [1] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
       KC_ESC,    KC_1,    KC_2,    KC_3,    KC_4,    KC_5,                         KC_6,    KC_7,    KC_8,    KC_9,    KC_0, KC_BSPC,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LCTL, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      KC_LEFT, KC_DOWN,   KC_UP,KC_RIGHT, XXXXXXX, XXXXXXX,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LSFT, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI, _______,  KC_SPC,     KC_ENT,   MO(3), KC_RALT\
                                      //`--------------------------'  `--------------------------'
  ),

  [2] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
       KC_TAB, JP_EXLM, JP_DQUO, JP_HASH,  JP_DLR, JP_PERC,                      JP_AMPR, JP_QUOT, JP_LPRN, JP_RPRN, JP_CIRC, KC_BSPC,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LCTL, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      JP_MINS,  JP_EQL, JP_LBRC, JP_RBRC,  JP_YEN,   JP_AT,
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      KC_LSFT, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      JP_UNDS, JP_PLUS, JP_LCBR, JP_RCBR, JP_PIPE, JP_TILD,
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI,   MO(3),  KC_SPC,     KC_ENT, _______, KC_RALT
                                      //`--------------------------'  `--------------------------'
  ),

  [3] = LAYOUT_split_3x6_3( \
  //,-----------------------------------------------------.                    ,-----------------------------------------------------.
        RESET, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,\
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      RGB_TOG, RGB_HUI, RGB_SAI, RGB_VAI, XXXXXXX, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,\
  //|--------+--------+--------+--------+--------+--------|                    |--------+--------+--------+--------+--------+--------|
      RGB_MOD, RGB_HUD, RGB_SAD, RGB_VAD, XXXXXXX, XXXXXXX,                      XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,\
  //|--------+--------+--------+--------+--------+--------+--------|  |--------+--------+--------+--------+--------+--------+--------|
                                          KC_LGUI, _______,  KC_SPC,     KC_ENT, _______, KC_RALT\
                                      //`--------------------------'  `--------------------------'
  )
};
</code></pre><h3 id="キーキャップが待ちきれなくて">キーキャップが待ちきれなくて</h3>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">この右下のトンガリがちょっと気になる <a href="https://t.co/W0A0z67cns">pic.twitter.com/W0A0z67cns</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1322092102637948928?ref_src=twsrc%5Etfw">October 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>キーキャップが待ちきれなくて上海問屋のキーキャップを付けてみて、いろいろ試行錯誤してみました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">あー。上下関係ないのか。だからみんなこんな感じにしてるのか？ <a href="https://t.co/KTBlPLespr">pic.twitter.com/KTBlPLespr</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1322180553529937921?ref_src=twsrc%5Etfw">October 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">どのくらい離すのが良いか実験中 <a href="https://t.co/ozK4WyBXY8">pic.twitter.com/ozK4WyBXY8</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1323161306266963969?ref_src=twsrc%5Etfw">November 2, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h3 id="キーキャップもいろいろあるのね">キーキャップもいろいろあるのね</h3>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">届いた。 <a href="https://t.co/jccNW1ZcEM">pic.twitter.com/jccNW1ZcEM</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1323194081036546048?ref_src=twsrc%5Etfw">November 2, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">もう一個 <a href="https://t.co/9y6qat4JiN">pic.twitter.com/9y6qat4JiN</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1323194549196390400?ref_src=twsrc%5Etfw">November 2, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">とりあえずセミコロンの位置につけてみた。 <a href="https://t.co/TDk5213IHi">pic.twitter.com/TDk5213IHi</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1323473987213565953?ref_src=twsrc%5Etfw">November 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>今はこのDSAプロファイルの青いグラデーションのキーキャップを使っています（Dukeはここにはいないですｗ）。
キーキャップのプロファイルはこれまた、<a href="https://salicylic-acid3.hatenablog.com/entry/2018/12/06/%E3%82%AD%E3%83%BC%E3%82%AD%E3%83%A3%E3%83%83%E3%83%97%E3%81%AE%E6%B9%AF%E3%81%AE%E3%81%8A%E8%AA%98%E3%81%84">サリチル酸さんのブログ</a>がわかりやすく書かれています（ほんとすごいなぁ。）。</p>
<h3 id="高さ調節">高さ調節</h3>
<p>そのまま使っていたのですが、やはりもうちょっと奥側に高さがほしいなと。
ダイソーなどで打っているケーブルクリップがお試しには良さそうだったのでこんな感じで内側がちょっとだけ高くなるような感じにして使っています。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">こうなった。とりあえず仮運用 <a href="https://t.co/VkhjawdhLM">pic.twitter.com/VkhjawdhLM</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1323999827810684931?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="まとめ">まとめ</h2>
<p>ということで、駆け足ですがCorne Light v2のビルドログでした。
いきなりフルキーボードから40％キーボードかつ異なる並びにしたので、
混乱しっぱなしですが、いい頭の運動になっているし、なにより作って動かすまでの間に
いろいろと調べたり試行錯誤できたのがすごく楽しかったです。</p>
<p>あとは、スプリット型＋Column Staggeredになったので「c」や「b」を間違えまくっていますが、
いい気付きでした。まだまだ記号とかの入力に慣れていないですが、ちょっとずつ身に着けていけたらなと。</p>
<p>ただ、もう次のキーボードをDIYしたい気持ちも出てきているので困っているところです。</p>
<p>書ききれてないこともあるかと思うので、ここはどうしてるの？これはどうやったの？などあれば、Tweetなりコメントなりを頂けたらなと思います。いやぁ、DIYキーボード楽しいわ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Windowsへの移行（その１）</title>
      <link>https://blog.johtani.info/blog/2020/10/18/restart-windows-no1/</link>
      <pubDate>Sun, 18 Oct 2020 23:01:50 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/10/18/restart-windows-no1/</guid>
      <description>ヒゲが不評だったのでいつも通りの長さに切りました。 前回「Windowsをお試し中」と書きましたが、いくつかソフトをインストールしたりしたので</description>
      <content:encoded><p>ヒゲが不評だったのでいつも通りの長さに切りました。</p>
<p>前回「<a href="https://blog.johtani.info/blog/2020/10/15/restart-windows/">Windowsをお試し中</a>」と書きましたが、いくつかソフトをインストールしたりしたので現状を忘れないようにブログに残しておきます（また同じ作業する予定なので）</p>
<h2 id="インストールしたものたち">インストールしたものたち</h2>
<p>今のところインストールしたのはこの辺。
特にコメントしたいものについてこのあと明記します。</p>
<ul>
<li>ドライバインストール from CD</li>
<li>MX Ergoのソフトインストール。接続はコネクター</li>
<li>Windowsアップデート</li>
<li>Chrome</li>
<li>Visual Studio ビルドツール(Lindera用)</li>
<li>JetBrains Toolbox
<ul>
<li>IntelliJ</li>
<li>CLion</li>
<li>Rider</li>
</ul>
</li>
<li>Visual Studio Code</li>
<li>Slack</li>
<li>Teams</li>
<li>Google Drive共有</li>
<li>Gitter</li>
<li><del>Divvy for Windows</del></li>
<li>Zoom</li>
<li>DeepL</li>
<li>Sophos</li>
<li><a href="https://help.twitter.com/ja/using-twitter/twitter-windows-10">Twitter for Windows</a></li>
<li>Office 365</li>
<li>AutoHotKey</li>
<li>WSL2 - Ubuntu</li>
<li>Windows Terminal</li>
<li><a href="https://www.rust-lang.org/tools/install">Rustup</a></li>
<li>Git for Windows</li>
<li>wsl-ssh-agent</li>
<li>PeaZip</li>
<li>Speccy</li>
<li>Hugo (WSL2のUbuntuにapt-get install hugoした)</li>
<li><a href="https://github.com/microsoft/PowerToys">PowerToys</a></li>
</ul>
<h3 id="autohotkey">AutoHotKey</h3>
<p>前回のブログに書いたように、カーソル系のEmacsショートカットが体に染みついて離れないのです。
そこで、<a href="https://linuxfan.info/windows-emacs-keybindings">ググって見つけた記事はこちら</a>。
<a href="https://www.autohotkey.com/">AutoHotKey</a>というツールが
Windowsに常駐して、特定のキー入力の時に、別の操作を実行してくれるツールです。</p>
<p>見つけた記事にあった、<a href="https://github.com/lintaro-jp/gtk-emacs-theme-like.ahk">https://github.com/lintaro-jp/gtk-emacs-theme-like.ahk</a>のキーマップをもとに、いくつか変更したものを使用させていただいています。</p>
<p>書き換えたのは、以下の通りです。</p>
<ul>
<li>Ctrl+\でIMEの切り替え（Wnn風）</li>
<li>Ctrl+kをEmacsと同じ挙動に（もとにしたキーマップでは、カーソルから行末までが削除されてしまう）</li>
<li>Win+sでもファイル保存（Ctrl+sと同じ挙動に。デフォルトではCortanaが起動して邪魔くさいので）</li>
</ul>
<p>すごく快適です。そのうち、もう少し自分が使いやすいようにキーマップを追加していったりするかもしれません。
特に、Macのショートカットがしみついているものについてなどです（仮想デスクトップ切り替えとか？）</p>
<h3 id="powertoys">PowerToys</h3>
<p>Divvyをインストールしたのですが、グローバルショートカットがうまく動かなくていまいちでした（なんでだろ？）
そこでググって見つけたのが<a href="https://www.atmarkit.co.jp/ait/articles/1912/26/news019.html">@ITのPowerToysの記事</a>でした。
<a href="https://github.com/microsoft/PowerToys">PowerToys</a>はMS製のOSSツールで、GitHub上で公開されています。
中には「ColorPicker」、「FancyZones」、「File Explowrer Add-ons」など、複数のツールが入っています。
「<a href="https://github.com/microsoft/PowerToys/wiki/FancyZones-Overview">FancyZones</a>」です。</p>
<p>Windowsでは、デフォルトで、アプリにフォーカスしているときにWin＋カーソルでウィンドウの配置場所をいくつか（左右どちらかに半分、4分の1(右上、左上など)）に配置してくれるAero Snapというものが動いています。が、上半分などデフォルトにない柔軟な配置を設定することができません。
MacではDivvyというツールをいれて、それを実施していたのですが、先ほど書いたようにWindows版はあるものの、なぜかグローバルショートカットが動かず、システムタスクトレイのアイコンをクリックしなければいけませんでした。</p>
<p>それを解消してくれるのがFancyZonesです。
FancyZonesの設定項目で、<a href="https://github.com/microsoft/PowerToys/wiki/FancyZones-Overview#picking-a-layout">自分で画面上にどのような割り当て領域を作るかを設定できます</a>。さらに、マルチディスプレイ、仮想デスクトップを使っている場合、それぞれのディスプレイかつ仮想デスクトップで個別にレイアウトが設定できるようになります。
レイアウトを設定した後は、Shiftキーを押しながらウィンドウを移動すると、設定した領域が出てきて、あとは、割り当てたい領域にウィンドウを移動するだけでその領域いっぱいにウィンドウをリサイズしてくれます。
また、設定を変えることで、Shiftキーを押さなくてもウィンドウ移動するだけで領域にリサイズすることも可能です。また、Aero Snapの機能をオーバーライドしてWin＋カーソルで動作させることも可能になります。</p>
<h3 id="wsl2--windows-terminal">WSL2 + Windows Terminal</h3>
<p>三宅さんからコメントを頂いたので。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">もう少し幅広い感じですかね。あわせて Windows Terminal 使うとわりと快適です。</p>&mdash; miyake (@kazuyukimiyake) <a href="https://twitter.com/kazuyukimiyake/status/1316602393283366913?ref_src=twsrc%5Etfw">October 15, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>なんとなく噂は聞いていましたがということで、<a href="https://docs.microsoft.com/ja-jp/windows/wsl/about">MSのドキュメント</a>を見ながらインストールしました。<a href="https://docs.microsoft.com/ja-jp/windows/wsl/install-win10">クイックスタート</a>に則って作業するだけで特に問題なくUbuntuまで無事インストールできました。
Windows Terminalも同様です（クイックスタートで入れることになる）。</p>
<h3 id="git-for-windows--wsl-ssh-agent">Git for Windows + wsl-ssh-agent</h3>
<p>少してこずったのがこちらです。
<a href="https://docs.microsoft.com/ja-jp/windows/wsl/tutorials/wsl-git">チュートリアル</a>に記述はあったのですが、どちらで作業するのか？などが少しわかりにくかったので。<a href="https://scrapbox.io/saitotetsuya/Windows_10%E3%81%AEssh-agent%E3%82%92WSL2%E3%81%8B%E3%82%89%E4%BD%BF%E3%81%86">ググって出てきた記事</a>をもとに作用しました。</p>
<p>RustのプロジェクトファイルをMacから丸ごとコピーしていたのですが、Git for Windowsを入れるまでは、CLion+Rust pluginの環境では、<code>.git</code>ファイルをきちんと認識してくれませんでした（この時WSL2と少し混同してた）。</p>
<p>無事環境は用意できました。
作業の手順は以下のような感じです。
Macで使用していた<code>.ssh</code>のディレクトリを持ってきて作業しました。</p>
<h4 id="windows側での作業">Windows側での作業</h4>
<ol start="0">
<li>Macから<code>.ssh</code>ディレクトリを<code>c:\\Users\johta\</code>にコピー</li>
<li>Git for Windowsのインストール</li>
<li>OpenSSHクライアントの設定
<ul>
<li>管理者権限でコマンドプロンプトを起動して
<ul>
<li><code>sc config ssh-agent start=auto</code></li>
<li><code>sc start ssh-agent</code></li>
</ul>
</li>
</ul>
</li>
<li>[wsl-ssh-agent(https://github.com/rupor-github/wsl-ssh-agent)のインストール
<ul>
<li>ダウンロードして<code>.7z</code>ファイルを展開</li>
<li><code>wsl-ssh-agent-gui.exe</code>と<code>npiperelay.exe</code>を<code>c:\\Users\johta\bin</code>に移動</li>
<li><code>c:\\Users\johta\bin\wsl-ssh-agent-gui.exe</code>のショートカットをスタートアップに作成</li>
<li>作成したスタートアップのリンク先に<code> -setenv -envname=WSL_AUTH_SOCK</code>を追加</li>
</ul>
</li>
<li>Git BashでOpenSSHを利用するように設定
<ul>
<li><code>setx GIT_SSH C:\\Windows\system32\OpenSSH\ssh.exe</code></li>
</ul>
</li>
<li><code>ssh-add</code>で秘密鍵を登録
<ul>
<li><code>ssh-add .ssh/id_rsa</code></li>
</ul>
</li>
</ol>
<h4 id="wsl2のubuntuでの作業">WSL2のUbuntuでの作業</h4>
<ol>
<li>Ubuntuはgitがすでに入っていたのでインストールはスキップ</li>
<li><code>git config --global user.name &quot;Jun Ohtani&quot;</code></li>
<li><code>git config --global user.email &quot;メールアドレス&quot;</code></li>
<li><code>mkdir .ssh</code></li>
<li><code>.bashrc</code>の最後行に<a href="https://github.com/rupor-github/wsl-ssh-agent#wsl-2-compatibility">wsl-ssh-agentのWSL2 compatibility</a>にある<code>export</code>から<code>fi</code>までをコピー。この時、パスを自分のパスに変更すること(私の場合<code>$HOME/winhome/.wsl</code>の部分を<code>/mnt/c/Users/johta/bin</code>に書き換えました)</li>
</ol>
<p>これで、WSL2側では<code>.ssh</code>のファイルを管理しなくても、Windows側のOpenSSHに接続してssh周りの処理をしてくれるようになりました。</p>
<h3 id="peazip">PeaZip</h3>
<p>wsl-ssh-agentが<code>.7z</code>形式で圧縮されていたので、ダウンロードしました。
とりあえずこれを入れてみたのですが、ほかにお勧めがあれば教えてほしいです。</p>
<h3 id="speccy">Speccy</h3>
<p>CPUの温度やスレッドごとのCPU使用率などを見てみたいので入れてみました。
これもとりあえず入れてみた感じなので、ほかにお勧めのソフトがあれば教えてもらえればと。
リソースマネージャーでもスレッドごとのCPU使用率は見れたのですが、温度が見れなかったので。</p>
<h3 id="hugo-">Hugo +</h3>
<p>WSL2にあるUbuntuに<code>sudo apt-get install hugo</code>してインストールしました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">そこで、VS Code の Remote Extension です。WSL2 から ‘code’ で起動できます😎</p>&mdash; Daiyu Hatakeyama (@dahatake) <a href="https://twitter.com/dahatake/status/1317257121658884097?ref_src=twsrc%5Etfw">October 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>あとは、畠山さんに教えてもらったVSCodeの<a href="https://github.com/Microsoft/vscode-remote-release">Remote - WSL</a>を入れて、
WLSに接続した状態でVSCodeを起動して記事のMarkdownを書くと、VSCodeのターミナルを開くとWSL2に接続してくれて、hugoコマンドが実行できます。</p>
<h2 id="まとめ">まとめ</h2>
<p>とりあえず、ブログが書ける環境ができました。
Mac上にあった各種プロジェクトのディレクトリをコピーして、Rustの開発もできるようになりました。</p>
<p>今後も環境構築は続いていきます。Javaとか入ってないし。
SDKMan使ってたけど、切り替えどうしよう？とか。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">そういえばSDKMAN!をJDK切替に使ってたけど、結局windows側のパスは通らんので自力インストールになった。scoopがよさげ</p>&mdash; きしだൠ(K8S(Kishidades)) (@kis) <a href="https://twitter.com/kis/status/1317274361854791680?ref_src=twsrc%5Etfw">October 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>IDE系から呼び出すターミナルはWSL2がいいなぁとか。</p>
<blockquote class="twitter-tweet"><p lang="und" dir="ltr"><a href="https://t.co/X4Jj4tih1I">https://t.co/X4Jj4tih1I</a></p>&mdash; そーだい@初代ALF (@soudai1025) <a href="https://twitter.com/soudai1025/status/1317270352431476737?ref_src=twsrc%5Etfw">October 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>ほかにもおすすめなどがあれば教えてください！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Windowsをお試し中</title>
      <link>https://blog.johtani.info/blog/2020/10/15/restart-windows/</link>
      <pubDate>Thu, 15 Oct 2020 11:04:02 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/10/15/restart-windows/</guid>
      <description>ヒゲが伸びてきて(試しに伸ばしてる)不評を買っている今日このごろです。 自宅で作業することが多くなってきたので、自作PCでもと思っていますが、</description>
      <content:encoded><p>ヒゲが伸びてきて(試しに伸ばしてる)不評を買っている今日このごろです。</p>
<p>自宅で作業することが多くなってきたので、自作PCでもと思っていますが、OSをどうしようか悩み中。
とりあえず、試しにWindowsにしてみるかということで、10年ぶりくらいにWindowsに帰ってきました
(この文章はMacで入力してますが)。</p>
<h2 id="むかしむかし">むかしむかし</h2>
<p>もともとはWindowsを使っていたのですが、Win7くらいに32bitと64bit混在の時期に、
Xkeymacsを利用するときに少し手間がかかるなぁと思ったのもあり、Macに移行しました。
Macだと自分がよく使うEmacsっぽいショートカットがデフォルトいろんなアプリで使用できる利点があったからです。</p>
<p>ただ、最近、<a href="https://nzxt.jp/products/detail/kraken-z.html">TL上で光るPC</a>とかを見てしまったのもあり、
自作PC(どちらかというとDIYかな?)熱が復活しました。
まだ、OSをWindows、Ubuntuのどちらをメインにしようかな?と悩んでいるところではありますが、まずはWndowsをちょっと試してみるかなと。</p>
<p>まずは、やりたいことをちょっとリストアップしておこうかなと思います(きっと、先人の知見が集まってくるはず!)。</p>
<h2 id="どうしてもほしい機能">どうしてもほしい機能</h2>
<p>大学の頃にSunOSやSolarisを使用し、Emacsでメールを読んだしていたせいで、Emacsのショートカット操作が体から抜けない状態です(抜こうとしてないという話もあるが)。
ですので、社会人になってからWindowsを利用していたときはXkeymacsというソフトのお世話になっていました(神アプリでした。まだあるのかな?)。</p>
<h3 id="ショートカット操作とか">ショートカット操作とか</h3>
<ul>
<li>カーソル移動(必須)
<ul>
<li>上下左右：<code>Ctrl+n、p、f、b</code></li>
<li>行頭、行末：<code>Ctrl+a、e</code></li>
</ul>
</li>
<li>編集
<ul>
<li>デリートキー：<code>Ctrl+d</code>
<ul>
<li><code>Ctrl+h</code>がバックスペースだけど使わないな、そういえば</li>
</ul>
</li>
<li>カーソルから行末までをカット：<code>Ctrl+k</code>
<ul>
<li>必須!</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>カーソル移動がホームポジションから移動しなくてもいいのもあって、多用してしまっています。
これ、WindowsとかUbuntuでいい感じにできるのあるのかな?
できれば、IMEやブラウザのURLのサジェストなどもこのカーソル移動で移動できると嬉しいです。
(昔は<code>M-%</code>とかで置換などもやってたけど最近はやらないな。)</p>
<h2 id="デスクトップ操作系">デスクトップ操作系</h2>
<ul>
<li>仮想デスクトップ
<ul>
<li>やることごとにデスクトップを切り替えて使うので</li>
</ul>
</li>
<li>ウィンドウのリサイズ?
<ul>
<li><a href="https://mizage.com/divvy/">Divvy</a>というアプリをmacOSで利用中</li>
<li>自分で登録したサイズ(例：画面右半分)にウィンドウサイズを変更などがショートカットで可能</li>
</ul>
</li>
</ul>
<p>仮想デスクトップはあると思うけど、ウィンドウのリサイズの便利なツールあるかなぁ?</p>
<h2 id="利用するアプリ">利用するアプリ</h2>
<h3 id="snsオンラインミーティング">SNS、オンラインミーティング</h3>
<ul>
<li>Twitter
<ul>
<li>macOSではTweetDeckと夜フクロウを使用</li>
</ul>
</li>
<li>Slack
<ul>
<li>ネイティブアプリ</li>
</ul>
</li>
<li>Gitter
<ul>
<li>ネイティブアプリ</li>
</ul>
</li>
<li>Chatwork
<ul>
<li>ブラウザ</li>
</ul>
</li>
<li>Teams
<ul>
<li>ネイティブアプリ</li>
</ul>
</li>
<li>Zoom
<ul>
<li>ネイティブアプリ</li>
</ul>
</li>
<li>Google Meet
<ul>
<li>ブラウザ</li>
</ul>
</li>
<li>Discord
<ul>
<li>ネイティブアプリ(最近使ってないな)</li>
</ul>
</li>
</ul>
<p>色々使ってますが、最悪ブラウザで使う感じかな?</p>
<h3 id="オンラインストレージ">オンラインストレージ</h3>
<ul>
<li>Google Drive
<ul>
<li>同期アプリ使ってる</li>
</ul>
</li>
<li>Dropbox
<ul>
<li>あんまり使ってない</li>
</ul>
</li>
</ul>
<h3 id="開発系">開発系</h3>
<ul>
<li>JetBrainsのIDE
<ul>
<li>Toolbox App</li>
<li>IntelliJ</li>
<li>CLion + Rust plugin</li>
<li>Rider</li>
</ul>
</li>
<li>zsh
<ul>
<li>macOSで標準になったから</li>
</ul>
</li>
<li>コマンド系
<ul>
<li>git</li>
<li>SDKman
<ul>
<li>ant</li>
<li>gradle</li>
<li>JDK</li>
</ul>
</li>
<li>Rust</li>
<li>Hugo</li>
</ul>
</li>
<li><a href="https://jasperapp.io/">Jasper</a>
<ul>
<li>GitHubのIssueとかを見るネイティブアプリ</li>
<li>自分が関連しているIssueとかが楽に見える(メールだと埋もれてしまう)</li>
</ul>
</li>
</ul>
<h3 id="エディタ">エディタ</h3>
<ul>
<li>Visual Studio Code
<ul>
<li>開発系では?と思われるかもだけど、Markdownエディタとして使ってる</li>
</ul>
</li>
<li>Emacs by Homebrew
<ul>
<li>最近は起動してない</li>
</ul>
</li>
</ul>
<h3 id="その他">その他</h3>
<ul>
<li>カレンダー
<ul>
<li>macOSのカレンダーで複数のGoogleカレンダーを取り込んでる</li>
</ul>
</li>
<li>ScanSnap
<ul>
<li>Ubuntuで使えるやつあるのかな?</li>
<li>Mac miniにつないであるので、画面共有とかで入れればそれでOK</li>
</ul>
</li>
<li>画面共有
<ul>
<li>winやubuntuからMac miniとか見えるかな?</li>
</ul>
</li>
<li>キーボード共有
<ul>
<li>macも使うので、KVMスイッチとかかな?</li>
</ul>
</li>
<li>マウス共有
<ul>
<li>MX ErgoのFlowを使うと行き来が簡単にできたので良さそう。</li>
<li>ただし、Ubuntuだとどうなるのか?</li>
</ul>
</li>
<li>システム監視系
<ul>
<li>CPUの温度とかCPU、メモリの使用率とか</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>とりあえずこんなところです。カーソル移動系が一番重要なので、そのへんから色々とちょっとずつ試していく予定。
ぜひオススメアプリとかあれば教えていただければと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Dozen0を作成した #DIYキーボード</title>
      <link>https://blog.johtani.info/blog/2020/10/11/build-dozen0/</link>
      <pubDate>Sun, 11 Oct 2020 23:53:26 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/10/11/build-dozen0/</guid>
      <description>今年の春くらいから、セパレートタイプのキーボードが気になっています。 また、なんか知らないですが、Twitterのタイムラインが自作キーボード</description>
      <content:encoded><p>今年の春くらいから、セパレートタイプのキーボードが気になっています。
また、なんか知らないですが、Twitterのタイムラインが自作キーボード(DIYキーボード)で盛り上がってる気がします。
(たぶん、気になってるから余計目についてる)。
これとか。このスライドから、自分がやってるのは「まだ」DIYキーボードだなということで、タイトルに使ってみました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr"><a href="https://twitter.com/hashtag/toruby?src=hash&amp;ref_src=twsrc%5Etfw">#toruby</a> &quot;DIYキーボードは実質Ruby&quot; 本日のスライドです! とちぎでこの話をできてよかった!!!q: <a href="https://t.co/RJot71Ngu9">https://t.co/RJot71Ngu9</a></p>&mdash; Kakutani Shintaro (@kakutani) <a href="https://twitter.com/kakutani/status/1304655402378752001?ref_src=twsrc%5Etfw">September 12, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="セパレートタイプのキーボード">セパレートタイプのキーボード</h2>
<p>今年の夏まではMac Book Pro 16インチのキーボードを利用していました(<a href="https://blog.johtani.info/blog/2020/03/26/working-facility/">参考:自宅の作業環境(2020)</a>)。
これまで、自宅で作業することよりも、外で仕事をすることが多く、また、外といっても様々な場所(サムライズムだったり、オフィスだったり、カフェだったり)で作業をすることが多かったので、ノートPCのキーボードにしていました。流石にキーボードを持って歩くほどは気にしていなかったので。</p>
<p>ただ、コロナウイルスの影響もあり、ほぼ自宅で作業することとなりました。
となると、前から気になっていたセパレートタイプのキーボードが俄然気になり始めます。
(といいつつ、<a href="https://blog.johtani.info/blog/2020/09/08/update-working-facility/#%E3%82%AD%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%83%91%E3%83%BC%E3%83%A0%E3%83%AC%E3%82%B9%E3%83%88">寄り道してたり</a>しますが)</p>
<p>気になっていたのはこのあたりなのですが、</p>
<ul>
<li><a href="https://ergodox-ez.com/">Ergodox EZ</a></li>
<li><a href="https://www.zsa.io/moonlander/">Moonlander Mark I</a></li>
</ul>
<p>流石にいきなり行くにはちょっと気が引けるなぁと(なかなかいいお値段)。
また、自分が日本語キーボードを利用しているというのもちょっとあります。</p>
<h2 id="キースイッチの感触を知りたくて">キースイッチの感触を知りたくて</h2>
<p>ただ、日々、楽しそうな自作キーボードの記事や画像が流れてきます。
で、思い出したのが自分の趣味。プラモデルです。色を塗ったりはしないですが、組み立てるのは楽しいなと。
じゃあ、趣味と実益を兼ねればいいのでは?(ほんとか?)となり、作る気になってきました。</p>
<p>ただ、全く知らない世界だし、どんなものなんだろう?と。
市販のキーボードでも、キーの押し具合が色々合ったり、形もまちまちです。
また、このご時世ですので、自宅からあまり出ていないので遊舎工房さんなどに遊びにも行けず。</p>
<p>そんなところに流れてきたのがキースイッチのテスターでした(世の中誘惑だらけ)。</p>
<ul>
<li><a href="https://yushakobo.jp/shop/b0200st-f2-1/">スイッチテスター人気スイッチ詰め合わせ 18個セット</a></li>
</ul>
<p>「なるほど、これでどんな感触かわかるじゃないか!」と、気づけばポチッと押していました。。。
届いて、「ふむふむ、これがこういう感触なのか。なるほど」ポチポチ押しながら、さらに自作キーボードについて調べていきます。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">いろんな感触なんだなぁ <a href="https://t.co/PaGDMIRJiv">pic.twitter.com/PaGDMIRJiv</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1308289480458084353?ref_src=twsrc%5Etfw">September 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="キースイッチのストロークの深さを知りたくて">キースイッチのストロークの深さを知りたくて</h2>
<p>まぁ、当たり前なのですが、先程のキースイッチテスターどこにもつながっていません。
キーボード触ってるとわかりますが、ストロークが違いがあるんですよ。
ただ、これだとわからない。けど知りたい。</p>
<p>で、ググっていると<a href="https://qiita.com/kamaboko123/items/c32ad91434ffc7f4ff8d">Arduinoを使ってLチカやってる人とか、キー入力させている人</a>がいるじゃないですか。
これでは?そういえば、Arduinoもどきうちにもあるぞ?と。
で、準備をしていたときに見かけてしまったのがこのツイートでした。。。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr"><a href="https://t.co/ALl7OcRVRa">https://t.co/ALl7OcRVRa</a> 高機能テスターとしてもおすすめ</p>&mdash; Kakutani Shintaro (@kakutani) <a href="https://twitter.com/kakutani/status/1312749674214744064?ref_src=twsrc%5Etfw">October 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><strong>やられてしまいました。。。</strong></p>
<h2 id="dozen0作りました">dozen0作りました</h2>
<p>ということで、前置き長かったですが、Dozen0というキーボード(マクロパッド?)を<a href="https://yushakobo.jp/shop/dozen0/">遊舎工房さんから購入して</a>作ってみました。
手順は<a href="https://github.com/yynmt/Dozen0/blob/master/docs/buildguide_jp.md">ビルドガイド</a>という形でまとまっています。</p>
<p>基本的にはこれに沿って作成しました。</p>
<h3 id="パーツと手順の確認">パーツと手順の確認</h3>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">さてと。 <a href="https://t.co/GTBJ7zDleN">pic.twitter.com/GTBJ7zDleN</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314798164919029765?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>まずは入っているパーツがきちんとあるか確認します。
また、ビルドガイドでどんな作業があるのかをざっと眺めておきます。
まぁ、プラモデルといっしょですよね。</p>
<p>キットにはキースイッチとキーキャップは含まれていません。これはご注意ください。
私は、キースイッチテスターとして入手していたキースイッチとキーキャップがどんな押し具合なのかを確認したかったので特に問題ありません。</p>
<h3 id="ハンダづけ">ハンダづけ</h3>
<p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B006MQD7M4/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B006MQD7M4&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B006MQD7M4/?tag=johtani-22">
      Amazon | 白光 ダイヤル式温度制御はんだこて FX600 | ハンダゴテ
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B004OR7UBM/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B004OR7UBM&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B004OR7UBM/?tag=johtani-22">
      Amazon | 白光 こて先 2C型 T18-C2 | ハンダゴテパーツ
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B017SQ0TUO/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B017SQ0TUO&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B017SQ0TUO/?tag=johtani-22">
      Amazon | 白光(HAKKO) セラミックヒーターはんだこて専用こて台 クリーニングスポンジ付き FH300-81 | ハンダゴテパーツ
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B0029LGAJI/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B0029LGAJI&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B0029LGAJI/?tag=johtani-22">
      Amazon | goot(グット) 高密度集積基板用 鉛入りはんだ Φ0.6mm スズ60%/鉛40% ヤニ入り SD-60 | ハンダゴテ
      </a>
    </p>
  </div>
</div></p>
<p>はんだ付け用に揃えた道具です。「自作キーボード ハンダゴテ」とかでググると出てきたものがこれだったので。
デフォルトのコテ先ではなく、2Cのコテ先に付け替えてから作業を開始しました。</p>
<p>私の持っていたソケットはCherry MX系なのですが、気が向いたらKailh Chocも試したくなるかも?ということで、
すべてはんだ付けしました。本格的にはんだ付けしたのは大学以来だからもう20年近く経ってますね。
ソケットの足をプリント基板にはんだ付けするのですが、2Cのコテ先が大きくなかなか手こずりました(<em>これがこのあと問題を引き起こしました</em>)。</p>
<p>ソケット、リセットスイッチ、ProMicroをはんだ付けすればはんだの作業は終了です。</p>
<p><blockquote class="twitter-tweet"><p lang="und" dir="ltr"><a href="https://t.co/jjUo162n54">pic.twitter.com/jjUo162n54</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314810257873399810?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="und" dir="ltr"><a href="https://t.co/p5atvtHfFN">pic.twitter.com/p5atvtHfFN</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314810285002088448?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<h3 id="スイッチの取付と組みたて">スイッチの取付と組みたて</h3>
<p>試したいキースイッチを選んでトッププレートにつけていきます。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">キーテスターからスイッチとキーキャップ持ってきて完成。動作確認はまだなのでまた、最初に戻るかもだけど、、、 <a href="https://t.co/ZmBd3bgOKI">pic.twitter.com/ZmBd3bgOKI</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314822558865264641?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>組み立ても終わり次は実際の動作確認です(ツイートしていますが、予想通りの展開でした)。</p>
<h3 id="ファームウェアの書き込み">ファームウェアの書き込み</h3>
<p>USBで接続して、ファームウェアの書き込みです。
書き込み手順は<a href="https://github.com/yynmt/Dozen0/blob/master/docs/buildguide_jp.md#6-%E3%83%95%E3%82%A1%E3%83%BC%E3%83%A0%E3%82%A6%E3%82%A7%E3%82%A2%E3%81%AE%E6%9B%B8%E3%81%8D%E8%BE%BC%E3%81%BF">ビルドガイド</a>にありますが、QMK Toolboxの使い方やQMK Configuratorの使い方は以下の<a href="https://salicylic-acid3.hatenablog.com/">サリチル酸さんのブログ</a>がわかりやすかったです。</p>
<ul>
<li><a href="https://salicylic-acid3.hatenablog.com/entry/qmk-toolbox">（初心者編）自作キーボードにファームウェアを書き込む</a></li>
<li><a href="https://salicylic-acid3.hatenablog.com/entry/qmk-configurator">（初心者編）QMK Configuratorを使ってキーマップを書き換えよう</a></li>
</ul>
<p>実際に書き込んで動かしてみると。。。。</p>
<p><blockquote class="twitter-tweet"><p lang="ja" dir="ltr">うーん、上半分が死んでる気がするなぁ。<br>デフォルトキーマップの書き込みはできたけど、カーソルの上とかバックスペースが入らないw</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314844321674190848?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">とりあえず、もう一回確認してみた。デフォルトのキーマップで、「Cut」「Paste」が強く押さないと入力されない。UpとDeleteは動くようになってた。CopyとBkSpがうんともすんとも言わない。下段のキーは全部動く。<a href="https://t.co/zEyINdAK6t">https://t.co/zEyINdAK6t</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314916612168794112?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<p>といった具合に上半分が動作があやしいです。</p>
<h3 id="再度はんだ付け">再度、はんだ付け</h3>
<p>ということで、ネジを外して上の段のソケットをはんだで温めてみると、片側だけ温めるとはずれるじゃないですか。
どうやら、うまくソケットの足が基盤にはんだ付けできていなかったようです。
この時、最初に作業したときに2C型のコテ先が大きくて、ソケットの足に入らなかったのを思い出したので、
最初にはんだごてについていたコテ先に戻してから作業をしました。
これだとソケットの足の隙間に入るんです。ということで、挙動のおかしい上段のソケットをすべてつけ直したところ無事動作しました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">上の段、全体的にソケットがうまくはんだ付けできてなかったみたいだった。今回はMX系のスイッチしかないので、Kailh Chocのソケットの導通は未確認。ざっとブログにまとめるか。</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1314927893680594944?ref_src=twsrc%5Etfw">October 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h3 id="現在のキーマップ">現在のキーマップ</h3>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201011/dozen0_music_play.png" />
    </div>
    <a href="/images/entries/20201011/dozen0_music_play.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>これが今のキーマップです。仕事中のBGMをラズパイ4の音楽プレーヤーで流していますが、
部屋から出るときや、打ち合わせがあるときに音楽の停止、開始などを画面共有経由でやっています。
が、わざわざマウス移動するのもめんどくさいなと。これに使えそうだというのもあったので、Dozen0を購入したというのがあります。
残念ながら、<code>Prev Track</code>とかがうまく動かないので、ショートカットを調べてキーマップ書き換える予定ですが、非常に便利になりました。</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、Dozen0を作ってみましたが、楽しかったです。すんなりと行かなかったのがまた良かったです。
試行錯誤して動いたときの喜びが何倍もありました。また、キースイッチの感触も知ることができました。
たぶんGateron Silentクリア軸もしくは赤軸くらいが良さそうだなぁと(こうやって沼へ。。。)。
キーマップの仕組みを調べたり、もっと面白い使い方できないかな?と探りつつ、次のステップに進もうかなと。</p>
<p>次は本格的なセパレートタイプのキーボードを作る予定です。が、お店のオープンを待ってからということになってます。はやくオープンしないかなぁ!!</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">がんばって!!</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1310887394879496193?ref_src=twsrc%5Etfw">September 29, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</content:encoded>
    </item>
    
    <item>
      <title>LinderaのFSTをDoubleArrayTrieに変更した話</title>
      <link>https://blog.johtani.info/blog/2020/10/05/switch-fst-2-da/</link>
      <pubDate>Mon, 05 Oct 2020 11:36:18 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/10/05/switch-fst-2-da/</guid>
      <description>2020/10/06 11:00くらいにマージされました。 @minoru_osuka さんが開発を引き継いだLinderaというKuromojiのRustクローンがあります(リポジトリ)</description>
      <content:encoded><blockquote>
<p>2020/10/06 11:00くらいにマージされました。</p>
</blockquote>
<p><a href="https://twitter.com/minoru_osuka">@minoru_osuka</a> さんが開発を引き継いだ<a href="https://qiita.com/mosuka/items/0fdaaf91f5530d427dc7">LinderaというKuromojiのRustクローン</a>があります(<a href="https://github.com/lindera-morphology/lindera">リポジトリ</a>)
。
最近趣味でRustを勉強しているので、こちらを少し手伝っています。</p>
<p>Rustの勉強仲間である<a href="https://twitter.com/takuya_b">@takuya_b</a>さんや<a href="https://twitter.com/ikawaha">@ikawaha</a>さんと話をしているときに、FST部分をDouble Array Trieに置き換えると速度が向上するのでは?という話が出まして、@takuya_bさんがDouble Array Trieを作るらしいという話になったので、下準備などをしつつ、作ってもらったライブラリ<a href="https://github.com/takuyaa/yada">yada</a>を組み込んでみたという話です。</p>
<h2 id="ベンチマークの追加">ベンチマークの追加</h2>
<p>下準備として、今のLindera(FST実装)がどのくらいの性能なのか?というのを計っておく必要があります。
幸いにも、Linderaのオリジナルの開発者の方が、<a href="https://github.com/bheisler/criterion.rs">criterion.rs</a>というライブラリを使った<a href="https://github.com/lindera-morphology/lindera/blob/581728bf790a331402ef7a200fd443c4f9244abd/lindera/benches/bench.rs">ベンチマークプログラム</a>を作成してくれていました。</p>
<p>ただ、1種類だけだと少し心もとないなというのと、長い文章やパターンを増やしたほうが良さそうだなということで、
ベンチマーク自体をいくつか追加しました(<a href="https://github.com/lindera-morphology/lindera/pull/74">追加したときのPR</a>)。</p>
<p>種類としては、5種類のベンチマークです。</p>
<ol>
<li>システム辞書のみのTokenizerのコンストラクタ呼び出し</li>
<li>カスタム辞書ありのTokenizerのコンストラクタ呼び出し</li>
<li>システム辞書のみのTokenizerの<code>tokenize</code>処理の呼び出し</li>
<li>カスタム辞書ありのTokenizerの<code>tokenize</code>処理の呼び出し</li>
<li>青空文庫の坊っちゃんのテキストをシステム辞書のみのTokenizerで<code>tokenize</code></li>
</ol>
<p>1,2はコンストラクタ部分だけの処理をベンチマークテストする目的で作成しました。
LinderaはTokenizerがtokenize処理するのに利用するデータをいくつか内部で保持しています。
これらはファイルにシリアライズされており、Tokenizerのオブジェクト生成時に読み込みやデシリアライズ処理が発生します。
この部分だけも速度を計測したい目的でコンストラクタだけを切り出しました。</p>
<p>3,4はTokenizerのメインの処理です。コンストラクタはベンチマークの対象外にしました。
純粋にtokenizeの処理だけを切り出して計測するためです。
カスタム辞書がある場合、ない場合は念の為切り出した形になっています。</p>
<p>5は長い文章(文章が多いのでバリエーションも増える)を扱いたいために別にしました。</p>
<p>これで、一応下準備が完了です。
ちなみに、Criterionは賢くて、前のベンチマークの結果と最新の結果を比較してくれる機能があります。
どんな感じで出てくるかはベンチマーク結果をご覧ください。</p>
<h2 id="yadaの組み込み">yadaの組み込み</h2>
<p>ベンチマークの準備をしていたところyadaがリリースされたので、Linderaへの組み込みを検討し始めました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">というわけで、またダブル配列を書いてしまったので crate として公開しました。フィードバックお待ちしております！ <a href="https://t.co/As7h0tfmjf">https://t.co/As7h0tfmjf</a></p>&mdash; takuya-a (@takuya_b) <a href="https://twitter.com/takuya_b/status/1307671030731694081?ref_src=twsrc%5Etfw">September 20, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h3 id="中身の理解">中身の理解</h3>
<p>lindera-fstを利用して、prefix searchしている処理があるので、そこで利用しているFSTをyadaに置き換えれば良さそうだと判断して、
処理を読んでいきます。</p>
<ul>
<li>Tokenizerは、<a href="https://github.com/lindera-morphology/lindera/blob/027ab8c7d5fdd6d1bc2dc7e8779adb1ddcf7f770/lindera-core/src/core/prefix_dict.rs">PrefixDict</a>という構造体でlindera-fstを利用している
<ul>
<li><a href="https://github.com/lindera-morphology/lindera/blob/027ab8c7d5fdd6d1bc2dc7e8779adb1ddcf7f770/lindera-core/src/core/prefix_dict.rs#L27-L64">prefixメソッド</a>が入力文字列を元に、FSTを前方一致検索して、ヒットした単語の情報をIteratorとして取り出せる(単語の情報は「入力文字列の先頭からの文字数」と「ヒットした単語のWordEntry構造体」)</li>
<li>PrefixDictのfstは辞書(例：ipadic)ごとに<code>lindera-&lt;辞書名&gt;-builder</code>で生成される</li>
</ul>
</li>
<li>システム辞書としては、デフォルトでは<code>lindera-ipadic-builder</code>でfstを構築している
<ul>
<li><a href="https://github.com/lindera-morphology/lindera/blob/027ab8c7d5fdd6d1bc2dc7e8779adb1ddcf7f770/lindera-ipadic-builder/src/lib.rs#L237-L249">構築処理はこの辺</a></li>
</ul>
</li>
</ul>
<p>という感じです。
また、辞書周りのファイルがそれぞれどんな役割なのか、どんなデータの持ち方をしているのか?といった点を、変更点の調査のついでに書き出してみました。<a href="https://github.com/lindera-morphology/lindera/blob/master/lindera-dictionary/FILES.md">lindera-dictionary/FILES.md</a>。TODOになっている部分も追記が終わっています(<a href="https://github.com/lindera-morphology/lindera/pull/77">PR</a>)</p>
<h3 id="変更点">変更点</h3>
<p>実際に変更したプログラムの詳細についてはの<a href="https://github.com/lindera-morphology/lindera/pull/76">PR</a>を見ていただくとして、簡単には以下の点になります。</p>
<ol>
<li>Rustのバージョンを1.46.0に(おもにREADME.md)
<ul>
<li>yadaが利用している機能に1.46.0で導入された機能があるため</li>
</ul>
</li>
<li>lindera-fstをyadaに変更(lindera-core/Cargo.toml, lindera-ipadic-builder/Cargo.toml)
<ul>
<li>合わせて、<code>dict.fst</code>というファイル名を<code>dict.da</code>に変更</li>
</ul>
</li>
<li><code>dict.da</code>に関して構築部分と検索部分を変更
<ul>
<li>FSTではFSTから返ってくる値(入力文字列に出てきた単語に関連する値)は<code>u64</code>だったが、yadaのDoubleArrayが<code>u32</code>しか扱えないため、<code>u32</code>に変更。テストの記述はしていないが、扱うデータ的に<code>u32</code>で問題なさそうだったので。</li>
<li>検索部分：PrefixDict構造体の<code>prefix</code>メソッドで<code>DoubleArray</code>の<code>prefix_common_search</code>を使用
<ul>
<li>DoubleArray自体が<code>prefix_common_search</code>のメソッドを持っていたので、処理が簡単に置き換え可能だった。FSTは<code>prefix</code>メソッド内で独自で前方一致検索を実装していた。</li>
</ul>
</li>
<li>構築部分：<code>lindera-ipadic-builder/src/lib.rs</code>の<code>build_dict</code>と<code>build_user_dict</code>の<code>dict.da</code>構築処理
<ul>
<li>ipadicのCSVファイルを読み込んで、見出し語をキーに、辞書にある単語情報のベクタを値とするBTreeMapを生成し、このBTreeMapに基づいてFSTを構築していた部分をDoubleArray構築処理に置き換えた。</li>
<li>シフト演算などで、実際の値(<code>dict.vals</code>)へのポインタを作っていたのだが、ここの処理を読み解くために<code>FILES.md</code>を書き出した。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>という感じです。そもそもデータ構造がどうなっているのか?から読み解いて、変更部分を洗い出して変更していった形になります。
取り込み作業中にいくつか<code>yada</code>に要望(<a href="https://github.com/takuyaa/yada/issues?q=is%3Aissue+is%3Aclosed">このへん</a>)を上げて、変更を取り込んでもらい、最終的にyadaのバージョン<code>0.3.2</code>で問題なく動きそうだという形になりました。<a href="https://twitter.com/takuya_b">@takuya_b</a>さん、対応ありがとうございました。</p>
<h4 id="エッジケースバグの発見">エッジケースバグの発見</h4>
<p>作っててよかった、テストケースでした(実際にはベンチマークテストですが)。
取り込み作業中に、Lindera本体の<code>cargo test</code>はすべてOKになるが、ベンチマークを取ろうとしたときに、坊っちゃんの文字列を入力にしたベンチマークが失敗するという事象が発生しました(<a href="https://github.com/lindera-morphology/lindera/pull/76#issuecomment-699735905">PRのコメント参照</a>)。
切り分けのために、入力の文章のどこでおかしくなるのか?DoubleArrayの<code>build</code>メソッドに渡している値がおかしくないか?などをすこしずつ調べていくと次のバグが判明したという感じです。</p>
<p>特定のデータ(ipadicの見出し語一覧)をDoubleArrayに入れて、特定の文字列(<code>「は相」</code>)を<code>common_prefix_search</code>にいれたら、
返ってくる情報(0から何バイト目の文字が一覧に存在した)が、不正な値が返ってくるというバグでした。
@takuya_bさんに見てもらいつつ(DoubleArrayの中身わからん。。。)、修正してもらいました。素早い対応ありがとうございます。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">yada 0.3.1 をリリースしています。特定の条件で不正な遷移を許すダブル配列が構築されてしまうバグを修正しています。このエッジケースは <a href="https://twitter.com/johtani?ref_src=twsrc%5Etfw">@johtani</a> さんが見つけてくださいました。ありがとうございました！ <a href="https://t.co/CiftZi5GDn">https://t.co/CiftZi5GDn</a></p>&mdash; takuya-a (@takuya_b) <a href="https://twitter.com/takuya_b/status/1311153143971864576?ref_src=twsrc%5Etfw">September 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>やはり、いろんな文字列入れてテストしてみるの重要ですね。
ということで、ベンチマークだけでなく、テストケースとしても坊っちゃんのファイルを読み込んでトークナイズするようにPRでテストケースを追加しています。</p>
<h2 id="ベンチマーク結果">ベンチマーク結果</h2>
<p>yadaを利用した変更が終わったので、再度<code>cargo bench</code>を実行して計測です。
計測としては、<code>master</code>ブランチでまず<code>cargo bench</code>を実行し、yadaの実装をしたブランチに切り替えてから<code>cargo bench</code>を実行します。
すると、Criterion? <code>cargo bench</code>が、最終的な結果に前回との差分でどのくらい性能が改善、改悪したかも合わせて出力してくれます。
実行環境と結果は以下のとおりです。</p>
<ul>
<li>MacBook Pro 16インチ
<ul>
<li>CPU：Core i7 6コア 2.6GHz</li>
<li>メモリ：32GB</li>
</ul>
</li>
</ul>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20201005/bench.png" />
    </div>
    <a href="/images/entries/20201005/bench.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>コンストラクタのベンチマークについては10%ほど性能が悪くなっています。
これは、FSTよりもDoubleArrayTrieのほうがデータが大きくなってしまうためだと思われます。
実際にファイルのサイズは次のようになりました。yada(DoubleArrayTrie)のほうが2倍以上大きいことがわかります。
また、このファイル以外にもLinderaが利用しているデータはありますが、それらは今回変更の対象にはなっていません。
なので、単純にこのファイルの読み込みの処理に時間がかかっているのだと想像できます。</p>
<ul>
<li><em><strong>2147765</strong></em> / FST / dict.fst</li>
<li><em><strong>5425152</strong></em> / yada / dict.da</li>
</ul>
<p><code>tokenize</code>のベンチマークについては、11%〜28%の改善が見られました。
文章から、内部に保持している辞書に存在する単語を見つけ出す処理に利用されるのがFST、DoubleArrayTrieです。
今回の変更では、この処理に利用しているデータ構造だけを変更しました。
実際には</p>
<ul>
<li>DoubleArrayTrieを用いた単語の検索処理</li>
<li>見つかった単語の持つ値(<code>data.vals</code>のオフセット情報)を元にシフト演算</li>
</ul>
<p>といった処理が実行されます。シフト演算は<code>u64</code>だったものが<code>u32</code>に変更されたくらいなので、大した処理量ではないかと。
大部分はDoubleArrayTrieを利用したルックアップ処理が速度向上に寄与していると思います。</p>
<h2 id="まとめ">まとめ</h2>
<p>最近Linderaに加えた変更、作ったPRについて少しブログにまとめてみました。
ちなみに、まだPRの段階でレビュー&amp;リリース待ちという感じです。</p>
<p>実際には作ってもらったライブラリを組み込んでみたというだけなのですが、速度が向上した結果が見れたのは面白いです。
また、基本的なデータ構造とかアルゴリズムの勉強にもなりました(2次元配列を1次元配列に押し込むとか)。このへんも今後も勉強していきたいです。</p>
<p>組み込む際に色々と協力していただいた@takuya_bさん、@ikawahaさん、巻き込んでくれた@minoru_osukaさんに改めて感謝いたします。</p>
<p>Rustや形態素解析のプログラムの勉強を兼ねて、今後もなにか改善できる部分がないかなどを見ていこうと思っています。
Rustで形態素解析をしたいという人がどのくらいいるかはわかりませんが、おかしなところや疑問点などあればコメントください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>検索対象のデータとデータソース(検索システムに関する妄想その3)</title>
      <link>https://blog.johtani.info/blog/2020/09/15/improve-search-no3/</link>
      <pubDate>Tue, 15 Sep 2020 11:23:34 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/09/15/improve-search-no3/</guid>
      <description>先日は「検索システムを構成するパーツ」ということで検索システムを構成しているパーツについて書いてみました。 大体、検索がうまくヒットしないとい</description>
      <content:encoded><p>先日は「<a href="/blog/2020/07/28/improve-search-no2/">検索システムを構成するパーツ</a>」ということで検索システムを構成しているパーツについて書いてみました。</p>
<p>大体、検索がうまくヒットしないといった場合に、問題になるのがコンテンツ自体のデータもしくは、転置インデックスのキーワードだったりします。
そこで今回は、前回のパーツの「データソース・コンテンツ」周りについて少し書いてみようと思います。言葉の定義、それぞれがどんなことをやるのか、とりあえず導入したあとにコンテンツ周りでどんな改善ができるかなどを書いてみます。</p>
<h2 id="言葉の定義">言葉の定義</h2>
<h3 id="コンテンツ">コンテンツ</h3>
<p>実際に検索させたいデータになります。
コンテンツにはWebページ、データベースのレコード(CMSで登録されたデータなど)、ファイルサーバーにある文書(PDF、Word、Excelなど)などになります。</p>
<h3 id="データソース">データソース</h3>
<p>コンテンツのマスタデータが保存されている先です。
よくあるデータソースとしては以下のものが考えられます。</p>
<ul>
<li>Webサイト - 自社もしくはインターネットに存在しているWebサイトです。</li>
<li>ファイルサーバー - ローカルネットワーク上のファイルサーバーもありますが、最近ではGoogle DriveやDropboxといった外部のWebサービスもあります。</li>
<li>RDB - CMSや自社システムでのデータの保存先です。</li>
</ul>
<h3 id="クローラー">クローラー</h3>
<p>コンテンツをデータソースから収集してくるプログラムのことです。
Webサイトやファイルサーバーからコンテンツを収集して検索エンジンに登録するところまでを担当します。
RDBにあるデータを検索エンジンに登録する場合はクローラーがデータを登録するというよりは、RDBにデータを登録するシステムが検索エンジンに登録する機能を持っていることが多いです。</p>
<h2 id="データの収集と登録">データの収集と登録</h2>
<h3 id="収集">収集</h3>
<p>クローラーを使用した収集の場合は、サービスの特性とデータソースによって、どの程度の頻度でクロールするのか、クロール対象はどこまでか?といったものを決める必要が出てきます。
これらが決まれば収集ができるかと(他に権限とかもありますが。)。
収集コンテンツは、そのままでは利用しにくかったり、利用できないことがあるので、次はデータの変換を行います。</p>
<h3 id="データ変換">データ変換</h3>
<p>コンテンツはそのままの形では検索エンジンには扱いにくいデータ形式である場合があります。</p>
<h4 id="webページの場合">Webページの場合</h4>
<p>Webページの場合、コンテンツにはHTMLタグが入っていたり、JavaScriptなど検索対象にはしたくないデータなどが入っています。これらを除去して、検索させたいものを取り出す必要があります。
また、Titleタグなど、いくつかメタデータとして扱えるものがHTMLで規定されているので、これらを別の項目として取り出して個別に検索できるようにすると便利です。
HTMLをパースしてデータを抜き出す処理ができるライブラリなどがあるので活用します。</p>
<h4 id="ファイル">ファイル</h4>
<p>PDFファイルなど、ファイルの場合もメタデータと呼ばれるファイル自体が持っている情報が存在します。作成者、更新日時、ファイル名、パスなどです。
これらも検索時に有効な情報になります。
また、ファイルから文字列を抜き出す処理も必要になります。
それぞれデータフォーマットが異なりますので、そのフォーマットに合わせて文章データを抜き出す処理が必要です。OSSや製品がありますので、それらを利用して、ファイルから文章を抜き出します。</p>
<h4 id="rdb">RDB</h4>
<p>RDBのデータの場合、データが正規化されています。
検索エンジンでは、非正規化のデータを登録して検索することが基本となるため、まずは非正規化して取り出す必要が出てきます。</p>
<p>例えば、ジャンルやカテゴリ、各種IDなどが実際のコンテンツのレコードに入っていると思いますが、これらをユーザーが入力したキーワードで検索したい場合などは、IDではなく表示名を取り出して、検索エンジンに登録する必要が出てきます。</p>
<h4 id="その他">その他</h4>
<p>データごとの変換処理について説明しました。
その他に、データのクリーニング処理などと言ったことも必要になってきます。例えば、HTMLのタイトルに必ずサイト名が入っているが、除去したいといった場合や、検索エンジン固有のデータの前処理などもあります。</p>
<h3 id="登録">登録</h3>
<p>最後は検索エンジンへの登録です。
最近の検索エンジンはJSON形式でデータを受け取る場合が多いので、JSONに変換することが多いです。
基本的には各種プログラミング言語のライブラリが用意されているのでこれらを利用するのが基本となります。</p>
<p>検索したい項目、検索させたい方法などを洗い出し、必要なデータを作成して検索エンジンに登録します。
登録と書いていますが、更新、削除などもここでの対象となります。</p>
<p>ここまでの流れで、データソースからコンテンツを取得し、変換して、検索エンジンへの登録が終わりました。</p>
<h2 id="検索の改善">検索の改善</h2>
<p>検索のログから分析して改善していくのが良いですが、ユーザーからの質問や意見などからも改善すべき点が見えてくると思います。
検索ログでは、次のようなものを元に、検索がうまく行かないものを見つけ出します。</p>
<ul>
<li>0件ヒット</li>
<li>0件クリック</li>
</ul>
<p>ヒットしていない検索ワードがある場合、コンテンツに問題がある場合があります。まずはこのあたりをとってみるのが良いかと。
そもそも、入力されたキーワードにマッチするコンテンツを扱っていないこともわかりますし、入力されたキーワードに似た単語を持ったコンテンツなども存在するはずです。
類義語の辞書を用意して、検索にヒットできるようにするといった分析と改善にも利用できます。</p>
<p>あとは、検索エンジン側の話ですが、形態素解析器などを利用している場合に、意図した区切りになっていないために、うまくキーワードがヒットしないと言ったこともあります。</p>
<h3 id="コンテンツの理解">コンテンツの理解</h3>
<p>実はこれが一番だったりします。
どんなコンテンツを自分たちが扱っているのか?どんなデータがどういった項目でコンテンツに入っているのか?といったところから、
コンテンツのデータを元に検索にヒットさせる方法が改善できます。</p>
<p>CMSなど人が入力したデータをコンテンツとして扱う場合、入力画面を改良することで、望んでいるデータを入れてもらえたり、不要なデータが入らなくなる可能性があります。
例えば、ECサイトなどで商品の説明文やタイトルにいろいろなキーワードが入っている場合などがあります。むりやりどんなキーワードでもヒットさせたいという入力者の意図もあるのですが、検索しているユーザーにはノイズになることも多いです。適切にカテゴリやジャンル、属性といった項目に分けることでおかしな入力データを減らすことも可能です。</p>
<p>Webサイトなどをクローリングしたものの場合は、サイトごとに文章の特徴があったり、重複している部分などが合ったりする場合があります。
これらもコンテンツをよく調べることで、不要な情報を除去したりといったことが可能になります。</p>
<h2 id="まとめ">まとめ</h2>
<p>簡単ですが、検索のデータソースやコンテンツにまつわる話を紹介しました。
もちろんここでは紹介しきれていない項目がいっぱいあります。
また、具体例ではなく概略をざっくりと書いているのでわかりにくい場合もあるかもしれません。
すこしユースケースを絞り込んで書いたほうがわかりやすくなるのかも?</p>
<p>次はUIとか書くかも?</p>
<p>不明点とか疑問点、指摘事項などあればコメントしていただければと。
要望などもお待ちしております。</p>
</content:encoded>
    </item>
    
    <item>
      <title>自宅の作業環境(2020/09)</title>
      <link>https://blog.johtani.info/blog/2020/09/08/update-working-facility/</link>
      <pubDate>Tue, 08 Sep 2020 11:17:25 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/09/08/update-working-facility/</guid>
      <description>自宅環境に少しアップデートがあったので更新版です。 お客さんのおかげで相変わらず自宅で作業させてもらってるのもあり、昨今のコロナウイルスの影響</description>
      <content:encoded><p>自宅環境に少しアップデートがあったので更新版です。
お客さんのおかげで相変わらず自宅で作業させてもらってるのもあり、昨今のコロナウイルスの影響で出かけることもないので、自室の作業環境が更新されている感じです。
<a href="%22/blog/2020/03/26/working-facility/%22">前回のブログ</a>はこちらです。
前回のまとめで触れていた2点について更新されています。</p>
<p>まずは、現状のデスクの上の写真です。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200908/my_desk.jpg" />
    </div>
    <a href="/images/entries/20200908/my_desk.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>基本的には前回紹介したものと大きくは変わっていません(?)。
変わったものについて紹介していきます。</p>
<h2 id="デスク">デスク</h2>
<p>先程の写真ではちょっとわかりにくいですが、スタンディングデスクになりました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">タイマーついてるの便利 <a href="https://t.co/E9engTIMnM">pic.twitter.com/E9engTIMnM</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1255668820510818304?ref_src=twsrc%5Etfw">April 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>Flexispotの天板と足の組み合わせにしました。
前のElectaの天板が明るめの色だったのもあって、メープルの天板です。
組み立てはちょっと大変でした。
足が重たいのと、天板にはネジ穴がなく電動ドライバーもないので、ネジ締めと机を起こすのがちょっと大変です。子供に手伝ってもらいながら組み立て設置をしました。
オンラインミーティングなどのときには基本、立って作業しています。
あとは、デフォルトのタイマー設定のままですが、45分ごとに立ったり座ったりを数回繰り返す感じです。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07G9FPCMP/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07G9FPCMP&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07G9FPCMP/?tag=johtani-22">
      Amazon | FLEXISPOT オフィスデスク用天板  スタンディングデスク120×60cm
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07LF51796/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07LF51796&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07LF51796/?tag=johtani-22">
      Amazon | FLEXISPOT スタンディングデスク 電動式 昇降デスク メモリー機能付き ブラック EN1B（天板別売り） 
      </a>
    </p>
  </div>
</div>
<h2 id="スタンディングデスクマット">スタンディングデスクマット</h2>
<p>フローリングに45分立ちっぱなしだと結構、足の裏がつかれるんですよ。なので、クッション性が高いフロアマットを立ってるときは利用しています。ちょっとめんどくさいのですが、椅子のときは壁に立て掛けて、立つときにマットを敷くようにして使っています。</p>
<p>ちなみに、ウレタン素材なので開封直後はすごい匂いでした。。。
数日は陰干しみたいなのが必要です、ご注意を。
ただ、これを利用してから膝やかかとが痛いことはなくなっています。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07MVY63GH/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07MVY63GH&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07MVY63GH/?tag=johtani-22">
      Amazon | 疲労軽減マット スタンディングデスクマット 滑り止め加工 立ち仕事 耐水 耐油 耐菌 (ブラック, 75cm*50cm*2cm)
      </a>
    </p>
  </div>
</div>
<h2 id="ケーブルトレー">ケーブルトレー</h2>
<p>スタンディングデスクだとケーブルをぶら下げた状態だと危なそうだなということで、ケーブルトレーも机の下に追加してあります。電源タップを配置して、ディスプレイなどの電源はこちらから取るようにして、ケーブルトレーからは電源タップとスタンディングデスクの電源コードを垂らすという感じです。実際にはLANケーブルも伸びてたりはしますが。。。
また、後で出てくるラズパイ+SSDも最近こちらに移設しました。
最初にチャコールグレーを発注したのですが、配送日が未定になったので、Lサイズのシルバーを発注するという慌てようでした。
結局、今は両方使っています。テーブルタップにラズパイと、結構乗せるものがあるんですよね。見えない場所だし、色が揃ってないのは気にならないかなと。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B00BP4S3D6/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B00BP4S3D6&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B00BP4S3D6/?tag=johtani-22">
      Amazon | プラス Garage ワイヤーケーブルトレー Lサイズ 幅63.7cm シルバー 
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B08494LN4W/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B08494LN4W&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B08494LN4W/?tag=johtani-22">
      Amazon | プラス Garage 配線ケーブルトレー 幅40cm チャコールグレー
      </a>
    </p>
  </div>
</div>
<h2 id="キーボードパームレスト">キーボード+パームレスト</h2>
<p>10数年ぶりに外付けキーボードじゃないかな?
少なくともMacに移行してからは、ずっとMac bookのキーボードを使ってました(極稀にMac mini起動後とかにMagic Keyboard使うくらい)。
外出して仕事をすることが多かったのもあり、体をキーボードに合わせる感じでした。</p>
<p>が、ずっと在宅で仕事をしていますし、<a href="https://item.rakuten.co.jp/f141500-sagamihara/11065-30029135/">ふるさと納税でRealforce(リンクは楽天)</a>がということで、申し込んでみました。2週間ちょっとで到着。
箱が厳重でびっくりしました。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07QCBJ7VT/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07QCBJ7VT&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07QCBJ7VT/?tag=johtani-22">
      Amazon | 東プレ REALFORCE SA for Mac ホワイト R2SA-JP3M-WH | Realforce
      </a>
    </p>
  </div>
</div>
<p>まだ深いキーに慣れつつあると言った感じです。
付属の2mmのキースペーサーでAPCを2.2mmの設定で使用しています。最初は3mm+APC 1.5mmでやっていたのですが、流石にキーが敏感すぎてちょっとキーボードに手をおいたまま考え事をしているだけで「lllllllllll」のようになってしまったので現在の組み合わせに落ち着いています。</p>
<p>あと、特殊な設定としてはスペースの横の<code>eng</code>/<code>kana</code>キーを設定でキーロックして使えなくしました。どうも、ご入力の原因になっているようだったので。
日本語の入力切替は、昨日までは<code>Ctrl+Space</code>を使用していましたが、このブログを書き始めてから<code>Ctrl+\</code>に変更しています。
むかーし、使っていたIME切り替えのショートカットです(知っている人は知っているやつ)。</p>
<p>マウスは使用しておらず、キーボードの手前にトラックパッドをおいて使っています。パームレストの部分は、<a href="https://magictray.samuraism.com/magictraypalm/">サムライズムさんのトラックパッドがきれいに入るmagicTrayPalm</a>です。
Mac book Proと同じような感じでトラックパッドが使えるのがいいですね。
興味がある方は、<a href="https://secure.samuraism.com/hardware-referral/FFDB5DB101C223387141F6A31888C4E6">侍割</a>のリンクをたどってもらうと、初めてサムライズムさんで買い物をする方に限り割引がつくようになってるみたいです(なんと、私もなんか割引になる!)。</p>
<h2 id="ディスプレイアーム">ディスプレイアーム</h2>
<p>外付けキーボードになったのですが、Mac Book Proの画面も活用したく、さらに上下での配置がやはり目線の移動距離が少ないので気に入っていました。ただ、前回のブログで紹介したアームでは少し高さが足りず。。。
思案しながらAmazonを回遊していたら、同じ会社の新しいアームがあり、調べると高さが取れそうだということで付け替えました。
アーム2本ありますが、今のところ1本だけを使っています。
2本目のアームにカメラを乗せるのもあり?と思いながらまだ試していませんが。玉突き式に前に使っていたアームがサンダーボルトディスプレイ(縦置き)のアームになっています。前にサンダーボルトにつけていたアームは重量オーバーで悲鳴を上げていたので。。。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B089GSLJ78/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B089GSLJ78&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B089GSLJ78/?tag=johtani-22">
      Amazon | HUANUO 2画面 アーム デュアル ガススプリング式 
      </a>
    </p>
  </div>
</div>
<h2 id="スマホ充電器">スマホ充電器</h2>
<p>寝室で利用していた斜めに立てかけるタイプのワイヤレス充電器を作業デスクに持ってきました。
実は部屋に時計がなく、Pixel 3 XLを使っているので、スマホの画面に常に表示されている時計を置き時計の代わりに使えそうだなと
(スマホで写真を撮っているので上の画像ではスマホがありませんが)。
結構便利です。ちょっと時間を確認したいときに、メニューバーの時計では流石に小さすぎるのでw</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07DJC28GS/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07DJC28GS&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07DJC28GS/?tag=johtani-22">
      Amazon | Anker PowerWave 7.5 Stand, Qi ワイヤレス充電器
      </a>
    </p>
  </div>
</div>
<h2 id="ミキサー">ミキサー</h2>
<p>音声のミキサーです。Yamahaのスピーカーは入力が2系統あるのですが、入力したい音声は3系統あります(ラズパイ4、Mac mini、メインディスプレイ)。2入力を1系統にまとめるプラグを試しに買ってみて、Mac miniとラズパイをまとめてみたのですが、残念ながら切り替わるときに数分無音状態が続くという問題が発生しました。
USB給電できるタイプを購入しました。スライダーとかいらないなと思っていたのですが、電話やミーティング時にBGMの音を下げたりするのに地味に便利だったりします。
ただ、スライダーを動かそうとすると本体も動いてしまっていたので、本体裏に滑り止めを貼ってあります。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07DRB8LJ3/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07DRB8LJ3&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07DRB8LJ3/?tag=johtani-22">
      Amazon | Maker hart Just Mixer ステレオ3入力音声ミキサー/電池とUSB電源可能 
      </a>
    </p>
  </div>
</div>
<h2 id="webカメラ">Webカメラ</h2>
<p>おっさんの顔がきれいに写っても仕方ないんですが、下からのアングルよりはいいかなぁということと、今後の勉強会で使えそうかな?ということで、購入してみました。
縦でも横でも使える便利なカメラです。また、あんまりないのですが、部屋の中などを写すときにカメラを持ち回せるの便利ですね。
ディスプレイの上に置いてしまうと高さがありすぎるということで、スピーカーの上に乗っけています。仮置きなのですが、このままになるんじゃないかな?
打ち合わせ中は画面共有してることが多いのでオンライン飲み会などで主に活躍してる気も。。。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B086R4VGVX/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B086R4VGVX&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B086R4VGVX/?tag=johtani-22">
      Amazon | ロジクール ウェブカメラ フルHD 1080P 60FPS StreamCam C980OW オフホワイト USB-C接続
      </a>
    </p>
  </div>
</div>
<h2 id="mac-miniのssd化">Mac miniのSSD化</h2>
<p>見えないところですが、Mac miniのHDDをSSDに差し替えました。
性能検証用のマシンとしてMac miniを利用し始めたのですが、
計測するたびに速度の差がありすぎるのでおかしいな?と。
よくよく考えてみると、このMac miniは<a href="https://support.apple.com/ja-jp/HT202574">Fusion Drive</a>だったんです。。。
開発などで使う分には大容量だし便利だったのですが、負荷計測に利用する場合は挙動をハンドリングできないので、弊害でしかないです。。。
ということで、頑張って差し替えました。想像以上の大手術でした。
「Mac mini HDD 交換」などでググると換装している方たちがいらっしゃいます。興味ある方はググってみてください。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">さて、復路 <a href="https://t.co/SwndXPoKqn">pic.twitter.com/SwndXPoKqn</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1287041775593385984?ref_src=twsrc%5Etfw">July 25, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B079SJ32TT/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B079SJ32TT&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B079SJ32TT/?tag=johtani-22">
      Amazon | シリコンパワー SSD 1TB SATA3 6Gb/s 2.5インチ 
      </a>
    </p>
  </div>
</div>
<h2 id="ラズパイ4">ラズパイ4</h2>
<p>Mac miniの負荷検証マシン化のため+Linuxを触りたかったというのもあり、ラズパイ4を追加しました。
Mac miniは主な使用用途が音楽再生だったのでラズパイ4で肩代わりできるだろう+その他の検証にもラズパイ4が使えそうだと。
現在はUbuntuをインストールして、画面共有で操作しています。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B082VVBKRP/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B082VVBKRP&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B082VVBKRP/?tag=johtani-22">
      Amazon | LABISTS Raspberry Pi 4 4B MicroSDHCカード64G
      </a>
    </p>
  </div>
</div>
<h2 id="まとめ">まとめ</h2>
<p>かなり快適になりました。キーボードもうるさくないですし、熱くもありません。マイクのポップガードは意味がなさそうなので外しました。</p>
<p>まだいくつか気になってるものもあるので、また更新するかもしれません。</p>
<ul>
<li>
<p>分離式キーボード - 胸を広げた状態でキーを打ったほうがいいのでは?という気がしています。ただ、最近はずっと日本語キーボードを使っているので選択肢が少ないんですよね。。。</p>
</li>
<li>
<p>サンダーボルト?USB-C?ドック - MacBook Proのポートがすべて埋まっているし、もう少しケーブルを減らせると幸せになれそうなきが。。。</p>
</li>
<li>
<p>モバイルディスプレイ - MacBook Proのディスプレイを使っているのですが、キーボードの上にかぶってるんですよね、結構。。。クラムシェルにしてiPadもしくはモバイルディスプレイに置き換えたほうがスッキリはしそうかなと。ただ、Touch IDが使えなくなったり、クラムシェル状態でMacBook Proの電源を入れられないというはなしなので、あまり意味がなさそうだなというのもあります。</p>
</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>meteredクレートの紹介</title>
      <link>https://blog.johtani.info/blog/2020/09/07/intro-metered-rs/</link>
      <pubDate>Mon, 07 Sep 2020 23:11:40 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/09/07/intro-metered-rs/</guid>
      <description>Rustで便利なクレートを見つけたので、紹介がてら、自分のメモのためにブログに残しておきます。 そもそもの問題 Rustで処理を書いていて、なん</description>
      <content:encoded><p>Rustで便利なクレートを見つけたので、紹介がてら、自分のメモのためにブログに残しておきます。</p>
<h2 id="そもそもの問題">そもそもの問題</h2>
<p>Rustで処理を書いていて、なんかちょっと遅いな?どこの処理で時間がかかってるんだろう?
ということがありませんか?ありますよね?</p>
<p>というのを調べるために、最初に思いつくのは自分で計測する方法です。
流石にそれはなぁ、と思ったのでググって出てきた方法を最初は使っていました。</p>
<ul>
<li><a href="https://qiita.com/pseudo_foxkeh/items/5d5226e3ffa27631e80d">Rustで実行時間計測</a></li>
</ul>
<p>3年前の記事ですが、とりあえず計測する分には問題なかったのでこちらの方が書いていたマクロを拝借していました。
が、ちょっと面倒なのが戻り値がある処理などのときに、このマクロを挟むのが結構めんどくさいなと。
また、処理の時間を測りたいのは基本的にはメソッドや関数単位であることが多いです。</p>
<p>で、さらにググっていて見つけたのが、<a href="https://crates.io/crates/metered">metered</a>でした。</p>
<h2 id="どんなもの">どんなもの?</h2>
<p>計測したい部分に<code>#[metric]</code>のようなアノテーションを追加することで計測対象としてくれます。
あとは、計測したものを保存するレジストリという場所を指定するだけです。
処理が終わったタイミングなどで、そのレジストリの内容を出力することで、次の情報を計測することができます。</p>
<ul>
<li>HitCount : 実行された回数</li>
<li>ErrorCount : エラーを返した数(<code>Result</code>を戻り値にしているメソッドが対象)</li>
<li>InFlight : 処理中の回数かな?</li>
<li>ResponseTime : レスポンスタイム(処理に何秒かかったか)</li>
<li>Throughput : スループット(1秒あたり何回呼ばれたか)</li>
</ul>
<p>とりあえず試してみたのは、ResponseTimeとThroughputです。他のメトリクスはまた後日(機会があれば)。
また、<code>metered::metric::Metric</code>トレイトというものが用意されているようで、これを実装した独自のメトリクスも扱うことができるようです。</p>
<h2 id="使い方">使い方</h2>
<p>使い方としては次のようになります。</p>
<ul>
<li>計測対象となるメソッドがある構造体に、メトリクスを保持するためのレジストリを用意</li>
<li>計測対象にしたい構造体のメソッドに<code>#[measure]</code>を追加(このとき、計測したいものも指定する。)</li>
</ul>
<p>あとは、実行したあとに構造体をダンプするとメトリクスが出力されます。</p>
<h3 id="レジストリの用意">レジストリの用意</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[derive(Default, Debug, Serialize)]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">NekoParser</span> {
    metric_reg: <span style="color:#a6e22e">NekoParserMetricRegistry</span>,
}
</code></pre></div><p><code>NekoParserMetricRegistry</code>という型はこのあとの<code>impl</code>のアノテーションで指定する名前になります。
実際にはこの型の構造体を自分で定義する必要はありません。
構造体の<code>derive</code>で<code>Default</code>を指定します。構造体のインスタンス化のときに<code>default()</code>メソッドを呼び出して初期化したいためです(おそらくレジストリの初期化をやってくれるのだと思う(<strong>要確認</strong>))。
レジストリの用意はこれだけです。</p>
<h3 id="レジストリの指定と計測対象の指定">レジストリの指定と計測対象の指定</h3>
<p>計測対象側です。少し長いですが、<code>NekoParser</code>のメソッドすべてを掲載しました。
まずは、1行目でレジストリの名前の指定(<code>registry = NekoParserMetricRegistry</code>)、レジストリのフィールド名(<code>registry_expr = self.metric_reg</code>)、レジストリの可視性(<code>visibility = pub(self)</code>)を定義します。
2行目では、<code>impl</code>ブロック全体で計測したいメトリクスを指定しています。今回は、スループットとレスポンスタイムを計測したかったので
<code>#measure([ResponseTime, Throughput])]</code>と2種類を指定しています。
メトリクスが2種類のため配列で指定していますが、1種類だけの場合は<code>[]</code>の記号は必要ありません。</p>
<p>あとは、計測したい各メソッドに<code>#[measure]</code>をつけるだけです。
なお、メソッドごとに<code>#[measure(ErrorCount)]</code>といったかたちで個別にメトリクスを指定することも可能です。
今回はお試しということもあり、すべて<code>#[measure]</code>だけになっています。
アノテーションを付けただけで、メソッド自体を変更はしていません。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[metered(registry = NekoParserMetricRegistry, registry_expr = self.metric_reg, visibility = pub(self))]</span>
<span style="color:#75715e">#[measure([ResponseTime, Throughput]</span>)]
<span style="color:#66d9ef">impl</span> NekoParser {
    <span style="color:#75715e">#[measure]</span>
    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">load_and_parse_neko</span>(<span style="color:#f92672">&amp;</span>self) {
        <span style="color:#66d9ef">let</span> file_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./data/chap04/neko.txt&#34;</span>;
        <span style="color:#66d9ef">let</span> file <span style="color:#f92672">=</span> File::open(file_path).unwrap();
        <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(file);
        <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> out <span style="color:#f92672">=</span> File::create(<span style="color:#e6db74">&#34;./data/chap04/neko.txt.lindera.json&#34;</span>).unwrap();
        buf.lines().filter_map(<span style="color:#f92672">|</span>item<span style="color:#f92672">|</span> item.ok()).for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> {
            <span style="color:#66d9ef">let</span> tokens <span style="color:#f92672">=</span> self.tokenize(line.as_str());
            self.output_tokens(<span style="color:#f92672">&amp;</span>tokens, <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> out);
        });
    }

    <span style="color:#75715e">#[measure]</span>
    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">output_tokens</span>(<span style="color:#f92672">&amp;</span>self, tokens: <span style="color:#66d9ef">&amp;</span>Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span>, buf: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> File) {
        writeln<span style="color:#f92672">!</span>(buf, <span style="color:#e6db74">&#34;{}&#34;</span>, serde_json::to_string(tokens).unwrap())
            .expect(<span style="color:#e6db74">&#34;Error during output json&#34;</span>);
    }

    <span style="color:#75715e">#[measure]</span>
    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">tokenize</span>(<span style="color:#f92672">&amp;</span>self, line: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span> {
        <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> tokenizer <span style="color:#f92672">=</span> lindera::tokenizer::Tokenizer::new(<span style="color:#e6db74">&#34;normal&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>);
        <span style="color:#66d9ef">let</span> lindera_tokens <span style="color:#f92672">=</span> tokenizer.tokenize(line);
        <span style="color:#66d9ef">let</span> tokens <span style="color:#f92672">=</span> lindera_tokens
            .iter()
            .map(<span style="color:#f92672">|</span>lindera_token<span style="color:#f92672">|</span> {
                <span style="color:#66d9ef">let</span> surface <span style="color:#f92672">=</span> lindera_token.text.to_string();
                <span style="color:#66d9ef">let</span> pos <span style="color:#f92672">=</span> lindera_token.detail[<span style="color:#ae81ff">0</span>].to_string();
                <span style="color:#66d9ef">let</span> pos1 <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> pos <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;UNK&#34;</span> {
                    lindera_token.detail[<span style="color:#ae81ff">1</span>].to_string()
                } <span style="color:#66d9ef">else</span> {
                    String::new()
                };
                <span style="color:#66d9ef">let</span> base <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> pos <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;UNK&#34;</span> {
                    lindera_token.detail[<span style="color:#ae81ff">6</span>].to_string()
                } <span style="color:#66d9ef">else</span> {
                    String::new()
                };
                Token {
                    surface,
                    base,
                    pos,
                    pos1,
                }
            })
            .collect();
        <span style="color:#66d9ef">return</span> tokens;
    }
}
</code></pre></div><h3 id="計測結果の出力">計測結果の出力</h3>
<p>最後は計測結果の出力です。
今回はテストメソッドで実行して結果を出力する処理を書きました。</p>
<p><code>let parser = NekoParser::default();</code>で構造体をインスタンス化します。
あとは、処理をそのまま実行します。</p>
<p>最後に出力結果をJSON形式の文字列にしてから出力しました。
<code>let serialized ... println!(&quot;{}&quot;, serialized);</code>という形です。
簡単ですね!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[cfg(test)]</span>
<span style="color:#66d9ef">mod</span> tests {
    <span style="color:#66d9ef">use</span> <span style="color:#66d9ef">crate</span>::chapter04::answer::NekoParser;
    <span style="color:#66d9ef">use</span> std::path::Path;
    <span style="color:#75715e">#[test]</span>
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">success_output_tokenlists</span>() {
        <span style="color:#66d9ef">let</span> parser <span style="color:#f92672">=</span> NekoParser::default();
        parser.load_and_parse_neko();
        <span style="color:#66d9ef">let</span> serialized <span style="color:#f92672">=</span> serde_json::to_string(<span style="color:#f92672">&amp;</span>parser).unwrap();
        println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, serialized);
        assert<span style="color:#f92672">!</span>(Path::new(<span style="color:#e6db74">&#34;./data/chap04/neko.txt.lindera.json&#34;</span>).exists());
    }
}
</code></pre></div><h3 id="出力結果">出力結果</h3>
<p>ここまで紹介したものの実行結果は次のような形でした。
レスポンスタイム、スループットともに、最小、最大、99パーセンタイルなどを出力してくれます。
出力はメソッド名ごとにくくられているのでとても便利です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#f92672">&#34;metric_reg&#34;</span>: {
        <span style="color:#f92672">&#34;load_and_parse_neko&#34;</span>: {
            <span style="color:#f92672">&#34;response_time&#34;</span>: {
                <span style="color:#f92672">&#34;samples&#34;</span>: <span style="color:#ae81ff">1</span>,
                <span style="color:#f92672">&#34;min&#34;</span>: <span style="color:#ae81ff">176128</span>,
                <span style="color:#f92672">&#34;max&#34;</span>: <span style="color:#ae81ff">177151</span>,
                <span style="color:#f92672">&#34;mean&#34;</span>: <span style="color:#ae81ff">176640.0</span>,
                <span style="color:#f92672">&#34;stdev&#34;</span>: <span style="color:#ae81ff">0.0</span>,
                <span style="color:#f92672">&#34;90%ile&#34;</span>: <span style="color:#ae81ff">177151</span>,
                <span style="color:#f92672">&#34;95%ile&#34;</span>: <span style="color:#ae81ff">177151</span>,
                <span style="color:#f92672">&#34;99%ile&#34;</span>: <span style="color:#ae81ff">177151</span>,
                <span style="color:#f92672">&#34;99.9%ile&#34;</span>: <span style="color:#ae81ff">177151</span>,
                <span style="color:#f92672">&#34;99.99%ile&#34;</span>: <span style="color:#ae81ff">177151</span>
            },
            <span style="color:#f92672">&#34;throughput&#34;</span>: {
                <span style="color:#f92672">&#34;samples&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;min&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;max&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;mean&#34;</span>: <span style="color:#ae81ff">0.0</span>,
                <span style="color:#f92672">&#34;stdev&#34;</span>: <span style="color:#ae81ff">0.0</span>,
                <span style="color:#f92672">&#34;90%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;95%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;99%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;99.9%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;99.99%ile&#34;</span>: <span style="color:#ae81ff">0</span>
            }
        },
        <span style="color:#f92672">&#34;output_tokens&#34;</span>: {
            <span style="color:#f92672">&#34;response_time&#34;</span>: {
                <span style="color:#f92672">&#34;samples&#34;</span>: <span style="color:#ae81ff">9964</span>,
                <span style="color:#f92672">&#34;min&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;max&#34;</span>: <span style="color:#ae81ff">143</span>,
                <span style="color:#f92672">&#34;mean&#34;</span>: <span style="color:#ae81ff">0.03592934564431955</span>,
                <span style="color:#f92672">&#34;stdev&#34;</span>: <span style="color:#ae81ff">1.5152489085107463</span>,
                <span style="color:#f92672">&#34;90%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;95%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;99%ile&#34;</span>: <span style="color:#ae81ff">0</span>,
                <span style="color:#f92672">&#34;99.9%ile&#34;</span>: <span style="color:#ae81ff">6</span>,
                <span style="color:#f92672">&#34;99.99%ile&#34;</span>: <span style="color:#ae81ff">143</span>
            },
            <span style="color:#f92672">&#34;throughput&#34;</span>: {
                <span style="color:#f92672">&#34;samples&#34;</span>: <span style="color:#ae81ff">174</span>,
                <span style="color:#f92672">&#34;min&#34;</span>: <span style="color:#ae81ff">39</span>,
                <span style="color:#f92672">&#34;max&#34;</span>: <span style="color:#ae81ff">71</span>,
                <span style="color:#f92672">&#34;mean&#34;</span>: <span style="color:#ae81ff">57.103448275862064</span>,
                <span style="color:#f92672">&#34;stdev&#34;</span>: <span style="color:#ae81ff">3.8417981983375835</span>,
                <span style="color:#f92672">&#34;90%ile&#34;</span>: <span style="color:#ae81ff">60</span>,
                <span style="color:#f92672">&#34;95%ile&#34;</span>: <span style="color:#ae81ff">61</span>,
                <span style="color:#f92672">&#34;99%ile&#34;</span>: <span style="color:#ae81ff">64</span>,
                <span style="color:#f92672">&#34;99.9%ile&#34;</span>: <span style="color:#ae81ff">71</span>,
                <span style="color:#f92672">&#34;99.99%ile&#34;</span>: <span style="color:#ae81ff">71</span>
            }
        },
        <span style="color:#f92672">&#34;tokenize&#34;</span>: {
            <span style="color:#f92672">&#34;response_time&#34;</span>: {
                <span style="color:#f92672">&#34;samples&#34;</span>: <span style="color:#ae81ff">9964</span>,
                <span style="color:#f92672">&#34;min&#34;</span>: <span style="color:#ae81ff">12</span>,
                <span style="color:#f92672">&#34;max&#34;</span>: <span style="color:#ae81ff">79</span>,
                <span style="color:#f92672">&#34;mean&#34;</span>: <span style="color:#ae81ff">16.897230028101177</span>,
                <span style="color:#f92672">&#34;stdev&#34;</span>: <span style="color:#ae81ff">2.331145559054724</span>,
                <span style="color:#f92672">&#34;90%ile&#34;</span>: <span style="color:#ae81ff">19</span>,
                <span style="color:#f92672">&#34;95%ile&#34;</span>: <span style="color:#ae81ff">20</span>,
                <span style="color:#f92672">&#34;99%ile&#34;</span>: <span style="color:#ae81ff">24</span>,
                <span style="color:#f92672">&#34;99.9%ile&#34;</span>: <span style="color:#ae81ff">46</span>,
                <span style="color:#f92672">&#34;99.99%ile&#34;</span>: <span style="color:#ae81ff">79</span>
            },
            <span style="color:#f92672">&#34;throughput&#34;</span>: {
                <span style="color:#f92672">&#34;samples&#34;</span>: <span style="color:#ae81ff">174</span>,
                <span style="color:#f92672">&#34;min&#34;</span>: <span style="color:#ae81ff">39</span>,
                <span style="color:#f92672">&#34;max&#34;</span>: <span style="color:#ae81ff">71</span>,
                <span style="color:#f92672">&#34;mean&#34;</span>: <span style="color:#ae81ff">57.103448275862064</span>,
                <span style="color:#f92672">&#34;stdev&#34;</span>: <span style="color:#ae81ff">3.819293076427424</span>,
                <span style="color:#f92672">&#34;90%ile&#34;</span>: <span style="color:#ae81ff">60</span>,
                <span style="color:#f92672">&#34;95%ile&#34;</span>: <span style="color:#ae81ff">61</span>,
                <span style="color:#f92672">&#34;99%ile&#34;</span>: <span style="color:#ae81ff">64</span>,
                <span style="color:#f92672">&#34;99.9%ile&#34;</span>: <span style="color:#ae81ff">71</span>,
                <span style="color:#f92672">&#34;99.99%ile&#34;</span>: <span style="color:#ae81ff">71</span>
            }
        }
    }
}
</code></pre></div><p>出力内容で気になったのは<code>output_tokens</code>と<code>tokenize</code>の<code>throughput</code>が全く同じ結果が出ていることです。
なにかバグを踏んでいる気がします。。。(時間を見つけてソースコード読んでみるか。)</p>
<h3 id="気をつけること">気をつけること</h3>
<p><code>metered</code>の導入時にわかりにくいコンパイルエラーが出たので備忘録として残しておきます。
(下からコンパイルエラーを読んでしまうくせがあったのが問題なのですが。。。)
エラーメッセージは次のとおりです。</p>
<pre><code>error[E0412]: cannot find type `ResponseTime` in this scope
  --&gt; src/chapter04/answer.rs:13:12
   |
13 | #[measure([ResponseTime, Throughput])]
   |            ^^^^^^^^^^^^ not found in this scope
   |
help: consider importing one of these items
   |
1  | use metered::ResponseTime;
   |
1  | use metered::common::ResponseTime;
   |

error[E0283]: type annotations needed
  --&gt; src/chapter04/answer.rs:12:1
   |
12 | #[metered(registry = NekoParserMetricRegistry, /* default = self.metrics */ registry_expr = self.metric_reg)]
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ cannot infer type
   |
   = note: cannot satisfy `_: std::default::Default`
   = note: required by `std::default::Default::default`
   = note: this error originates in a derive macro (in Nightly builds, run with -Z macro-backtrace for more info)

</code></pre><p>最初のメッセージにはわかりやすく出ていますが、<code>ResponseTime</code>を<code>use</code>せずに利用しようとした場合に以下のようなエラーが出ていました。
ターミナル画面が狭かったので<code>E0283</code>のエラーが目に入り、何を言ってるんだろう?という状態になってしまいました。
スクロールアップしたら、答えが載っているのに。。。</p>
<h3 id="コード全体">コード全体</h3>
<p>元ネタは<a href="https://blog.johtani.info/blog/2020/09/07/reboot-nlp100-ch04/">NLP100本ノックの第4章</a>です。
<a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter04/answer.rs">コードの全体</a>はGitHubのソースをご覧ください。</p>
<h2 id="まとめ">まとめ</h2>
<p><code>metered</code>を簡単ですが紹介してみました。
導入自体も簡単で、想像していたような使い方ができたので満足しています。
ほかにもプロファイラなどはあるのかもしれませんが、まずはこれを使っていこうかと思っています。</p>
<p>バグらしきものがありそうだったりするので、そのへんは今後調査してみようかと。
まだ、ちょっと試してみただけなので、metered自体のオーバーヘッドや、独自のメトリクスの実装方法、メソッドではなく関数に対して利用する場合にはどうするのか?などいくつか疑問点があるので、今後試してみてまたブログに残しておこうと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第4章終了(言語処理100本ノック2020)</title>
      <link>https://blog.johtani.info/blog/2020/09/07/reboot-nlp100-ch04/</link>
      <pubDate>Mon, 07 Sep 2020 11:17:07 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/09/07/reboot-nlp100-ch04/</guid>
      <description>Rustで言語処理100本ノックの第4章です。 前回はこちら。 今回は早めに続きをやりました。 「形態素解析」ですしね。 第4章の概要 吾輩は猫である</description>
      <content:encoded><p>Rustで言語処理100本ノックの第4章です。</p>
<p>前回は<a href="/blog/2020/09/04/reboot-nlp100-ch03">こちら</a>。</p>
<p>今回は早めに続きをやりました。
「形態素解析」ですしね。</p>
<h2 id="第4章の概要">第4章の概要</h2>
<p>吾輩は猫であるの文章が用意されていて、MaCabで形態素解析した結果をファイルに保存したところからが開始となります。</p>
<p>が、せっかくRustでやっているのでKuromojiのRust版である<a href="https://github.com/lindera-morphology/lindera">Lindera</a>を利用して形態素解析した結果を保存する部分から作成しました。
3章に引き続き、大きな流れのところの説明だけにしておきます。</p>
<h3 id="形態素解析">形態素解析</h3>
<p>もとの<code>neko.txt</code>が文章が1行ごとになっているので、そのまま1行ずつ読みならが、形態素解析していきます。読み込みの部分は3章とあまり変わらないので割愛します。
以下は、形態素解析の処理と形態素解析結果用の構造体です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[derive(Clone, Debug, Serialize, Deserialize)]</span>
<span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Token</span> {
    surface: String,
    base: String,
    pos: String,
    pos1: String,
}
</code></pre></div><p>まずは構造体です。今回の問題では、必要な情報は4種類だったのでそれを構造体にしました。</p>
<ul>
<li>表層形（surface）</li>
<li>基本形（base）</li>
<li>品詞（pos）</li>
<li>品詞細分類1（pos1）</li>
</ul>
<p><code>derive</code>でSerialize、Deserializeを付与しているのは、形態素解析の結果をJSON文字列として保存し、あとのそれぞれの課題で読み出すためにserde_jsonを利用するためです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">tokenize</span>(line: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> tokenizer <span style="color:#f92672">=</span> lindera::tokenizer::Tokenizer::new(<span style="color:#e6db74">&#34;normal&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>);
    <span style="color:#66d9ef">let</span> lindera_tokens <span style="color:#f92672">=</span> tokenizer.tokenize(line);
    <span style="color:#66d9ef">let</span> tokens <span style="color:#f92672">=</span> lindera_tokens
        .iter()
        .map(<span style="color:#f92672">|</span>lindera_token<span style="color:#f92672">|</span> {
            <span style="color:#66d9ef">let</span> surface <span style="color:#f92672">=</span> lindera_token.text.to_string();
            <span style="color:#66d9ef">let</span> pos <span style="color:#f92672">=</span> lindera_token.detail[<span style="color:#ae81ff">0</span>].to_string();
            <span style="color:#66d9ef">let</span> pos1 <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> pos <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;UNK&#34;</span> {
                lindera_token.detail[<span style="color:#ae81ff">1</span>].to_string()
            } <span style="color:#66d9ef">else</span> {
                String::new()
            };
            <span style="color:#66d9ef">let</span> base <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> pos <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;UNK&#34;</span> {
                lindera_token.detail[<span style="color:#ae81ff">6</span>].to_string()
            } <span style="color:#66d9ef">else</span> {
                String::new()
            };
            Token {
                surface,
                base,
                pos,
                pos1,
            }
        })
        .collect();
    <span style="color:#66d9ef">return</span> tokens;
}
</code></pre></div><p>次が形態素解析の処理です。
入力に1行分の文章を受け取り、出力として、さきほどの構造体をベクタに入れたもの<code>Vec&lt;Token&gt;</code>を返します。
内部ではLinderaの<code>Tokenizer</code>を<code>normal</code>モードでインスタンス化してその<code>tokenizer()</code>メソッドを叩いているだけです。
インスタンス化のときの第2引数は辞書のディレクトリですが、今回はデフォルト辞書(IPADIC)を利用しています。
戻り値はLinderaが用意したToken構造体なので、これを今回作成した<code>Token</code>構造体に詰め替えているだけです。</p>
<p>注意点としてLinderaはMeCabとは異なり、未知語(辞書に出てこない単語)の処理が実装されていないので、品詞が<code>&quot;UNK&quot;</code>の場合にはその他の情報が取得できないので、空文字を構造体に設定するようにしました。</p>
<h3 id="結果の保存">結果の保存</h3>
<p>形態素解析の結果はJSONで保存しました。
もとのファイルが1文が1行になっていたので、
1行を読み込み、形態素解析し、それをVecで取り出して、1行1配列JSONの形で保存するようにしてあります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">output_tokens</span>(tokens: <span style="color:#66d9ef">&amp;</span>Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span>, buf: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> File) {
    writeln<span style="color:#f92672">!</span>(buf, <span style="color:#e6db74">&#34;{}&#34;</span>, serde_json::to_string(tokens).unwrap()).expect(<span style="color:#e6db74">&#34;Error during output json&#34;</span>);
}
</code></pre></div><p><code>serde_json::to_string</code>に<code>Vec&lt;Token&gt;</code>を渡しているだけですが、構造体に<code>derive</code>をつけているのでよしなにやってくれます(便利ー)。</p>
<h3 id="jsonの読み込み処理">JSONの読み込み処理</h3>
<p>1行1JSONの読み込み処理です。
今回も3章のように読み込みながら、各文章ごとの形態素解析結果に対して処理を実施するために、処理を実行するための<code>trait</code>を<code>Command</code>として用意し、それぞれの問題で形態素解析結果に対して処理を書くような実装にしました。また、設問37で「猫」と共起している単語を処理するという課題があるので、文章に「猫」が入っているものだけを処理できるようにするための<code>Filter</code>も用意し、これをJSONの読み込み処理のイテレータの<code>filter</code>にわたすようにしています。
特にフィルタリングが必要ない場合ように、<code>NonFilter</code>を予め実装済みです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">trait</span> Command {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">execute</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, tokens: <span style="color:#66d9ef">&amp;</span>Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span>);
}

<span style="color:#66d9ef">trait</span> Filter {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">is_target</span>(<span style="color:#f92672">&amp;</span>self, line: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; <span style="color:#66d9ef">bool</span>;
}

<span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">NonFilter</span> {}

<span style="color:#66d9ef">impl</span> Filter <span style="color:#66d9ef">for</span> NonFilter {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">is_target</span>(<span style="color:#f92672">&amp;</span>self, line: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; <span style="color:#66d9ef">bool</span> {
        <span style="color:#66d9ef">true</span>
    }
}

<span style="color:#75715e">// ch04-30. 形態素解析結果の読み込み
</span><span style="color:#75715e"></span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">load_json</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Command</span><span style="color:#f92672">&gt;</span>(cmd: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> T) {
    load_json_with_filter(cmd, <span style="color:#f92672">&amp;</span>NonFilter {});
}

<span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">load_json_with_filter</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Command</span>, U: <span style="color:#a6e22e">Filter</span><span style="color:#f92672">&gt;</span>(cmd: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> T, filter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">U</span>) {
    <span style="color:#66d9ef">let</span> file <span style="color:#f92672">=</span> File::open(<span style="color:#e6db74">&#34;./data/chap04/neko.txt.lindera.json&#34;</span>).unwrap();
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(file);
    buf.lines()
        .filter_map(<span style="color:#f92672">|</span>item<span style="color:#f92672">|</span> item.ok())
        .filter(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> filter.is_target(line))
        .for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> {
            <span style="color:#66d9ef">let</span> tokens <span style="color:#f92672">=</span> parse_line_json(line.as_str());
            cmd.execute(<span style="color:#f92672">&amp;</span>tokens);
        });
}
</code></pre></div><p><code>output_tokens</code>では<code>serde_json::to_string</code>を呼び出してましたが、読み込みでは、<code>serde_json::from_str</code>を使うと構造体にしてくれます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">parse_line_json</span>(line: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">return</span> serde_json::from_str(line).unwrap();
}
</code></pre></div><p>あとは、設問ごとに<code>Command</code>トレイトを実装していく形です。
たとえば、32.の動詞の原形を出力する場合は次のようになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">ExtractVerbBase</span> {
    out: <span style="color:#a6e22e">File</span>,
}
<span style="color:#66d9ef">impl</span> Command <span style="color:#66d9ef">for</span> ExtractVerbBase {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">execute</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, tokens: <span style="color:#66d9ef">&amp;</span>Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span>) {
        tokens
            .iter()
            .filter(<span style="color:#f92672">|</span>token<span style="color:#f92672">|</span> token.pos <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;動詞&#34;</span>)
            .for_each(<span style="color:#f92672">|</span>token<span style="color:#f92672">|</span> {
                writeln<span style="color:#f92672">!</span>(self.out, <span style="color:#e6db74">&#34;{}&#34;</span>, token.base).expect(<span style="color:#e6db74">&#34;Error during writeln&#34;</span>);
                println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, token.base);
            })
    }
}
</code></pre></div><p>標準出力とは別にファイルにも出力できるように<code>ExtractVerbBase</code>に<code>out</code>でファイルを保持しています。</p>
<h3 id="34-名詞の連接">34. 名詞の連接</h3>
<p>「名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．」という課題だったのですが、最初は読み間違えて、名詞の連接の最も長いものだけを出力するようにしてました。。。
やっぱり、出力結果とかのサンプルは用意しといてほしいなぁ。。。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">impl</span> Command <span style="color:#66d9ef">for</span> ExtractMaxConjunctionNoun {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">execute</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, tokens: <span style="color:#66d9ef">&amp;</span>Vec<span style="color:#f92672">&lt;</span>Token<span style="color:#f92672">&gt;</span>) {
        <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> nouns <span style="color:#f92672">=</span> vec<span style="color:#f92672">!</span>[];
        <span style="color:#75715e">// TODO 参照保持でどうにかしたいけどなぁ。
</span><span style="color:#75715e"></span>        tokens.iter().map(<span style="color:#f92672">|</span>token<span style="color:#f92672">|</span> token.clone()).for_each(<span style="color:#f92672">|</span>token<span style="color:#f92672">|</span> {
            <span style="color:#66d9ef">if</span> token.pos <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;名詞&#34;</span> {
                nouns.push(token);
            } <span style="color:#66d9ef">else</span> {
                <span style="color:#66d9ef">if</span> nouns.len() <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span> {
                    self.buffer.push(nouns.clone());
                }
                nouns <span style="color:#f92672">=</span> vec<span style="color:#f92672">!</span>[]
            }
        });
    }
}
</code></pre></div><p>名詞の場合に、nounsにバッファリングしつつ、違う品詞が来たら出力するという処理になっています。
<code>clone</code>を呼び出していますが、これを参照を引き回す感じにできるといいのかもなぁ(結構めんどくさい)。</p>
<h3 id="36-頻度上位10語">36. 頻度上位10語</h3>
<p>頻度を数えるのにはBTreeMapを利用しています。
数えながら、Top10を保持する方法がいい気がしたのですが、いい入れ物を見つけられなかったので、数え上げたあとにBTreeMapのIteratorを回しながら、キーバリューのVecをまず生成します。
その生成したVecに値でソートし、その後Iteratorから最初の10件を取得して表示する方法にしました。</p>
<p>ソートして取り出すという処理がついでにかかっています。。。
<a href="https://doc.rust-lang.org/std/collections/struct.BinaryHeap.html">BinaryHeap</a>がなにか使えそうな気もしたのですが、いい方法が思いつきませんでした。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">print_top10</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self) {
        <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> key_values: Vec<span style="color:#f92672">&lt;</span>(<span style="color:#f92672">&amp;</span>String, <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">u32</span>)<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span>
            self.terms_count.iter().collect::<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>(<span style="color:#f92672">&amp;</span>String, <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">u32</span>)<span style="color:#f92672">&gt;&gt;</span>();
        key_values.sort_by(<span style="color:#f92672">|</span>x, y<span style="color:#f92672">|</span> y.<span style="color:#ae81ff">1.</span>cmp(<span style="color:#f92672">&amp;</span>x.<span style="color:#ae81ff">1</span>));
        key_values.iter().take(<span style="color:#ae81ff">10</span>).for_each(<span style="color:#f92672">|</span>(key, value)<span style="color:#f92672">|</span> {
            writeln<span style="color:#f92672">!</span>(<span style="color:#f92672">&amp;</span>self.out, <span style="color:#e6db74">&#34;{}, {}&#34;</span>, key, value).expect(<span style="color:#e6db74">&#34;Error during writeln&#34;</span>);
            println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}, {}&#34;</span>, key, value);
        });
    }
</code></pre></div><h2 id="まとめ">まとめ</h2>
<p>形態素解析結果をちゃんと眺めてはいないですが、処理としてはこんなところかなと。
グラフはめんどくさいのでスキップしてしまいました。。。
Kibana/Esに食わせて見てみるのもありかなぁ?</p>
<p>次は係り受け解析です。Rustで使えるライブラリとかあるかなぁ?</p>
</content:encoded>
    </item>
    
    <item>
      <title>第3章終了(言語処理100本ノック2020)</title>
      <link>https://blog.johtani.info/blog/2020/09/04/reboot-nlp100-ch03/</link>
      <pubDate>Fri, 04 Sep 2020 23:46:25 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/09/04/reboot-nlp100-ch03/</guid>
      <description>Rustで言語処理100本ノックの第3章です。 前回はこちら。 少し間が空きましたが、再開しました。 間が空いた理由は。。。「正規表現」ですかね。</description>
      <content:encoded><p>Rustで言語処理100本ノックの第3章です。</p>
<p>前回は<a href="/blog/2020/05/12/reboot-nlp100-ch02-12to19">こちら</a>。</p>
<p>少し間が空きましたが、再開しました。
間が空いた理由は。。。「正規表現」ですかね。。。
苦手なんです、正規表現。
なので、28はちょっとギブアップしてしまいました。</p>
<h2 id="第3章の概要">第3章の概要</h2>
<p>個別に説明はせずに大きな流れのところだけ。
それぞれの問題の解については<a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter03/answer.rs">リポジトリ</a>を御覧ください(興味ある人いるのかなぁ?)</p>
<p>第3章はNDJSON(new line delimited JSON)という、
1行に1JSONという形式のデータを格納したファイルがgzipで圧縮された状態で提供されます。
まずは、このJSONファイルからJSONを読み込むのが主な処理になります。</p>
<p>読み込んだデータに「イギリス」のWikipediaの記事が入っているので、そこから正規表現で必要なデータを抽出します。</p>
<p>最後の問題が少し特殊で、抜き出した情報の「国旗」のファイル名を元に、MediaWikiのREST APIを叩いて、結果を取得し、その一部の情報を抜き出すというものです。</p>
<h3 id="jsonの読み込み処理">JSONの読み込み処理</h3>
<p>gzipファイルを読み込んでから、1行ずつ抜き出してVecに入れる処理が次のようになります。
今回のgzipファイルは大した量が入っていないので、全部先に抜き出す処理としてまとめました。
もっと巨大なファイルの場合は個別のJSONに対する処理を
<code>buf.lines().map()</code>のmapのなかで実行する形にすると思います。
gzipのファイルを開くのに<code>flate2</code>というクレート(ライブラリ)を利用しました。便利なのは、<code>BufReader</code>に<code>lines()</code>というメソッドがあるところですかね。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">// https://docs.rs/flate2/1.0.14/flate2/read/struct.GzDecoder.html
</span><span style="color:#75715e"></span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">extract_ndjson_from_gzip</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">let</span> f <span style="color:#f92672">=</span> File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found?&#34;</span>);
    <span style="color:#66d9ef">let</span> gz <span style="color:#f92672">=</span> GzDecoder::new(f);
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(gz);
    <span style="color:#66d9ef">let</span> lines: Vec<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> buf.lines().map(<span style="color:#f92672">|</span>l<span style="color:#f92672">|</span> l.unwrap()).collect();
    <span style="color:#66d9ef">return</span> lines;
}
</code></pre></div><p>こちらは、上記のメソッドで抜き出したVecを元に、記事の情報を抜き出す処理をしています。
JSONをパースして構造体<code>Article</code>にデシリアライズするために、<code>serde</code>というライブラリを使用しています。
<code>serde</code>自体は様々なデータ形式(JSON、YAMLなど)をパースするためのフレームワークです。今回はJSONなので、<code>serde_json</code>の実装を利用しています。
また、JSON文字列から構造体にデシリアライズするのを簡単にできるように構造体に<code>#[derive(Deserialize)]</code>をつけています。
あとは、<code>let article: Article = serde_json::from_str(json.as_str())</code>という処理を実行すればserde_jsonがJSONをパースして構造体に変換してくれます。形式がわかっているJSONの扱いはこれが楽ですね。変数に型を明記してあるので、型の推論もしてくれてるようです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[derive(Deserialize)]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Article</span> {
    title: String,
    text: String,
}

<span style="color:#75715e">// ch03-20. JSONデータの読み込み
</span><span style="color:#75715e">// https://serde.rs/
</span><span style="color:#75715e"></span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">load_json</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, target_title: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>Article<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> results <span style="color:#f92672">=</span> vec<span style="color:#f92672">!</span>[];
    <span style="color:#66d9ef">let</span> ndjson <span style="color:#f92672">=</span> extract_ndjson_from_gzip(input_file_name);
    <span style="color:#66d9ef">for</span> json <span style="color:#66d9ef">in</span> ndjson {
        <span style="color:#66d9ef">let</span> article: <span style="color:#a6e22e">Article</span> <span style="color:#f92672">=</span> serde_json::from_str(json.as_str()).expect(<span style="color:#e6db74">&#34;json parse error&#34;</span>);
        <span style="color:#66d9ef">if</span> article.title <span style="color:#f92672">==</span> target_title {
            results.push(article);
        }
    }
    <span style="color:#66d9ef">return</span> results;
}
</code></pre></div><p>後続の処理ではパースした<code>Article</code>から記事情報を取得して色々と処理をしています。</p>
<h3 id="正規表現">正規表現</h3>
<p>正規表現用のクレート<code>regex</code>がRustに用意されています。<code>Regex::new(正規表現)</code>で、正規表現をコンパイルし、あとは、この構造体のメソッドを利用して文字列を処理していきます。
問題では、マッチするかどうか、マッチした一部の文字列を抜き出す、不要なタグを削除するといった処理を正規表現で行いました(Rust書くよりも正規表現の書き方とかを調べるのに大半の時間をもっていかれてます。。。)。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">extract_category_lines</span>(article: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Article</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">let</span> re <span style="color:#f92672">=</span> Regex::new(<span style="color:#e6db74">r&#34;\[\[Category:(.*)\]\]&#34;).expect(&#34;syntax error in regex&#34;</span>);
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> lines <span style="color:#f92672">=</span> vec<span style="color:#f92672">!</span>[];
    article.lines_from_text().iter().for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> {
        <span style="color:#66d9ef">if</span> re.is_match(line) {
            lines.push(line.to_string());
        }
    });
    <span style="color:#66d9ef">return</span> lines;
}
</code></pre></div><h3 id="mediawiki-apiリクエスト処理">MediaWiki APIリクエスト処理</h3>
<p>最後の問題で国旗のファイル名を元に<a href="https://www.mediawiki.org/wiki/API:Imageinfo">MediaWiki API</a>を叩いて、URLの文字列を取得しましょうという問題がありました。
ファイル名をREST APIの引数に渡してHTTP経由でリクエストを送信し、返ってくるJSONレスポンスからURLを抜き出すという処理です。</p>
<p>HTTPのリクエストの送受信に<code>reqwest</code>というクレートを利用しました。
ちょっと長いけど、APIコールしている箇所はこんな形です。</p>
<p>この関数には<code>async</code>とついています。非同期処理の関数です。内部で2回ほど(リクエスト送信の結果待ちとレスポンスのパース待ち)<code>.await</code>があります。</p>
<p><code>client</code>に<code>.get(URL)</code>や<code>query(&amp;[])</code>といったメソッドが用意されているので、URLやクエリパラメータを用意して<code>send()</code>でリクエスト送信します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">
async <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">call_api</span>(file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Result<span style="color:#f92672">&lt;</span>String, String<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">let</span> client <span style="color:#f92672">=</span> reqwest::Client::new();
    <span style="color:#66d9ef">let</span> file_name2 <span style="color:#f92672">=</span> format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;File:{}&#34;</span>, file_name);
    <span style="color:#75715e">//let mut file_name2 = file_name.to_string().replace(&#34; &#34;, &#34;_&#34;);
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> query <span style="color:#f92672">=</span> [
        (<span style="color:#e6db74">&#34;action&#34;</span>, <span style="color:#e6db74">&#34;query&#34;</span>),
        (<span style="color:#e6db74">&#34;format&#34;</span>, <span style="color:#e6db74">&#34;json&#34;</span>),
        (<span style="color:#e6db74">&#34;prop&#34;</span>, <span style="color:#e6db74">&#34;imageinfo&#34;</span>),
        (<span style="color:#e6db74">&#34;iiprop&#34;</span>, <span style="color:#e6db74">&#34;url&#34;</span>),
        (<span style="color:#e6db74">&#34;titles&#34;</span>, file_name2.as_str()),
    ];
    <span style="color:#66d9ef">let</span> result <span style="color:#f92672">=</span> client
        .get(<span style="color:#e6db74">&#34;https://en.wikipedia.org/w/api.php&#34;</span>)
        .query(<span style="color:#f92672">&amp;</span>query)
        .send()
        .await;

    <span style="color:#66d9ef">match</span> result {
        Ok(response) <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">match</span> response.status() {
            StatusCode::OK <span style="color:#f92672">=&gt;</span> {
                <span style="color:#66d9ef">let</span> body <span style="color:#f92672">=</span> response.json::<span style="color:#f92672">&lt;</span>MediaWikiResponse<span style="color:#f92672">&gt;</span>().await;
                <span style="color:#66d9ef">match</span> body {
                    Ok(obj) <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">match</span> obj.get_url() {
                        Some(url) <span style="color:#f92672">=&gt;</span> Ok(url),
                        None <span style="color:#f92672">=&gt;</span> Err(String::from(<span style="color:#e6db74">&#34;Cannot get url...&#34;</span>)),
                    },
                    Err(error) <span style="color:#f92672">=&gt;</span> Err(error.to_string()),
                }
            }
            _ <span style="color:#f92672">=&gt;</span> Err(String::from(format<span style="color:#f92672">!</span>(
                <span style="color:#e6db74">&#34;Status code is {}.&#34;</span>,
                response.status()
            ))),
        },
        Err(error) <span style="color:#f92672">=&gt;</span> {
            <span style="color:#66d9ef">let</span> error_msg <span style="color:#f92672">=</span> format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;Error occurred... {:?}&#34;</span>, error);
            println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, error_msg.as_str());
            Err(error_msg)
        }
    }
}
</code></pre></div><p>あとは、呼び出し元で非同期の処理を実行するために、<code>tokio</code>というクレートを利用しています。
<code>block_on()</code>で<code>call_api()</code>の実行をして、結果が返ってくるのを待ち受けています。結果が返ってきて、問題なければ、<code>call_api</code>の戻り値<code>Result&lt;String, String&gt;</code>の左側のStringの値が取り出され、<code>get_image_url</code>の戻り値となります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">
<span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">get_image_url</span>(file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; String {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> _rt <span style="color:#f92672">=</span> tokio::runtime::Runtime::new().expect(<span style="color:#e6db74">&#34;Fail initializing runtime&#34;</span>);
    <span style="color:#66d9ef">let</span> task <span style="color:#f92672">=</span> call_api(file_name);
    _rt.block_on(task).expect(<span style="color:#e6db74">&#34;Something wrong...&#34;</span>)
}
</code></pre></div><p>一応、非同期に関して説明してみましたが、合っているのかどうか。。。
クレートの関係などはまだちょっと自身がないです。。。
あとは、エラーの処理の仕方とかももうちょっと勉強したいかな。</p>
<h2 id="まとめ">まとめ</h2>
<p>一応、3章を終わらせました。だいぶ強引かつ1つスキップしましたが。。。
次は<a href="https://nlp100.github.io/ja/ch04.html">第4章の形態素解析</a>です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>TerraformでAzure Cognitive Searchのクラスターを起動</title>
      <link>https://blog.johtani.info/blog/2020/08/18/azure-search-with-terraform/</link>
      <pubDate>Tue, 18 Aug 2020 18:26:47 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/08/18/azure-search-with-terraform/</guid>
      <description>負荷を計測するために、数回、Azure Cognitive Searchのクラスターを起動したり、停止したりしてました。 これは、Terraformでやると楽でき</description>
      <content:encoded><p>負荷を計測するために、数回、Azure Cognitive Searchのクラスターを起動したり、停止したりしてました。
これは、<a href="https://www.terraform.io/">Terraform</a>でやると楽できるのでは?と思ったので、やってみました。
1パーティションのクラスターなので、全然大したことはないのですが、メモを残しておくためにブログに書いておきます。</p>
<p>基本的には<a href="https://www.terraform.io/docs/providers/azurerm/r/search_service.html">Terraformの公式ドキュメント</a>にあったものを自分用に変数を抽出しただけです。</p>
<h2 id="ファイルたち">ファイルたち</h2>
<p>単にクラスターを起動するためだけなので、2種類のファイルだけ作成しました(1個でもいいかも)。</p>
<ul>
<li><code>variables.tf</code> - 変数用のファイル。いくつかの設定を変数として定義しました。</li>
<li><code>terraform.tf</code> - Terraform本体のファイル。</li>
</ul>
<h3 id="variablestf"><code>variables.tf</code></h3>
<p>まずは<code>variables.tf</code>です(ファイル内の並びは異なりますが。。。)。terraform.tfで利用する5つの変数です。</p>
<ul>
<li>resource_group - リソースグループ名。既存とは異なるリソースグループ名にしました(変に壊してもいいので)。</li>
<li>machine_type - 価格レベル(SKU)。<a href="https://www.terraform.io/docs/providers/azurerm/r/search_service.html#sku">公式ドキュメントに設定できる値の一覧</a>があります。今回はWikipediaのデータを登録していたのでそれが入るサイズにしています。</li>
<li>region - 場所(リージョン)。リージョンの一覧はどこにあるんだろう?<a href="https://azure.microsoft.com/en-us/global-infrastructure/geographies/#geographies">これを参考</a>にしましたが。</li>
<li>partition_size - パーティションのサイズ(=Elasticsearchでのシャードかな?)です。今回は1つでの性能を計測したかったので1にしてあります。</li>
<li>search_cluster_name - Azure Cognitive Searchのサービス名。<code>search.windows.net</code>名前空間で一意である必要があったので、他の人が使わなそうな名前をつけています。</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">variable <span style="color:#e6db74">&#34;machine_type&#34;</span> {
  default = <span style="color:#e6db74">&#34;standard&#34;</span>
}

variable <span style="color:#e6db74">&#34;region&#34;</span> {
  default = <span style="color:#e6db74">&#34;East Asia&#34;</span>
}

variable <span style="color:#e6db74">&#34;resource_group&#34;</span> {
  default = <span style="color:#e6db74">&#34;johtani-wiki-test&#34;</span>
}

variable <span style="color:#e6db74">&#34;search_cluster_name&#34;</span> {
  default = <span style="color:#e6db74">&#34;johtani-wikipedia&#34;</span>
}

variable <span style="color:#e6db74">&#34;partition_size&#34;</span> {
  default = <span style="color:#ae81ff">1</span>
}
</code></pre></div><h3 id="terraformtf"><code>terraform.tf</code></h3>
<p><code>terraform.tf</code>は以下の通り。<a href="https://www.terraform.io/docs/providers/azurerm/r/search_service.html">Terraformの公式ドキュメントにある例</a>に<code>provider</code>を追加しただけのものになります。</p>
<ul>
<li>provider - プロバイダーの設定。Azureを利用するという宣言です。<code>features {}</code>がないとエラーになります。空でも必ず指定が必要です。認証周りについては、<a href="https://www.terraform.io/docs/providers/azurerm/guides/azure_cli.html"><code>azure-cli</code>経由で認証する方式</a>を採用しました。<code>azure-cli</code>はHomebrewでインストールしています。</li>
<li>azurerm_resource_group - リソースグループの設定。<code>variables.tf</code>の<code>resource_group</code>と<code>region</code>を利用しています。必須項目はこの2種類だけです。</li>
<li>azurerm_search_service - Azure Cognitive Searchの設定。<code>partition_count</code>以外は必須項目です。<code>variables.tf</code>と<code>azurerm_resource_group</code>の設定を利用しています。今回は利用していませんが、レプリカ数なども指定できるようになっています。</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># Azureのproviderを指定。</span>
provider <span style="color:#e6db74">&#34;azurerm&#34;</span> {
  features {}
}

<span style="color:#75715e"># リソースグループの設定</span>
resource <span style="color:#e6db74">&#34;azurerm_resource_group&#34;</span> <span style="color:#e6db74">&#34;wiki-test&#34;</span> {
  name     = var.resource_group
  location = var.region
}

<span style="color:#75715e"># Azure Cognitive Searchの設定</span>
resource <span style="color:#e6db74">&#34;azurerm_search_service&#34;</span> <span style="color:#e6db74">&#34;search_service&#34;</span> {
  name                = var.search_cluster_name
  resource_group_name = azurerm_resource_group.wiki-test.name
  location            = azurerm_resource_group.wiki-test.location
  sku                 = var.machine_type
  partition_count     = var.partition_size
}
</code></pre></div><p>以上がファイルです。ほぼ公式ドキュメントのサンプル通りですねw</p>
<h2 id="デプロイとか">デプロイとか</h2>
<p>ファイルの準備ができたら実際にデプロイします。
Azureの環境への認証にはAzure CLIを利用して、事前にログインした状態にします。
実際にデプロイするまでの手順は次のようになります。</p>
<ol>
<li><code>az login</code> - Azureの認証。ブラウザが起動してログイン画面が表示されます。無事認証がOKなら、<code>az</code>コマンドでAzure Cloudの情報が取得できます。</li>
<li><code>terraform init</code> - 初回だけです。Terraformのワーキングディレクトリの初期化処理を実行します。</li>
<li><code>terraform plan</code> - Terraform &lt; 0.12の場合は実行。最新版だともういらないみたいだ。</li>
<li><code>terraform apply</code> - 実際にAzure上にクラスターを起動します。Terraformが隠蔽してくれているので、実際にどんなことをやっているかはわかってないですが。</li>
</ol>
<p>以上で、Azure Cognitive Searchのクラスターが起動します。
すごく簡単です。Webコンソールでチェックすれば、起動していることも確認できます。</p>
<p>この状態では、クラスターが起動しただけなので、あとは必要に応じてデータをロードしたり、アプリから検索したりと行ったことが可能になります。
そのへんの話はまた機会があれば。</p>
<h2 id="destroy">Destroy</h2>
<p>今回は負荷テスト用にクラスターを起動していましたので、必要なくなれば、クラスターを削除します。Terraformを導入したもう一つの理由がこの簡略化です。
<code>terraform destroy</code>を実行するだけで、Azure Cognitive Searchのクラスターおよび、リソースグループが削除されます。</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、ほぼ公式ドキュメントのままですが、TerraformでAzure Cognitive Searchのクラスターを起動する方法の紹介でした。
ブログを書いていて、1点気になったのは、ロケーション(リージョン)の一覧はどこにあるんだろう?という点です。azコマンドとかで出てくるのかなぁ?</p>
</content:encoded>
    </item>
    
    <item>
      <title>検索システムを構成するパーツ(検索システムに関する妄想その2)</title>
      <link>https://blog.johtani.info/blog/2020/07/28/improve-search-no2/</link>
      <pubDate>Tue, 28 Jul 2020 12:34:42 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/07/28/improve-search-no2/</guid>
      <description>先日は「システムの特徴と検索機能について」という感じでふんわり書きました。 まぁ、頭の中でぼんやり考えてることを文章にしてみた感じです。 他にも</description>
      <content:encoded><p>先日は「<a href="/blog/2020/07/27/improve-search-no1/">システムの特徴と検索機能について</a>」という感じでふんわり書きました。
まぁ、頭の中でぼんやり考えてることを文章にしてみた感じです。
他にもぼんやりしてるものはいくつかあるので今日も書いてみることに。
検索システム?みたいなツイートも見かけたので、検索システムってこんなイメージですというブログを書いてみました。</p>
<h2 id="検索システム機能を構成するパーツ">検索システム(機能)を構成するパーツ</h2>
<p>今回はシステムに組み込まれる検索機能を構成するパーツについて書き出してみようかなと思います。
パーツといってもユーザー、UI、コンテンツなども入れています。</p>
<ul>
<li>ユーザー</li>
<li>検索UI
<ul>
<li>検索窓
<ul>
<li>オートコンプリート</li>
</ul>
</li>
<li>検索結果画面
<ul>
<li>ファセット</li>
<li>ソート</li>
<li>ハイライト</li>
</ul>
</li>
<li>詳細画面
<ul>
<li>レコメンド</li>
</ul>
</li>
</ul>
</li>
<li>検索エンジン</li>
<li>コンテンツ</li>
<li>検索ログ
<ul>
<li>クリックログとかも</li>
</ul>
</li>
<li>サービス提供者</li>
</ul>
<p>ざっくり書くとこんな感じです。
システム構成だったり、機能だったり、アクターだったりといろいろなものが混ざってしまっていますが、登場するものはこんなものです。</p>
<p>ざっくりした繋がりの図はこんな感じです。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200731/search_system_overview.jpg" />
    </div>
    <a href="/images/entries/20200731/search_system_overview.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>それぞれの役割について見ていきましょう。</p>
<h3 id="ユーザー">ユーザー</h3>
<p>サイト、システムのユーザーです。
検索UIを経由して望んだコンテンツを探します。
探す目的は、サイトによって異なります。
「何かを購入(ECサイトやオークション)する」だったり、「情報(レシピや社内文書)を見つける」だったりします。
図では「キーワード」と記載しましたが、最近では自然文(文章)を受け付ける検索もあります。</p>
<h3 id="検索ui">検索UI</h3>
<p>サイト、システムが提供するUIです。ユーザーはこのUIにキーワード(質問)を入力し、検索結果を取得します。
UIにはいくつものパーツがさらに存在します。簡単に例を上げると以下のようなものです。</p>
<ul>
<li>検索窓 - キーワードを入れるための入力ボックスです。
<ul>
<li>オートコンプリート(自動補完) - サイトによっては、検索窓に何かを入力すると、キーワードを保管したりサジェストしたりしてくれます。</li>
</ul>
</li>
<li>検索結果画面 - 質問にマッチしたコンテンツの一覧を表示する画面です。一覧以外にもいくつか情報が表示されます。
<ul>
<li>検索結果一覧 - コンテンツの一覧です。何かしらの基準(日付順や人気順など)によってソートされたものが表示されます。</li>
<li>ファセット - 検索結果が持っている属性(価格帯、カテゴリ、メーカー名など)の一覧で、絞り込み検索のヒントです。</li>
<li>ハイライト - 入力したキーワードがどこにマッチしたかがわかるように、強調表示されたスニペット(情報の一部)が出ます。</li>
</ul>
</li>
</ul>
<p>検索APIとUIに分かれている場合が多いでしょうか?
処理の流れとしては、検索窓に入力されたキーワードを検索エンジンに問い合わせができるクエリに書き換えてからリクエストを投げます。
あとは、検索エンジンからのレスポンスにある検索結果を表示できる形に変換して表示するのが役割です。
また、検索ログの出力もこの部分で担当することが多いです(もしくは、検索エンジン自体がログ出力の機能を持っている場合もあります)。</p>
<h3 id="検索エンジン">検索エンジン</h3>
<p>検索に特化したデータ構造を内部に持っているサーバーもしくはサービスです。
<a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a>や<a href="https://lucene.apache.org/solr/">Apache Solr</a>、<a href="https://azure.microsoft.com/ja-jp/services/search/">Azure Cognitive Search</a>などは転置インデックスと呼ばれるデータ構造になっています。
検索エンジンの検索処理に対しての主な役割は次の2つです。</p>
<ul>
<li>クエリにマッチするコンテンツの集合を決定する</li>
<li>マッチしたコンテンツを特定の条件で並び替える(ランキング)</li>
</ul>
<p>クエリを受け取り、検索結果のリストを返すのが処理の大きな流れです。
その他に、ファセット、ハイライトといった付加的な処理を実行することがあります。</p>
<p>また、データ登録(インデキシング/インデクシング)の処理もあります。</p>
<ul>
<li>登録するコンテンツを検索に特化したデータ構造にして格納する</li>
</ul>
<p>転置インデックスの場合は、入力されたデータ(文章)から単語列を作り出して、単語からコンテンツのIDが判別できる形にする処理になります。</p>
<p><a href="https://www.algolia.com/">Algolia</a>や<a href="https://www.elastic.co/jp/app-search/">Elastic App Search</a>のようなSaaSであったり、RDBの機能を利用するといった選択肢もあります。</p>
<h3 id="データソースコンテンツ">データソース・コンテンツ</h3>
<p>実際に提供したいコンテンツになります。
コンテンツが保存されている場所は、サイトによって異なります。</p>
<ul>
<li>Webの検索サイト - インターネット上のホームページ</li>
<li>ECサイト - データベースに格納されているアイテムのデータ</li>
<li>社内文書検索 - ファイルサーバーやWikiなどのファイル、文書</li>
</ul>
<p>といった感じです。
実際には、データソースからコンテンツを検索エンジンに登録する場合は、いくつかの処理(いわゆる前処理)が必要になります。
社内文書検索やWebの検索サイトの場合は、データを収集するためのクローラーが必要ですし、
収集したデータから、検索エンジンに登録するデータを加工したり(HTMLタグを除去したり、メタデータ(URL、収集日、タイトル)を付与したり)もします(ETL処理とか言われる)。</p>
<h3 id="検索ログ">検索ログ</h3>
<p>検索を提供するだけであれば、必要ありません。
が、実際に検索がどのような使われ方をしているか?を知るために必要な機能になります。
この検索ログがユーザーのニーズを読み解くための情報になります。
検索ログには次のような情報が入ります。</p>
<ul>
<li>検索窓に入力された文字列</li>
<li>入力された文字列でヒットした件数</li>
</ul>
<p>検索結果を出したタイミングでのログです。他にも実際にヒットしたコンテンツのIDなどをログに残したり、他のユーザーと区別をつけるために、ユーザーのセッションごとにIDを発行してログに残したりもします。</p>
<p>また、検索に満足してもらえているかを見るために、実際に検索結果のどのコンテンツに興味をもったのか?という情報も検索ログとして残すことがあります。クリックログなどとも呼ばれます。検索結果のどのドキュメントが実際にクリックされたか(詳細画面に遷移したか)という情報です。1回の検索結果に対してクリックされるごとにログが残ります。
もちろん、結果に満足しない場合は、クリックされずに、キーワードを変えたり、絞り込み条件がクリックされたりします。</p>
<p>ECサイトなどの場合は更に、実際に購入されたかどうかといった情報もユーザーのニーズを読み解くための情報となります。
詳細画面へのクリックログや購入ログについては、検索以外からの流れ(広告やDMだったり、レコメンドだったり)なども考えられます。
これらのログを元に、検索を改善していくことになります。</p>
<h3 id="サービス提供者">サービス提供者</h3>
<p>サイト・システムの提供者です。
コンテンツの準備、検索UIにはどんな物が必要なのか、サイト・システムにとって良い検索とはなにか?、検索ログからユーザーのニーズを分析して何を改善していくのか?といったことを考えます。
例としては次のようなことです。</p>
<ul>
<li>検索結果に表示するコンテンツとはなにか?</li>
<li>コンテンツの持っている項目・属性の何を検索対象とするか?</li>
<li>検索結果の並び順(ランキング)がどんなものがよいのか?</li>
<li>検索UIにはどんなものを表示するのか?</li>
</ul>
<h2 id="検索機能を作るときの流れ">検索機能を作るときの流れ</h2>
<p>検索機能を構成するパーツにどんなものがあるかを紹介しました。
実際にシステムに検索機能を追加する場合は、最低限、次のものが必要になります。</p>
<ul>
<li>検索UI</li>
<li>検索エンジン(RDB?SaaS?ミドルウェア?)</li>
<li>データソース・コンテンツ</li>
</ul>
<p>とりあえずこれらがあれば、検索機能を作ることはできるかと。
ただ、作っただけでは、いいか悪いかの判断がつかないので、どういった使われ方をしているかを知るために検索ログをとったりして、
改善をしていく必要が出てきます。</p>
<h2 id="まとめ">まとめ</h2>
<p>わかってるよそんなことはと言われそうな感じになったかもしれないですが、
検索の機能を構成するパーツについて紹介してみました。
細かなはなしは色々とありますが、大まかにはこのような役割のパーツがあります。</p>
<p>実際にはこれらのパーツを用意すればよいわけではなく、それぞれで検索を良くしていくためにどんなことを考えていくのか?などが出てきます。そのあたりの話はまた、別のブログで書いていく感じでしょうか。こんな感じで書いてくと、終わらない気がしてきたけど。。。
次はどんなはなしを書くかなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>システムの特徴と検索機能について(検索システムに関する妄想その1)</title>
      <link>https://blog.johtani.info/blog/2020/07/27/improve-search-no1/</link>
      <pubDate>Mon, 27 Jul 2020 18:28:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/07/27/improve-search-no1/</guid>
      <description>今年の頭からシステムの検索周りを手伝う仕事をフリーランスとしてやっています。 検索の仕組みを知れば知るほど面白くなってきたからという理由になる</description>
      <content:encoded><p>今年の頭からシステムの検索周りを手伝う仕事をフリーランスとしてやっています。
検索の仕組みを知れば知るほど面白くなってきたからという理由になるのかな?
LuceneやSolr、Elasticsearchなどを長く触っているというのもあるかと思います。</p>
<p>ということで、検索についていつも考えています。
頭の中でまとまっていない状況ですが、システムにおける検索機能についていくつか頭の中にあることを書き出して、
いろんな方にダメ出しやコメントをもらいたいなと思ったので、色々と書いてみようかと。
思いつきのままに書いているので、はなしがあちこち飛ぶ可能性もありますが、あしからず。</p>
<h2 id="検索って難しい">検索って難しい</h2>
<p>「「検索」とは、データの集合から目的のデータを探し出すこと」<a href="https://ja.wikipedia.org/wiki/%E6%A4%9C%E7%B4%A2">By Wikipedia</a></p>
<p>一言で「検索」といっても、使う人、ユースケースによっていろいろな「検索」があります。
例えば、新しいスマホを買ったときに、スクリーンロックの時間を設定する機能を「検索」したりします。
また、PCで仕事をしているときに、ファイルの中身をある文字列で「検索」したりもします。
TSUTAYAに行って、欲しかった本がおいてあるかどうか店内の端末で「検索」もします。
Rustを書いていて、こんなことをやるライブラリありそうだよな?と思ってGoogleでウェブの検索をしたりもします。</p>
<p>私が特にそうだと思いますが、なにかあったらまず検索をするという生活をしています。
ただ、このとき、「検索」といっても望んでいる挙動が違ったりするものです。
以下は自分が「検索」しているときに想定していることになります。</p>
<ul>
<li>ファイル内の検索をしているときはgrep的な検索を想定していることが多い。</li>
<li>書籍の検索をしているときは、特定の項目(著者など)に対してgrep的な検索を想定しているが、名前の読みなどでも検索されることを想定している(漢字覚えてなかったりする。。。)。</li>
<li>Rustを書いているときに機能をGoogleで検索するときは、いい感じに検索してくれることを望んでいる(入力するキーワードが曖昧なことが多々ある。例えば、そのものズバリの名前をしらないときとか)</li>
</ul>
<p>あくまでも私が想像している挙動です。他の人とは違う可能性もあります。
なので、「検索」といってもさまざまな要素があるし、想定しているシーンも異なるので「難しい」なと思っています。
また、そんな「検索」ですが、世の中的にはあって当たり前だと思われていたり、お金や時間がかかるものと思われてなかったりもします。ま、けどそういったことも含めてやればやるほど面白いなと感じている今日このごろです。</p>
<p>前置きはこのくらいにして、今回はシステムの特徴と検索機能について感じていることを書いてみようと思います。</p>
<h2 id="システムの特徴と検索機能">システムの特徴と検索機能</h2>
<p>先ほども書きましたが、検索は今やシステムに欠かせない機能となっています。
が、あればいいというものでもないのではないかなと。とりあえず検索できるべきだということで
検索機能を追加しても使いにくいものや、想定している動きをしない場合は使われないものになってしまいます。</p>
<p>システムでの検索機能は特に、「情報検索」(<a href="https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E6%A4%9C%E7%B4%A2">Wikipediaはこちら</a>)と呼ばれたりもします。Wikipediaによるとこんな説明です。</p>
<blockquote>
<p>情報検索（じょうほうけんさく）とは、コンピュータを用いて大量のデータ群から目的に合致したものを取り出すこと。検索の対象となるデータには文書や画像、音声、映像、その他さまざまなメディアやその組み合わせとして記録されたデータなどが含まれる。</p>
</blockquote>
<p>「目的」と呼ばれるものは「ユーザーのニーズ」と呼ばれたりもします。
「合致したもの」というのがシステムが返す「検索結果」になります。検索結果は大体の場合、何かしらの順序でソートされていることが多いです。
ざっくり話をすると、「ユーザーのニーズ」を元に「(何かしらの順序でソートされている)検索結果」を返すという処理です。</p>
<p>ただ、この「ユーザーのニーズ」や「何かしらの順序でソートされている検索結果」はシステムの特性、特徴によってぜんぜん違うものになります。検索エンジンを入れただけで解決するものではありません。</p>
<p>また、システムは提供する側のニーズもあります。
ECサイトであればより多くのユーザーに購買してもらったり、
コミュニティサイトの場合は利用ユーザーを増やしてコンテンツや広告の収入を増やしたりといったニーズがあります。</p>
<p>これらの両方のニーズが検索機能に影響を与えたりもします。</p>
<p>いくつか例を上げてみましょう。</p>
<h3 id="書籍の検索の場合">書籍の検索の場合</h3>
<p>ユーザーのニーズは、「ある本を探す」ことです。そのためにユーザーがクエリを入力します。
クエリは、例にも出しましたがタイトルや著者名の読みだったりします。
検索窓が1つしかないというよりは、著者やタイトル、出版社などそれぞれの項目ごとに検索できるほうが便利だったりしますよね。
検索結果については、完全一致したものが一番最初に出てきてほしいこともあれば、出版年月日の降順で並べたいことなど、
その時々でやりたいことが変わったりもします。</p>
<p>場合によっては、説明文などでも検索できると嬉しいこともあります。また、システムからは離れますが、図書館や書店で時々、「〇〇について書かれている本ありますか?」といった聞き方をしたりもします。</p>
<p>また、書店としては、探している本を見つけてもらうために検索端末などを用意しますが、そ
れ以外の本も買ってもらえるといいですよね。
オンラインの書店などでは、検索結果や書籍詳細の画面に関連書籍が出ていることもあります。</p>
<p>検索とは少し異なり、探索(なにか面白い本とかないかな?というようなニーズ)をしに、書店に行くこともあります。
書店で平積みされた本やポップなどを見て新しい本に興味を持つこともあります。</p>
<h3 id="オークションサイトやecサイトでの検索の場合">オークションサイトやECサイトでの検索の場合</h3>
<p>ユーザーのニーズは、「欲しい物を探す」ことです。
ユーザーが入力するクエリは、幅広いものになると思います。製品の型番を入力する人もいれば、
メーカー名や製品名だったり、ジャンルで絞り込んで検索することもあります。</p>
<p>探されるもの(コンテンツ、アイテム)も多数に渡ります。
検索窓は1つかもしれませんが、検索結果には、絞り込み条件(ファセット)がいくつか並んで、絞り込んでいける仕組みが用意されていることが多いです。</p>
<p>検索結果のソートは、価格順だったり人気順だったりします。
ただ、オークションサイトの場合は新しいもの順や、終了日時の早い物順だったりします。</p>
<p>サイト提供者のニーズとしては、より多くのアイテムを購入してもらうこと(売上)です。
また、オークションサイトの場合は、アイテムを提供している人のニーズも影響してくるでしょう。
売りたい人はより多くの人の目に止まってほしいと思うはずなので、
様々な情報を付与していかに目にとまるか?といったことを考えてくると思います。</p>
<p>また、様々な商品を扱うECサイトの場合は、さらに色々と大変になってきます。たとえば、「iPhone」で検索されたときに、
iPhoneそのものが上位に来るべきなのか、ケースなどの周辺商品なのかだったりといった問題が出てきます。
商品の提供者が多数に渡る場合は、同一商品でもさまざまなお店から提供されてしまうために、検索結果一覧に多数同じ商品が並んだりもしますよね。</p>
<h3 id="レシピサイトでの検索の場合">レシピサイトでの検索の場合</h3>
<p>ユーザーのニーズは「レシピを探す」です。が、探し方はユーザーによって様々です。
冷蔵庫にある材料を入力して検索することもあれば、食べたいものが決まっていてそのレシピを検索することもあります。
このとき、重要なのは類義語だったりするでしょう。食材やレシピは同じものでも様々な名前(例:パクチー、コリアンダー、シャンツァイ(香菜)など)を持っていたりします。また、部位や形によっても名前が変わったりもします。</p>
<p>検索結果は人気順で並ぶことが多いでしょうか?
ただ、レシピの提供がユーザーによるものなのか、サイト運営者が提供しているものかによっても変わってくるでしょう。</p>
<p>サイト提供者のニーズとしては、レシピコミュニティサイトの場合は、ユーザー数の増加や広告の売上などがあるでしょう。
調味料などのメーカーがレシピサイトをやっている場合は、調味料の売上だったりします。この場合は、検索がどの程度売上に寄与しているのか?などを測ることが難しかったりしそうです。</p>
<h3 id="社内文書検索の場合">社内文書検索の場合</h3>
<p>ユーザーのニーズは「文書を探す」です。探し方はファイル名であったり、ファイルのなかに出てくる単語だったりします。
社内用語・略語のような特殊な単語で検索されることもあるでしょう。ユーザーによっては、ぼんやりとした「こんな資料を探している」といったふんわりとした検索をしたくなることもあります。</p>
<p>検索結果の表示順は「それっぽいもの」が上位に出てくることが望まれそうです。
ただ、古い文書が出てきても役に立たないこともあります。新しい文書のほうが役に立つことが多いので、最近作られたものというのも重要な情報になります。ただし、権限によっては見ることができなかったり、そもそも探すこともNGだったりもします。</p>
<p>社内文書検索の提供者は、素早く検索できるものを提供することで仕事の効率を上げてもらったり、無駄を省くことができることを期待しているでしょう。</p>
<p>昔からですが、社内の文書は様々な場所に散らばっていることが多いです。顧客管理システム、ファイルサーバー、Wiki、ウェブサイトなどこれらをまとめて検索できるシステムなどが望まれていることも多いです(使われるかはまた別ですが。。。)。</p>
<h3 id="スマートスピーカーの場合">スマートスピーカーの場合</h3>
<p>ちょっと特殊な面白い例かなと思ってます。
音声で検索(というかお願い?)します。
システムとしては、ユーザーのニーズを理解するのに2段階あるのかなと。</p>
<ul>
<li>音声認識</li>
<li>認識した文章・キーワードで検索(場合によってはコマンド発行)</li>
</ul>
<p>これだけでも難易度が増します。</p>
<p>さらに、画面のないスピーカーの場合は、結果は1件だけになります。これって結構難しいことだと思うんです。
画面があれば、検索結果を上位10件などのリストで表示して、あとはユーザーに選んでもらうことができますが、
音声の場合は1件だけしか返せません。
また、レスポンスタイムもシビアなものだと想定されます。ずっとスピーカーに黙っていられると困りますよね?
きっと大変なんだろうなぁ(妄想)。</p>
<p>このように、検索と言っても、システムごとに要求・想定されるものは変わってきます。</p>
<h2 id="まとめ">まとめ</h2>
<p>例をいくつか上げましたが、ざっくりしすぎて発散してますかね。。。
想像している部分もあるので、この通りではないと思います。
ただ、システムによって、「検索」といってもシステムの特性上、
さまざまな物事、思惑が絡んでくるというのは想像してもらえたと思います?(思いたい)。</p>
<p>システムに検索機能を追加すると言っても、探したいものが何なのか?、探してもらうものはどういったものなのか?、
検索機能を追加することで何を達成したいのか?など考えることは色々あります。
どうやって、検索機能を実装するのか、その検索機能を実装するためにはどんな情報が必要なのか?などの検索機能のコアな部分を考えるだけでなく、提供しているシステム、コンテンツがどんなものかなど、システム全体を考えながら検索機能を考えていく事が検索をより良いものとして行くことだと思います。</p>
<p>また、検索されるものも検索する人もシステムが成長するのに合わせて変化していきいます。システム同様、一度作ればおしまいというものではないので、やることはいっぱいあるのかなと。</p>
<p>次は、検索のパーツについてなにか書こうかなぁ。</p>
<h3 id="ボヤキ">ボヤキ</h3>
<p>もう少しまとめてから書いたほうがいいのかもなぁ。
もしくは、出てくる要素を整理するとか。
ユーザー、コンテンツ、コンテンツ提供者とかで。
ふんわりとしたブログになってしまった。
個別のシステムごとにもっと書けることもありそう。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第15章</title>
      <link>https://blog.johtani.info/blog/2020/07/09/hap15-rust-the-book/</link>
      <pubDate>Thu, 09 Jul 2020 18:59:38 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/07/09/hap15-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 Rust the book - 第5章 Rust the book - 第6章 Rust the book - 第8章</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
<li><a href="/blog/2020/04/02/chap5-rust-the-book/">Rust the book - 第5章</a></li>
<li><a href="/blog/2020/04/07/chap6-rust-the-book/">Rust the book - 第6章</a></li>
<li><a href="/blog/2020/04/16/chap8-rust-the-book/">Rust the book - 第8章</a></li>
<li><a href="/blog/2020/05/14/chap9-rust-the-book/">Rust the book - 第9章</a></li>
<li><a href="/blog/2020/05/28/chap10-rust-the-book/">Rust the book - 第10章</a></li>
<li><a href="/blog/2020/06/04/chap13-rust-the-book/">Rust the book - 第13章</a></li>
</ul>
<p>14章は飛ばして、15章です(Cargoはまた別途調べればいいかな?と思って)。</p>
<h2 id="第15章-スマートポインタ">第15章 スマートポインタ</h2>
<p>たぶん、これを理解すれば、参照とベクタや構造体とかの組み合わせがもう少し効率よく使えるようになるのかなぁ?</p>
<ul>
<li>ポインタの強い版?
<ul>
<li>参照カウント方式のスマートポインタ型 - Luceneとかで実装されてた気がするなぁ
<ul>
<li>複数の所有者!?</li>
</ul>
</li>
</ul>
</li>
<li>DerefとDropトレイトを実装している構造体</li>
</ul>
<h3 id="ヒープのデータを指すboxtを使用する">ヒープのデータを指すBox<T>を使用する</h3>
<p>これはコンパイルエラー。<code>let y</code>のタイミングで借用してるので、書き換えでエラーになる。</p>
<pre><code>fn main() {
    let mut x = 5;
    let y = &amp;x;

    assert_eq!(5, x);
    assert_eq!(5, *y);
    x = 6;
    assert_eq!(6, x);
    assert_eq!(6, *y);
}
</code></pre><p>こっちはOK。</p>
<pre><code>fn main() {
    let mut x = 5; // in stack
    let y = Box::new(x); // in heap

    assert_eq!(5, x);
    assert_eq!(5, *y);
    x = 6;
    assert_eq!(6, x);
    assert_eq!(6, *y);
}
</code></pre><p>余談:コンパイラが変なワーニングを出してくれた。</p>
<pre><code>use std::ops::Deref;

impl&lt;T, Z&gt; Deref for MyBox&lt;T, Z&gt; {
    type Target = T;

    fn deref(&amp;self) -&gt; &amp;T {
        &amp;self.0
    }
}

struct MyBox&lt;T, Z&gt;(T, Z);

impl&lt;T, Z&gt; MyBox&lt;T, Z&gt; {
    fn new(x: T, y: Z) -&gt; MyBox&lt;T, Z&gt; {
        MyBox(x, y)
    }
}

fn main() {
    let x = 5;
    let z = &quot;10&quot;;
    let y = MyBox::new(x, z);

    assert_eq!(5, x);
    assert_eq!(5, *y);
}

</code></pre><h3 id="derefトレイトでスマートポインタを普通の参照のように扱う">Derefトレイトでスマートポインタを普通の参照のように扱う</h3>
<ul>
<li><code>参照外し型強制</code> : 日本語ムズカシイネ</li>
<li>Derefを自分で実装しないといけない場面がちょっと想像できてない。たぶん、Boxとかの説明に必要なので出てきたって感じなんだろうけど。</li>
</ul>
<h3 id="dropトレイトで片付け時にコードを走らせる">Dropトレイトで片付け時にコードを走らせる</h3>
<ul>
<li>こっちは、リソース開放とかでいい感じにできそうだってのはわかった。</li>
<li>Dropはどんなときに実装するんだろう?Tantivyだとオブジェクトプールとかで使ってた。</li>
</ul>
<h3 id="rctは参照カウント方式のスマートポインタ">Rc<T>は、参照カウント方式のスマートポインタ</h3>
<ul>
<li>これ、ここで作ったConsのリストを追っかけるためのサンプルも書いてほしい。</li>
</ul>
<pre><code>#[derive(Debug)]
enum List {
    Cons(i32, Rc&lt;List&gt;),
    Nil,
}

fn print_typename&lt;T&gt;(_: T) {
    println!(&quot;{}&quot;, std::any::type_name::&lt;T&gt;());
}

use List::{Cons, Nil};
use std::rc::Rc;
use std::borrow::Borrow;

fn main() {
    let z = Cons(5, Rc::new(Cons(10, Rc::new(Nil))));
    let a = Rc::new(z);
    let _b = Cons(3, Rc::clone(&amp;a));
    let _c = Cons(4, Rc::clone(&amp;a));
    match &amp;(*a) {
        Cons(v1, v2) =&gt; {
            print_typename(v2);
            println!(&quot;{}, {:?}&quot;, v1, v2);
        },
        Nil =&gt; println!(&quot;Nil!!&quot;)
    };
}
</code></pre><h3 id="refcelltと内部可変性パターン">RefCell<T>と内部可変性パターン</h3>
<ul>
<li></li>
</ul>
<h3 id="循環参照はメモリをリークすることもある">循環参照は、メモリをリークすることもある</h3>
</content:encoded>
    </item>
    
    <item>
      <title>Berlin Buzzwordsにオンライン出張してた</title>
      <link>https://blog.johtani.info/blog/2020/07/06/attend-berlin-buzzwords/</link>
      <pubDate>Mon, 06 Jul 2020 12:12:44 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/07/06/attend-berlin-buzzwords/</guid>
      <description>6月7日の週に開催されたBerlin Buzzwordsにオンライン出張してました。 Berlin Buzzwordsとは? ベルリンで開催されている、Big</description>
      <content:encoded><p>6月7日の週に開催された<a href="https://berlinbuzzwords.de/">Berlin Buzzwords</a>にオンライン出張してました。</p>
<h2 id="berlin-buzzwordsとは">Berlin Buzzwordsとは?</h2>
<p>ベルリンで開催されている、Big Data、Scarability、Storage and Searchabilityに関するカンファレンスです。
今年はコロナウイルスの影響で、オンラインで開催されました。
また、同時期に検索に関する他のカンファレンス(以下の2つ)もベルリンで毎年開催されているのですが、今年はこれら3つのカンファレンスが1つのチケットで参加できる形で開催されました。</p>
<ul>
<li><a href="https://mices.co/">MICES</a> - MIX-CAMP E-COMMERCE SEARCH</li>
<li><a href="https://haystackconf.com/">HAYSTACK</a> - The Search Relevance Conference! sponsored by OpenSource Connections</li>
</ul>
<p>MICES、HAYSTACKは初参加ですが、検索に関するいくつかのトピックが聞けたので楽しかったです。</p>
<p>6/7から6/12まで(がんばって)参加したので、その感想などをブログにとどめておきます。</p>
<h2 id="オンラインってどんな感じで開催されてた">オンラインってどんな感じで開催されてた?</h2>
<p>まずは、オンライン開催がどのような感じだったのかをメモしておきます。</p>
<ul>
<li>有料のオンラインカンファレンス(事前にチケット購入が必要)</li>
<li>参加者用Slack
<ul>
<li>カンファレンス数日前まではここで連絡とか質問が可能だった(もちろん、メールも来ましたけど)。</li>
</ul>
</li>
<li>基本的なプラットフォームは<a href="brella.io">Brella</a>のバーチャルイベントプラットフォーム
<ul>
<li>参加者同士のSNS機能 - 参加者同士の興味によって参加登録時に似たような人ですよとマッチングしたり。ビデオチャット機能もあり。</li>
<li>カンファレンスのスケジュール確認 - セッションのスケジュールの他に、参加者同士でのチャットのスケジュールも可能。<strong>一番便利だったのは自分のタイムゾーンも表示してくれること</strong>。</li>
<li>ストリームチャネル - セッションが行われている場所への誘導</li>
<li>スピーカー・スポンサーのリスト - スピーカーやスポンサーを探せる機能。スポンサー企業からは参加者も見ることができる</li>
</ul>
</li>
<li>セッションはYouTubeライブ
<ul>
<li>ストリーム中だったらちょっと戻ったり、ポーズもできるので、便利だった</li>
</ul>
</li>
<li>セッション後の質疑応答には<a href="https://meet.jit.si/">Jitsi</a>というオープンソース!?のビデオカンファレンスの部屋が用意されてた(GitHubで公開されてるのか。https://github.com/jitsi)。</li>
<li>2日目、3日目はLTとかが終わったあとに、オンライン飲み会やってたっぽい(不参加)</li>
</ul>
<p>主催者側も初めてだとは思うのですが、目立ったトラブルはなかったです。
ちょっとだけ遅れたりしてましたが、それほど影響はなかったです。
オンラインでの開催の一番のネックは、日本だと時差が辛いということです。
ベルリンが開催地なのですが、スピーカーや参加者はアメリカからの方が結構います。
そのため、開始時間が日本の23時といった具合になりました。</p>
<h2 id="面白かった気になったセッション">面白かった&amp;気になったセッション</h2>
<p>いくつか面白かった&amp;もう一度見ないとなと思ったセッションと感想を。</p>
<h3 id="natural-language-queries-at-salesforce-scale">Natural Language queries at Salesforce scale</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/natural-language-queries-salesforce-scale">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>Salesforceでどのような自然言語のようなクエリに対して書き換え、サジェストのようなことをやっているか?という話です。
Salesforceはテナント(企業)ごとに、データ構造などがカスタマイズ可能なため、
それぞれ個別に入力クエリ(例: new leads in sf)に対して、どういったパーツ(時間?場所?状態?)なのか?、どのフィールドへの条件なのか?といったものをNERのディープラーニングモデルとして捉えて解析しているという話でした。
企業毎にパーソナライズもされていると。実際にはパイプラインの一部でこの処理をやっており、それ以外にも処理はされているという話もありました。評価の話もされています。</p>
<h3 id="ama---ai-powered-search">AMA - AI-Powered Search</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/ask-me-anything-ai-powered-search">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>ManningでMEAP(絶賛書いているところ)の<a href="https://www.manning.com/books/ai-powered-search">AI-Powered Search</a>の著者2名がAMA形式でいろんな質問に答えていく感じのやつです。
最初は近況報告(Treyさんがカンファレンス直前に転職してた)と、書籍がどんなものかを簡単に紹介したあと、質問に答えていく形式で2時間あります。ディープラーニングのモデルに関する話なども出てきています。
もう一度見たいと思ってたやつなので見ないとな。。。
(パネルっぽいセッションは、ヒントがなにもないので結構辛い)</p>
<h3 id="ask-me-anything-lucene-9">Ask Me Anything: Lucene 9</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/ask-me-anything-lucene-9">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>LuceneのPMCメンバーのUweさんが今後のLucene/Solrのいくつかの質問に答える感じのAMAです。
出てきた話(質問の前の)としては、Lucene 8の現状(Bloc-Max WANDとか)や、Java 11対応になるよとかです。
QAでは、SIMDの話、Approximate Nearest Neighborがどんな感じか?などの話でした。</p>
<h3 id="from-commercial-search-to-owned-search">From commercial search to owned search</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/commercial-search-owned-search">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>カルフールスペインがECの検索をどのように導入したかという概要レベルの話でした。
モノリシックなものをマイクロサービスでk8s上に載せ替えたという大きなアーキテクチャ以降の話です。
Empathy.coが提供しているものを最終的には使用したみたいだけど、
どんな検索がされているのかといったニーズの調査ができるようになり、検索に絡んだKPIが改善した話でした。
COVID-19に絡んだクエリの変化についてもちょっと話が出てました。</p>
<h3 id="neural-search-in-practice">Neural Search in Practice</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/neural-search-practice-0">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>Zalandoの検索の一部でNIR(Neural IRモデル)を利用してクエリの改善をやって、それをどうやってトレーニングして、テストしたかなどの話。
NIRを利用することで、複数の言語に対して改善が見られたという話だった。
今までは、クエリをいくつかの処理を元に翻訳して、入力された単語がカテゴリーに対するものなのか、スタイルに関するものなのか?などを判別して、クエリの補強?を行っている手法だった。
これに対して、ディープラーニングでクエリに対するクリックデータを元にトレーニングして、どういうクエリに対してどんなアイテムを出すのか?というモデルで検索を改善していた。
ヒット件数が0件だったり少ないものを対象にして上記の処理を入れているらしい。
(ということで、ディープラーニングをしっかり勉強しないといけないみたいなので、どうにかしたい。。。)</p>
<h3 id="top-10-lessons-learned-in-search-projects-the-past-10-years">Top 10 Lessons learned in search projects the past 10 years</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/top-10-lessons-learned-search-projects-past-10-years">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>10年検索プロジェクトをやってきた10個の気づきという感じのセッション。
ごく当たり前のことなんだけど、検索の導入・改善に関して、こういう事あって、何も考えないとこうなっちゃうよね。
だから、こんなことをやるべきだよね?という話です。
たとえば、検索窓はあるけど分析すらしていない状況(レッスン1)だとまずはこういうのやらないとね。とか、
検索クエリの分析・改善ばかりして、コンテンツの分析・改善を怠っていないか?という当たり前の話です。
当たり前なんだけどまとめてくれてるのは、やはりいいなぁと。</p>
<h3 id="click-logs-and-insights-putting-the-search-experts-in-your-audience-to-work">Click logs and insights: Putting the search experts in your audience to work</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/click-logs-and-insights-putting-search-experts-your-audience-work">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>検索ログとクリックログがあったときに、どういったことに使えるのかを料理のレシピに見立ててデモをするセッションで説明が面白かったです。
「こんなログがあったときに、ログのこの項目とこの項目を材料にすると、こんなのができますね」というのを、Elasticsearchにログを取り込んで、JupyterNotebookでデモをしていました。やはり動くものがあるとわかりやすいですねぇ。</p>
<h3 id="mixing-and-matching-diversifying-search-results">Mixing and Matching: Diversifying Search Results</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/mixing-and-matching-diversifying-search-results">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>これまたパネルセッションです。
検索結果の多様性に関するディスカッションでした。
これは、ECだからこその課題でもあるのかなと。検索自体は「何かを見つける」ための手段です。
普通に考えた場合は、ピンポイントで探していたものが見つかるのが嬉しいです。
が、例えば、ユーザーが検索した単語そのものが入っているだけのものが見つかるよりも、似たような商品も一緒に出てきてほしいことありますよね?また、ECサイトだと、回遊してほしいというのもあります。ということで、それぞれの方がどんな観点で多様性を考えているのかという話をするディスカッションになっていました。</p>
<h3 id="thought-vectors-knowledge-graphs-and-curious-death-of-keyword-search">Thought Vectors, Knowledge Graphs, and Curious Death(?) of Keyword Search</h3>
<ul>
<li><a href="https://berlinbuzzwords.de/session/thought-vectors-knowledge-graphs-and-curious-death-keyword-search">セッションのページはこちら(2020年7月現在)</a></li>
</ul>
<p>AI-Powered Searchの著者の一人、Treyさんのセッション。ベクトルを検索にどうやって使うのか、
ベクトルで表現できるものはどんなものがあるのか?どんな検索エンジンで使えるのか?という話でした。
歴史的な話も交えつつ、検索だとこのへんで使えるんじゃないか?というような話でした。</p>
<h3 id="録画は">録画は?</h3>
<p>気になったセッションをいくつか書き出してみました。
ちなみに、全セッションのビデオが公開されています。興味がある方は、ご覧いただければと。</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The recordings from this year&#39;s Berlin Buzzwords / MICES / Haystack joint event are now online. You can watch them all on our YouTube channel. Thank you once again to all of our wonderful speakers.  <a href="https://t.co/hTRDmKNdpd">https://t.co/hTRDmKNdpd</a></p>&mdash; Berlin Buzzwords (@berlinbuzzwords) <a href="https://twitter.com/berlinbuzzwords/status/1280089146942464000?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="感想そして来年は">感想そして来年は?</h2>
<p>楽しかったです。検索に関する話が色々聞けるのはやっぱ楽しいですね。サイトの特性(ECなのか、Wikipediaなのかなど)によっても「良い検索」の定義も変わるので、サービスなどがどんなものか、そしてそれを良くするためには検索はどんなことができるのか?といった話や、技術的な濃い話までいろいろな話を聞けました。
ただ、パネルは英語の聞き取りが辛いですね。。。あと、時差が。日本にいながらにして時差ボケは辛い。。。
オンライン飲み会には流石に参加できませんでした(4時とか5時から始まるし)。</p>
<p>来年もオンラインで開催されたら間違いなく参加します。
オフラインのみだった場合はどうなるかなぁ。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>luceneutil - Analyzer性能テストへのkuromojiの追加</title>
      <link>https://blog.johtani.info/blog/2020/06/25/analyzer-perf-test-with-luceneutil/</link>
      <pubDate>Thu, 25 Jun 2020 11:01:58 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/06/25/analyzer-perf-test-with-luceneutil/</guid>
      <description>luceneutil - マニアックなツールのセットアップの続きです。 今回も誰得?なブログなので興味ない場合は飛ばしましょう。 一応、luceneutilのREAD</description>
      <content:encoded><p><a href="/blog/2020/06/23/how-to-use-luceneutil/">luceneutil - マニアックなツールのセットアップ</a>の続きです。
今回も誰得?なブログなので興味ない場合は飛ばしましょう。</p>
<p>一応、<code>luceneutil</code>のREADMEにある<code>localrun.py</code>を動かせるところまでいったんですが、そこで一旦本題を思い返してみました。</p>
<blockquote>
<p>Kuromojiの性能が落ちてるらしいし、Analyzer系のベンチマーク測ってるグラフに載せたほうがいいよね。</p>
</blockquote>
<p>これが、そもそも動かしてみようと思った本題です。
READMEに書いてある手順で動いたんですが、よくよく調べてみると、Analyzer系の性能テストをやってるのは、別物っぽいぞと。
なんとなく、「<a href="http://satob.hatenablog.com/entry/2019/07/17/003751">ソフトウェア考古学</a>」っぽくなってきましたね。</p>
<h2 id="analyzerの性能テストやってるのは">Analyzerの性能テストやってるのは?</h2>
<p>Analyzerのパフォーマンステストのグラフに出てきたTokenizerの名前を元にluceneutilのリポジトリを検索してみました。
<code>EgdeNGrams</code>当たりで検索するとヒットしたのが以下のファイルです。</p>
<ul>
<li><a href="https://github.com/mikemccand/luceneutil/blob/master/src/main/perf/TestAnalyzerPerf.java"><code>src/main/perf/TestAnalyzerPerf.java</code></a></li>
<li><code>src/main/perf/TestAnalyzerPerf4x.java</code></li>
<li><a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/sumAnalyzerPerf.py"><code>src/python/sumAnalyzerPerf.py</code></a></li>
</ul>
<p>本命は<code>src/main/perf/TestAnalyzerPerf.java</code>っぽいですね。
これを動かしている人はどれかな?ということで、今度はこのファイル名で検索します。</p>
<ul>
<li><a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/runAnalyzerPerf.py"><code>src/python/runAnalyzerPerf.py</code></a></li>
</ul>
<p>どうやら、このPythonのファイルが先程のJavaファイルを実行して、性能を計測しているみたいです。
最初に出てきた、<code>sumAnalyzerPerf.py</code>は<code>runAnalyzerPerf.py</code>の実行結果をAnalyzerの計測結果のグラフに追加する処理をしているようだということまでがわかりました。</p>
<h2 id="kuromojiをtestanalyzerperfに追加してみる">KuromojiをTestAnalyzerPerfに追加してみる</h2>
<p>動かす対象がわかったので、あとは、やることを追加しましょうと。</p>
<ol>
<li>Kuromojiの実行を<code>TestAnalyzerPerf</code>に追加
<ul>
<li>引数を追加して日本語版のWikipediaのファイルも読み込むようにする</li>
</ul>
</li>
<li><code>runAnalyzerPerf.py</code>に引数の追加とクラスパスの追加
<ul>
<li>JapaneseAnalyzerは<code>lucene/analysis/kuromoji</code>にビルドされるのでクラスパスを追加</li>
<li>引数に日本語版のWikipediaのファイルを追加</li>
</ul>
</li>
<li>日本語版のWikipediaのデータの用意</li>
</ol>
<p>こんなものかな?と。</p>
<p>1と2はそれほど大変ではないので、3をまずはというところから始めてみました。</p>
<h2 id="wikipediaのxmlから1行1データのtsvファイルに">WikipediaのXMLから1行1データのTSVファイルに</h2>
<p>まずは、どんなファイルを想定して動いているのかな?ということで、英語版のファイルがどんなものかを探してみることに。</p>
<p><a href="%5B77%E8%A1%8C%E7%9B%AE%5D(https://github.com/mikemccand/luceneutil/blob/master/src/main/perf/TestAnalyzerPerf.java#L77)"><code>TestAnalyzerPerf.java</code></a>では入力ファイルから1行ずつ文字列を読み込んでAnalyzerに処理させているだけというのがわかっています。なので、とりあえず、1行ずつ読めるような形式だと。
次に、<a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/runAnalyzerPerf.py#L55"><code>runAnalyzerPerf.py</code>の55行目で</a><code>enwiki-20130102-lines.txt</code>というものを読み込んでいます。
が、これがよくわからないw</p>
<p>前回とりあえず動いたときに、<a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/constants.py"><code>src/python/constants.py</code></a>にいろんなファイルへのパスとかが記載されていたのを覚えていたので、その当たりから調べてみます。
<a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/constants.py#L52">52行目に</a>約665万件のドキュメントだというような記載があります。
前回の<code>localrun.py</code>のファイルと似たような構造(1行に1記事が埋め込まれたTSVファイル)だろうと判断して、それを作りそうなプログラムを探してみました。
Wikiとかで検索して見つけたのがこのソースたちでした。</p>
<ul>
<li><a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/wikiXMLToText.py#L160"><code>src/python/wikiXMLToText.py</code></a> - それっぽい名前ですよね?
<ul>
<li>中身を見ると、第1引数のファイル(XML)から<code>page</code>タグごとにデータを抜き出し、処理をしてからタブ区切りで第2引数のファイルに書き出してます。</li>
</ul>
</li>
<li><a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/WikipediaExtractor.py"><code>src/python/WikipediaExtractor.py</code></a> - これもそれっぽいですね。
<ul>
<li><code>WikipediaExtractory.py</code>は界隈では有名な<code>https://github.com/attardi/wikiextractor</code>みたいです。こっちはなんとなく使い方はわかっています。</li>
</ul>
</li>
<li><a href="https://github.com/mikemccand/luceneutil/blob/master/src/python/combineWikiFiles.py"><code>src/python/combineWikiFiles.py</code></a> - これまたそれっぽい。
<ul>
<li>中身を見ると、1番目のコードで出力したデータに、2番めのコードで出力されたデータを元になにかしら処理をして、引数で与えられたファイルに出力する感じになってます。</li>
</ul>
</li>
</ul>
<p>ということで、完全に憶測ですが、日本語のWikipediaのXMLファイルを元に次のような流れでファイル作ってみればいいのかな?と。</p>
<ol>
<li><code>jawiki-20200620-pages-articles.xml.bz2</code>をwikimediaからダウンロードして、<code>unbzip2</code>で展開</li>
<li><code>python src/python/wikiXMLToText.py jawiki-20200620-pages-articles.xml jawiki-lines.txt</code>で、1行ごとのデータを作る
<ul>
<li>ちなみに、このプログラムは2箇所ほど修正しました。<code>username</code>タグが存在しない<code>page</code>が出力されなさそうなので、デフォルトで<code>username: &quot;&quot;</code>みたいなデータを<code>attr</code>ってディクショナリに設定しました。</li>
</ul>
</li>
<li><code>cat jawiki-20200620-pages-articles.xml | python -u src/python/WikipediaExtractor.py -b102400m -o extracted</code>で別途XMLからデータを抽出</li>
<li><code>python src/python/combineWikiFiles.py jawiki-lines.txt extracted/AA/wiki_00 jawiki-20200620-lines.txt</code>で2と3の出力をかけ合わせる</li>
</ol>
<p>で、まぁ、一応ファイルはできたんですが。。。
前回のブログ記事でセットアップしたときにダウンロードされたファイルは<code>title</code>、<code>日付</code>、<code>本文</code>の3つのカラムしかないんですが、上記の手順で作り出したファイルにはいろんなカラムが存在するんですよね(2が出力する項目が結構ある)。。。</p>
<h2 id="性能テスト用プログラムの書き換え">性能テスト用プログラムの書き換え</h2>
<p>入力ファイルは出来上がったので、あとは、性能テストを走らせる部分の修正です。
<a href="https://github.com/mikemccand/luceneutil/pull/69">修正部分はプルリク</a>見たほうが明確なので省略で。</p>
<h2 id="実行してみた">実行してみた</h2>
<p>で、実行してみました。
せっかくなので、ブランチを<code>branch_8_5</code>と<code>master</code>を対象にしてやってみました。
実際には<code>src/python/runAnalyzerPerf.py</code>にディレクトリ名やブランチ名が直書きされてたので書き換えつつ実施した結果はこちら。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200625/analyzer_perf_test.png" />
    </div>
    <a href="/images/entries/20200625/analyzer_perf_test.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>他の英語系のAnalyzerと同じグラフに載せてみたのですが、遅いので、下の方に表示されてます。
で、下に凹んだ部分(<code>23 June</code>のところ)が<code>branch_8_5</code>で実行したときの性能値でした。
実際に遅くなってますね。ただ、他のAnalyzerの振れ幅も大きいので、別のグラフにしたほうがわかりやすくなるのかもなぁと思った次第です。</p>
<p>ということで、一応動いたんでプルリク出してみました。
採用されるかなぁ?</p>
<p>一応誰得ブログはこれでおしまい。
Noriとかもこの感じで対応できるんじゃないかな?</p>
</content:encoded>
    </item>
    
    <item>
      <title>luceneutil - マニアックなツールのセットアップ</title>
      <link>https://blog.johtani.info/blog/2020/06/23/how-to-use-luceneutil/</link>
      <pubDate>Tue, 23 Jun 2020 21:58:29 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/06/23/how-to-use-luceneutil/</guid>
      <description>LuceneのFSTの修正に関連して、Kuromojiのパフォーマンス問題が出ているようです。 この問題自体はLucene 8.6.0以降で直る</description>
      <content:encoded><p><a href="https://issues.apache.org/jira/browse/LUCENE-9286">LuceneのFSTの修正に関連して、Kuromojiのパフォーマンス問題が出ているようです</a>。
この問題自体はLucene 8.6.0以降で直る予定のようなのです(<a href="https://github.com/elastic/elasticsearch/issues/57142">Elasticsearchへの影響範囲についてはこれが参考になるかな?</a>)。
で、これに関連して、ベンチマーク計らないとねという話が出ていて、
昔から、LuceneのMikeさんがやっている<a href="http://people.apache.org/~mikemccand/lucenebench/analyzers.html">ベンチマークのグラフ</a>に<a href="https://github.com/mikemccand/luceneutil/issues/64">載せるのがいいよね</a>という話になっていました。
どうも、これについては、Luceneの中にあるbenchmarkというプロジェクトではなく、MikeさんのGitHubリポジトリにあるプログラムで計測しているようです。</p>
<p>じゃあ、手元でこれどうやって動かすんだろう?ということでやってみたブログになります。
おそらく、99.99%の人は興味ないと思うのでスルーしていただくのがいいと思います。備忘録のために書いてます。</p>
<h2 id="とりあえずgit-clone">とりあえずgit clone</h2>
<p>公開されているリポジトリは<a href="https://github.com/mikemccand/luceneutil/">こちら</a>です。</p>
<h2 id="手順通りに">手順通りに?</h2>
<p>とりあえず、READMEにセットアップなどのやり方があったんで追っていくことに。
とりあえず動くまでの手順は以下のとおりです。</p>
<ol>
<li>ディレクトリを決める
<ul>
<li><code>$LUCENE_BENCH_HOME</code>が起点になります。名前は何でもいいみたいです。</li>
<li><code>cd $LUCENE_BENCH_HOME</code></li>
</ul>
</li>
<li>リポジトリをクローン - <code>git clone https://github.com/mikemccand/luceneutil.git util</code>です。
<ul>
<li>コレ自体は問題なし。</li>
<li><code>cd util</code></li>
</ul>
</li>
<li>セットアップスクリプトを実行 - <code>python src/python/setup.py -download</code>
<ul>
<li>ここがすごく時間かかります。6GBのファイルをゆっくりダウンロードしてきますので、一晩置いておきましょう(起きたら終わってた)</li>
<li><code>$LUCENE_BENCH_HOME/data</code>に<code>enwiki-20120502-lines-1k.txt.lzma</code>と<code>wikimedium500.tasks</code>いうファイルがダウンロードされている。</li>
</ul>
</li>
<li>ダウンロードした圧縮ファイルを展開 - <code>unlzma enwiki-20120502-lines-1k.txt.lzma</code>
<ul>
<li>macOSに<code>lzma</code>のコマンド入ってるんですね。知らなかった。</li>
<li>終わったら<code>cd ../</code>で<code>$LUCENE_BENCH_HOME</code>に戻っておく</li>
</ul>
</li>
<li>ベンチマーク対象となるLuceneを用意 - <code>git clone https://github.com/apache/lucene-solr.git</code>
<ul>
<li>READMEには<code>lucene_candidate</code>と<code>lucene_baseline</code>って名前でって書いてあったのですが、これだと、この後の実行フェーズでエラーになりました。</li>
<li><code>trunk</code>と<code>patch</code>というディレクトリにそれぞれ変更しました。<code>localrun.py</code>を実行したらこのディレクトリ名だったので(相変わらず、自分、行きあたりばったりな対応してるなぁ。。。)</li>
<li>とりあえず動くかどうかを確認したかったので、<code>trunk</code>と<code>patch</code>はどちらもリポジトリを<code>clone</code>したものになってます。動いたのを確認したら、タグを指定して比較したいブランチをチェックアウトする予定。</li>
</ul>
</li>
<li><code>trunk</code>と<code>patch</code>をビルド - <code>ant jar</code>
*</li>
<li><code>localrun.py</code>を実行 - <code>cd util</code>そして。。。
<ul>
<li>5.で記述したディレクトリ以外に1箇所Pythonのコードを書き換えた。</li>
<li><code>src/python/benchUtil.py</code>内部に<code>hppc-0.8.1.jar</code>のファイルの存在チェックをしているのだが、2020/06/23時点でのLuceneのリポジトリの依存関係だと<code>hppc-0.8.2.jar</code>になっており、ファイルが見つからないエラーが出たため、<code>0.8.2</code>に書き換えた。970行目付近。</li>
<li>改めて実行したら成功した。</li>
</ul>
</li>
</ol>
<h2 id="まだ途中">まだ途中</h2>
<p>とりあえず、実行するところまではできましたが、結果の見方とかちゃんと調べないとなぁ。
いくつかローカルで対応したものについてはあとでGitHubにIssue立てとくべきだな。</p>
<p>と、動くのを確認したので、日本語周りの準備をしてみてるところです。</p>
<ul>
<li>
<p>終わったこと</p>
<ul>
<li>日本語のWikipediaのデータ<code>jawiki-20200620-pages-articles.xml.bz2</code>をダウンロードして展開</li>
</ul>
</li>
<li>
<p>試している途中</p>
<ul>
<li>WikipediaExtractorでXMLからデータを抽出 - <code>cat ~/tmp/wiki/jawiki-20200620-pages-articles.xml | python -u src/python/WikipediaExtractor.py -b102400m -o extracted</code>
<ul>
<li>これは手順が違うかも????</li>
</ul>
</li>
<li><code>python src/python/wikiXMLToText.py ~/tmp/wiki/20200620-pages-articles.xml ./hoge.txt</code>
<ul>
<li>これで、<code>title</code>、<code>日付</code>、<code>本文</code>が抜き出せそう?</li>
<li>この後に<code>combineWikiFiles.py</code>の実行かな?</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>って感じです。
誰得だろうこれ???</p>
</content:encoded>
    </item>
    
    <item>
      <title>Azure Cognitive Searchでの日本語向けAnalyzerの違い</title>
      <link>https://blog.johtani.info/blog/2020/06/09/difference-analyzers-in-azure-search/</link>
      <pubDate>Tue, 09 Jun 2020 17:44:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/06/09/difference-analyzers-in-azure-search/</guid>
      <description>Azure Cognitive Searchで日本語を扱うときに、形態素解析器を使いたい場合、2種類のAnalyzerが用意されています。今回はこれらの違いがどんなもの</description>
      <content:encoded><p>Azure Cognitive Searchで日本語を扱うときに、形態素解析器を使いたい場合、2種類のAnalyzerが用意されています。今回はこれらの違いがどんなものかを見ていくことにします。</p>
<h2 id="analyzerとは">Analyzerとは?</h2>
<p>まずは、その前にAnalyzerとは何者か?というのを少しだけ。
Azure Cognitive Searchは<a href="https://ja.wikipedia.org/wiki/%E8%BB%A2%E7%BD%AE%E3%82%A4%E3%83%B3%E3%83%87%E3%83%83%E3%82%AF%E3%82%B9">転置インデックス</a>を内部で作成して、検索を行っています。
この、転置インデックスは、「単語」がどのドキュメントに入っているか?を素早く見つけることができるデータ構造となっています。</p>
<p>Azure Cognitive Searchは、この「単語」を入力された文章から生成するときに、Analyzerというものを利用します。
Analyzerは入力された文章をある規則に則って単語に分割する機能です。
この「ある規則」が、各種言語や用途によって様々に用意されています。
今回はこの中の<code>ja.lucene</code>と<code>ja.microsoft</code>という2種類のAnalyzerについて違いを見ていきます。</p>
<h2 id="2種類のanalyzerの違いはどんなもの">2種類のAnalyzerの違いはどんなもの?</h2>
<p>このAnalyzerの挙動を見るためのエンドポイントとして<code>analyze</code>というAPIがあります(<a href="https://blog.johtani.info/blog/2020/03/19/azure-search-analyze-plugin/">詳細は昔のブログを参照</a>)。</p>
<p>このAPIを利用して、Wikipediaのいくつかの文章を単語に区切って見て、
<code>ja.microsoft</code>がどんな動きをしているのか想像してみます(残念ながら<code>ja.microsoft</code>の仕様?や挙動についてはページが見つからないため)。</p>
<h3 id="もとの文章と解析結果一部抜粋">もとの文章と解析結果(一部抜粋)</h3>
<p>文章は、手元のElasticsearchに登録したjawikiのデータからランダムに抽出しています。また、自前のツールで生成したWikipediaのデータなので、まだ一部、見苦しい文字列になっています(そっちもなおさないと)。</p>
<h4 id="1-砂川熊本県">1. 砂川（熊本県）</h4>
<blockquote>
<p>thumb|250px|right|上砂川橋より上流方砂川（すながわ）は、熊本県宇城市・八代郡氷川町を流れる二級河川。</p>
</blockquote>
<p>この文字列から抽出された単語で特徴的なものを一部抜粋しました。</p>
<table>
<thead>
<tr>
<th><code>ja.microsoft</code></th>
<th><code>ja.lucene</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>250px</td>
<td>250</td>
</tr>
<tr>
<td></td>
<td>px</td>
</tr>
<tr>
<td>上砂</td>
<td>上, 上砂川</td>
</tr>
<tr>
<td>川橋</td>
<td>砂川</td>
</tr>
<tr>
<td></td>
<td>橋</td>
</tr>
<tr>
<td>宇城</td>
<td>宇</td>
</tr>
<tr>
<td>市</td>
<td>城市</td>
</tr>
<tr>
<td>二級</td>
<td>二</td>
</tr>
<tr>
<td>^</td>
<td>級</td>
</tr>
</tbody>
</table>
<p>まず、最初の<code>250px</code>ですが、<code>ja.microsoft</code>では、<code>px</code>が単位であると判定しているのか、数値と合わせた単語として抽出されています。この場合、<code>250</code>で検索しても、この文字列はヒットしない形になるので、ノイズが減ることが考えられるかと。</p>
<p><code>上砂川橋</code>という文字は、分割の仕方が別れました。
<code>ja.lucene</code>では、<code>上砂川</code>という単語が地名として辞書に存在するために、このような分割になっています。<code>ja.microsoft</code>のデータは品詞の情報が取れないのですが、<code>上砂</code>、<code>川橋</code>ともに、名詞として辞書に存在しているのではないかなと。<code>ja.lucene</code>には<code>川橋</code>という単語は存在していないようでした。</p>
<p><code>宇城市</code>(うきし)については、2005年に合併でできた市のようで、<code>ja.lucene</code>が利用している辞書には存在しない可能性があり、<code>宇城</code>という文字が抽出できてないと思われます。</p>
<p>最後は<code>二級</code>です。<code>ja.lucene</code>では、数字と助数詞として分割されています。こちらも何かしらのロジックにより、<code>二級</code>という1単語でヒットできるように数字と単位?が合わせた単語で出てくる仕組みが<code>ja.microsoft</code>なのかなと。</p>
<h4 id="2-uefa-u-18女子選手権2000">2. UEFA U-18女子選手権2000</h4>
<blockquote>
<p>UEFA U-18女子選手権2000は第3回目のUEFA U-18女子選手権である。決勝トーナメントは2000年7月27日から8月4日までフランスで行われ、ドイツが初優勝を果たした。</p>
</blockquote>
<p>この文字列から抽出された単語で特徴的なものを一部抜粋しました。</p>
<table>
<thead>
<tr>
<th><code>ja.microsoft</code></th>
<th><code>ja.lucene</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>u-18, u</td>
<td>u</td>
</tr>
<tr>
<td>18, nn18</td>
<td>18</td>
</tr>
<tr>
<td>第3回目</td>
<td>第</td>
</tr>
<tr>
<td></td>
<td>3</td>
</tr>
<tr>
<td></td>
<td>回</td>
</tr>
<tr>
<td></td>
<td>目</td>
</tr>
<tr>
<td>トーナメント, トナメント</td>
<td>トーナメント</td>
</tr>
<tr>
<td>2000年</td>
<td>2000</td>
</tr>
<tr>
<td></td>
<td>年</td>
</tr>
<tr>
<td>7月</td>
<td>7</td>
</tr>
<tr>
<td></td>
<td>月</td>
</tr>
<tr>
<td>27日</td>
<td>27</td>
</tr>
<tr>
<td></td>
<td>日</td>
</tr>
</tbody>
</table>
<p>数字を含む単語<code>第3回目</code>や<code>2000年</code>、<code>7月</code>などは、<code>ja.microsoft</code>は先程と同様、数字と単位の組み合わせを1単語として出力しています。</p>
<p>また、<code>トーナメント</code>という単語を<code>トナメント</code>という形で、長音を除去した形で出力しています。
今回の文字列ではないですが、この他に、<code>センター</code>を<code>センター</code>と<code>センタ</code>の2パターンの単語で出力するといったことを行っています。
<code>ja.lucene</code>の場合、単語の最後に長音がある場合だけ<code>センタ</code>として、長音を除去した単語が出力されます。
これは、長音の表記ゆれに対応するためではないかなと。たとえば、<code>インターフェース</code>と<code>インタフェース</code>、<code>インターフェイス</code>のように、人や文章によって、間にでてくる長音を使ったり使わなかったりという表記ゆれに対応するためだと思われます。
その他にも、<code>イプロゥヴェト</code>を<code>イプロゥベト</code>に、<code>ネクスト</code>を<code>ネキスト</code>に、<code>バラエティ</code>を<code>バラエチ</code>にも変換するなどといった処理をしてくれるようです。カタカナの表記ゆれには強そうですね(これどうやってるんだろう?)。</p>
<p><code>ja.microsoft</code>では、<code>nn18</code>というちょっと変わった単語も出てきていました。純粋な数字の場合は<code>nn</code>と入力してくれるようで、数字だけで検索したい場合に利用できるのかな?これはドキュメントに書いておいてほしいかも?</p>
<h3 id="共通点">共通点</h3>
<p><code>ja.lucene</code>、<code>ja.microsoft</code>ともに、共通している動作として、「てにをは」といった単語は除去されていました。違いがあるものとしては、「より」(助詞-格助詞-一般)、「されている」(動詞-自立、動詞-接尾、助詞-接続助詞、動詞-非自立)、「ある」(助動詞)といったものは<code>ja.microsoft</code>では除去されずに出てきていました。
ストップワード的に「てにをは」あたりを除去をしている感じでしょうか?</p>
<p>アルファベットで構成されている単語についても、基本はそのまま出力される挙動のようでした。</p>
<h2 id="じゃあどっちがいいの">じゃあどっちがいいの?</h2>
<p>残念ながらどちらがいいかは、一長一短かなぁと。
<code>ja.lucene</code>に関しては、Luceneの仕組みを利用しているので、Elasticsearchなどを使えば、個別の単語についてより詳細の情報を取得することが可能です(品詞、読みなど)。<code>ja.microsoft</code>については、残念ながら手の入れようがないので、そういう動きのものだという割り切った使い方になるでしょうか?
ただ、長音の除去による表記ゆれなどについては、便利な機能なので、そのあたりの問題に対応したい場合は、<code>ja.microsoft</code>を活用するのも良いかと思います。</p>
<p>個人的には、より細かい単語としてインデックスに登録できるもののほうが、柔軟な検索には対応できるのではないかなぁと考えています(Kuromojiの辞書をUniDicにするとか?も考えますが、これはAzure Searchではできないですが)。</p>
<h2 id="まとめ">まとめ</h2>
<p>Wikipediaのデータをいくつか使って、<code>ja.microsoft</code>と<code>ja.lucene</code>の違いについて、考察してみました。何かの役に立てばと。
他に、これはどんな感じになるの?などありましたら、コメントいただければと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第13章</title>
      <link>https://blog.johtani.info/blog/2020/06/04/chap13-rust-the-book/</link>
      <pubDate>Thu, 04 Jun 2020 17:37:29 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/06/04/chap13-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 Rust the book - 第5章 Rust the book - 第6章 Rust the book - 第8章</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
<li><a href="/blog/2020/04/02/chap5-rust-the-book/">Rust the book - 第5章</a></li>
<li><a href="/blog/2020/04/07/chap6-rust-the-book/">Rust the book - 第6章</a></li>
<li><a href="/blog/2020/04/16/chap8-rust-the-book/">Rust the book - 第8章</a></li>
<li><a href="/blog/2020/05/14/chap9-rust-the-book/">Rust the book - 第9章</a></li>
<li><a href="/blog/2020/05/28/chap10-rust-the-book/">Rust the book - 第10章</a></li>
</ul>
<p>11章、12章はちょっと飛ばして、13章です。</p>
<h2 id="第13章">第13章</h2>
<p>イテレータ、クロージャです。
12章の話もちょっと出てくるのか。</p>
<h3 id="クロージャ">クロージャ</h3>
<p>基本的に、「変数には値が束縛されている」という固定観念がずっと頭にこびりついたままなので、クロージャに慣れないんだろうなぁ。そろそろこの固定概念をどうにかしないと。</p>
<ul>
<li>匿名関数で、変数に保存したり引数に渡せる</li>
<li>ちょっと面白い話(ワークアウト)で実際に考えられる手法の説明がいくつか行われる</li>
</ul>
<ol>
<li>関数でリファクタリング
<ul>
<li>これが自分がよくやるパターンかなぁ。クロージャになれてないので。。。</li>
</ul>
</li>
<li>クロージャーを変数に束縛
<ul>
<li>呼び出しは関数みたいな感じ(ここで少し混乱)</li>
<li>これだと、結局呼び出されたタイミングが複数回あるよね? -&gt; あはりそうだった</li>
</ul>
</li>
</ol>
<p>ここで、閑話休題で、クロージャの型推論とか注釈の話。
クロージャは狭い文脈だし、外に公開しているものでもないので、戻り値なども定義してなくてもいいよねとのこと。書くことも可能?なので、書いてわかりやすくするのもありなんだろうな。</p>
<p>推論についてはこれまで通りで、2回異なる型の変数で呼び出すと、2回目で怒られていた。</p>
<ol start="3">
<li>遅延評価(クロージャを保持する構造体!?)
<ul>
<li><code>Fn</code>トレイト</li>
<li>トレイトとMatchの組み合わせだからこのへんで説明する形になるのか。</li>
<li>これを真似すれば、いくつか処理を簡素化できるかもしれないなぁ、たしかに。</li>
<li>なければ実行するみたいな処理を書きたいことがよくあるし。Javaだとnullで定義しといて、nullだったらみたいなのがあるから。</li>
</ul>
</li>
</ol>
<ul>
<li>
<p><code>Cacher</code>はサンプルだからこの名前でいいけど、自分だと、どんな名前にするかなぁ?</p>
</li>
<li>
<p>振る舞いは難しくなるのか。<code>Cacher実装の限界</code>を読むと。</p>
</li>
<li>
<p>関数にするとスコープが変わるのでアクセスできなくなると。。。コンパイラが教えてくれるのは便利だな。</p>
</li>
<li>
<p>環境から値をキャプチャする3つの方法</p>
<ul>
<li>多分この話が一番クロージャに意味がある話なんだと思う。</li>
</ul>
</li>
</ul>
<h3 id="イテレータ">イテレータ</h3>
<p>回しましょう。</p>
<ul>
<li>便利。ただ、こういう書き方に自分が慣れてないので、そっちを補正しないとなぁ。</li>
<li>どれがイテレータ?っていうのを判別するのがちょっとむずかしい(慣れの問題かなぁ)</li>
<li>イテレータアダプタ便利。どんなのがあるのか?とかがやっとわかってきた。</li>
<li>パフォーマンスに関しては、うーん、どうなんだろう?という感想だった。</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第10章</title>
      <link>https://blog.johtani.info/blog/2020/05/28/chap10-rust-the-book/</link>
      <pubDate>Thu, 28 May 2020 18:06:55 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/28/chap10-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 Rust the book - 第5章 Rust the book - 第6章 Rust the book - 第8章</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
<li><a href="/blog/2020/04/02/chap5-rust-the-book/">Rust the book - 第5章</a></li>
<li><a href="/blog/2020/04/07/chap6-rust-the-book/">Rust the book - 第6章</a></li>
<li><a href="/blog/2020/04/16/chap8-rust-the-book/">Rust the book - 第8章</a></li>
<li><a href="/blog/2020/05/14/chap9-rust-the-book/">Rust the book - 第9章</a></li>
</ul>
<h2 id="第10章">第10章</h2>
<p>ジェネリック、トレイト、ライフタイムです。
手強そう。</p>
<p>いきなり関数の切り出し方みたいな話が始まって面食らいました。</p>
<h3 id="ジェネリックなデータ型">ジェネリックなデータ型</h3>
<p>ジェネリックはJavaにもあるので、それほど理解に苦しむことはなかったです。
また、OptionやResultですでに経験済みでしたし。</p>
<p>ただ、<code>impl&lt;T&gt; Point&lt;T&gt;{</code>、このメソッド定義は少し最初は戸惑いました。
言われてみれば、なるほどなんですけど。</p>
<p>コンパイル時にコンパイラが単相化を行うことにより、必要最低限なコードを生成してくるというのは理にかなっているなぁと。</p>
<h3 id="トレイト-共通の振る舞いを定義する">トレイト: 共通の振る舞いを定義する</h3>
<p>出だしにもありますが、「インターフェイス」という機能に類似していると考えると割とすんなりと理解が進みました。
ただ、Javaだと、インターフェースはクラスとセットなため、トレイとの実装に関する記述方法は少し戸惑いが。</p>
<p>デフォルト実装との組み合わせはAbstractに似た処理になるなと考えながら読みすすめました。</p>
<p>「トレイト境界」という日本語には少し違和感を覚えましたが、線引をして、制限をかけるという理解でいいのかな?</p>
<p>実際には<code>#[derive()]</code>などで、トレイトを自分で実装する必要がないなどの、便利機能も用意されており、このあたりのコードの追い方がまだ少し慣れていないかもなぁと。便利なんですけど。。。</p>
<p>少しだけ気になったので、動作確認したのは次の実装です。</p>
<p>トレイトで宣言されている関数と構造体が独自に実装する関数の名前がかぶるとどうなるのかという実験です。
構造体独自のメソッドが優先される感じになりそう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Tweet</span> {
    <span style="color:#66d9ef">pub</span> username: String,
    <span style="color:#66d9ef">pub</span> content: String,
    <span style="color:#66d9ef">pub</span> reply: <span style="color:#66d9ef">bool</span>,
    <span style="color:#66d9ef">pub</span> retweet: <span style="color:#66d9ef">bool</span>,
}
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> Summary {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">summarize_author</span>(<span style="color:#f92672">&amp;</span>self) -&gt; String;
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">summarize</span>(<span style="color:#f92672">&amp;</span>self) -&gt; String {
        <span style="color:#75715e">// {}さんからもっと読む
</span><span style="color:#75715e"></span>        format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;(Read more from {}...)&#34;</span>, self.summarize_author())
    }
}
<span style="color:#66d9ef">impl</span> Tweet {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">summarize_author</span>(<span style="color:#f92672">&amp;</span>self) -&gt; String {
        format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;hoge {}&#34;</span>, self.username)
    }
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">to_string</span>(<span style="color:#f92672">&amp;</span>self) -&gt; String {
        format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;fuga&#34;</span>)
    }
}
<span style="color:#66d9ef">impl</span> Summary <span style="color:#66d9ef">for</span> Tweet {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">summarize_author</span>(<span style="color:#f92672">&amp;</span>self) -&gt; String {
        format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;@{}&#34;</span>, self.username)
    }
}
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">summary</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Summary</span><span style="color:#f92672">&gt;</span>(hoge: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">T</span>) {
    println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, hoge.summarize_author());
}
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">main</span>() {
    <span style="color:#66d9ef">let</span> tweet <span style="color:#f92672">=</span> Tweet {
        username: String::from(<span style="color:#e6db74">&#34;horse_ebooks&#34;</span>),
        content: String::from(<span style="color:#e6db74">&#34;of course, as you probably already know, people&#34;</span>),
        reply: <span style="color:#a6e22e">false</span>,
        retweet: <span style="color:#a6e22e">false</span>,
    };
    println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, tweet.summarize_author());
    summary(<span style="color:#f92672">&amp;</span>tweet);
    println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, Summary::summarize_author(<span style="color:#f92672">&amp;</span>tweet));
}
</code></pre></div><h3 id="ライフタイムで参照を有効化する">ライフタイムで参照を有効化する</h3>
<p>言われてみればそうですが、プログラマが色々考えないとまぁ、行けないんですねという感想。</p>
<p>ただ、借用チェッカーが賢くやってくれるおかげで、全てにライフタイム注釈をつけなくて良くなっているというのがわかりました。
逆に言うと、なんとなくRustを書き始めてしまったので、それを知らずに書いたせいで、コンパイラに怒られてても「?」となっていたのかと。。。</p>
<p>疑問点がいくつかあって、</p>
<ul>
<li>通常はどんなライフタイム注釈をみんな書いてるんだろう?<code>'a</code>とかざっくりしすぎてる?</li>
<li>1つのメソッド、関数にライフタイム注釈が大量に出てくるような書き方をした場合は設計がおかしいのでは?って考えたほうがいいのかも?</li>
<li>ジェネリックな型とライフタイム引数の順序を入れ替えてみても動くだろ?とおもって入れ替えてみたら怒られた。</li>
</ul>
<p>あとは、構造体+ジェネリックが絡んできたら少しこんがらがってきそうっという感じです。
まぁ、これから先は実際に書いてみないことにはわからないんだろうなと。</p>
<h2 id="まとめ">まとめ</h2>
<p>読みました。
実際にはプログラムを書きながら慣れていく感じだろうなぁと。
まだまだ、あれ?ジェネリックってどう書くんだっけ?とか、ライフタイム注釈どうやって付けて、使うときはどうすんだ?みたいになりながら、
出てくるサンプルを少し変えてみてはどうやって動くんだろうこの場合?みたいなことをやってました。
次は、11章、12章を少しだけ自習しつつ、13章に入る予定です(知り合いと一緒に読みすすめてる)。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Azure Cognitive Searchのリクエストのロギング</title>
      <link>https://blog.johtani.info/blog/2020/05/26/logging-azure-search-request/</link>
      <pubDate>Tue, 26 May 2020 17:22:47 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/26/logging-azure-search-request/</guid>
      <description>今回はAzure Cognitive SearchのSDKを利用したアプリケーションが、実際にAzure Cognitive Searchに対して送信しているリクエストのパラメータ</description>
      <content:encoded><p>今回はAzure Cognitive SearchのSDKを利用したアプリケーションが、実際にAzure Cognitive Searchに対して送信しているリクエストのパラメータとボディをログに保存する方法について紹介します。</p>
<h2 id="動機">動機</h2>
<p>アプリケーションでリクエストを組み立てて、SDK経由で送信していると、最終的にAzure Cognitive Searchに対して送信されているリクエストのパラメータや検索条件などをひと目で見たいことがあります。
アプリケーションでは、ソート条件や、クエリ文字列の組み立てなどの処理は異なる場所で行われたりしますので。</p>
<p>また、公式リファレンスでは、機能の説明はRest APIの使い方と組み合わせで説明されることが多いです。</p>
<p>ということで、SDKを利用しているアプリからAzure Searchへ送信されているリクエストをログに保存する方法を調べてみました。</p>
<h2 id="方法">方法</h2>
<p>調べてみるといくつかの手段を取ることができそうだとわかりました。実際に調べて実装する方法を4種類ほど試してみたのでブログに残しておきます。なお、2020年4月時点でのSDKとAzureの仕組みに基づいたブログになります。最新版ではお手軽な方法があるかもしれません。</p>
<ol>
<li>DelegatingHandlerを利用する</li>
<li>ServiceClientTracingの機能を利用する</li>
<li>Azure Application Insightsを活用する</li>
<li>Azure Cognitive Searchのコンソールにある診断情報の機能を利用する</li>
</ol>
<p>1、2はAzure Cognitive SearchのSDKのリファレンスから当たりを付けて見つけた方法です。
3はApplication Insights、4はAzure Cognitive Searchの機能になります。
これらの方法について個別に説明していきます。</p>
<h3 id="1-delegatinghandleを利用する">1. DelegatingHandleを利用する</h3>
<p>まずは、SDKでロギングの機能がないかを調べて見つけた機能がこちらです。
SDKの<a href="https://docs.microsoft.com/ja-jp/dotnet/api/microsoft.azure.search.searchserviceclient.-ctor?view=azure-dotnet#Microsoft_Azure_Search_SearchServiceClient__ctor_System_String_Microsoft_Azure_Search_SearchCredentials_System_Net_Http_HttpClientHandler_System_Net_Http_DelegatingHandler___">SearchServiceClient</a>のコンストラクタの引数に<a href="https://docs.microsoft.com/en-us/dotnet/api/system.net.http.delegatinghandler?view=netframework-4.8">DelegatingHandler</a>というものが渡せることを発見しました。</p>
<p>これは、.NET FrameworkのHttpのAPIに存在するクラスで、HTTPのクライアントがHTTPの送受信時に、処理を挟むことができる機能です。フレームワーク側で、<a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.http.logging.logginghttpmessagehandler?view=dotnet-plat-ext-3.1&amp;viewFallbackFrom=netframework-4.8">LoggingHttpMessaggeHandler</a>というクラスを用意してくれていましたが、残念ながらこちらは、リクエストとレスポンスのヘッダのみをロギングするクラスでした。
ということで、リクエストボディをログに出力したい場合は独自に拡張してやる必要があります。なお、ロギングには<code>Microsoft.Extensions.Logging</code>の<code>ILogger</code>を使用します。</p>
<p>また、Azure Cognitive SearchのSDK側に違う問題点もありました。<a href="https://docs.microsoft.com/ja-jp/azure/search/tutorial-csharp-create-first-app#add-the-runqueryasync-method">チュートリアルにあるように</a>、検索するときには、SDKは次のような使い方を想定しています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C#" data-lang="C#">        <span style="color:#75715e">// Create a service and index client.
</span><span style="color:#75715e"></span>        _serviceClient = <span style="color:#66d9ef">new</span> SearchServiceClient(searchServiceName, <span style="color:#66d9ef">new</span> SearchCredentials(queryApiKey));
        _indexClient = _serviceClient.Indexes.GetClient(<span style="color:#e6db74">&#34;hotels&#34;</span>);
</code></pre></div><p>インデックス用のクライアントを取得するために、<code>GetClient(インデックス名)</code>というメソッドを使用します。この<a href="https://docs.microsoft.com/ja-jp/dotnet/api/microsoft.azure.search.indexesgetclientextensions.getclient?view=azure-dotnet#Microsoft_Azure_Search_IndexesGetClientExtensions_GetClient_Microsoft_Azure_Search_IIndexesOperations_System_String_"><code>GetClient</code>メソッドのバリエーション</a>として、DelegatingHandlerを受け取るメソッドがないのです。。。</p>
<p>ということで、DelegationHandlerを活用する方法としては、以下の2つを実装する必要があります。</p>
<ol>
<li>CustomなLoggingHttpMessageHandlerクラスを実装</li>
<li>GetClientと同等の処理をDelegatingHandlerを引数にしたものを実装する</li>
</ol>
<p>以上の2つを実装し、アプリケーション側から2で作成したGetClientを呼び出すことで、リクエストをボディも含めてログ出力することが可能になります。以下は実装例です。</p>
<p>CustomHttpMessageHandlerクラス。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C#" data-lang="C#"><span style="color:#66d9ef">using</span> System;
<span style="color:#66d9ef">using</span> System.Collections.Generic;
<span style="color:#66d9ef">using</span> System.Diagnostics;
<span style="color:#66d9ef">using</span> System.Net.Http;
<span style="color:#66d9ef">using</span> System.Net.Http.Headers;
<span style="color:#66d9ef">using</span> System.Text;
<span style="color:#66d9ef">using</span> System.Threading;
<span style="color:#66d9ef">using</span> System.Threading.Tasks;
<span style="color:#66d9ef">using</span> Microsoft.Extensions.Logging;

<span style="color:#66d9ef">namespace</span> AzureSearchSample
{
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CustomLoggingHttpMessageHandler</span> : DelegatingHandler
    {
        <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">readonly</span> ILogger _logger;
        <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">readonly</span> <span style="color:#66d9ef">bool</span> _logContent = <span style="color:#66d9ef">false</span>;

        <span style="color:#66d9ef">public</span> CustomLoggingHttpMessageHandler(ILogger logger, <span style="color:#66d9ef">bool</span> logContent)
        {
            <span style="color:#66d9ef">if</span> (logger == <span style="color:#66d9ef">null</span>)
            {
                <span style="color:#66d9ef">throw</span> <span style="color:#66d9ef">new</span> ArgumentNullException(nameof(logger));
            }

            <span style="color:#66d9ef">this</span>._logger = logger;
            <span style="color:#66d9ef">this</span>._logContent = logContent;
        }


        <span style="color:#66d9ef">protected</span> <span style="color:#66d9ef">override</span> <span style="color:#66d9ef">async</span> Task&lt;HttpResponseMessage&gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)
        {
            <span style="color:#66d9ef">if</span> (request == <span style="color:#66d9ef">null</span>)
            {
                <span style="color:#66d9ef">throw</span> <span style="color:#66d9ef">new</span> ArgumentNullException(nameof(request));
            } 
            <span style="color:#66d9ef">await</span> Log.RequestStart(<span style="color:#66d9ef">this</span>._logger, request, <span style="color:#66d9ef">this</span>._logContent);
            <span style="color:#66d9ef">var</span> stopwatch = <span style="color:#66d9ef">new</span> Stopwatch();
            HttpResponseMessage response = <span style="color:#66d9ef">await</span> <span style="color:#66d9ef">base</span>.SendAsync(request, cancellationToken).ConfigureAwait(<span style="color:#66d9ef">false</span>);
            stopwatch.Stop();
            <span style="color:#66d9ef">await</span> Log.RequestEnd(<span style="color:#66d9ef">this</span>._logger, response, stopwatch.Elapsed);

            <span style="color:#66d9ef">return</span> response;
        }

        <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Log</span>
        {
            <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EventIds</span>
            {
                <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">readonly</span> EventId RequestStart = <span style="color:#66d9ef">new</span> EventId(<span style="color:#ae81ff">100</span>, <span style="color:#e6db74">&#34;RequestStart&#34;</span>);
                <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">readonly</span> EventId RequestEnd = <span style="color:#66d9ef">new</span> EventId(<span style="color:#ae81ff">101</span>, <span style="color:#e6db74">&#34;RequestEnd&#34;</span>);
            }

            <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">async</span> Task RequestStart(ILogger logger, HttpRequestMessage request, <span style="color:#66d9ef">bool</span> logContent)
            {
                StringBuilder message = <span style="color:#66d9ef">new</span> StringBuilder();
                message.AppendLine(<span style="color:#e6db74">$&#34;Sending HTTP request {request.Method} {request.RequestUri}&#34;</span>);
                <span style="color:#66d9ef">if</span> (logger.IsEnabled(LogLevel.Trace))
                {
                    LogHttpHeaders(message, request.Headers);
                    <span style="color:#66d9ef">await</span> LogHttpContent(message, request.Content, logContent);
                    logger.Log(
                        LogLevel.Trace,
                        EventIds.RequestStart,
                        message,
                        <span style="color:#66d9ef">null</span>,
                        (state, ex) =&gt; state.ToString());
                }
            }

            <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">async</span> Task RequestEnd(ILogger logger, HttpResponseMessage response, TimeSpan duration)
            {
                StringBuilder message = <span style="color:#66d9ef">new</span> StringBuilder();
                message.AppendLine(
                    <span style="color:#e6db74">$&#34;Recieving HTTP response after {duration.TotalMilliseconds}ms - {response.StatusCode}&#34;</span>);
                <span style="color:#66d9ef">if</span> (logger.IsEnabled(LogLevel.Trace))
                {
                    LogHttpHeaders(message, response.Headers);
                    <span style="color:#66d9ef">await</span> LogHttpContent(message, response.Content, <span style="color:#66d9ef">false</span>);
                    logger.Log(
                        LogLevel.Trace,
                        EventIds.RequestEnd,
                        message,
                        <span style="color:#66d9ef">null</span>,
                        (state, ex) =&gt; state.ToString()
                        );
                }
            }

            <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">void</span> LogHttpHeaders(StringBuilder message, HttpHeaders headers)
            {
                <span style="color:#66d9ef">if</span> (headers == <span style="color:#66d9ef">null</span>) <span style="color:#66d9ef">throw</span> <span style="color:#66d9ef">new</span> ArgumentNullException(nameof(headers));
                <span style="color:#66d9ef">foreach</span> (KeyValuePair&lt;<span style="color:#66d9ef">string</span>, IEnumerable&lt;<span style="color:#66d9ef">string</span>&gt;&gt; header <span style="color:#66d9ef">in</span> headers)
                {
                    <span style="color:#66d9ef">foreach</span> (<span style="color:#66d9ef">string</span> <span style="color:#66d9ef">value</span> <span style="color:#66d9ef">in</span> header.Value)
                    {
                        message.AppendLine(<span style="color:#e6db74">$&#34;{header.Key}: {value}&#34;</span>);
                    }
                }
            }

            <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">async</span> Task LogHttpContent(StringBuilder message, HttpContent content,
                <span style="color:#66d9ef">bool</span> logContent)
            {
                <span style="color:#66d9ef">if</span> (content != <span style="color:#66d9ef">null</span>)
                {
                    <span style="color:#66d9ef">foreach</span> (KeyValuePair&lt;<span style="color:#66d9ef">string</span>,IEnumerable&lt;<span style="color:#66d9ef">string</span>&gt;&gt; header <span style="color:#66d9ef">in</span> content.Headers)
                    {
                        <span style="color:#66d9ef">foreach</span> (<span style="color:#66d9ef">string</span> <span style="color:#66d9ef">value</span> <span style="color:#66d9ef">in</span> header.Value)
                        {
                            message.AppendLine(<span style="color:#e6db74">$&#34;{header.Key}: {value}&#34;</span>);
                        }
                    }

                    <span style="color:#66d9ef">if</span> (logContent)
                    {
                        <span style="color:#66d9ef">string</span> contentBody = <span style="color:#66d9ef">await</span> content.ReadAsStringAsync();
                        message.AppendLine(contentBody);
                    }
                }
            }
        }
    }
}
</code></pre></div><p>GetClientの実装</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C#" data-lang="C#">
        <span style="color:#66d9ef">private</span> ISearchIndexClient GetClient(<span style="color:#66d9ef">string</span> indexName, <span style="color:#66d9ef">params</span> DelegatingHandler[] handlers)
        {
            <span style="color:#66d9ef">var</span> rootHandler = _searchServiceClient.HttpMessageHandlers.OfType&lt;HttpClientHandler&gt;().SingleOrDefault();
            <span style="color:#66d9ef">var</span> indexClient =
                <span style="color:#66d9ef">new</span> SearchIndexClient(_searchServiceClient.SearchServiceName, indexName,
                    _searchServiceClient.SearchCredentials, rootHandler, handlers)
                {
                    SearchDnsSuffix = _searchServiceClient.SearchDnsSuffix
                };

            indexClient.HttpClient.Timeout = _searchServiceClient.HttpClient.Timeout;
            <span style="color:#66d9ef">return</span> indexClient;
        }
</code></pre></div><p>出力されるログ例</p>
<pre><code>2020/04/14 19:17:53.591|TRACE|Sending HTTP request POST https://サービス名.search.windows.net/indexes('インデックス名')/docs/search.post.search?api-version=2019-05-06
client-request-id: be02140f-3a07-48cc-b018-d8aa5e819bc3
Accept-Language: en-US
Accept: application/json; odata.metadata=none
api-key: APIキー
User-Agent: FxVersion/4.700.20.11803
User-Agent: OSName/MacOs
User-Agent: OSVersion/Darwin.19.4.0.Darwin.Kernel.Version.19.4.0.Wed.Mar.4.22.28.40.PST.2020.root.xnu.6153.101.6.15RELEASE.X86.64
User-Agent: Microsoft.Azure.Search.SearchIndexClient/10.100.19.52907
Content-Type: application/json; charset=utf-8
{
  &quot;count&quot;: false,
  &quot;facets&quot;: [],
  &quot;queryType&quot;: &quot;simple&quot;,
  &quot;scoringParameters&quot;: [],
  &quot;search&quot;: &quot;azure&quot;,
  &quot;searchMode&quot;: &quot;any&quot;
}
 |AzureSearchSample.SearchService|EventId_Id=100, EventId_Name=RequestStart, EventId=RequestStart
</code></pre><h4 id="メリットデメリット">メリット、デメリット</h4>
<p>Azure Cognitive Searchの検索の処理だけを対象にリクエストのログを出力することが可能です。また、影響範囲はアプリケーションだけに閉じていますので、デバッグ目的などでログ出力したい場合に、自分だけの手元でログの確認が可能になります。</p>
<p>デメリットとしては、独自に実装しなければいけない範囲が広いことです。</p>
<h3 id="2-serviceclienttracingの機能を利用する">2. ServiceClientTracingの機能を利用する</h3>
<p><code>Microsoft.Rest.ClientRuntime</code>というライブラリをAzure Cognitive Searchは利用しています。
このライブラリに<a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.rest.serviceclienttracing?view=azure-dotnet"><code>ServiceClientTracing</code></a>というクラスが存在します。
なにやら、クライアントの処理のトレースができそうです。</p>
<p>Azure Cognitive SearchのSDKの実装がGitHubに公開されており、検索リクエストの処理を投げる直前に、このトレースの仕組がONになっていると、<a href="https://github.com/Azure/azure-sdk-for-net/blob/a080ceda8df5e397aadd4b7c48c83431c5203b59/sdk/search/Microsoft.Azure.Search.Data/src/Generated/DocumentsOperations.cs#L798"><code>ServiceClientTracing.SendRequest</code>メソッドを呼び出していました</a>。</p>
<p>実際に<code>SendRequest</code>メソッドに送られたものに対して何かしらの処理を行うのは、<a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.rest.iserviceclienttracinginterceptor?view=azure-dotnet"><code>IServiceClientTracingIntercepter</code></a>インターフェースを実装したクラスになります。
このインターフェースの実装が<a href="https://www.nuget.org/packages/Microsoft.Rest.ClientRuntime.Log4Net/">Log4Net</a>に存在します。Log4Netを利用している場合は、これを活用すれば楽ができます。</p>
<p>実際に<code>ServiceClientTracing</code>を有効にするには、以下の2行を呼び出すだけです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C#" data-lang="C#">ServiceClientTracing.IsEnabled = <span style="color:#66d9ef">true</span>;
ServiceClientTracing.AddTracingInterceptor(<span style="color:#66d9ef">new</span> Log4NetTracingInterceptor());
</code></pre></div><p>あとは、Rest.ClientRuntimeがよしなにやってくれます。
1とは異なり、トレースログなので、リクエストのボディについても出力してくれます。</p>
<pre><code>2020-05-26 19:09:04,058 [1] DEBUG Microsoft.Rest.Tracing.Log4Net.Log4NetTracingInterceptor [(null)] - invocationId: 1
instance: Microsoft.Azure.Search.DocumentsProxyOperations
method: SearchPost
parameters: {searchRequest=Microsoft.Azure.Search.Models.SearchRequest,clientRequestId=,cancellationToken=System.Threading.CancellationToken}
2020-05-26 19:09:04,164 [1] DEBUG Microsoft.Rest.Tracing.Log4Net.Log4NetTracingInterceptor [(null)] - invocationId: 1
request: Method: POST, RequestUri: 'https://サービス名.search.windows.net/indexes('インデックス名')/docs/search.post.search?api-version=2019-05-06', Version: 2.0, Content: System.Net.Http.StringContent, Headers:
{
  client-request-id: 591cb14f-e5c2-4a85-977d-01d1f6431ddc
  Accept-Language: en-US
  Accept: application/json; odata.metadata=none
  api-key: APIキー
  Content-Type: application/json; charset=utf-8
}

Body:
{
{
  &quot;count&quot;: false,
  &quot;facets&quot;: [],
  &quot;queryType&quot;: &quot;simple&quot;,
  &quot;scoringParameters&quot;: [],
  &quot;search&quot;: &quot;azure&quot;,
  &quot;searchMode&quot;: &quot;any&quot;
}
}

2020-05-26 19:09:04,459 [Thread Pool Worker] DEBUG Microsoft.Rest.Tracing.Log4Net.Log4NetTracingInterceptor [(null)] - invocationId: 1
response: StatusCode: 200, ReasonPhrase: 'OK', Version: 1.1, Content: System.Net.Http.StreamContent, Headers:
{
  Cache-Control: no-cache
  Pragma: no-cache
  request-id: 591cb14f-e5c2-4a85-977d-01d1f6431ddc
  elapsed-time: 72
  OData-Version: 4.0
  Preference-Applied: odata.include-annotations=&quot;*&quot;
  Strict-Transport-Security: max-age=15724800; includeSubDomains
  Date: Tue, 26 May 2020 10:09:04 GMT
  Content-Type: application/json; odata.metadata=none
  Expires: -1
  Content-Length: 376
}
</code></pre><h4 id="メリットデメリット-1">メリット、デメリット</h4>
<p>Log4Netを利用しているアプリの場合、2行だけを追加することで実装が完了するのがお手軽な点です。</p>
<p>難点としては、Rest Client全てにたいしてトレース処理が入ってしまうので、Azure Cognitive Search以外にもRestクライアントを利用しているものが存在した場合、ログの量が増えてしまいます。また、検索以外の処理でもトレースされてしまうのもデメリットになります。</p>
<p>Log4Net以外のログ機構を使用している場合は、自分で<code>IServiceClientTracingInterceptor</code>を実装する必要も出てきます(<a href="https://stackoverflow.com/questions/46943669/microsoft-rest-serviceclienttracing-how-to-output-tracing-results-to-console-o">参考:StackOverflow</a>)。</p>
<h3 id="3-azure-application-insightsを活用する">3. Azure Application Insightsを活用する</h3>
<p>ここから紹介する3と4については、ログの出力先がAzure上になります。</p>
<p><a href="https://docs.microsoft.com/ja-jp/azure/azure-monitor/app/app-insights-overview">AzureのApplication Insights</a>を利用する方法です。
Azure?.NET?のアプリケーションパフォーマンスモニタリングのサービスです。</p>
<p>Application Insightsを自分のアプリケーションに設定することで、アプリケーションのパフォーマンス監視に関する情報がAzure上のApplication Insightsリソースに送信されるようになります。</p>
<p>ただ、Application Insightsのデフォルトの機能では、URL程度の情報だけが送信されます(<a href="https://blog.shibayan.jp/entry/20190405/1554459340">参考:しばやんさんのブログ</a>)</p>
<p>こちらも拡張機能が用意されており、ITelemetryInitializerのインターフェースを実装したクラスを用意することで、独自の情報をApplication Insightsに出力することが可能となります。詳細については<a href="https://blog.shibayan.jp/entry/20190405/1554459340">しばやんさんのブログ</a>を参考にしてもらうのが良いかと。</p>
<p>Httpリクエストを出力する実装例は次のとおりです。ただ、ちょっとうまく行かないパターンがあったので、コメントアウトとして残してあったりします(なんでだろう?)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C#" data-lang="C#"><span style="color:#66d9ef">using</span> System.Collections.Generic;
<span style="color:#66d9ef">using</span> System.Net.Http;
<span style="color:#66d9ef">using</span> System.Net.Http.Headers;
<span style="color:#66d9ef">using</span> Microsoft.ApplicationInsights.Channel;
<span style="color:#66d9ef">using</span> Microsoft.ApplicationInsights.DataContracts;
<span style="color:#66d9ef">using</span> Microsoft.ApplicationInsights.Extensibility;

<span style="color:#66d9ef">namespace</span> AzureSearchWebSample.ApplicationInsights
{
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">HttpRequestInitializer</span> : ITelemetryInitializer
    {
        <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> Initialize(ITelemetry telemetry)
        {

            <span style="color:#66d9ef">if</span> (!(telemetry <span style="color:#66d9ef">is</span> DependencyTelemetry dependency))
            {
                <span style="color:#66d9ef">return</span>;
            }

            HttpRequestMessage requestMessage = <span style="color:#66d9ef">null</span>;
            HttpRequestHeaders requestHeaders;
            <span style="color:#66d9ef">if</span> (dependency.TryGetOperationDetail(<span style="color:#e6db74">&#34;HttpRequest&#34;</span>, <span style="color:#66d9ef">out</span> <span style="color:#66d9ef">var</span> details) &amp;&amp;
                details <span style="color:#66d9ef">is</span> HttpRequestMessage request)
            {
                requestMessage = request;
                requestHeaders = request.Headers;            
                <span style="color:#66d9ef">if</span> (requestMessage.Method == HttpMethod.Post)
                {
                    <span style="color:#66d9ef">string</span> contentBody = requestMessage.Content.ReadAsStringAsync().GetAwaiter().GetResult();
                    dependency.Properties.Add(<span style="color:#e6db74">&#34;RequestBody&#34;</span>,contentBody);
                }
            }
            <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">if</span> (dependency.TryGetOperationDetail(<span style="color:#e6db74">&#34;HttpRequestHeaders&#34;</span>, <span style="color:#66d9ef">out</span> details) &amp;&amp;
                     details <span style="color:#66d9ef">is</span> HttpRequestHeaders headers)
            {
                requestHeaders = headers;
            }
            <span style="color:#66d9ef">else</span>
            {
                <span style="color:#66d9ef">return</span>;
            }

            <span style="color:#66d9ef">foreach</span> (KeyValuePair&lt;<span style="color:#66d9ef">string</span>,IEnumerable&lt;<span style="color:#66d9ef">string</span>&gt;&gt; header <span style="color:#66d9ef">in</span> requestHeaders)
            {
                <span style="color:#66d9ef">foreach</span> (<span style="color:#66d9ef">string</span> <span style="color:#66d9ef">value</span> <span style="color:#66d9ef">in</span> header.Value)
                {
                    dependency.Properties.Add(header.Key, <span style="color:#66d9ef">value</span>);
                }
            }
            <span style="color:#75715e">//この実装の場合は出力されなかった。なぜ?
</span><span style="color:#75715e"></span>            <span style="color:#75715e">// if (requestMessage != null &amp;&amp; requestMessage.Method == HttpMethod.Post)
</span><span style="color:#75715e"></span>            <span style="color:#75715e">// {
</span><span style="color:#75715e"></span>            <span style="color:#75715e">//     string contentBody = requestMessage.Content.ReadAsStringAsync().GetAwaiter().GetResult();
</span><span style="color:#75715e"></span>            <span style="color:#75715e">//     dependency.Properties.Add(&#34;RequestBody&#34;,contentBody);
</span><span style="color:#75715e"></span>            <span style="color:#75715e">// }
</span><span style="color:#75715e"></span>        }
    }
}
</code></pre></div><h4 id="メリットデメリット-2">メリット、デメリット</h4>
<p>すでにApplication Insightsを利用している場合はついでに情報が出力されるので便利です。かつ、常にリクエストログを見れるようにしておく場合はとても便利だと思います。</p>
<p>Application Insightsを利用していない場合は、そこから導入しなければならなくなるので、手間が増えるかもしれないです。</p>
<h3 id="4-azure-cognitive-searchのコンソールにある診断情報の機能を利用する">4. Azure Cognitive Searchのコンソールにある診断情報の機能を利用する。</h3>
<p>最後は<a href="https://docs.microsoft.com/ja-jp/azure/search/search-monitor-logs">Azure Cognitive Searchの診断ログ</a>を有効にする方法です。
ここまで説明してきた方法の中で、一番お手軽な方法です。。。</p>
<p>これまでは、リクエストボディを出力する方法を考えていましたが、Azure Cognitive Search側の診断ログを有効にすると、リクエストボディで送信したものが、<a href="https://docs.microsoft.com/ja-jp/azure/search/search-monitor-logs#properties-schema">Azure Cognitive Search側で、クエリパラーメータとして、診断ログに出力</a>されます(診断ログの<code>Query_s</code>)。</p>
<p>あとは、<a href="https://docs.microsoft.com/ja-jp/azure/search/search-monitor-logs#query-log-information">Azureのコンソール</a>で当該時間のログを見ればよいだけです。以下は出力されたログの一部です。Description_sにはURLのパスが記載されています。</p>
<p>診断ログ例(一部)</p>
<pre><code>Description_s      POST /indexes('multi-field-test')/docs/search.post.search
Query_s            ?api-version=2019-05-06&amp;searchMode=Any&amp;search=azure&amp;queryType=Simple&amp;$count=false
</code></pre><h4 id="メリットデメリット-3">メリット、デメリット</h4>
<p>アプリケーション側に手を入れる必要がなのでお手軽です。
一度設定しておけばコンソール側でログをいつでも見れるので便利です。</p>
<p>リクエスト量が多くなってしまうと、ログの量も多くなり、費用がかさむ恐れがあります。また、複数の人が触る環境の場合は自分で送信したリクエストがどれだったのか?といった状況に陥る可能性はあります。</p>
<h2 id="その他は">その他は?</h2>
<p>Azureに対してではないですが、昔似たようなことをやるときにやっていた方法として、ローカルにプロキシサーバーを起動し、そのプロキシサーバー経由でアプリケーションから、Azureに接続することで、リクエストを保存する方法もあります。
ざんねんながら、未調査ですがアプリなどにはそれほど手を入れる必要はないかと思います。</p>
<h2 id="まとめ">まとめ</h2>
<p>ちょっと送信リクエストの内容が見てみたいという話でしたが、いろいろな手段が存在しました。
自分の状況、環境に合わせて手段を選択肢てみるのがいいかと思います。
まずは、簡単な診断ログあたりからでしょうか?</p>
</content:encoded>
    </item>
    
    <item>
      <title>Microsoft Build(2020)のAzure Cognirive Searchのセッションを見たのでメモ</title>
      <link>https://blog.johtani.info/blog/2020/05/22/watching-azure-cognirive-search-session-at-ms-build/</link>
      <pubDate>Fri, 22 May 2020 12:10:41 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/22/watching-azure-cognirive-search-session-at-ms-build/</guid>
      <description>Mircosoft Buildというイベントが今週ありました(MSの方やお客さんに教えてもらった)。 そこで、Azure Cognitive Searchのセッション(MyBuil</description>
      <content:encoded><p><a href="https://mybuild.microsoft.com/">Mircosoft Buildというイベント</a>が今週ありました(MSの方やお客さんに教えてもらった)。</p>
<p>そこで、Azure Cognitive Searchのセッション(<a href="https://mybuild.microsoft.com/sessions/391f3d09-50b4-476a-9acf-4fd2d4927a9e">MyBuild - Cognitive Search： The pocket-knife for knowledge mining</a>)があったので、見てみました。
内容がどんなものかをメモっておきます。
最初はCognitive Searchがどんなものよという説明でした。</p>
<h2 id="ビルトインスキルの拡充の話">ビルトインスキルの拡充の話</h2>
<p>データソースからデータを取り出し、エンリッチし、検索エンジンに保存するという、<a href="https://docs.microsoft.com/ja-jp/azure/search/cognitive-search-concept-intro">パイプラインが組めるようになっています</a>。</p>
<p>このパイプラインで利用できる処理のことがスキルと呼ばれています。ここで利用できる<a href="https://docs.microsoft.com/ja-jp/azure/search/cognitive-search-predefined-skills">ビルトインスキル</a>が拡充されますよという話でした。ちょっとだけ抜き出すと以下のとおりです。</p>
<ul>
<li>Azure Machine Learning</li>
<li><a href="https://docs.microsoft.com/ja-jp/azure/search/cognitive-search-skill-text-translation">Text translation</a></li>
<li>Brand detection</li>
<li>Object detection</li>
</ul>
<p><a href="https://docs.microsoft.com/ja-jp/azure/search/cognitive-search-predefined-skills">スキルのリファレンス</a>には載ってるものと載ってないものがあるので、今後追加されてくのかな?
Brand detectionがどんなものなんだろう?ってのがちょっと気になりました。どっかにデモとかあるかなぁ?</p>
<h2 id="スキルセットのための新機能--debug-sessionのデモ">スキルセットのための新機能 : Debug Sessionのデモ</h2>
<p>上記のスキルを組み合わせてパイプラインを組んで、データソースから取り出したデータをエンリッチしてから、検索エンジンに入れる処理をかけるのですが、その処理のデバッグ用に新しいGUIの機能が追加されてますよという紹介とデモでした。</p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/search/cognitive-search-tutorial-debug-sessions#start-your-debug-session">Tutorial： Use Debug sessions to diagnose, fix, and commit changes to your skillset - Azure Cognitive Search | Microsoft Docs</a></li>
</ul>
<h2 id="manage-identityの話">Manage Identityの話</h2>
<p>Azure Cognitive Searchにデータを登録するパイプランの最初の段階で、各種データソースにアクセスが必要です。
このアクセス時にコネクションの設定にアカウントキーなども含めてましたが、これをコネクション設定ではなく、専用の管理機能が用意されましたよという話でした。</p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/search/search-howto-managed-identities-data-sources">Set up a connection to a data source using a managed identity (preview) - Azure Cognitive Search | Microsoft Docs</a></li>
</ul>
<h2 id="qa">QA</h2>
<ul>
<li>Similarityとかの話
<ul>
<li><a href="https://docs.microsoft.com/ja-jp/azure/search/index-similarity-and-scoring#similarity-ranking-algorithms">BM25になってるよ</a>とか</li>
</ul>
</li>
<li>SDKの話とか</li>
<li>ほかにRelevancyの話
<ul>
<li>Analyzerをデフォルトのままじゃなくてちゃんと考えて使いましょう(例:<a href="https://docs.microsoft.com/ja-jp/azure/search/index-add-language-analyzers#language-analyzer-list">各言語用のAnalyzerがいっぱいあるよ</a>とか)</li>
</ul>
</li>
</ul>
<p>こんな感じでした。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elastic Stack 7.7がリリースされた</title>
      <link>https://blog.johtani.info/blog/2020/05/15/elastic-stack-7_7/</link>
      <pubDate>Fri, 15 May 2020 11:17:17 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/15/elastic-stack-7_7/</guid>
      <description>ElasticのWorkplace Searchを触ってみています(その1、その2)が、Elastic Stackの7.7.0がリリースされてし</description>
      <content:encoded><p>ElasticのWorkplace Searchを触ってみています(<a href="https://blog.johtani.info/blog/2020/05/01/intro-workplace-search/">その1</a>、<a href="https://blog.johtani.info/blog/2020/05/07/install-workplace-search/">その2</a>)が、Elastic Stackの7.7.0がリリースされてしまいました。</p>
<p>簡単ですが、リリースブログを見ながら気になった点をメモしとこうかと。</p>
<h2 id="本家のリリースブログ">本家のリリースブログ</h2>
<p>いっぱいあるんですよ、これが。まぁ、内容かぶってるのもあるんですが。</p>
<h3 id="elastic-stack">Elastic Stack</h3>
<ul>
<li><a href="https://www.elastic.co/jp/blog/elastic-stack-7-7-0-released">Elastic Stack 7.7.0をリリース</a></li>
<li><a href="https://www.elastic.co/blog/elasticsearch-7-7-0-released">Elasticsearch 7.7.0 released</a></li>
<li><a href="https://www.elastic.co/blog/kibana-7-7-0-released">Kibana 7.7.0 released</a></li>
<li><a href="https://www.elastic.co/blog/logstash-7-7-0-released">Logstash 7.7.0 released</a></li>
</ul>
<h3 id="enterprise-search系">Enterprise Search系</h3>
<ul>
<li><a href="https://www.elastic.co/blog/elastic-enterprise-search-7-7-0-released">Elastic Enterprise Search 7.7 released, featuring Workplace Search GA | Elastic Blog</a></li>
<li><a href="https://www.elastic.co/blog/elastic-app-search-7-7-0-released">Elastic App Search updates： Scale your way with new configuration options | Elastic Blog</a></li>
<li><a href="https://www.elastic.co/jp/blog/elastic-workplace-search-7-7-0-released">Elastic Workplace Search： 新しい、統合された働き方へ</a></li>
</ul>
<h3 id="observability系">Observability系</h3>
<ul>
<li>
<p><a href="https://www.elastic.co/blog/elastic-observability-7-7-0-released">Elastic Observability 7.7 released | Elastic Blog</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/blog/elastic-apm-7-7-0-released">Elastic APM 7.7.0 released with service maps, async profiler, and more | Elastic Blog</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/blog/elastic-logs-7-7-0-released">Elastic Logs 7.7.0 released with PCF support, MQTT support, and more | Elastic Blog</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/blog/elastic-metrics-7-7-0-released">Elastic Metrics 7.7.0 released with enhanced Prometheus integration and PCF support, | Elastic Blog</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/blog/elastic-uptime-monitoring-7-7-0-released">Elastic Uptime Monitoring 7.7.0 released with alerting, anomaly detection and more | Elastic Blog</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/blog/introducing-the-new-alerting-framework-for-observability-security-and-the-elastic-stack">New alerting framework released for Observability, Security and the Elastic Stack | Elastic Blog</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/blog/elastic-security-7-7-0-released">Introducing Elastic Security 7.7.0 | Elastic Blog</a></p>
</li>
</ul>
<h2 id="個人的に気になったもの">個人的に気になったもの</h2>
<p>で、全部読んだわけではないのですが、個人的に気になった機能についてメモを残しておきます。
多分に検索によっている可能性がありますが、お気になさらず。
また、ブログとドキュメントを元に気になるところをピックアップしてます。
実際の実装とか動作に関してはまだ未調査の段階です。</p>
<h3 id="async-search---elasticsearch">Async Search - Elasticsearch</h3>
<p>ElasticsearchのAPIとして非同期に検索できるAPIが実装されました
(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/async-search.html">公式リファレンス</a>)。
大量データを検索が終了してから検索結果を返すのではなく、クエリは実行しつつわかったところまでは都度取得したいという要望だったり、Coldインデックスのように遅いストレージ上にあるデータへの検索でも少しずつ取得できるようにという具合だと思います。ログとか大量に扱う場合にまぁ、ほしいですよね。今回はEsのリリースがメインで、今後はKibanaなどとの連携だったり色々改善していくよという話みたいです(<a href="https://www.elastic.co/blog/kibana-7-7-0-released">Kibanaの一部の機能は利用しているみたい</a>)。</p>
<h4 id="ライセンスとサブスクリプションのレベルhttpswwwelasticcojpsubscriptions">ライセンスと<a href="https://www.elastic.co/jp/subscriptions">サブスクリプションのレベル</a></h4>
<ul>
<li>Elastic License</li>
<li>Basic</li>
</ul>
<h4 id="気になるところと予想とか">気になるところ(と、予想とか)</h4>
<ul>
<li>仕組みがどうなっているか?
<ul>
<li>Search APIと同等のリクエストパラメータが使える。が、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/async-search.html">いくつか制約はありそう</a></li>
<li>おそらくTask Manageerで検索してる処理が管理され、レスポンスにはIDが発行されて、そのIDで処理の進み具合を取得する感じ。</li>
</ul>
</li>
<li>Shard単位?Segment単位?
<ul>
<li>内部的にはShard単位で検索処理を扱ってるので、Shard単位がとりあえず楽そう?</li>
</ul>
</li>
<li>Aggsとかも?
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/async-search.html#get-async-search">対象の模様</a></li>
</ul>
</li>
<li>Sortは?
<ul>
<li>有効。ソートのフィールドがインデックス対象だった場合は、そのデータの統計値を使ってShardをソートするっぽい(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/async-search.html#submit-async-search">公式リファレンスのNOTE参照</a>)</li>
</ul>
</li>
</ul>
<h4 id="関連しそうなgithubのissuesとか">関連しそうなGitHubのIssuesとか</h4>
<p>まだ読んでない</p>
<ul>
<li><a href="https://github.com/elastic/elasticsearch/issues/49091">Async search · Issue #49091 · elastic/elasticsearch</a></li>
<li><a href="https://github.com/elastic/elasticsearch/pull/49471">Add a listener to track the progress of a search request locally by jimczi · Pull Request #49471 · elastic/elasticsearch</a></li>
<li><a href="https://github.com/elastic/elasticsearch/pull/49092">Pre-sort shards based on the max/min value of the primary sort field by jimczi · Pull Request #49092 · elastic/elasticsearch</a></li>
<li><a href="https://github.com/elastic/elasticsearch/issues/55464">Async search： store intermediate results · Issue #55464 · elastic/elasticsearch</a></li>
</ul>
<h3 id="reduced-consumption-of-heap---elasticsearch">Reduced consumption of heap - Elasticsearch</h3>
<p><a href="https://www.elastic.co/blog/significantly-decrease-your-elasticsearch-heap-memory-usage">ヒープの使用量が7.7で減ってるよというブログ</a>も別途ありました。</p>
<p>元になっているLuceneの実装に関するIssueが「<a href="https://issues.apache.org/jira/browse/LUCENE-8635">［LUCENE-8635］ Lazy loading Lucene FST offheap using mmap - ASF JIRA</a>」です。ヒープ上に展開していたデータをmmapで扱えるようにすることで、ヒープのメモリを削減してる。</p>
<p>で関連して、これがLuceneで操作できるようになって(「<a href="https://github.com/elastic/elasticsearch/pull/42838">Use reader attributes to control term dict memory useage by s1monw · Pull Request #42838 · elastic/elasticsearch</a>」)、そこからEsに取り込まれたと。</p>
<p><code>_id</code>をoff-heapにする話は[ここ](<a href="https://github.com/elastic/elasticsearch/pull/52405">Move the terms index of <code>_id</code> off-heap. by jpountz · Pull Request #52405 · elastic/elasticsearch</a>)にあった。</p>
<p>これに関連して、ヒープサイズの推奨について再考が必要かも?というIssue(<a href="https://github.com/elastic/elasticsearch/issues/52561">Reconsider our recommendations for heap size? · Issue #52561 · elastic/elasticsearch</a>)も上がってる。</p>
<h3 id="painless-lab---kibana">Painless Lab - Kibana</h3>
<p>ほんとにPain<code>less</code>?と言う話はさておき、<a href="https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-execute-api.html">_script APIでexecute</a>はできていましたが、やはりもう少し楽に実行したいですよね?ということで画面ができたみたいです。</p>
<p>シンタックスハイライトとかもできるので、より簡単に使えるようになってますね。
まだ、ベータなので足りないContextとかもあるっぽいけど。</p>
<h4 id="ライセンスとサブスクリプションのレベルhttpswwwelasticcojpsubscriptions-1">ライセンスと<a href="https://www.elastic.co/jp/subscriptions">サブスクリプションのレベル</a></h4>
<ul>
<li>Elastic License</li>
<li>Basic?</li>
</ul>
<p>まだベータだからか、サブスクリプションのページにはなかった。
<a href="https://www.elastic.co/guide/en/kibana/7.7/painlesslab.html">公式ドキュメント</a>と<a href="https://github.com/elastic/kibana/tree/master/x-pack/plugins/painless_lab">GitHubのリポジトリ</a>から、Elastic Licenseであることは判明。</p>
<h4 id="気になるところと予想とか-1">気になるところ(と、予想とか?)</h4>
<ul>
<li>制限事項がどんなものか?
<ul>
<li>使えるContextがまだ少ない?まだGAではない</li>
</ul>
</li>
<li>サジェストがどのくらい効くのか?
<ul>
<li>シンタックスハイライトはあるけど。<a href="https://github.com/elastic/kibana/issues/59962">まだかなぁ?</a></li>
</ul>
</li>
</ul>
<h3 id="その他elasticsearch関連">その他Elasticsearch関連</h3>
<p>7.7リリースのハイライトが別途用意されています。</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/release-highlights-7.7.0.html">7.7.0 release highlights | Elasticsearch Reference ［7.7］ | Elastic</a></li>
</ul>
<p>こっちで気になるのは、ClassificationとかTransformsあたりかなぁ。ML関連の機能で。</p>
<h3 id="workplace-search-ga">Workplace Search GA</h3>
<p>これは別のブログに書くかな。</p>
<h4 id="気になるところ">気になるところ</h4>
<ul>
<li>Betaと何が変わったのか?</li>
<li>APIとかどういう形で提供されるのか?そもそも提供されるのか?</li>
</ul>
<p>どんな機能?みたいなのも書く予定です。</p>
<h3 id="data-visualizerでfilebeatのconfigをサジェスト---kibana-and-filebeat">Data VisualizerでFilebeatのConfigをサジェスト - Kibana and Filebeat</h3>
<p>Kibanaの画面からデータをちょっと?(100MB)だけアップロードして、Elasticsearchにサクッとデータ登録できる機能が実はあります。この機能の拡張として、<a href="https://www.elastic.co/guide/en/kibana/7.7/release-highlights-7.7.0.html#file-uploader-highlights">Filebeatの設定だとこんな感じに作れるよ?というのを提示してくれる</a>ようになったみたいです。
手元にあるファイルを定期的に読み込むときに、設定を書く下地ができるのは便利じゃないかな?</p>
<h4 id="ライセンスとサブスクリプションのレベルhttpswwwelasticcojpsubscriptions-2">ライセンスと<a href="https://www.elastic.co/jp/subscriptions">サブスクリプションのレベル</a></h4>
<ul>
<li>Elastic License</li>
<li>Basic</li>
</ul>
<p><a href="https://www.elastic.co/jp/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer">Data Visualizerってどんなもの?というのはここにブログ</a>があります。残念ながらKibanaとかの公式リファレンスには使い方のページがないんだよなぁ。</p>
<h3 id="apmのサービスマップ---apm-and-kibana">APMのサービスマップ - APM and Kibana</h3>
<p>アプリケーションアーキテクチャのどこでAPMが動作しているか、どこと通信しているかという、APMのエージェントが入っているアプリ間のつながりをKibana上で可視化できる機能っぽいです。まだ、ベータみたいですが、面白そう。<a href="https://docs.microsoft.com/ja-jp/azure/azure-monitor/app/app-map?tabs=net">Azure Application Insightのアプリケーションマップ</a>に似てる気がします。</p>
<h4 id="ライセンスとサブスクリプションのレベルhttpswwwelasticcojpsubscriptions-3">ライセンスと<a href="https://www.elastic.co/jp/subscriptions">サブスクリプションのレベル</a></h4>
<ul>
<li>Elastic License</li>
<li>Platinum</li>
</ul>
<h4 id="気になるところ-1">気になるところ</h4>
<ul>
<li>どうやって作ってる(つながりをどうやて判断してる)のか?
<ul>
<li>分散トレーシングのデータを元に誰がどこと通信してるってのは取れるんじゃなかろうか?</li>
</ul>
</li>
</ul>
<h3 id="log-categorization-and-contextual-examples---kibana-and-elasticsearch">Log categorization and contextual examples - Kibana and Elasticsearch</h3>
<p>7.6から入ってたっぽいですが、<a href="https://www.elastic.co/blog/elastic-logs-7-6-0-released">ログの分類</a>をMLの機能を使ってできるものが、LogsのUIとかでさらに使いやすくなった模様。</p>
<h4 id="ライセンスとサブスクリプションのレベルhttpswwwelasticcojpsubscriptions-4">ライセンスと<a href="https://www.elastic.co/jp/subscriptions">サブスクリプションのレベル</a></h4>
<ul>
<li>Elastic License</li>
<li>Platinum</li>
</ul>
<h4 id="気になるところ-2">気になるところ</h4>
<ul>
<li>7.6の機能から追いかけないとなぁ。。。
<ul>
<li>似たようなログをカテゴリーごとに分析できるようになるので、プラスMLの仕組みでこれまでとは異なる種類のログが出始めたみたいなことが分析できそう。</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>眺めるだけで時間かかりましたが、まぁ、相変わらずいっぱいありますね。
興味のあるところの濃淡が出た感じになりましたが、気になる点をピックアップしてみました。
おかしなところとか、ここはどういう意味?などあればツッコミお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第9章</title>
      <link>https://blog.johtani.info/blog/2020/05/14/chap9-rust-the-book/</link>
      <pubDate>Thu, 14 May 2020 18:43:26 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/14/chap9-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 Rust the book - 第5章 Rust the book - 第6章 Rust the book - 第8章</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
<li><a href="/blog/2020/04/02/chap5-rust-the-book/">Rust the book - 第5章</a></li>
<li><a href="/blog/2020/04/07/chap6-rust-the-book/">Rust the book - 第6章</a></li>
<li><a href="/blog/2020/04/16/chap8-rust-the-book/">Rust the book - 第8章</a></li>
</ul>
<h2 id="第9章">第9章</h2>
<p>エラー処理です。
NLP100とか、いくつかのプログラムを書いていて、なんとなくは扱っていますが、きちんと勉強しないと。</p>
<p>とりあえず、「Rustには例外は存在しません。」が一番知っておくことかな。</p>
<h3 id="panicで回復不能なエラー">panic!で回復不能なエラー</h3>
<ul>
<li><code>panic!</code>マクロでスタックを巻き戻して掃除をして終了。
<ul>
<li>異常終了(<code>panic = 'abort'</code>)にもできる。</li>
</ul>
</li>
<li>「<code>RUST_BACKTRACE</code>を0以外の変数にセットして実行」
*</li>
</ul>
<h3 id="resultで回復可能なエラー">Resultで回復可能なエラー</h3>
<ul>
<li><code>expect()</code>は気持ち悪い名前じゃないかなぁ?</li>
<li><a href="https://doc.rust-jp.rs/book/second-edition/ch09-02-recoverable-errors-with-result.html#a%E3%82%A8%E3%83%A9%E3%83%BC%E3%82%92%E5%A7%94%E8%AD%B2%E3%81%99%E3%82%8B">ここ</a>で<code>io::Error</code>ではないものもエラーが発生する場合には</li>
</ul>
<h3 id="panicすべきかするまいか">panic!すべきかするまいか</h3>
<h2 id="まとめ">まとめ</h2>
<p>「Rustには例外は存在しない」ので、回復不能か可能かを考えつつ処理を書こうと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第2章の12から19まで(言語処理100本ノック2020)</title>
      <link>https://blog.johtani.info/blog/2020/05/12/reboot-nlp100-ch02-12to19/</link>
      <pubDate>Tue, 12 May 2020 12:23:29 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/12/reboot-nlp100-ch02-12to19/</guid>
      <description>Rustで言語処理100本ノックの第2章の残りです。 前回はこちら。 ちなみに、標準入力から受け取る処理は書いてないです。 出力に関してはファイル</description>
      <content:encoded><p>Rustで言語処理100本ノックの第2章の残りです。</p>
<p>前回は<a href="/blog/2020/05/08/rebootnlp100-ch02-10to11">こちら</a>。</p>
<p>ちなみに、標準入力から受け取る処理は書いてないです。
出力に関してはファイル分割、保存と支持があるもの以外は文字列として取り出すところで終わっています。</p>
<h2 id="12-1列目をcol1txtに2列目をcol2txtに保存">12. 1列目をcol1.txtに，2列目をcol2.txtに保存</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">extract_column</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, num: <span style="color:#66d9ef">usize</span>, output_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) {
    <span style="color:#66d9ef">let</span> input_f <span style="color:#f92672">=</span> File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>);
    <span style="color:#66d9ef">let</span> read_buf <span style="color:#f92672">=</span> BufReader::new(input_f);
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> output_f <span style="color:#f92672">=</span> OpenOptions::new()
        .write(<span style="color:#66d9ef">true</span>)
        .create(<span style="color:#66d9ef">true</span>)
        .open(output_file_name)
        .expect(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;can&#39;t open file[{}] with write option&#34;</span>, output_file_name).as_str());
    read_buf.lines().for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> <span style="color:#66d9ef">match</span> line {
        Ok(line) <span style="color:#f92672">=&gt;</span> {
            <span style="color:#66d9ef">let</span> columns: Vec<span style="color:#f92672">&lt;</span>_<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> line.split(<span style="color:#e6db74">&#39;\t&#39;</span>).collect();
            writeln<span style="color:#f92672">!</span>(output_f, <span style="color:#e6db74">&#34;{}&#34;</span>, columns[num]);
            output_f.flush().expect(<span style="color:#e6db74">&#34;Error during flush&#34;</span>);
        }
        Err(_) <span style="color:#f92672">=&gt;</span> panic<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;parse error &#34;</span>),
    });
}
</code></pre></div><p>13で出力結果を利用するので入力として出力ファイル名も受け取るようにしました。
問題としては、1列目と2列目を別々に出力すればいいので、1回の処理で書いても良かったのですが、1回1ファイルの出力という形で実装しました(効率は悪い)。</p>
<p>改行コードあたりを考えるのがめんどくさかったので<code>writeln!</code>マクロでファイルに書き出しています。が、普通に<code>write</code>メソッドで改行コードを追加しても良かったのかなと。</p>
<p>あとは、出力先ファイルが存在しない場合だけ<code>open</code>するように<code>OpenOptions</code>を利用してみています。</p>
<p><code>flush</code>を呼び出すべきなのかどうか?を調べないとな。。。</p>
<h2 id="13-col1txtとcol2txtをマージ">13. col1.txtとcol2.txtをマージ</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">merge_files</span>(col1_file: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, col2_file: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, output_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) {
    <span style="color:#66d9ef">let</span> col1_buf <span style="color:#f92672">=</span> BufReader::new(File::open(col1_file).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    <span style="color:#66d9ef">let</span> col2_buf <span style="color:#f92672">=</span> BufReader::new(File::open(col2_file).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> output_f <span style="color:#f92672">=</span> OpenOptions::new()
        .write(<span style="color:#66d9ef">true</span>)
        .create(<span style="color:#66d9ef">true</span>)
        .open(output_file_name)
        .expect(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;can&#39;t open file[{}] with write option&#34;</span>, output_file_name).as_str());
    col1_buf
        .lines()
        .zip(col2_buf.lines())
        .for_each(<span style="color:#f92672">|</span>(col1, col2)<span style="color:#f92672">|</span> {
            <span style="color:#66d9ef">let</span> col1 <span style="color:#f92672">=</span> col1.expect(<span style="color:#e6db74">&#34;parse error col1&#34;</span>);
            <span style="color:#66d9ef">let</span> col2 <span style="color:#f92672">=</span> col2.expect(<span style="color:#e6db74">&#34;parse error col2&#34;</span>);
            writeln<span style="color:#f92672">!</span>(output_f, <span style="color:#e6db74">&#34;{}\t{}&#34;</span>, col1, col2);
            output_f.flush().expect(<span style="color:#e6db74">&#34;Error during flush&#34;</span>);
        });
}
</code></pre></div><p>2つのファイル名を入力として受け取り、タブでくっつけて出力します。
<code>zip</code>を利用することで、2つのイテレーターを同時に回しています。</p>
<h2 id="14-先頭からn行を出力">14. 先頭からN行を出力</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">head</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, lines: <span style="color:#66d9ef">usize</span>) -&gt; String {
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> head <span style="color:#f92672">=</span> String::new();
    buf.lines().take(lines).for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> {
        head.push_str(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}\n&#34;</span>, line.expect(<span style="color:#e6db74">&#34;parse error&#34;</span>)).as_str());
    });
    <span style="color:#66d9ef">return</span> head;
}
</code></pre></div><p>イテレーターのメソッドに<a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.take"><code>take</code></a>があります。
これを利用することで、引数に指定した数のエレメントが取得できるので、これでheadが実現できます。</p>
<h2 id="15-末尾のn行を出力">15. 末尾のN行を出力</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">tail</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, lines: <span style="color:#66d9ef">usize</span>) -&gt; String {
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> tail <span style="color:#f92672">=</span> String::new();
    <span style="color:#66d9ef">let</span> line_count <span style="color:#f92672">=</span> word_count(input_file_name);
    buf.lines().skip(line_count <span style="color:#f92672">-</span> lines).for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> {
        tail.push_str(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}\n&#34;</span>, line.expect(<span style="color:#e6db74">&#34;parse error&#34;</span>)).as_str());
    });
    <span style="color:#66d9ef">return</span> tail;
}
</code></pre></div><p>tailの場合は少し複雑で、11で作成した行数をカウントするメソッドで総行数を取り出し、そこから引数で指定された行数を引き算した数(=出力しない行数)を、イテレーターの<code>skip</code>メソッドの引数に渡しています。これにより、指定された数のエレメントをスキップしたあとの処理がかけます。</p>
<h2 id="16-ファイルをn分割する">16. ファイルをN分割する</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">split_files</span>(
    input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>,
    num: <span style="color:#66d9ef">usize</span>,
    output_file_prefix: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>,
    output_file_suffix: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>,
) {
    <span style="color:#66d9ef">let</span> total <span style="color:#f92672">=</span> word_count(input_file_name) <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">f64</span>;
    <span style="color:#66d9ef">let</span> lines_in_file <span style="color:#f92672">=</span> total <span style="color:#f92672">/</span> num <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">f64</span>;
    <span style="color:#66d9ef">let</span> lines_in_file <span style="color:#f92672">=</span> lines_in_file.ceil() <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">usize</span>; <span style="color:#75715e">//
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));

    <span style="color:#66d9ef">let</span> output_files: Vec<span style="color:#f92672">&lt;</span>File<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> create_file_vec(output_file_prefix, num, output_file_suffix);

    println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;split file each {} lines.&#34;</span>, lines_in_file);
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> lines <span style="color:#f92672">=</span> buf.lines();

    <span style="color:#66d9ef">for</span> <span style="color:#66d9ef">mut</span> output_f <span style="color:#66d9ef">in</span> output_files {
        <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> current <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>;
        <span style="color:#66d9ef">while</span> current <span style="color:#f92672">&lt;</span> lines_in_file <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span> {
            <span style="color:#66d9ef">let</span> line <span style="color:#f92672">=</span> lines.next();
            <span style="color:#66d9ef">if</span> <span style="color:#66d9ef">let</span> Some(line_rs) <span style="color:#f92672">=</span> line {
                <span style="color:#66d9ef">if</span> <span style="color:#66d9ef">let</span> Ok(line_str) <span style="color:#f92672">=</span> line_rs {
                    writeln<span style="color:#f92672">!</span>(output_f, <span style="color:#e6db74">&#34;{}&#34;</span>, line_str);
                }
            }
            current <span style="color:#f92672">=</span> current <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>;
        }
        output_f.flush().expect(<span style="color:#e6db74">&#34;error during flush&#34;</span>);
    }
}

<span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">create_file_vec</span>(output_file_prefix: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, num: <span style="color:#66d9ef">usize</span>, output_file_suffix: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>File<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> files <span style="color:#f92672">=</span> Vec::with_capacity(num);
    <span style="color:#66d9ef">for</span> i <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">0</span>..num {
        <span style="color:#66d9ef">let</span> output_file_name <span style="color:#f92672">=</span> format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}{}{}&#34;</span>, output_file_prefix, i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, output_file_suffix);
        <span style="color:#66d9ef">let</span> output_f <span style="color:#f92672">=</span> OpenOptions::new()
            .write(<span style="color:#66d9ef">true</span>)
            .create(<span style="color:#66d9ef">true</span>)
            .open(output_file_name.as_str())
            .expect(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;can&#39;t open file[{}] with write option&#34;</span>, output_file_name).as_str());
        files.push(output_f);
    }
    <span style="color:#66d9ef">return</span> files;
}
</code></pre></div><p>ちょっと長いですね。</p>
<p>入力としては、分割するファイル数<code>N</code>が指定されます。まずは、<code>総行数/N</code>で各ファイルに保存されるべき行数を計算します。
次に、2つ目の関数をつかって、必要な数のファイルオブジェクトをベクトルとして生成します。</p>
<p>ファイルオブジェクトのベクトルの要素を元にしたfor文を回しつつ、それぞれのファイルに必要な行数を出力している処理になっています。</p>
<p>総行数が<code>N</code>で割り切れない場合に<code>ceil</code>で切り上げした行数にするというちょっとした処理を入れてあります。</p>
<h2 id="17-１列目の文字列の異なり">17. １列目の文字列の異なり</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">count_uniq_words</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>, col: <span style="color:#66d9ef">usize</span>) -&gt; <span style="color:#66d9ef">usize</span> {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> words <span style="color:#f92672">=</span> HashSet::new();
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    buf.lines().for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> <span style="color:#66d9ef">match</span> line {
        Ok(line_str) <span style="color:#f92672">=&gt;</span> {
            <span style="color:#66d9ef">let</span> columns: Vec<span style="color:#f92672">&lt;</span>_<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> line_str.split(<span style="color:#e6db74">&#39;\t&#39;</span>).collect();
            words.insert(columns[col].to_string());
        }
        Err(_) <span style="color:#f92672">=&gt;</span> panic<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;parse error &#34;</span>),
    });
    <span style="color:#66d9ef">return</span> words.len();
}
</code></pre></div><p><code>HashSet</code>を利用することでユニーク性を担保して、最後はHashSetの数を数え上げれば終了です。</p>
<h2 id="18-各行を3コラム目の数値の降順にソート">18. 各行を3コラム目の数値の降順にソート</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">sort_on_col3</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; String {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> lines: <span style="color:#a6e22e">BTreeSet</span><span style="color:#f92672">&lt;</span>Line<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> BTreeSet::new();
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    buf.lines().for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> <span style="color:#66d9ef">match</span> line {
        Ok(line_str) <span style="color:#f92672">=&gt;</span> {
            <span style="color:#66d9ef">let</span> columns: Vec<span style="color:#f92672">&lt;</span>_<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> line_str.split(<span style="color:#e6db74">&#39;\t&#39;</span>).collect();
            <span style="color:#66d9ef">let</span> num: <span style="color:#66d9ef">u32</span> <span style="color:#f92672">=</span> columns[<span style="color:#ae81ff">2</span>].parse().expect(<span style="color:#e6db74">&#34;parse error&#34;</span>);
            <span style="color:#66d9ef">let</span> line <span style="color:#f92672">=</span> Line {
                line: <span style="color:#a6e22e">line_str</span>,
                num,
            };
            lines.insert(line);
        }
        Err(_) <span style="color:#f92672">=&gt;</span> panic<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;parse error&#34;</span>),
    });
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> sorted <span style="color:#f92672">=</span> String::new();
    lines.iter().for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> {
        sorted.push_str(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}\n&#34;</span>, line.line).as_str());
    });

    <span style="color:#66d9ef">return</span> sorted;
}

<span style="color:#75715e">#[derive(Eq)]</span>
<span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Line</span> {
    line: String,
    num: <span style="color:#66d9ef">u32</span>,
}

<span style="color:#66d9ef">impl</span> Ord <span style="color:#66d9ef">for</span> Line {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">cmp</span>(<span style="color:#f92672">&amp;</span>self, other: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Self</span>) -&gt; <span style="color:#a6e22e">Ordering</span> {
        <span style="color:#66d9ef">let</span> ord <span style="color:#f92672">=</span> other.num.cmp(<span style="color:#f92672">&amp;</span>self.num);
        <span style="color:#66d9ef">if</span> <span style="color:#66d9ef">let</span> Ordering::Equal <span style="color:#f92672">=</span> ord {
            other.line.cmp(<span style="color:#f92672">&amp;</span>self.line)
        } <span style="color:#66d9ef">else</span> {
            ord
        }
    }
}

<span style="color:#66d9ef">impl</span> PartialOrd <span style="color:#66d9ef">for</span> Line {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">partial_cmp</span>(<span style="color:#f92672">&amp;</span>self, other: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Self</span>) -&gt; Option<span style="color:#f92672">&lt;</span>Ordering<span style="color:#f92672">&gt;</span> {
        Some(self.cmp(other))
    }
}

<span style="color:#66d9ef">impl</span> PartialEq <span style="color:#66d9ef">for</span> Line {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">eq</span>(<span style="color:#f92672">&amp;</span>self, other: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Self</span>) -&gt; <span style="color:#66d9ef">bool</span> {
        self.eq(other)
    }
}
</code></pre></div><p><code>Line</code>という、行の文章と第3カラム目の値をもった構造体を作成しました。そこにEq、Ord、PartialOrd、PartialEqを実装し、3カラム目での大小比較できるようにしました。
この構造体をBTeeSetに格納していき、イテレーターで回すことで、ソートされた状態にしてあります。
同一数値の場合は行の降順でソートできるようにOrdを実装してあります。</p>
<h2 id="19-各行の1コラム目の文字列の出現頻度を求め出現頻度の高い順に並べる">19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">sort_on_frequency</span>(input_file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; String {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> names: <span style="color:#a6e22e">HashMap</span><span style="color:#f92672">&lt;</span>String, <span style="color:#66d9ef">u32</span><span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> HashMap::new();
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(File::open(input_file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>));
    buf.lines().for_each(<span style="color:#f92672">|</span>line<span style="color:#f92672">|</span> <span style="color:#66d9ef">match</span> line {
        Ok(line_str) <span style="color:#f92672">=&gt;</span> {
            <span style="color:#66d9ef">let</span> columns: Vec<span style="color:#f92672">&lt;</span>_<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> line_str.split(<span style="color:#e6db74">&#39;\t&#39;</span>).collect();
            <span style="color:#66d9ef">let</span> name_str <span style="color:#f92672">=</span> columns[<span style="color:#ae81ff">0</span>].to_string();
            <span style="color:#66d9ef">let</span> count <span style="color:#f92672">=</span> names.entry(name_str).or_insert(<span style="color:#ae81ff">0</span>);
            <span style="color:#f92672">*</span>count <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>;
        }
        Err(_) <span style="color:#f92672">=&gt;</span> panic<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;parse error&#34;</span>),
    });
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> sorted <span style="color:#f92672">=</span> String::new();
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> sorted_names: Vec<span style="color:#f92672">&lt;</span>(<span style="color:#f92672">&amp;</span>String, <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">u32</span>)<span style="color:#f92672">&gt;</span> <span style="color:#f92672">=</span> names.iter().collect();
    sorted_names.sort_by(<span style="color:#f92672">|</span>(aname, acount), (bname, bcount)<span style="color:#f92672">|</span> {
        <span style="color:#66d9ef">let</span> ord <span style="color:#f92672">=</span> bcount.cmp(acount);
        <span style="color:#66d9ef">if</span> <span style="color:#66d9ef">let</span> Ordering::Equal <span style="color:#f92672">=</span> ord {
            bname.cmp(aname)
        } <span style="color:#66d9ef">else</span> {
            ord
        }
    });
    sorted_names.iter().for_each(<span style="color:#f92672">|</span>(name, count)<span style="color:#f92672">|</span> {
        sorted.push_str(format<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{} {}\n&#34;</span>, count, name).as_str());
    });
    <span style="color:#66d9ef">return</span> sorted;
}

</code></pre></div><p>もうすこしうまくできる気がしますが、いったんこれで。
数え上げのためにまずはHashMapに<code>第1カラムの文字列, 個数</code>という組み合わせでデータを入れていきます。
出来上がったHashMapをタプルのベクターに変換し、変換したベクターの<code>sort_by</code>メソッドに比較用の関数を渡すことで個数の降順に並べています。同一個数の場合は文字列の降順になっています。
で、最後に並びかわったベクターのイテレーターを使って出力しておしまいです。
内部的には最悪3回回る感じでしょうか?
最初からベクトルに入れつつソートできる仕組みにするようなのがいいのかなぁ?</p>
<h2 id="まとめ">まとめ</h2>
<p>Unixコマンドの勉強になりましたw
あとは、HashMapなどの勉強にもなりました。
最後の方は効率がいまいち良くない気もしてはいますが、とりあえず第3章に進もうかと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第2章の10から11まで(言語処理100本ノック2020)</title>
      <link>https://blog.johtani.info/blog/2020/05/08/rebootnlp100-ch02-10to11/</link>
      <pubDate>Fri, 08 May 2020 23:37:20 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/08/rebootnlp100-ch02-10to11/</guid>
      <description>気づいたら1ヶ月サボってました、ごめんなさい。。。 Rustで言語処理100本ノックの第2章をはじめました。 前回はこちら。 確認用のUnixコマ</description>
      <content:encoded><p>気づいたら1ヶ月サボってました、ごめんなさい。。。</p>
<p>Rustで言語処理100本ノックの第2章をはじめました。</p>
<p>前回は<a href="/blog/2020/05/08/reboot-nlp100-finish-ch01">こちら</a>。</p>
<h2 id="確認用のunixコマンド">確認用のUnixコマンド</h2>
<p>確認用のファイルを先に生成して置きました。
これで、Rustでコードを書いて、作成済みの確認ファイルを元に<code>assert_eq!</code>でチェックするという方式を取ろうかと。</p>
<p>で、コマンド群は<a href="https://github.com/johtani/nlp100-rust/blob/master/commands/chap02.sh">こちら</a>です。</p>
<p>Unix/Linuxコマンド、昔から使っています。が、なにかちょっとした文字列処理やファイル処理をやるときは、Javaのプログラム(最近だとPython)を書くというのが基本になってるので、結構、使ったことの無いコマンドが今回ありました。使ったことがなかったのはこちらです。</p>
<ul>
<li><code>sed</code></li>
<li><code>tr</code></li>
<li><code>expand</code></li>
<li><code>paste</code></li>
<li><code>cut</code></li>
<li><code>split</code></li>
</ul>
<p><code>sed</code>とかは普通さわってるだろ?って思われそうですね。。。</p>
<p>で、コマンドを<code>man</code>で調べつつやりました、macOS上で。
これがまたいくつか罠があったので書き残しておきます(自分が知らないだけかもしれないので、おかしいところがあったらツッコミお願いします。)。</p>
<h3 id="sedコマンドでのタブの扱い"><code>sed</code>コマンドでのタブの扱い</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#75715e">## sed command for macOS. If using Linux, use &#34;\t&#34; for tab character</span>
cat $INPUT_FILE_NAME | sed -e <span style="color:#e6db74">&#39;s/	/ /g&#39;</span> &gt; $OUTPUT_DIR/11_sed.txt
</code></pre></div><p><code>\t</code>で行けると思ったのですが、うまく動きませんでした。<code>&lt;tab&gt;</code>みたいな書き方もあると思うのですが、これも駄目で、結局タブ文字をそのまま打ち込みました。。。
これ、めんどくさくないですか???
ちなみに、ターミナルで動作確認して、GitHubにあげてあるシェルファイルにコピペしてたのですが、CLionに貼り付けたらタブ文字がスペースに変換されてしまってて20分くらい悩みました。。。</p>
<h3 id="splitコマンドに-nオプションがない"><code>split</code>コマンドに<code>-n</code>オプションがない</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">
LINES<span style="color:#f92672">=</span><span style="color:#e6db74">`</span>cat $OUTPUT_DIR/10.txt<span style="color:#e6db74">`</span>
SPLIT_LINES<span style="color:#f92672">=</span><span style="color:#e6db74">`</span>echo $LINES/$N | bc<span style="color:#e6db74">`</span>
split -a <span style="color:#ae81ff">1</span> -l $SPLIT_LINES $INPUT_FILE_NAME $OUTPUT_DIR/16_
</code></pre></div><p><code>split</code>コマンドについてググると、<code>-n</code>で指定した数のファイルに分割できるという記事がいくつも出てくるのですが、<code>man split</code>をターミナル上でやるとそんなオプションがないと。。。
macOSがBSD系だからっぽいです。
ということで、行数を元に、指定した数(<code>N</code>)で行数を割ってから、指定行数ごとにファイルを分割する方式にしました。</p>
<p>これらのコマンドの違いはHomebrewとかでインストールするとなくなるのかなぁ?(めんどくさいので確認してないですが。。。)</p>
<p>ってことで、2章のそれぞれの課題の正解ファイルの生成はこれでできたはずです。</p>
<h2 id="10-行数のカウント">10. 行数のカウント</h2>
<p><code>wc -l</code>ですね。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">// ch02-10 行数のカウント
</span><span style="color:#75715e"></span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">word_count</span>(file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; <span style="color:#66d9ef">usize</span> {
    <span style="color:#66d9ef">let</span> f <span style="color:#f92672">=</span> File::open(file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>);
    <span style="color:#66d9ef">let</span> buf <span style="color:#f92672">=</span> BufReader::new(f);
    <span style="color:#66d9ef">return</span> buf.lines().count();
}
</code></pre></div><p>ファイルを読み込んで行数を数えます。
文字列として読み込んで改行コードの数を数えるというのもありかな?と思いましたが、Rustの<code>BufReader</code>に<code>lines()</code>という行のイテレータ?が取れることがわかったので、それでカウントを取りました。</p>
<h2 id="11-タブをスペースに置換">11. タブをスペースに置換</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">// ch02-11 タブをスペースに置換
</span><span style="color:#75715e"></span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">tab_2_space</span>(file_name: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; String {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> f <span style="color:#f92672">=</span> File::open(file_name).expect(<span style="color:#e6db74">&#34;file not found&#34;</span>);
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> contents<span style="color:#f92672">=</span> String::new();
    f.read_to_string(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> contents).expect(<span style="color:#e6db74">&#34;read error&#34;</span>);
    <span style="color:#66d9ef">return</span> contents.replace(<span style="color:#e6db74">&#34;\t&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>);
}
</code></pre></div><p>こっちはファイル全体を文字列に読み込んでしまってから、文字列の<code>replace</code>で置換するという方式です。
ファイルが大きい場合にこれでいいのか?という問題がある気がしますが、まずはこの実装にしました。
やるとしたら、<code>read</code>メソッドと<code>buffer</code>を用意して、少しずつ読みながら、置換して吐き出す感じでしょうか?
ちゃんとした文字コードの区切りで取れるかどうかを気にしないと行けないと思うので、思ったよりはめんどくさくなりそう。</p>
<p><code>BufReader</code>をつかって<code>read_line</code>のほうがましかも?</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、サボっていたのを再開しました。
Rustのコードを書く前に、Unixコマンドの処理に結構悩みましたw</p>
<p>Rustのコードとしてはファイル処理なので、今後も役立つ気がしてます。
ということで、頑張っていくぞと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第1章の08から09まで(言語処理100本ノック2020)</title>
      <link>https://blog.johtani.info/blog/2020/05/08/reboot-nlp100-finish-ch01/</link>
      <pubDate>Fri, 08 May 2020 16:40:57 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/08/reboot-nlp100-finish-ch01/</guid>
      <description>Rustで言語処理100本ノックのリファクタリングの続き。 前回はこちら。 とっくに終わってたのに、ブログ書いてなかった。。。 08. 暗号文 pub fn cipher(text: &amp;amp;str) -&amp;gt;</description>
      <content:encoded><p>Rustで言語処理100本ノックのリファクタリングの続き。</p>
<p>前回は<a href="/blog/2020/04/09/reboot-nlp100-ch01-03to05/">こちら</a>。</p>
<p>とっくに終わってたのに、ブログ書いてなかった。。。</p>
<h3 id="08-暗号文">08. 暗号文</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">cipher</span>(text: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; String {
    <span style="color:#66d9ef">return</span> String::from_iter(text.chars().map(<span style="color:#f92672">|</span>x<span style="color:#f92672">|</span> {
        <span style="color:#66d9ef">if</span> x.is_ascii_alphanumeric() <span style="color:#f92672">&amp;&amp;</span> x.is_lowercase() {
            <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> b <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>; <span style="color:#ae81ff">4</span>];
            x.encode_utf8(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> b);
            b[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">219</span> <span style="color:#f92672">-</span> b[<span style="color:#ae81ff">0</span>];
            char::from(b[<span style="color:#ae81ff">0</span>])
        } <span style="color:#66d9ef">else</span> {
            x
        }
    }));
}
</code></pre></div><p>Rustの文字列はUTF-8でエンコードされたテキストを保持しているので、文字コード自体は意識していないです。
<code>chars()</code>でUnicodeスカラー値のイテレータが返ってくるので、1文字ずつ扱えるようになります。</p>
<p>ただ、1文字をバイトとして扱うのに手こずりました。
<code>encode_utf8</code>というメソッドを利用して1バイトだけ取り出して、計算するというのをやっています。
文字種の判別のメソッドが用意されているのは便利ですね。</p>
<p>なんかもうちょっとスマートにできないのかな?と思いつつ動いたのでこれになってます。</p>
<h3 id="09-typoglycemia">09. Typoglycemia</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">typoglycemia</span>(text: <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span>) -&gt; String {
    <span style="color:#66d9ef">return</span> text
        .split_whitespace()
        .map(<span style="color:#f92672">|</span>word<span style="color:#f92672">|</span> {
            <span style="color:#66d9ef">if</span> word.len() <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">4</span> {
                word.to_string()
            } <span style="color:#66d9ef">else</span> {
                <span style="color:#66d9ef">let</span> original <span style="color:#f92672">=</span> word.chars().collect::<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>char<span style="color:#f92672">&gt;&gt;</span>();
                <span style="color:#66d9ef">let</span> first <span style="color:#f92672">=</span> original.get(<span style="color:#ae81ff">0</span>).unwrap();
                <span style="color:#66d9ef">let</span> last <span style="color:#f92672">=</span> original.last().unwrap();
                <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> typo <span style="color:#f92672">=</span> original[<span style="color:#ae81ff">1</span>..original.len() <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]
                    .iter()
                    .map(<span style="color:#f92672">|</span>x<span style="color:#f92672">|</span> x.clone())
                    .collect::<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>char<span style="color:#f92672">&gt;&gt;</span>();
                <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> rng <span style="color:#f92672">=</span> thread_rng();
                typo.shuffle(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> rng);
                <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> typo <span style="color:#f92672">=</span> String::from_iter(typo.iter());
                typo.insert(<span style="color:#ae81ff">0</span>, first.clone());
                typo.push(last.clone());
                typo
            }
        })
        .collect::<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">&gt;&gt;</span>()
        .join(<span style="color:#e6db74">&#34; &#34;</span>);
}
</code></pre></div><p>一応、1行で記述できたかな?
まずは、スペースで単語ごとに区切った後に、<code>word</code>(単語)の長さによって、処理を分岐し、単語が5文字以上の場合にランダムに並び替えを行うというのをやっています。
文字単位で処理を行うために、<code>chars()</code>で1文字ずつ取り出しています。
最初と最後の文字だけはそのままに、間の文字をランダムにシャッフルするというのをやるのに、もとの<code>word</code>のスライスからコピーした文字列を作り出してから組み立て直すということをやっています。</p>
<p>コピーしないでゴニョゴニョする方法ってあるのかなぁ?
思いつかなかったので、結構泥臭い感じの実装になってしまいました。</p>
<h2 id="まとめ">まとめ</h2>
<p>めんどくさいので、コードをGitHubのソースコードからではなく、ブログにコードスニペットとしてコピペしました。Hugoでいい感じにGitHubのコードスニペット表示するのないかなぁ?</p>
<p>ということで、2年越しで1章が終了しました。
2章もやらないとなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ElasticのWorkplace Searchを触ってみる - その2 - インストールと起動</title>
      <link>https://blog.johtani.info/blog/2020/05/07/install-workplace-search/</link>
      <pubDate>Thu, 07 May 2020 11:32:50 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/07/install-workplace-search/</guid>
      <description>前回はWorkplace Searchの概要について書きましたが、今回はインストールと構成要素について説明します。なお、2020/5/7時点で</description>
      <content:encoded><p>前回はWorkplace Searchの概要について書きましたが、今回はインストールと構成要素について説明します。なお、2020/5/7時点での情報を元に本記事は書いていますのでご注意ください。基本的にはインストールと起動方法についての手順を元に書いています。所々に考察を挟んだ形の記事になっていますので、気になるところだけ呼んでいただければと。</p>
<h2 id="記事一覧">記事一覧</h2>
<ul>
<li><a href="/blog/2020/05/01/intro-workplace-search/">ElasticのWorkplace Searchを触ってみる - その1</a></li>
</ul>
<h2 id="インストール">インストール</h2>
<p>インストール方法は<a href="https://www.elastic.co/guide/en/workplace-search/current/workplace-search-install.html">公式リファレンス</a>もしくは<a href="https://www.elastic.co/jp/downloads/enterprise-search">ダウンロードページ</a>に記載があります。
現時点ではMacもしくはLinuxが対象でWindowsはまだサポート対象外となっています。</p>
<h3 id="必要なもの">必要なもの</h3>
<p>インストールに必要なものは以下になります。</p>
<ul>
<li>Elasticsearch 7.6.x + Platinum license
<ul>
<li><a href="https://www.elastic.co/jp/elasticsearch/service">Elastic CloudのElasticsearch Service</a> もしくは</li>
<li><a href="https://www.elastic.co/downloads/elasticsearch">ダウンロード</a>してローカルで起動</li>
</ul>
</li>
<li>enterprise-search-7.6.0.tar.gz
<ul>
<li><a href="https://www.elastic.co/jp/downloads/enterprise-search">ダウンロードページはこちら</a></li>
</ul>
</li>
<li>Java 8もしくは11
<ul>
<li>Long Term Supportの対象であるJavaです。2020/5/7時点では8か11</li>
</ul>
</li>
</ul>
<p>今回はローカルにElasticsearchの7.6.2をインストールしてから試してみます。30日間のトライアルライセンスが有効になっているので、Platinumの機能を試すことができます。</p>
<p>Javaの8か11が必要になります。Elasticsearchには7.xからJDKが同梱されるようになりましたが、Workplace SearchがJettyを元に動作しているからです(enterprise-search-7.6.0.tar.gzにjettyというフォルダあり)。</p>
<h3 id="インストール手順">インストール手順</h3>
<p>大まかには以下の3つです。</p>
<ol>
<li>Java 8もしくは11のインストール
<ul>
<li>14でも大丈夫でした(ローカルにはSDKMANでインストールした14.0.1が利用された)。</li>
</ul>
</li>
<li>Elasticsearchのインストール(Elastic Cloudの場合はクラスタの起動)
<ul>
<li>今回はローカルにインストール</li>
</ul>
</li>
<li>Workplace Searchのインストール</li>
</ol>
<p>Javaはもともとインストールされていたので、今回は2と3をインストールしました。どちらもローカルで起動するので、2つをダウンロードして<code>tar.gz</code>ファイルを展開するだけになります。</p>
<h3 id="起動方法と設定">起動方法と設定</h3>
<p><a href="https://www.elastic.co/guide/en/workplace-search/current/workplace-search-install.html#running-enterprise-search">起動方法に起動前の設定の手順</a>も記載があります。
設定しながら起動していきます。</p>
<h4 id="elasticsearchの起動">Elasticsearchの起動</h4>
<p>既存のElasticsearchのクラスターがあり、Platinumのライセンスが有効になっている場合はこの手順は必要ありません。</p>
<ol>
<li>Elasticsearchの設定ファイルでSecurity機能をオンに
<ul>
<li>7.1から基本的な<a href="https://www.elastic.co/jp/subscriptions">セキュリティ機能はベーシックの機能</a>に含まれています。</li>
</ul>
</li>
<li>Elasticsearchを起動
<ul>
<li>まずは起動(パスワードなどを設定するために必要)</li>
</ul>
</li>
<li>Elasticsearchのパスワードの設定
<ul>
<li>Elasticsearchでデフォルトで用意されているユーザーのパスワードを設定してします。手順では自動で生成させる方法ですが、独自に設定することも可能です。</li>
</ul>
</li>
</ol>
<h4 id="workplace-searchの起動">Workplace Searchの起動</h4>
<p>Elasticsearchが起動したらWorkplace Searchの設定をして起動します。</p>
<ol>
<li>Esへの接続設定を<code>config/enterprise-search.yml</code>に指定
<ul>
<li>Esのパスワード設定時に生成された<code>elastic</code>というユーザーのパスワードをここで指定。</li>
<li>yamlファイルに記載があるが、<code>${ELASTICSEARCH_PASSWORD:changeme}</code>という記述をした場合に環境変数を読み込める</li>
</ul>
</li>
<li><code>allow_es_settings_modification: true</code>を<code>config/enterprise-search.yml</code>
<ul>
<li><a href="https://www.elastic.co/guide/en/workplace-search/current/workplace-search-install.html#elasticsearch-cluster-settings">ここに記載があるような変更</a>をWorkplace SearchがEsのクラスターに対して実行する模様。Workplace Search以外でも使用しているElasticsearchクラスターの場合は<code>allow_es_settings_modification</code>を有効にする代わりに、リンク先にあるような設定を自分で追加する。</li>
</ul>
</li>
<li><code>secret_management.encryption_keys</code>を複数設定
<ul>
<li><a href="https://www.elastic.co/guide/en/workplace-search/current/encryption-keys.html">Encryption Keysのガイド</a>に少し詳しい説明がある。</li>
<li>opensslコマンドとかで作ればいいかな??</li>
<li>1.と同じような設定をしようとしたがうまくいかなかったので、ファイルにキーを設定する方式にしました(バグ?)</li>
</ul>
</li>
<li>起動するときにデフォルトユーザーパスワードを指定
<ul>
<li>指定しなければ勝手に生成してコンソールに出力してくれるので、そちらの方がいいかと。</li>
<li>今回は手順通りに指定した。</li>
</ul>
</li>
<li>起動確認のため<code>http://localhost:3002</code>にアクセス</li>
</ol>
<p>起動するとログが流れ、問題がなければ次のようにデフォルトユーザーの情報が出力されます。</p>
<pre><code>#########################################################

*** Default user credentials have been setup. These are only printed once, so please ensure they are recorded. ***
      username: enterprise_search
      password: pas...ple

#########################################################
</code></pre><p>そして無事起動に成功したことも出力されます。</p>
<pre><code>#########################################################

Success! Elastic Workplace Search is starting successfully.

In a few moments, you'll be able to login at the following address:

* URL: http://localhost:3002
  * If this is your first time starting Workplace Search, check the console output above for your user authentication credentials.
  * Visit the documentation: https://swiftype.com/documentation/enterprise-search

Secret session key has been generated.

Set the key in your config file to persist user sessions through process restarts:

secret_session_key: c23...3


#########################################################
</code></pre><p>ブラウザで画面にアクセスすると、次のような画面が表示されました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200507/first_page.png" />
    </div>
    <a href="/images/entries/20200507/first_page.png" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="起動時のエラー">起動時のエラー</h4>
<p>いくつかのパターンも試してどんなエラーが出るのかを見てみました。
おまけですね。</p>
<h5 id="elasticsearchが見つからないエラー">Elasticsearchが見つからないエラー</h5>
<p>Esを起動しないでWorkplace Searchを起動してみました。</p>
<p>200秒間アクセスしようと試みて駄目だったらエラーで終了みたいです。</p>
<pre><code>
[2020-05-07T03:48:33.645+00:00][13709][2002][app-server][INFO]: Failed to connect to Elasticsearch backend. Make sure it is running.
...
[2020-05-07T03:51:54.038+00:00][13709][2002][app-server][INFO]: Could not connect to Elasticsearch backend after 200s. Terminating...
[2020-05-07T03:51:54.039+00:00][13709][2002][app-server][ERROR]: 
--------------------------------------------------------------------------------

Error: Workplace Search is unable to connect to Elasticsearch. Ensure a healthy Elasticsearch cluster is running at http://127.0.0.1:9200 for user elastic.

--------------------------------------------------------------------------------

</code></pre><h5 id="elasticsearchのsecurityがオフのときのエラー">ElasticsearchのSecurityがオフのときのエラー</h5>
<p>ちなみにSecurityをオフにしたままWorkplace Searchを起動した場合は以下のようなエラーが出ました。</p>
<pre><code>[2020-05-07T03:46:53.474+00:00][13567][2002][app-server][ERROR]: 
--------------------------------------------------------------------------------

Elastic Workplace Search requires Elasticsearch security features to be enabled.
Please enable Elasticsearch security features as outlined here:
  https://www.elastic.co/guide/en/workplace-search/current/workplace-search-install.html

--------------------------------------------------------------------------------
</code></pre><h2 id="構成要素">構成要素</h2>
<p>ここまでインストールして起動してきました。
では、Workplace Searchがどういったコンポーネントから構成されているかを予測してみましょう(あくまで外から見た予想となります。そのうちElastic社のウェビナーとかイベントで内部の発表とかあるかも?)。</p>
<h3 id="インストールページの記載から">インストールページの記載から</h3>
<p>インストールページに<a href="https://www.elastic.co/guide/en/workplace-search/current/workplace-search-install.html#_minimum_hardware">最小ハードウェア</a>という記載があり、そこで何が動く可能性があるかというのがわかります。</p>
<p>起動するものはこんな感じみたいです。</p>
<ul>
<li>Elasticsearch - 外部でもOK</li>
<li>App Server - Workplace SearchのWeb機能</li>
<li>Worker - クローラーとかかな?</li>
<li>Filebeat - Workplace Searchのログ収集用</li>
<li>その他プロセス - なんだろ?</li>
</ul>
<p>という具合です。</p>
<h3 id="設定などからの予想">設定などからの予想</h3>
<p>次は設定項目や起動時のログなどからの予想です。</p>
<ul>
<li>Worklpace Search配下のセキュアなデータストア - アクセストークンなどの管理のため</li>
<li>JRubyアプリケーション - App ServerはJRuby上で動いているRailsアプリ</li>
<li>Filebeatも起動している - Workplace Searchのログ収集のため?
<ul>
<li>Filebeatの接続設定はWorkplace Searchの設定値を利用</li>
</ul>
</li>
</ul>
<h3 id="では構成要素は">では構成要素は?</h3>
<p>ということで、現時点でわかった構成要素は以下のとおりです。</p>
<ol>
<li>Elasticsearch</li>
<li>Workplace Search App Server - Railsアプリ on JRuby
<ul>
<li>Webアプリとは別に(内部?で)、いくつかのワーカーが存在する</li>
<li>管理画面と検索画面の2種類が存在</li>
</ul>
</li>
<li>Filebeat - ログ収集</li>
</ol>
<p>といった感じです。
まだ起動したばかりなのでこのくらいでしょうか。ログを見るともう少しわかりそうな気がします。</p>
<p>基本的には、EsをバックエンドにしたRailsのミドルウェアになります。コネクターや検索画面はすべてWorkplace Searchのミドルウェア経由でアクセスする形になりますので、普通に検索で利用するユーザーにはElasticsearchの存在は見えない作りになっています。</p>
<h2 id="次は">次は?</h2>
<p><a href="https://www.elastic.co/guide/en/workplace-search/current/workplace-search-getting-started.html">Getting Started</a>を元に、どんなアクターがいて、どんな機能が提供されているのか、どんな利用方法なのか?というのを見ていこうと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ElasticのWorkplace Searchを触ってみる - その1</title>
      <link>https://blog.johtani.info/blog/2020/05/01/intro-workplace-search/</link>
      <pubDate>Fri, 01 May 2020 16:29:04 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/05/01/intro-workplace-search/</guid>
      <description>2月のElastic社のブログですが、Enterprise Searchとこれまで呼んでいた製品をWorkplace Searchという製品名に</description>
      <content:encoded><p>2月のElastic社のブログですが、Enterprise Searchとこれまで呼んでいた製品を<a href="https://www.elastic.co/jp/workplace-search">Workplace Search</a>という製品名に変更し、App Searchなどを含む製品群を<a href="https://www.elastic.co/jp/enterprise-search">Enterprise Search</a>という名前に変更しました(ちょっとややこしい)。
Workplace Search自体はまだβ版という位置づけですが、ダウンロードして試すことが可能です。</p>
<p>きちんと触ったことがないので、ちょっと触って見ようかなと思い、何回かに分けてブログを書いてみます。まずは概要とかから。</p>
<h2 id="workplace-searchとは">Workplace Searchとは?</h2>
<p>Elasticsearchをバックエンドに利用するElastic社が提供するアプリケーション(ミドルウェア?)の1つです。</p>
<p>もともとはSwiftypeという会社が作っていた、Site Search、App Searchと同じような系列で開発されている統合検索の検索エンジンミドルウェアという感じです。
製品ページを見るとわかりますが、様々なデータソースから、データをクロールしてElasticsearchに保存することで、統合された検索を提供することができるようになる製品です。
最近は会社のドキュメントがさまざまな場所(Google Drive、Saleseforce、GitHub、Dropboxなど)に保存されています。
それぞれで検索窓などはありますが、1箇所で検索することで横断的に検索でき、仕事の効率があがりますよね?ということで作られている製品です。</p>
<h3 id="提供利用方法は">提供(利用)方法は?</h3>
<p>今後の提供方法としては、Elastic Cloudで利用できるSaaS形式のものと、独自に(クラウドのコンピューティングエンジンやオンプレのサーバーなどで)Workplace Searchのアプリを起動する方法(オンプレ版)があります。後者の場合は、Elasticsearchのクラスターを用意する必要があります。なお、後者の場合、Elasticsearchの<a href="https://www.elastic.co/jp/subscriptions">サブスクリプションのプラチナライセンス</a>が必要になるようです(<a href="https://www.elastic.co/jp/downloads/enterprise-search">ダウンロードページ</a>に記載あり)</p>
<p>まだ、β版という位置づけなので、今後どのように変更されるかはわかりませんが、今回は2020年5月1日時点でのベータ版(7.6.0)を元にどんなものかを紹介します。現時点で利用できるのはβ版のオンプレ版で、MacやLinuxで利用可能です。</p>
<h3 id="ダウンロードとインストール">ダウンロードとインストール</h3>
<p><a href="https://www.elastic.co/jp/downloads/enterprise-search">ダウンロードページ</a>にインストール方法などの記載があります。</p>
<p>インストールして触って見るところはまた後日。</p>
<h2 id="ライセンス価格は">ライセンス、価格は?</h2>
<p>まだ不明です。SaaS版の提供はまだです。
オンプレ版については少なくとも、<a href="https://www.elastic.co/jp/subscriptions">Elasticのサブスクリプション</a>のプラチナが必要になります。こちらは、価格は公開されていません。Elastich社もしくはパートナー企業での問い合わせが必要になります。
(たぶん、ドキュメントレベルのセキュリティとかSSOとかの仕組みがプラチナで提供されているのでそのあたりを使っているのでは?と想像してます。)</p>
<h2 id="想定できそうな用途は">想定できそうな用途は?</h2>
<p>社内の文書検索でしょうか。ただ、いわゆる昔ながらのエンタープライズサーチと呼ばれている、社内のファイルサーバーなどの文書検索ではなく、クラウドサービスを複数利用している会社が利用する想定になっています。</p>
<p>例えば、開発者は社内Wiki(Confluence)で社内文書を書き、Issue管理にはJIRAやGitHubを活用しており、ただ、社内での説明にはGoogle Driveを利用しているといった場合です。こういう場合、あの機能についての説明や資料ってどこだっけ?というので、あちこち探し回ったりしないといけないです。また、営業部門やサポート、マーケティングなどが絡んでくると更に、SalesforceやZendeskといったデータソースも出てきます。
情報が散らばっていて、それらを探し出したりまとめるだけで時間を取られている場合などに便利かもしれません。</p>
<h2 id="次は">次は?</h2>
<p>実際にインストールしてから起動して、どんな感じで使えるのかといったところを見て、どんな機能が提供されているのかを見ていこうと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書の更新についての注意点</title>
      <link>https://blog.johtani.info/blog/2020/04/27/note-updating-dictionary/</link>
      <pubDate>Mon, 27 Apr 2020 10:44:15 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/27/note-updating-dictionary/</guid>
      <description>先日、Elasticsearchでのカスタム辞書の利用方法についてブログを書きました。 辞書の設定方法について記載しましたが、今回は辞書の更新</description>
      <content:encoded><p>先日、<a href="/blog/2020/04/22/custom-dictionary-after-7-4/">Elasticsearchでのカスタム辞書の利用方法についてブログ</a>を書きました。</p>
<p>辞書の設定方法について記載しましたが、今回は辞書の更新について書いていなかったので、書いてみようと思います。
ここで「辞書」としているのは、Kuromojiのユーザー辞書、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-graph-tokenfilter.html">Synonym Graph Token FilterのSynonym辞書</a>(いわゆる類義語辞書)のことになります。サードパーティのAnalyzer等に関する話ではありません。</p>
<h2 id="辞書更新に関する制限事項">辞書更新に関する制限事項</h2>
<p>辞書の更新について、大原則と制限事項が存在します。</p>
<h3 id="大原則辞書の更新データも更新">大原則(辞書の更新=データも更新)</h3>
<p>ElasticsearchはAnalyzerが切り出した単語を元に転置インデックスを作成して、検索を行っています(<a href="https://noti.st/johtani/halVGM">この仕組みに関するスライドはこちらを参照のこと</a>)。
Analyzerが辞書を持っている場合、その辞書を元に単語を切り出して転置インデックスに利用します。
また、検索クエリの単語に対してもこのAnalyzerの辞書が利用されます。</p>
<p>辞書に新しい単語を追加するということは、その単語に関連するドキュメントも更新しないと行けないということになります。</p>
<p>例えば、Kuromojiを利用していて、「グランベリーパーク」という単語「グランベリー」「パーク」という単語に分割できるような新しい単語として辞書に追加する場合を考えてみましょう。ユーザーが「グランベリー」で検索しても検索結果として出てきてほしいという場合です。</p>
<p>辞書に「グランベリーパーク」を登録していない頃に登録されたドキュメントは「グランベリーパーク」という1単語として転置インデックスの見出し語を切り出します(Kuromojiはカタカナの連続している文字列については未知語として1単語にし、「名詞-一般」の品詞を付与)。</p>
<p>更新前でのドキュメントのAnalyze結果</p>
<pre><code>「グランベリーパーク」「で」「ショッピング」
</code></pre><p>もし、辞書に「グランベリーパーク」を「グランベリー」「パーク」から構成される新規の単語として登録しそれを使用した場合、辞書を更新したあとから、「グランベリーパーク」という単語がAnalyzerからは出てこなくなります。</p>
<p>更新後でのドキュメントのAnalyze結果</p>
<pre><code>「グランベリー」「パーク」「で」「ショッピング」
</code></pre><p>ということは、辞書更新以前のドキュメントは「グランベリーパーク」という見出し語に対して登録されているので、辞書更新以前に登録されているドキュメントは検索にヒットしなくなります。</p>
<p>このように転置インデックスを利用している検索エンジンでは、単語の区切りが変更されるような辞書の更新があった場合、最低でも影響があるドキュメントについては再登録が必要となるわけです。</p>
<p>これが大原則(辞書更新=データも更新)となります。
基本的には辞書の更新を行った場合は、ドキュメントの再インデックス(再登録)が必要となります。</p>
<h3 id="elasticsearchでの制限事項">Elasticsearchでの制限事項</h3>
<p>Elasticsearchでは、辞書の更新に関して実装上の制限事項が存在しています。
内部的な実装として、ElasticsearchではAnalyzerのインスタンス(正確にはAnalyzerのFactoryのインスタンス)の生成がインデックスに関する内部のインスタンスが生成されたタイミングの1回のみとなっています。</p>
<p>このインスタンスの生成時に設定ファイル(辞書を含む)を読み込んでいます。</p>
<p>言い換えると、辞書(ファイル、インデックス設定に関わらず)の読み込みは、インデックスが作られたタイミングのみということになります。
なおここで言う「インデックスが作られたタイミング」というのは、以下の2パターンです。</p>
<ol>
<li>インデックス新規作成時</li>
<li>インデックスオープン時</li>
</ol>
<p>では、ここから辞書を更新してそれを既存のインデックスに適用する方法について説明しましょう。</p>
<h2 id="辞書の更新方法ファイル編">辞書の更新方法(ファイル編)</h2>
<p>前回のブログで説明しましたが、Elasticsearch 7.4よりも古いバージョンでは、ファイルでKuromojiのユーザー辞書を設定していました。まずはこちらの方法について説明します。前提として、すでにユーザー辞書を設定したKuromoji Tokenizerがインデックスに設定されているものとします(<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-kuromoji-tokenizer.html">ユーザー辞書の設定方法については公式リファレンスを御覧ください</a>)。</p>
<p>辞書ファイルに新規にエントリーを追加しただけでは、設定は読み込まれていません。新規辞書を反映させるためには以下の手順が必要となります。</p>
<ol>
<li>更新した辞書ファイルの配布
<ul>
<li>複数ノードでElasticsearchのクラスターを構成している場合はすべてのノードに更新した辞書ファイルを配布する必要があります。</li>
</ul>
</li>
<li>インデックスのクローズ(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-close.html">公式リファレンス</a>)
<ul>
<li>設定ファイルを再読込させるために一度インデックスをクローズします。</li>
<li>クローズするので、書き込み、検索などの処理を停止する必要があります。もし停止していない場合はクライアント側ではインデックスがクローズされているという旨のエラーを受け取ります(400で、<code>index_closed_exception</code>)。</li>
</ul>
</li>
<li>インデックスのオープン(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html">公式リファレンス</a>)
<ul>
<li>設定ファイルを読み込みます。これで、新規追加された単語が読み込まれます。</li>
</ul>
</li>
<li>再インデックス
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html"><code>_update_by_query</code></a>を利用することで、対象のインデックスのデータを再インデックスすることができます。条件無しでAPIを呼び出すとすべてのデータが再度登録されます。</li>
<li><code>_source</code>が<code>false</code>の場合は<code>_update_by_query</code>は利用できません。元データをもう一度外部からElasticsearchに対して登録する必要があります。</li>
</ul>
</li>
</ol>
<p><a href="https://gist.github.com/johtani/25e971ded639e3bea3229ebf861e62be#file-1_-_-json">Kibanaでの手順をGistにしてあります</a>。手順はこちらをご覧ください。</p>
<h2 id="辞書の更新方法インデックス設定編">辞書の更新方法(インデックス設定編)</h2>
<p>ファイルの場合とは少し手順が異なります。
インデックスの設定としてユーザー辞書を登録しているため、ファイルをElasticsearchのクラスターにあるノードに配布する必要がありません。
また、辞書の設定はインデックスの設定に指定してありますが、こちらは動的に設定変更できる項目ではないため、インデックスを先にクローズする必要があります。</p>
<ol>
<li>インデックスのクローズ(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-close.html">公式リファレンス</a>)
<ul>
<li>辞書の設定を更新するにはインデックスをクローズする必要があります。辞書の設定は動的に更新できる項目にはなっていないためです。</li>
<li>オープンしているインデックスで更新しようとした場合は<code>illegal_argument_exception</code>で<code>Can't update non dynamic settings...</code>というメッセージが返ってきます。</li>
</ul>
</li>
<li>辞書の更新(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.6/indices-update-settings.html">公式リファレンス:インデックス設定の更新</a>)
<ul>
<li><code>user_dictionary_rules</code>に単語と追加します。</li>
</ul>
</li>
<li>インデックスのオープン(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html">公式リファレンス</a>)
<ul>
<li>設定ファイルを読み込みます。これで、新規追加された単語が読み込まれます。</li>
</ul>
</li>
<li>再インデックス
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html"><code>_update_by_query</code></a>を利用することで、対象のインデックスのデータを再インデックスすることができます。条件無しでAPIを呼び出すとすべてのデータが再度登録されます。</li>
<li><code>_source</code>が<code>false</code>の場合は<code>_update_by_query</code>は利用できません。元データをもう一度外部からElasticsearchに対して登録する必要があります。</li>
</ul>
</li>
</ol>
<p><a href="https://gist.github.com/johtani/25e971ded639e3bea3229ebf861e62be#file-2_-_-json">Kibanaでの手順をGistにしてあります</a>。手順はこちらをご覧ください。</p>
<h2 id="第3の方法新規インデックス作成">第3の方法(新規インデックス作成)</h2>
<p>ここまで、インデックスのクローズ、オープンで既存のインデックスに対して辞書を更新する方法について説明しました。
ただ、残念なことにAmazon Elasticsearch ServiceではElasticsearchが提供しているすべてのAPIが利用できるわけではありません(<a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/aes-supported-es-operations.html">Amazon ESの利用可能なAPIの一覧はこちら</a>)。
(<code>_close</code>は駄目だけど<code>_open</code>は呼べるのかな???)</p>
<p>ということで、新規にインデックスを作成して、新しい辞書の設定を反映したインデックスを用意し、そこにデータをコピーもしくは登録するという方法になります(<a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/custom-packages.html">Amazon ESのカスタム辞書のドキュメントに手順がありますね</a>)。</p>
<p>手順としては以下のとおりです。</p>
<ol>
<li>辞書の更新(用意)
<ul>
<li>新しい単語などを登録した辞書を用意します。</li>
<li>ファイル、インデックス設定どちらでもOKです。</li>
<li>ファイルの場合は、既存のファイル名とは異なるファイル名にしたほうが混乱がなくなります。</li>
</ul>
</li>
<li>新規インデックス作成
<ul>
<li>1.で作成した辞書を元に新規インデックスを作成します。</li>
</ul>
</li>
<li>新規インデックスにデータコピー
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.6/docs-reindex.html"><code>_reindex</code> API</a>を利用するとデータコピーが簡単です。<code>source</code>と<code>dest</code>を指定するだけです。</li>
<li><code>_source</code>が<code>false</code>の場合は<code>_update_by_query</code>は利用できません。元データをもう一度外部からElasticsearchに対して登録する必要があります。</li>
</ul>
</li>
<li>アプリケーション側で新規インデックスを利用するように変更
<ul>
<li><code>_alias</code>を使用しておくと切り替えが簡単です(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.6/indices-add-alias.html">公式リファレンスはこのあたり</a>)。</li>
</ul>
</li>
</ol>
<p>考慮すべき点としては、サービスを提供しながら行う場合は、3.の<code>_reindex</code>を実行し始めたタイミング以降の登録・更新データの扱いについてでしょうか。</p>
<h2 id="まとめ">まとめ</h2>
<p>辞書の更新に関する大原則、制限事項、手順などについて説明しました。
辞書の変更は検索に大きく影響がでます。そのあたりをきちんと考慮しながら更新しましょう。
ユーザー辞書、カスタム辞書を扱う際の参考にしていただければと。
他にもユーザー辞書で気をつけないといけないこともありますが、今日はこのあたりで。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Kuromojiのカスタム辞書をインデックスの設定で指定</title>
      <link>https://blog.johtani.info/blog/2020/04/22/custom-dictionary-after-7-4/</link>
      <pubDate>Wed, 22 Apr 2020 10:30:56 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/22/custom-dictionary-after-7-4/</guid>
      <description>Elasticsearchで日本語を扱うときに、カスタム辞書を使いたいという要望がよくあります。 AWSのElasticsearch Servi</description>
      <content:encoded><p>Elasticsearchで日本語を扱うときに、カスタム辞書を使いたいという要望がよくあります。
<a href="https://aws.amazon.com/jp/about-aws/whats-new/2020/04/custom-dictionary-files-now-supported-on-amazon-elasticsearch-service/">AWSのElasticsearch Serviceでカスタム辞書ファイルを読み込める機能が発表されたようです</a>。</p>
<p>実は、Elasticsearchの7.4からファイルを使用しなくても日本語のTokenizerでカスタム辞書を利用することができるようになっています。</p>
<h2 id="カスタム辞書をインデックスの設定で指定">カスタム辞書をインデックスの設定で指定</h2>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.4/analysis-kuromoji-tokenizer.html#analysis-kuromoji-tokenizer">やり方はドキュメントに記載</a>があります。</p>
<p>トークナイザーの設定をインデックスの設定に記述しますが、このときに
<code>user_dictionary_rules</code>という設定を利用することでカスタム辞書を指定できます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">PUT</span> <span style="color:#960050;background-color:#1e0010">custom_dic_sample</span>
{
  <span style="color:#f92672">&#34;settings&#34;</span>: {
    <span style="color:#f92672">&#34;index&#34;</span>: {
      <span style="color:#f92672">&#34;analysis&#34;</span>: {
        <span style="color:#f92672">&#34;tokenizer&#34;</span>: {
          <span style="color:#f92672">&#34;kuromoji_user_dict&#34;</span>: {
            <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;kuromoji_tokenizer&#34;</span>,
            <span style="color:#f92672">&#34;mode&#34;</span>: <span style="color:#e6db74">&#34;extended&#34;</span>,
            <span style="color:#f92672">&#34;user_dictionary_rules&#34;</span>: [
              <span style="color:#e6db74">&#34;グランベリーパーク,グランベリー パーク,グランベリー パーク,カスタム名詞&#34;</span>,
              <span style="color:#e6db74">&#34;高輪ゲートウェイ,高輪 ゲートウェイ,タカナワ ゲートウェイ,カスタム名詞&#34;</span>]
          }
        },
        <span style="color:#f92672">&#34;analyzer&#34;</span>: {
          <span style="color:#f92672">&#34;my_analyzer&#34;</span>: {
            <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;custom&#34;</span>,
            <span style="color:#f92672">&#34;tokenizer&#34;</span>: <span style="color:#e6db74">&#34;kuromoji_user_dict&#34;</span>
          }
        }
      }
    }
  }
}
</code></pre></div><p>辞書の内部は<code>&quot;単語,出てきてほしい単語列(スペース区切り),読みの単語列(スペース区切り),品詞名&quot;</code>になります。配列で設定可能で、複数の単語を登録したい場合は、カンマ区切りで登録していきます(上記例では2つの単語を登録しています)。</p>
<h2 id="_analyzeを利用して設定の確認">_analyzeを利用して設定の確認</h2>
<p>実際に上記の設定がうまく動作するかは<code>_analyze</code>のエンドポイントを利用します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">custom_dic_sample/_analyze</span>
{
  <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;グランベリーパークがオープンしました。&#34;</span>,
  <span style="color:#f92672">&#34;analyzer&#34;</span>: <span style="color:#e6db74">&#34;my_analyzer&#34;</span>
}
</code></pre></div><p>出力は以下のようになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;tokens&#34;</span> : [
    {
      <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;グランベリー&#34;</span>,
      <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">6</span>,
      <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
      <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">0</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;パーク&#34;</span>,
      <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">6</span>,
      <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">9</span>,
      <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
      <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">1</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;が&#34;</span>,
      <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">9</span>,
      <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">10</span>,
      <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
      <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">2</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;オープン&#34;</span>,
      <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">10</span>,
      <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">14</span>,
      <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
      <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">3</span>
    },
    <span style="color:#960050;background-color:#1e0010">...(省略)</span>
  ]
}
</code></pre></div><p>ちなみに、デフォルトの<code>kuromoji</code>を利用した場合は、<code>グランベリーパーク</code>が1単語として出力されます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">_analyze</span>
{
  <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;グランベリーパークがオープンしました。&#34;</span>,
  <span style="color:#f92672">&#34;analyzer&#34;</span>: <span style="color:#e6db74">&#34;kuromoji&#34;</span>
}

<span style="color:#960050;background-color:#1e0010">##</span> <span style="color:#960050;background-color:#1e0010">レスポンス</span>
{
  <span style="color:#f92672">&#34;tokens&#34;</span> : [
    {
      <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;グランベリーパーク&#34;</span>,
      <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">9</span>,
      <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
      <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">0</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;オープン&#34;</span>,
      <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">10</span>,
      <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">14</span>,
      <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
      <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">2</span>
    }
  ]
}
</code></pre></div><p>これで、カスタム辞書を使用することで<code>グランベリー</code>で検索された場合に、<code>グランベリーパーク</code>もヒットするという仕組みです。</p>
<p><code>_analyze</code>は<code>explain</code>というパラメータも持っており、こちらを利用することで、単語の品詞情報なども取得できます。これを使うことで、実際に設定がきちんと動作しているかの確認に利用できます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">custom_dic_sample/_analyze</span>
{
  <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;グランベリーパークがオープンしました。&#34;</span>,
  <span style="color:#f92672">&#34;analyzer&#34;</span>: <span style="color:#e6db74">&#34;my_analyzer&#34;</span>,
  <span style="color:#f92672">&#34;explain&#34;</span>: <span style="color:#66d9ef">true</span>
}
<span style="color:#960050;background-color:#1e0010">##</span> <span style="color:#960050;background-color:#1e0010">レスポンス(一部のみ)</span>
{
  <span style="color:#f92672">&#34;detail&#34;</span> : {
    <span style="color:#f92672">&#34;custom_analyzer&#34;</span> : <span style="color:#66d9ef">true</span>,
    <span style="color:#f92672">&#34;charfilters&#34;</span> : [ ],
    <span style="color:#f92672">&#34;tokenizer&#34;</span> : {
      <span style="color:#f92672">&#34;name&#34;</span> : <span style="color:#e6db74">&#34;kuromoji_user_dict&#34;</span>,
      <span style="color:#f92672">&#34;tokens&#34;</span> : [
        {
          <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;グランベリー&#34;</span>,
          <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">0</span>,
          <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">6</span>,
          <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
          <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">0</span>,
          <span style="color:#f92672">&#34;baseForm&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;bytes&#34;</span> : <span style="color:#e6db74">&#34;[e3 82 b0 e3 83 a9 e3 83 b3 e3 83 99 e3 83 aa e3 83 bc]&#34;</span>,
          <span style="color:#f92672">&#34;inflectionForm&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;inflectionForm (en)&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;inflectionType&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;inflectionType (en)&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;partOfSpeech&#34;</span> : <span style="color:#e6db74">&#34;カスタム名詞&#34;</span>,
          <span style="color:#f92672">&#34;partOfSpeech (en)&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;positionLength&#34;</span> : <span style="color:#ae81ff">1</span>,
          <span style="color:#f92672">&#34;pronunciation&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;pronunciation (en)&#34;</span> : <span style="color:#66d9ef">null</span>,
          <span style="color:#f92672">&#34;reading&#34;</span> : <span style="color:#e6db74">&#34;グランベリー&#34;</span>,
          <span style="color:#f92672">&#34;reading (en)&#34;</span> : <span style="color:#e6db74">&#34;guramberi&#34;</span>,
          <span style="color:#f92672">&#34;termFrequency&#34;</span> : <span style="color:#ae81ff">1</span>
        },
        {
</code></pre></div><p><code>partOfSpeech</code>にカスタム辞書で設定した<code>カスタム名詞</code>が出力されていますね。
<code>_analyze</code>のAPIはこのように、アナライザーの挙動の確認に非常に便利なので是非活用してみてください。</p>
<p><a href="https://github.com/johtani/analyze-api-ui-plugin">KibanaでこのAPIを使うためのプラグイン</a>も開発していますので、こちらも合わせて利用してみてください。</p>
<h2 id="注意点">注意点</h2>
<p>ちなみに、<code>user_dictionary</code>と<code>user_dictionary_rules</code>を<a href="https://github.com/elastic/elasticsearch/blob/master/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/KuromojiTokenizerFactory.java#L62">両方指定した場合はエラー</a>となります。
ファイルをベースにしつつ、追加の設定をするという使い方はできないので、注意しましょう。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">インデックスの設定で書くことにより、バックアップ・リストアは楽になるかな。ただ、クラスターステートに取り込まれるから、あまりにも巨大なカスタム辞書だと心配かなぁ。ファイルの場合はクラスターステートには取り込まれないので、そこは圧迫しない。</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1252790960108400641?ref_src=twsrc%5Etfw">April 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>それ以外の注意点については、別途ブログを書きました。
<a href="/blog/2020/04/27/note-updating-dictionary/">「辞書の更新についての注意点」</a>、こちらも合わせてご覧ください。</p>
<h2 id="まとめ">まとめ</h2>
<p>カスタム辞書をファイルではなくインデックスの設定値として設定する方法を紹介しました。こちらは、Elasticsearch 7.4で導入された機能になります。7.4以降を利用している場合はこちらを利用することも検討してはいかがでしょうか?
また、<code>_analyze</code> APIも便利なので合わせて活用してみてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第8章</title>
      <link>https://blog.johtani.info/blog/2020/04/16/chap8-rust-the-book/</link>
      <pubDate>Thu, 16 Apr 2020 18:17:30 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/16/chap8-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 Rust the book - 第5章 Rust the book - 第6章 第8章 7章はパ</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
<li><a href="/blog/2020/04/02/chap5-rust-the-book/">Rust the book - 第5章</a></li>
<li><a href="/blog/2020/04/07/chap6-rust-the-book/">Rust the book - 第6章</a></li>
</ul>
<h2 id="第8章">第8章</h2>
<p>7章はパッケージなので後回しにして、8章に入ります。
8章はコレクションです。</p>
<h3 id="ベクタ型">ベクタ型</h3>
<ul>
<li>ベクタは同じ型の値だけ保持可能。
<ul>
<li>ジェネリクスで型を指定可能 - <code>Vec&lt;i32&gt;</code>とか。</li>
</ul>
</li>
<li><code>vec!</code>マクロで初期値とか設定すると便利。</li>
<li>ベクタに値を追加するのは<code>push</code>。もちろん値が変わるので元のベクタには<code>mut</code>が必要</li>
<li>ベクタのスコープ(ライフサイクル)は要素に対する参照があるのとないので話が変わってくる
<ul>
<li>メモリの確保などの影響で、ベクタ全体に対して借用の規則が矯正されると。</li>
</ul>
</li>
<li>ベクタの値を読むのはいくつか方法あり
<ul>
<li><code>get</code>メソッドはOptionを返す</li>
<li><code>&amp;v[2]</code>の添字記法の場合はパニックの可能性あり</li>
</ul>
</li>
<li>走査(唐突に参照外しが出てきた)
<ul>
<li>単純に値を取り出す場合は<code>for - in &amp;v</code></li>
</ul>
</li>
<li>Enumをベクタにいれることで、異なる型も保持可能(まぁ、Enumの型では固定されるけど)。
<ul>
<li>これだけのためにEnumを使うことってあるのかな?</li>
<li>トレイとオブジェクトに関する文章はちょっとわかりにくい。。。</li>
</ul>
</li>
</ul>
<p>説明以外のメソッドなどについてはAPIドキュメント見ましょうと(リンクも張ってくれてると嬉しいなぁと思ったり。まぁ、バージョンとかの絡みがあるから難しいか)。</p>
<h3 id="文字列型">文字列型</h3>
<ul>
<li>
<p>文字列はUTF-8でエンコードされた文字を扱うための型。</p>
</li>
<li>
<p><code>str</code>は文字列データへの参照。</p>
</li>
<li>
<p><code>String</code>型は言語のコアではなく、標準ライブラリに入っている文字列型。</p>
<ul>
<li>他にもあるのか。。。<code>OsString</code>とか。。。</li>
</ul>
</li>
<li>
<p>文字リテラルはDisplayトレイトを実装していると。</p>
</li>
<li>
<p><code>.to_string()</code> = <code>String::from</code></p>
</li>
<li>
<p>Stringはコレクションだから追加とかが可能なのか、なるほど。</p>
</li>
<li>
<p><code>push_str</code>と<code>push</code></p>
</li>
<li>
<p><a href="https://doc.rust-jp.rs/book/second-edition/ch08-02-strings.html#a%E6%BC%94%E7%AE%97%E5%AD%90%E3%81%BE%E3%81%9F%E3%81%AFformat%E3%83%9E%E3%82%AF%E3%83%AD%E3%81%A7%E9%80%A3%E7%B5%90"><code>+</code>演算子での参照</a>。</p>
<ul>
<li><code>&amp;String</code>は<code>&amp;str</code>に型強制(キャスト?)してくれる。してくれる場合としてくれない場合もあるのかな?<code>s2</code>の所有権は奪わない形で扱うので<code>s2</code>はこのあとも使えていると。</li>
<li>ここでは、<code>s1</code>を変更したあとに所有権が<code>s3</code>に持っていかれてる?</li>
<li><code>format!</code>を使うとどの所有権も奪わないので、これを使うほうが考え方は簡単そう。ただし、効率がいいかはわからん。</li>
</ul>
</li>
<li>
<p>添字記法でのアクセスを<code>String</code>は許容していない</p>
<ul>
<li>文字の境界が必ずしも1バイトとは限らないから。</li>
<li>スライスも同様。</li>
</ul>
</li>
<li>
<p>基本的には<code>.chars()</code>で文字としてアクセスするのが良い。</p>
</li>
<li>
<p>逆にバイト表現を得る方法はどうするんだろう?</p>
<ul>
<li><a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter01/answer.rs#L155">NLP100本ノックでは<code>encode_utf8</code>メソッド使ったけど。</a></li>
</ul>
</li>
</ul>
<h3 id="ハッシュマップ">ハッシュマップ</h3>
<ul>
<li>いろんな呼び方あるよね。Rustではハッシュマップだよ。</li>
<li>ハッシュマップは<code>use</code>しないと使えない</li>
<li>キーは1つの型、値も1つの型</li>
<li>タプルのベクタから<code>collect</code>で生成。なるほど。
<ul>
<li>タプルのベクタだと、タプルの中身は同じものであることが言える?
<ul>
<li>-&gt; 言える。エレメント数が異なるとコンパイルエラーになった</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://doc.rust-jp.rs/book/second-edition/ch08-03-hash-maps.html#a%E3%83%8F%E3%83%83%E3%82%B7%E3%83%A5%E3%83%9E%E3%83%83%E3%83%97%E3%81%A8%E6%89%80%E6%9C%89%E6%A8%A9">所有権周りの話。</a>
<ul>
<li>これ、ベクタのときに話してほしい感じがした。</li>
<li>値を渡すか参照を渡すかによって話が変わってくる。詳しくは10章</li>
<li>このあたりが自分が混乱していた元だ。</li>
</ul>
</li>
<li><code>entry</code>と<code>insert</code>の違い
<ul>
<li><code>entry</code>の戻り値は<code>Entry</code>というenumで<code>or_insert</code>というメソッドがありそれを使うと存在しない場合だけinsertが呼ばれる。
<ul>
<li>これ便利だ。毎回<code>exist</code>あたりで存在チェックしてた気がする。</li>
</ul>
</li>
</ul>
</li>
<li><code>or_insert</code>は可変参照<code>&amp;mut V</code>を返す。
<ul>
<li>これを<code>let count</code>で束縛するときに、中身が可変かどうかをcountには指定しないのか。。。</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>一応、大学などで習ってた(はず)ですが、
スタックとヒープを意識して考えないといけないなぁというのを何度か意識させられた感じです。</p>
<p>あと、これはRustに限らずですが、それぞれがどんな関数を持っているか、どんなメソッドを持っているか、どんなマクロが存在するかなどを探すときにみんなどうしてるんだろう?
人に教えてもらっているのか、APIリファレンスを探すのか、そういったところをみんながどういう感じにプログラミング言語を勉強しているか、業務で書いているのかと言うのが気になりました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第1章の03から06まで(言語処理100本ノック2020)</title>
      <link>https://blog.johtani.info/blog/2020/04/09/reboot-nlp100-ch01-03to05/</link>
      <pubDate>Thu, 09 Apr 2020 18:14:55 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/09/reboot-nlp100-ch01-03to05/</guid>
      <description>Rustで言語処理100本ノックのリファクタリングの続き。 前回はこちら。 コードも載せたほうが見やすいかなぁ? 03. 円周率 2年前はこちら。 どちらか</description>
      <content:encoded><p>Rustで言語処理100本ノックのリファクタリングの続き。</p>
<p>前回は<a href="/blog/2020/04/08/reboot-nlp10-with-rust/">こちら</a>。</p>
<p>コードも載せたほうが見やすいかなぁ?</p>
<h3 id="03-円周率">03. 円周率</h3>
<p><a href="/blog/2018/02/19/nlp100-ch01-03to04/#03-%E5%86%86%E5%91%A8%E7%8E%87">2年前はこちら</a>。
どちらかというとJavaっぽい書き方かな?
入れ物を用意して、入力を整形して、それからループを回す感じで書いてました。</p>
<p><a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter01/answer.rs#L50">今回は1行で収めてみました</a>。
Rustっぽく、<code>return</code>を省略してみました。
あとは、Iteratorを組み合わせる感じでやってます。
アルファベットの文字数ということで、<code>is_alphabetic()</code>メソッドでfilterしてます。</p>
<h3 id="04-元素記号">04. 元素記号</h3>
<p><a href="/blog/2018/02/19/nlp100-ch01-03to04/#04-%E5%85%83%E7%B4%A0%E8%A8%98%E5%8F%B7">2年前はこちら</a>。
エラー処理が多いのと、文字の扱いがちょっと</p>
<p>ここまで同様に<a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter01/answer.rs#L64">極力イテレータを利用する</a>という方針でリファクタリングしました。
あとは、エラー処理を除去してます。
正常系だけのテストなのでスッキリさせました。</p>
<p>こういった、ストリーム系?の書き方の場合にエラー処理をどう入れるかってところはちょっと悩みどころになるんじゃないかなぁ?と思いつつ、イレギュラーなものは後回しで(あとにやるのかなぁ?)</p>
<h3 id="05-n-gram">05. n-gram</h3>
<p><a href="/blog/2018/03/20/nlp100-ch01-05to06/#05-n-gram">2年前はこちら</a>。
これまでと同じく、入れ物を作ってから処理をしてます。</p>
<p>同じく、<a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter01/answer.rs#L82">極力イテレータを利用する形</a>で実装しました。
いくつか型の変換が必要なので、<code>.map()</code>を呼び出して詰め替えたりしています。</p>
<h3 id="06-集合">06. 集合</h3>
<p><a href="/blog/2018/03/20/nlp100-ch01-05to06/#06-%E9%9B%86%E5%90%88">2年前はこちら</a>。
独自に実装しています。</p>
<p>せっかく<code>05</code>で文字n-gramの配列を返す処理を実装しているので、
そちらを呼び出して、Setに入れるという処理に書き換えました。
その後の集合に対する処理については特にリファクタリングしてないです。</p>
<h2 id="まとめ">まとめ</h2>
<p>2年前にやってたところまでは追いつきました。
07は特にリファクタリングする必要がないので、次は08からの予定です。</p>
<p>リファクタリングしているときになるのは、速度とかでしょうか。
実装の違いでなにか差が出るのかどうかはちょっと気になるところですが、
今回の目的ではないので、目をつぶって進める予定です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>言語処理100本ノック、再び</title>
      <link>https://blog.johtani.info/blog/2020/04/08/reboot-nlp10-with-rust/</link>
      <pubDate>Wed, 08 Apr 2020 18:33:45 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/08/reboot-nlp10-with-rust/</guid>
      <description>今回もツイートから。 言語処理100本ノックの2020年版を公開しました。最近の自然言語処理の研究動向を反映し、深層ニューラルネットワークに関</description>
      <content:encoded><p>今回もツイートから。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">言語処理100本ノックの2020年版を公開しました。最近の自然言語処理の研究動向を反映し、深層ニューラルネットワークに関する問題を追加しました。留学生も一緒に取り組めるように多言語化を進め、その第１弾として英訳を部分公開しています（40番以降は順次公開予定）。 <a href="https://t.co/52h362PIQQ">https://t.co/52h362PIQQ</a></p>&mdash; Naoaki Okazaki (@chokkanorg) <a href="https://twitter.com/chokkanorg/status/1247312205671874561?ref_src=twsrc%5Etfw">April 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><a href="https://nlp100.github.io/ja/">言語処理100本ノック</a>が2020年版になったそうです。
そうです、2年前に初めて、<a href="/blog/2018/03/20/nlp100-ch01-05to06/">準備運動</a>で止まっていたんです!(衝撃的な続かなさ。。。)</p>
<p>ということで、Rust the bookも読んでいることだし、過去のプログラムをチェックしつつ再開しようかなと。
ということで、いくつかリファクタリングしてみました。</p>
<p><a href="https://github.com/johtani/nlp100-rust/blob/master/src/chapter01/answer.rs">ソースはリポジトリ</a>を御覧ください。</p>
<h3 id="00-文字列の逆順">00. 文字列の逆順</h3>
<p><a href="https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs#L6">2年前の実装</a>では、<code>chars()</code>メソッドで取り出したあとに、<code>collect()</code>でVecにしていたのですが、<a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.rev">RustのIteratorトレイトに<code>rev()</code>という便利なメソッドが存在していました</a>。</p>
<p>ということで、これを取り出すと、最初の文字列の逆順で文字を取り出すIteratorが取得できます。
あとは、<a href="https://doc.rust-lang.org/std/iter/trait.FromIterator.html#tymethod.from_iter">Stringが実装してくれている<code>from_iter</code></a>に渡せば文字列が出来上がります。</p>
<h3 id="01-パタトクカシーー">01. 「パタトクカシーー」</h3>
<p>ストリーム処理っぽい書き方に変更しました。
2年前はIteratorを取り出して、詰替していましたが、
<code>enumerate()</code>で添字と文字のタプルのイテレータに変換し、
<code>filter</code>で添字が偶数のときだけフィルタリングして、
<code>map</code>で対象の文字をまとめたイテレータにします。
で、最後はそれを元に文字列を生成することにしました。
<code>iter</code>を使わないでそのまま<code>String::from_iter</code>の引数に渡すことも可能ですね。</p>
<h3 id="02-パトカータクシーパタトクカシーー">02. 「パトカー」＋「タクシー」＝「パタトクカシーー」</h3>
<p>2年前は<a href="https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs#L30">2つのIteratorを<code>loop</code>で回して頑張って結合してました</a>。
ではなく、<a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.zip"><code>zip()</code></a>を使って、2つのイテレータを組み合わせる方法に替えてみました。
このとき、2つの文字列が違う文字数の場合の処理として、長い方から取り出した文字をあとに結合する処理を追加で記述しました。
ちょっとスマートな感じになりましたかね?</p>
<p><code>zip</code>したあとに出てきたタプルの文字列を結合するのに<code>format!</code>マクロを使いましたが、他にいい方法有るかなぁ?</p>
<h2 id="まとめ">まとめ</h2>
<p>とりあえず最初の3つをリファクタリングしてみました。
残りもやりつつ、準備運動以降もがんばるぞと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第6章</title>
      <link>https://blog.johtani.info/blog/2020/04/07/chap6-rust-the-book/</link>
      <pubDate>Tue, 07 Apr 2020 19:27:11 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/07/chap6-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 Rust the book - 第5章 第6章 Enumです。matc</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
<li><a href="/blog/2020/04/02/chap5-rust-the-book/">Rust the book - 第5章</a></li>
</ul>
<h2 id="第6章">第6章</h2>
<p>Enumです。<code>match</code>式に大活躍</p>
<h3 id="enumを定義する">Enumを定義する</h3>
<ul>
<li>列挙型は取りうる値をすべて<em>列挙</em>できる。これが名前の由来</li>
<li>列挙型と列挙子
<ul>
<li>2連コロン(<code>::</code>)で列挙子を指定可能</li>
</ul>
</li>
<li>列挙子にデータ(構造体も)が格納可能。
<ul>
<li><a href="https://doc.rust-lang.org/stable/std/net/enum.IpAddr.html">標準ライブラリに実装例あり。</a></li>
</ul>
</li>
<li>疑問:<code>Write(String)</code>とかはタプルの表現になるのかな?
<ul>
<li>と思ったが、タプルでは1つだけの変数を持つものは定義(正確には定義できるが、内部で普通の変数にもどされてるっぽい)できなかった。</li>
</ul>
</li>
<li>メソッド定義も可能
<ul>
<li>関連関数もできる? -&gt; できる</li>
</ul>
</li>
</ul>
<h4 id="optionの紹介">Optionの紹介</h4>
<ul>
<li>Rustに<code>null</code>はない。代わりにOptionがある</li>
<li>Noneを指定する場合に型が必要。Someの場合はすでに値が入るから推測可能なため。</li>
</ul>
<h3 id="match制御フロー演算子">match制御フロー演算子</h3>
<ul>
<li>アーム -&gt; matchしたときの処理のこと
<ul>
<li>短い場合は波括弧は不要</li>
</ul>
</li>
<li>returnなしでmatchが書いてあるだけだと、慣れない場合に値を返していることに気づかないかも(実際気づけてないかも)</li>
<li>Enumが値を持っているときに、値の束縛がmatch式で可能</li>
<li>すべての列挙子を網羅していないことをコンパイラが検知してくれるのはすごく助かる。
<ul>
<li>ただし、<code>_</code>を利用していなければだけど</li>
</ul>
</li>
</ul>
<h3 id="if-letで簡潔な制御フロー">if letで簡潔な制御フロー</h3>
<ul>
<li>enumで1つのパターンのときに処理をしたい場合に使えるmatchの糖衣構文</li>
<li>elseもかけるよ。</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>enumに慣れていないので、値や構造体を持つenumを利用するという想像ができないことがありそうだなぁと読みながら思いました。
それになれると、色々とプログラムがシンプルに書ける部分が多くなりそうかな。</p>
</content:encoded>
    </item>
    
    <item>
      <title>KuromojiのCLIコマンドにJSON出力とラティス出力を追加</title>
      <link>https://blog.johtani.info/blog/2020/04/06/update-kuromoji-cli/</link>
      <pubDate>Mon, 06 Apr 2020 12:26:40 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/06/update-kuromoji-cli/</guid>
      <description>Kuromoji-CLIの使い方などについては過去のブログを御覧ください。 KuromojiのCLIコマンドとpicocliとGraalVM G</description>
      <content:encoded><p>Kuromoji-CLIの使い方などについては過去のブログを御覧ください。</p>
<ul>
<li><a href="/post/2020/02/28/kuromoji-cli/">KuromojiのCLIコマンドとpicocliとGraalVM</a></li>
<li><a href="https://github.com/johtani/kuromoji-cli">GitHubリポジトリ</a></li>
</ul>
<p>Issueだけ上げていたJSON出力対応をしました。
また、ラティス(後述)の出力対応もしました。</p>
<h2 id="json出力">JSON出力</h2>
<p><a href="https://github.com/lindera-morphology/lindera-cli#output-format">Lindera</a>がJSON出力に対応してるのでそれを真似しました。
<code>-o json</code>で指定していただくと、次のようなJSONが出力されます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">% echo <span style="color:#e6db74">&#34;春眠暁を覚えず&#34;</span> | build/graal/kuromoji -o json
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">[
  {
    <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;春眠&#34;</span>,
    <span style="color:#f92672">&#34;detail&#34;</span>: [
      <span style="color:#e6db74">&#34;名詞&#34;</span>,
      <span style="color:#e6db74">&#34;一般&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;春眠&#34;</span>,
      <span style="color:#e6db74">&#34;シュンミン&#34;</span>,
      <span style="color:#e6db74">&#34;シュンミン&#34;</span>
    ]
  },
  {
    <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;暁&#34;</span>,
    <span style="color:#f92672">&#34;detail&#34;</span>: [
      <span style="color:#e6db74">&#34;名詞&#34;</span>,
      <span style="color:#e6db74">&#34;一般&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;暁&#34;</span>,
      <span style="color:#e6db74">&#34;アカツキ&#34;</span>,
      <span style="color:#e6db74">&#34;アカツキ&#34;</span>
    ]
  },
  {
    <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;を&#34;</span>,
    <span style="color:#f92672">&#34;detail&#34;</span>: [
      <span style="color:#e6db74">&#34;助詞&#34;</span>,
      <span style="color:#e6db74">&#34;格助詞&#34;</span>,
      <span style="color:#e6db74">&#34;一般&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;を&#34;</span>,
      <span style="color:#e6db74">&#34;ヲ&#34;</span>,
      <span style="color:#e6db74">&#34;ヲ&#34;</span>
    ]
  },
  {
    <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;覚え&#34;</span>,
    <span style="color:#f92672">&#34;detail&#34;</span>: [
      <span style="color:#e6db74">&#34;動詞&#34;</span>,
      <span style="color:#e6db74">&#34;自立&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;一段&#34;</span>,
      <span style="color:#e6db74">&#34;未然形&#34;</span>,
      <span style="color:#e6db74">&#34;覚える&#34;</span>,
      <span style="color:#e6db74">&#34;オボエ&#34;</span>,
      <span style="color:#e6db74">&#34;オボエ&#34;</span>
    ]
  },
  {
    <span style="color:#f92672">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;ず&#34;</span>,
    <span style="color:#f92672">&#34;detail&#34;</span>: [
      <span style="color:#e6db74">&#34;助動詞&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;*&#34;</span>,
      <span style="color:#e6db74">&#34;特殊・ヌ&#34;</span>,
      <span style="color:#e6db74">&#34;連用ニ接続&#34;</span>,
      <span style="color:#e6db74">&#34;ぬ&#34;</span>,
      <span style="color:#e6db74">&#34;ズ&#34;</span>,
      <span style="color:#e6db74">&#34;ズ&#34;</span>
    ]
  }
]
</code></pre></div><h2 id="viterbiラティス出力">Viterbiラティス出力</h2>
<p>Kuromojiが内部でトークンをどのように切り出すかを計算するためのViterbiのラティスです。これを<a href="http://www.graphviz.org/">Graphviz</a>というツールのDOTフォーマットのファイルとして出力できるメソッドがデバッグ用ですが、Kuromojiに用意されています。
こちらを呼び出して出力するオプション<code>-v</code>(もしくは<code>--viterbi</code>)を追加しました。
<strong>注意点として、このオプションを指定すると、DOTファイルが標準出力に出力され、トークンの結果は標準エラーに出力されます。</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">echo <span style="color:#e6db74">&#34;春眠暁を覚えず&#34;</span> | build/graal/kuromoji -v &gt; viterbi.dot
</code></pre></div><p><code>.dot</code>ファイル自体は画像ではないので、Graphvizの<code>dot</code>コマンドにて画像ファイルに変換する必要があります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">dot -Tpng viterbi.dot -oviterbi.png
</code></pre></div><p>これで、PNGファイルが出来上がります。出来上がったファイルはこんな感じです。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200406/viterbi.png" />
    </div>
    <a href="/images/entries/20200406/viterbi.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>画像を見ていただくと、緑色のパスがあるのがわかります。
こちらが、Kuromojiが実際に採用した形態素のリストです。それ以外のパスは不採用だったパスとなります。</p>
<p>ちなみに、macOSで1行で画像表示まで行うにはこんな感じで実行します。
<code>open</code>コマンドでpreviewアプリを指定することで画像が表示できます。
Windowsは。。。わかりません、すみません。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">% echo <span style="color:#e6db74">&#34;春眠暁を覚えず&#34;</span> | build/graal/kuromoji -v -o json | dot -Tpng | open -f -a preview.app
</code></pre></div><h2 id="まとめ">まとめ</h2>
<p>これ以前に複数辞書対応などもしていました。
今回は2つの出力形式を追加してみました。
特にViterbiラティス出力については、内部的にどのようなコスト計算で最終的な結果が出てきているかという理解に役立ちます。想定していない切れ方の場合は、そもそも想定している形態素になっていないか、コスト計算で不採用だったかなどを確認できますので、使ってみると面白いかもです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第5章</title>
      <link>https://blog.johtani.info/blog/2020/04/02/chap5-rust-the-book/</link>
      <pubDate>Thu, 02 Apr 2020 15:09:18 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/04/02/chap5-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた Rust the book - 第4章 第5章 構造体です。勝手知ったるなんとやら?</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
<li><a href="/blog/2020/03/26/chap4-rust-the-book/">Rust the book - 第4章</a></li>
</ul>
<h2 id="第5章">第5章</h2>
<p>構造体です。勝手知ったるなんとやら?オブジェクト指向的な部分は問題ないかなぁと。</p>
<h3 id="定義とインスタンス化">定義とインスタンス化</h3>
<ul>
<li><code>struct</code>で定義</li>
<li>インスタンスの生成は引数は順不同でOK</li>
<li>構造体のインスタンスを可変にするとフィールドの値も変更可能
<ul>
<li>特定のフィールドのみ可変にすることは不可能</li>
</ul>
</li>
<li>インスタンス化する関数の最後でreturnなしでインスタンスの返却を暗黙にできる(return書いてほしいな。。。)</li>
<li>インスタンス化時にフィールド初期化省略記法が可能(これはちょっと便利?)</li>
<li>構造体更新記法<code>..user1</code>のように、明示的に設定されていない他のフィールドをコピーしてくれる機能あり</li>
</ul>
<h4 id="タプル構造体">タプル構造体</h4>
<ul>
<li>タプル構造体!? <code>struct Color(i32, i32, i32);</code>
<ul>
<li>いつ使うんだろう?</li>
</ul>
</li>
</ul>
<h4 id="ユニット様構造体">ユニット様構造体</h4>
<ul>
<li>ユニット様構造体 = フィールドのない構造体。トレイトを実装したいけどインスタンスで持つ値はない場合に利用</li>
</ul>
<h4 id="ライフタイム">ライフタイム</h4>
<ul>
<li>構造体が参照を持つときにライフタイムという話が出てくる。なるほど。
<ul>
<li>ライフタイム指定子が必要になる -&gt; 10章での話</li>
</ul>
</li>
</ul>
<h3 id="プログラム例">プログラム例</h3>
<ul>
<li>タプルを引数かぁ。タプルは慣れないので構造体作りそう</li>
<li>Debugトレイトと<code>{:?}</code>という書き方
<ul>
<li><code>derive(Debug)</code>でデバッグ用のトレイトを自動で実装=継承してくれる</li>
<li><code>{:#?}</code>だとpretty printになる(改行とか入る)</li>
</ul>
</li>
</ul>
<p>この辺の便利なトレイとは<a href="https://doc.rust-jp.rs/book/second-edition/appendix-03-derivable-traits.html">付録C</a>にあるらしい。この辺はやりながら覚えるしかないか。</p>
<h3 id="メソッド記法">メソッド記法</h3>
<ul>
<li>最初の引数は必ず<code>self</code></li>
<li><code>impl</code>は構造体とは別の場所に書く = Javaのクラスとは違う</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Rectangle</span> {
    width: <span style="color:#66d9ef">u32</span>,
    height: <span style="color:#66d9ef">u32</span>,
}

<span style="color:#66d9ef">impl</span> Rectangle {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">area</span>(<span style="color:#f92672">&amp;</span>self) -&gt; <span style="color:#66d9ef">u32</span> {
        self.width <span style="color:#f92672">*</span> self.height
    }
}
</code></pre></div><ul>
<li>参照じゃない<code>self</code>も使えるらしい。どういうときに使うんだろう?</li>
</ul>
<h4 id="関連関数">関連関数</h4>
<ul>
<li><code>self</code>なしの関数をimplにかける。Javaのスタティックメソッドみたいな感じ</li>
</ul>
<h4 id="その他">その他</h4>
<ul>
<li><code>impl</code>ブロックがあちこちにかける。これはつらいな。。。</li>
<li>2つにわかれた<code>impl</code>ブロックに同じメソッドを書いてみたら、CLionのプラグインではエラーを検知してもらえなかった。
<ul>
<li>cargo buildではきちんとエラーが表示された。</li>
</ul>
</li>
<li>
<blockquote>
<p>複数のimplブロックが有用になるケースは第10章で見ますが、そこではジェネリック型と、トレイトについて議論します。</p>
</blockquote>
<ul>
<li>人の構造体に自分のトレイトを適用したりもできる。</li>
</ul>
</li>
</ul>
<h4 id="実験">実験</h4>
<p>スコープとかどうなりそう?って実験もしてみた。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">main</span>() {
    <span style="color:#66d9ef">trait</span> Hoge {
        <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">trim</span>(<span style="color:#f92672">&amp;</span>self);
    }

    <span style="color:#66d9ef">impl</span> Hoge <span style="color:#66d9ef">for</span> String {
        <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">trim</span>(<span style="color:#f92672">&amp;</span>self) {
            println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;hogehoge {}&#34;</span>, <span style="color:#f92672">&amp;</span>self);
        }
    }
    <span style="color:#66d9ef">let</span> c <span style="color:#f92672">=</span> String::from(<span style="color:#e6db74">&#34;hoge&#34;</span>);
    c.trim();
    println<span style="color:#f92672">!</span>(<span style="color:#e6db74">&#34;{}&#34;</span>, fuga(<span style="color:#f92672">&amp;</span>c));
}

<span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">fuga</span>(d: <span style="color:#66d9ef">&amp;</span>String) -&gt; <span style="color:#66d9ef">&amp;</span><span style="color:#66d9ef">str</span> {
    d.trim()
}
</code></pre></div><p>出力はこんな感じ</p>
<pre><code>hogehoge hoge
hoge
</code></pre><h2 id="まとめ">まとめ</h2>
<p>気になったのは以下の点。そのうち分かるようになってくるのかな。</p>
<ul>
<li>構造体更新記法はどういったときに使うのを想定して作ったんだろう?とか</li>
<li>可変長引数はマクロじゃないとだめ</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>自宅の作業環境(2020)</title>
      <link>https://blog.johtani.info/blog/2020/03/26/working-facility/</link>
      <pubDate>Thu, 26 Mar 2020 19:11:51 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/03/26/working-facility/</guid>
      <description>すもけさんが在宅勤務環境をブログに書いてておもしろそうだな(あと、アフィリンク貼れるな)と思ったので自分の環境も書いてみようかなと。 私自身は</description>
      <content:encoded><p><a href="https://www.smokeymonkey.net/2020/03/wfh.html">すもけさんが在宅勤務環境をブログ</a>に書いてておもしろそうだな(あと、アフィリンク貼れるな)と思ったので自分の環境も書いてみようかなと。</p>
<p>私自身は昨今の新型コロナウイルスの影響というのではなく、もう10年以上自宅でも作業ができる環境を整えています。
前前職の頃から家でも仕事をすることがよくあったので。今は、お客さんに恵まれていてリモートができる形で働かせていただいてます。</p>
<p>机周りはこんな感じです。机はこれも15年以上前から使用しているエレクターの机です。椅子も15年以上前から使っているアーロンチェア(購入時は高いと思ったけど、これだけ使えているので大満足)です。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200327/my_desk.jpg" />
    </div>
    <a href="/images/entries/20200327/my_desk.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>メインのPCはMac Book Pro16インチを使っています。それとは別にMac miniがMac bookの後ろに隠れています。こっちはブラウジングや音楽を聞いたりするために使っています。</p>
<p>まずはディスプレイ周りから。</p>
<h2 id="ディスプレイ周り">ディスプレイ周り</h2>
<h3 id="ディスプレイ">ディスプレイ</h3>
<p>Acerの31インチディスプレイです。昨年の夏に購入しました。28インチの4Kディスプレイを探していたのですが、セールか何かで2-3000円の違いでこのサイズが買えたので、このサイズになっちゃいました。Mac Book ProはDisplay port-USB-Cケーブルで接続し、PS4をHDMIで接続しています。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B0771DQYT5/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B0771DQYT5&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B0771DQYT5/?tag=johtani-22">
      【Amazon.co.jp 限定】Acer モニター ディスプレイ ET322QKwmiipx 31.5インチ
      </a>
    </p>
  </div>
</div></p>
<p>マイクの左側に写っているのはAppleのサンダーボルトディスプレイです。縦にしてブラウザやミュージックプレイヤーなどで音楽かけながら仕事をしています。
サブディスプレイって感じです。
Mac Book ProとMac miniの両方を操作するのですが、できれば同じキーボード+トラックパッドがいいなということで、<a href="https://symless.com/synergy">Synergy</a>というソフトを使って、Mac Book Proのキーボードとマウスでネットワーク越しにMac miniを操作しています。ただ、マシンの再起動後などにこのソフトが起動していない場合もあるので、Mac Book Proの右側にMac miniのためのトラックパッドがおいてあります。</p>
<h3 id="ディスプレイアーム">ディスプレイアーム</h3>
<p>ディスプレイはそれぞれディスプレイアームを使っています。写真のようにMac Book Proを下に、外部ディスプレイを上にという上下で作業をしています。
16インチの画面だと普通のディスプレイの足だとメインディスプレイにかぶってしまうので、アームに切り替えました。
最初はグリーンハウスの激安を使っていたのですが、31インチのディスプレイだとアームの高さが足りなかったので、2つ目のガススプリング式のアームを追加で購入しました。ガススプリング式なのに5千円しないので、かなりお得感があります。ただ、ディスプレイは基本的に動かさないので、どちらかというとディスプレイの足の部分を効率良く使うことが目的です。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B072MYML17/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B072MYML17&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B072MYML17/?tag=johtani-22">
      Amazon | グリーンハウス 液晶ディスプレイアーム 4軸 クランプ式
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07C2DBDNC/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07C2DBDNC&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07C2DBDNC/?tag=johtani-22">
      Amazon | HUANUO PC モニター アーム 液晶ディスプレイ アーム ガススプリング式
      </a>
    </p>
  </div>
</div>
<h2 id="サウンド関連">サウンド関連</h2>
<h3 id="スピーカー">スピーカー</h3>
<p>基本、音楽を聞きながら作業をするので、小型だけどしっかりと音がでるスピーカーを買いました。スタイルもいいし気に入ってます。
入力が2系統あるので、31インチディスプレイのヘッドフォン出力とMac miniのからの出力の2つをつないでいます。
仕事中はMac miniで音楽流しつつ、テレカンやゲームのときはメインディスプレイからの音が出る形です。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B00CY6RCJU/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B00CY6RCJU&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B00CY6RCJU/?tag=johtani-22">
      Amazon | ヤマハ パワードスピーカー NX-50
      </a>
    </p>
  </div>
</div></p>
<h3 id="マイク">マイク</h3>
<p>前職がDeveloper Advocateということで、インタビューなどに行くこともあるかも?ということで小さめのも運びができるコンデンサマイクを購入してました。結局それほど持ち運びはしてないのですが、値段の割には音がいいんじゃないかな?一応、無指向と単一指向の2種類を切り替えることが可能です。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B001R76D42/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B001R76D42&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B001R76D42/?tag=johtani-22">
      Amazon | SAMSON マイク ポータブル USB コンデンサ Go Mic | コンデンサ | 楽器
      </a>
    </p>
  </div>
</div>
<h3 id="マイクスタンド">マイクスタンド</h3>
<p>マイクはあったのですが、クリップ式でした。PCのディスプレイにクリップしていると、打鍵音を拾ってしまうので、マイクスタンドを最近導入しました。やすかったです。
前まではヘッドフォンのケーブルに付いているマイクで喋っていたのですが、服によってはマイクに擦れてしまい、ノイズが載っていました。
マイクスタンド+スピーカーでもスピーカーの音をマイクが拾わないので、最近はヘッドフォンを使用しないで気楽に話ができるようになっています。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07Q8KVZWX/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07Q8KVZWX&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07Q8KVZWX/?tag=johtani-22">
      Amazon | Luling Arts マイクスタンド アーム 
      </a>
    </p>
  </div>
</div></p>
<h2 id="その他">その他</h2>
<h3 id="モニタスタンド">モニタスタンド</h3>
<p>Mac Book Proの後ろで見えにくいですが、Mac miniが下に入っていて、上にMac mini用のキーボード、スピーカー、Qiの充電パッドが載せてあります。
Mac miniのキーボードをじゃまにならないように置きたいという目的で購入しました。3次元で空間が利用できるので結構重宝してます。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B0081MNH5E/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B0081MNH5E&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B0081MNH5E/?tag=johtani-22">
      Amazon | サンワサプライ 机上液晶モニタスタンド(D200・黒) MR-LC303BK
      </a>
    </p>
  </div>
</div></p>
<h3 id="充電関連">充電関連</h3>
<p>スマホ充電用にスピーカーの横においてあります。ただ、スイートスポットを探すのがちょっと大変で、時々充電できてないことがあったりします。。。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07FT2PHKM/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07FT2PHKM&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07FT2PHKM/?tag=johtani-22">
      Amazon | DesertWest Qi 急速充電ワイヤレス充電器 【Qi認証済み/PSEマーク付き】15W 
      </a>
    </p>
  </div>
</div></p>
<p>USBでの様々な充電用にはMercariさんのカンファレンスでいただいたAnkerの充電器をおいてあります。Mi Band 4、キーボードなどの充電向けです。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B00PK27K5Q/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B00PK27K5Q&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B00PK27K5Q/?tag=johtani-22">
      Amazon | Anker PowerPort 6(60W 6ポート USB急速充電器) 
      </a>
    </p>
  </div>
</div></p>
<h2 id="まとめ">まとめ</h2>
<p>簡単ですが作業環境でした。だいたい満足しています。もうちょっと改善したいなと思っているのは次の点です。</p>
<ul>
<li>USB-Cドック?ハブ?みたいなものがほしい
<ul>
<li>充電ケーブル、マイク、ディスプレイケーブル、有線LANの4つのケーブルがMac Book Proに刺さっており、ポートが全滅です。出先から帰ったときにつなぐのも少し面倒なので、1つか2つのポートでまとめられるといいなぁと思っています。が、ドック系はいい値段するのでまだ置き換わっていない感じです。</li>
</ul>
</li>
<li>スタンディングデスクが気になる
<ul>
<li>自宅でずっと座っているのはやはり気になるもので。スタンディングデスクにして時々たって作業したいなぁと思うことがあります。ただ、使ったことがないので、どれがいいのかわからず手を出せない状況です。</li>
</ul>
</li>
<li>ウェブカメラいる?
<ul>
<li>Mac Book Proのカメラでオンラインミーティングなどしています。気にするほどのことではないとは思いますが、メインディスプレイを見ながらしゃべると下からカメラで撮影されている形になります。。。</li>
</ul>
</li>
</ul>
<p>今のところはこんな形です。なにかの参考になればと。
あと、なにかおすすめのもの(特にスタンディングデスク)があれば教えて下さい!</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the book - 第4章</title>
      <link>https://blog.johtani.info/blog/2020/03/26/chap4-rust-the-book/</link>
      <pubDate>Thu, 26 Mar 2020 17:12:11 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/03/26/chap4-rust-the-book/</guid>
      <description>前回の記事はこちら。自分用のメモなので、読みにくいかもです。 Rust the Bookを読み始めた 第4章 第4章です。たぶん、これがいちばん大事な概念だと思</description>
      <content:encoded><p>前回の記事はこちら。自分用のメモなので、読みにくいかもです。</p>
<ul>
<li><a href="https://blog.johtani.info/blog/2020/03/23/start-reading-rust-the-book/">Rust the Bookを読み始めた</a></li>
</ul>
<h2 id="第4章">第4章</h2>
<p>第4章です。たぶん、これがいちばん大事な概念だと思います、Rustの。
そして、つまみ食いしながらRust書いてましたが、ここがきちんと理解できないまま書いてたってのもあります。。。</p>
<h3 id="所有権とは">所有権とは?</h3>
<ul>
<li><code>drop</code>関数ってのがあって、明示的に呼ぶことも可能。次のような感じで。2つ目の<code>println!</code>はエラーになる。sがもう無いのに借用しようとしてるから。</li>
</ul>
<pre><code>fn main() {
    let mut s = String::from(&quot;hello&quot;);
    s.push_str(&quot;, world!&quot;);
    println!(&quot;{}&quot;, s);
    drop(s);
    println!(&quot;{}&quot;, s);
}
</code></pre><ul>
<li>ムーブ - shallow copyではない。以下の2行目がムーブ。</li>
</ul>
<pre><code>let s1 = String::from(&quot;hello&quot;);
let s2 = s1;

println!(&quot;{}, world!&quot;, s1);
</code></pre><p>スタックとヒープの話が絡んでくる。あんまり意識すること無いよなぁ。
スタック = 固定長のデータを入れる場所。ポインタ、数値など
ヒープ = 可変長のデータが入る場所。可変の文字列とか。</p>
<ul>
<li>クローン - ヒープのデータをコピーすること。</li>
<li>コピー - スタックに収まるデータの場合はクローンが必要なくコピーで事足りる。
<ul>
<li>CopyトレイととDropトレイとは同居できない。</li>
<li>タプルのコピーはややこしそう</li>
</ul>
</li>
<li><a href="https://doc.rust-jp.rs/book/second-edition/ch04-01-what-is-ownership.html#a%E6%89%80%E6%9C%89%E6%A8%A9%E3%81%A8%E9%96%A2%E6%95%B0">所有権と関数</a>でまた、スタックに入れられるような変数と可変のオブジェクトの違いが出てくる。
<ul>
<li><code>takes_ownership(s: String)</code>が参照を受け取れば問題なく、このあとも使える。</li>
<li>戻り値でもムーブが発生</li>
</ul>
</li>
</ul>
<h3 id="参照と借用">参照と借用</h3>
<ul>
<li>借用 - 関数の引数に参照を取ること</li>
<li>可変な参照<code>&amp;mut</code>は1つ(不変な参照も含めて1つ)しか許さない
<ul>
<li>データの競合を防ぐため。</li>
<li>不変な参照を複数用いるのはOK</li>
<li>実際に変更が実行されるタイミングでエラーと判定される場合もある。</li>
</ul>
</li>
</ul>
<pre><code>let mut s = String::from(&quot;hello&quot;);

{
    let r1 = &amp;mut s;

} // r1はここでスコープを抜けるので、問題なく新しい参照を作ることができる

let r2 = &amp;mut s;
</code></pre><ul>
<li>ダングリング参照はテスト書くときとかにやってるかも。。。</li>
</ul>
<pre><code>fn dangle() -&gt; &amp;String { // dangleはStringへの参照を返す

    let s = String::from(&quot;hello&quot;); // sは新しいString

    &amp;s // String sへの参照を返す
} // ここで、sはスコープを抜け、ドロップされる。そのメモリは消される。
  // 危険だ
</code></pre><h3 id="スライス型">スライス型</h3>
<ul>
<li>部分的な参照。開始位置+長さで構成されているっぽい</li>
<li><code>&amp;str</code>の説明がよくわからなかった。</li>
</ul>
<p><a href="https://doc.rust-jp.rs/book/second-edition/ch04-03-slices.html#a%E5%BC%95%E6%95%B0%E3%81%A8%E3%81%97%E3%81%A6%E3%81%AE%E6%96%87%E5%AD%97%E5%88%97%E3%82%B9%E3%83%A9%E3%82%A4%E3%82%B9">引数としての文字列スライス</a>のテクニックは色々と使いまわせそう。</p>
<h2 id="まとめ">まとめ</h2>
<p>所有権、これまで特に難しいと思ってたのは、固定長の変数と、可変長の変数の違いを意識してなかったのが原因っぽい。
まぁ、Vecとかがどうなるのかとか、他にもいくつか気になるところはあるので、もうちょっとやらないといけないなと思いました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ElasticsearchのAnalyze APIのVisual Studio Codeのクライアントプラグイン</title>
      <link>https://blog.johtani.info/blog/2020/03/25/vsc-es-analzye-plugin/</link>
      <pubDate>Wed, 25 Mar 2020 11:10:05 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/03/25/vsc-es-analzye-plugin/</guid>
      <description>先日、Visual Studio Codeのプラグインを作ってみた(Azure Search Analyze Client)というブログを書きました。 このプラグインを作ってたタイミン</description>
      <content:encoded><p>先日、<a href="/blog/2020/03/19/azure-search-analyze-plugin">Visual Studio Codeのプラグインを作ってみた(Azure Search Analyze Client)</a>というブログを書きました。
このプラグインを作ってたタイミングで、Elasticの河村さん経由で、<a href="https://msdevjp.connpass.com/event/169431/">Microsoft Open Tech Night #9 w/ Elastic</a>でなにかLTしませんか?という打診がありました。</p>
<p>仕組み的には似たようなものだし、Elasticsearch用の拡張機能も作れるし、発表のネタにもなるし一石二鳥では?ということで、LTを快諾し、昨日発表してきました。</p>
<h2 id="資料とか">資料とか</h2>
<p>発表資料やGitHubのリポジトリなどは、以下のとおりです。</p>
<ul>
<li>発表資料 : <a href="https://noti.st/johtani/vDhbXW/analyze-api-vs-code">Analyze APIのVS Codeプラグインを作ってみた</a></li>
<li>GitHub Repository : <a href="https://github.com/johtani/vs-code-es-analyze-client">https://github.com/johtani/vs-code-es-analyze-client</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=johtani.elasticsearch-analyze-api-client">Visual Studio Code Marketplaceページ</a></li>
</ul>
<h2 id="機能">機能</h2>
<p>まだ、必要最低限の機能を実装した感じです。</p>
<ul>
<li>Analyze APIのパラメータ入力用のエディタ起動(<code>Elasticsearch Analyze Client: Create Elasticsearch Analyze Request</code>)</li>
<li>Esにリクエストを送信して結果の表示</li>
</ul>
<p>インストールからリクエスト送信して結果が出てくるまでのデモです。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200325/plugin-demo.gif" />
    </div>
    <a href="/images/entries/20200325/plugin-demo.gif" itemprop="contentUrl"></a>
  </figure>
</div>

<p>1点LTのデモのときに話すのを忘れていましたが、<code>.esanalyze</code>という拡張子のファイルであれば、このプラグインが入力値を見つけ出して、「<code>Analyze text with analyzers</code>」というコマンド送信用のリンクをエディタ画面に表示する機能があります。
ですので、パラメータ入力用のエディタを起動し、値を設定したあとにファイルを<code>hoge.esanalyze</code>というような名前で保存してもらえれば、後日そのファイルを開くことでリクエストが再送できます。</p>
<h3 id="azure-search版との機能の違い">Azure Search版との機能の違い</h3>
<p>先日のAzure Search向けのクライアントとの違いがいくつかあります。
ElasticsearchのAnalyze APIの方が多機能であるため、プラグインとしても違いがあったほうがいいかなと。</p>
<ol>
<li>入力がJSON形式</li>
<li>結果画面に詳細表示切り替えボタンを追加</li>
</ol>
<p>現時点では対応していませんが、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html#analyze-api-custom-analyzer-ex">Analyze APIはカスタムのtokenizer、filter、char_filterの設定を入力として受け付けることが可能です</a>。そのときに指定するのはJSON形式でtokenizerなどの設定を記述します。
今後、これらの対応をすることを考えると、入力全体をJSON形式で読み込めるほうがわかりやすいかなということで、入力はJSON形式で入力してもらうことを想定しました。</p>
<p>結果画面に詳細表示切り替えボタンを追加したのは、2つの理由があります。1つはAzure SearchのAnalyze APIよりもTokenの情報としていくつか他の情報も存在するためです。複数のAnalyzerとの比較をする場合は、単語列だけを比較したいですが、Analyzer個別の詳細情報を見たい場合もあるので、切り替えができたほうがよいかなと。
2つ目の理由はまだ実装していませんが、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html#explain-analyze-api">explainパラメータの出力</a>への対応のためです。
explainパラメータを指定すると、カスタムAnalyzerの場合に、Analyzerの設定にあるchar_filter、tokenizer、token_filterのそれぞれのステップでの単語列の出力が結果として返ってきます。この結果には標準の出力よりもさらに多くのtokenの情報(例えば、kuromojiだと品詞情報、読み、原型など)が追加されてきます。これらの表示を切り替えることができたほうがよいかと。</p>
<p>これらは、実は<a href="https://github.com/johtani/analyze-api-ui-plugin/blob/master/docs/GETTING_STARTED.md#3-show-analyzed-result-of-custom-analyzer">Kibanaのプラグインとしてすでに実装済み</a>になっています。
同等の機能は実装できるかなという目論見もあり、そちらに合わせた感じにしてあります。</p>
<h2 id="今後の対応">今後の対応</h2>
<p>現時点では、Analyzer名の指定のみが可能となっています。Kibanaのプラグインと同程度の機能はGitHubのIssueとして登録してみました。</p>
<ul>
<li><a href="https://github.com/johtani/vs-code-es-analyze-client/issues/5">explainパラメータ対応</a></li>
<li><a href="https://github.com/johtani/vs-code-es-analyze-client/issues/3">fieldパラメータ対応</a></li>
<li><a href="https://github.com/johtani/vs-code-es-analyze-client/issues/4">custom analyzer対応</a></li>
</ul>
<p>その他に、インデックス名やアナライザ名の自動補完みたいな機能があると便利かも?と妄想していたりします(実装が大変かもですが。。。)。Kibanaのプラグインの場合は、Mappingやインデックス名を調べるときに、KibanaのConsoleからチェックすればよかったのですが、このプラグイン単体だとそのあたりの情報の取得に他のツール(Kibanaだったり、REST API Clientだったり)を使わないといけないという問題点はあるかなぁと。</p>
<p>あとは、結果画面がこのままで本当に見やすいかどうか?なども気になってはいます。</p>
<h2 id="まとめ">まとめ</h2>
<p>まだまだ、作ってみたというレベルのプラグインです。
どのくらいの人に使ってもらえるかもわかりませんが、こんな機能あるといい?など要望があればリクエストいただければと。
Twitterで聞いていただいてもいいですし、GitHubのIssueとして登録していただいても構いません。
そもそもいらないなぁなんて意見でももちろん大歓迎です。フィードバックお待ちしてます!</p>
</content:encoded>
    </item>
    
    <item>
      <title>Rust the Bookを読み始めた</title>
      <link>https://blog.johtani.info/blog/2020/03/23/start-reading-rust-the-book/</link>
      <pubDate>Mon, 23 Mar 2020 10:57:22 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/03/23/start-reading-rust-the-book/</guid>
      <description>自転車本を読み始めましたが、その前にRust the bookを読んだほうが良いかも?と知り合いと話をしていてなったので、先にRust the bookを読</description>
      <content:encoded><p>自転車本を読み始めましたが、その前にRust the bookを読んだほうが良いかも?と知り合いと話をしていてなったので、先にRust the bookを読み始めてます。
コツコツ読むってのが苦手なので、知り合いと小規模オンライン読書会しながら読むことになりました(基本的になにか書きながら、使い方を調べるので、存在そのものを知らない記述や使用法などがあったりする)。</p>
<ul>
<li><a href="https://doc.rust-jp.rs/book/second-edition/">日本語版Rust the book</a></li>
<li><a href="https://doc.rust-lang.org/book/title-page.html">Rust the book</a></li>
</ul>
<p>基本は日本語版を読んでいます。まずは1章から3章あたり。</p>
<p>気になった点などを。自分用のメモなので、読みやすさとかは考えてないです(あとで自分が死ぬパターン?)。</p>
<h2 id="1章">1章</h2>
<ul>
<li>
<p>rustfmt便利。</p>
<ul>
<li>CLionのRustプラグインでは、保存時にrustfmtするというオプションがある。デフォルトはオフ。&ldquo;Run rustfmt on Save&rdquo;</li>
</ul>
</li>
<li>
<p>cargoの<code>--bin</code>オプション。意識してつけたことなかった=デフォルトだった。</p>
<ul>
<li>ライブラリにするときは<code>--lib</code></li>
</ul>
</li>
</ul>
<h2 id="2章">2章</h2>
<ul>
<li>「変数を値に束縛」という言い回しにまだ慣れない。
<ul>
<li>「代入」という言い方に慣れているから?</li>
<li>ただ、エラーにはassignってあるな。&ldquo;error[E0384]: cannot assign twice to immutable variable <code>x</code>&rdquo;</li>
</ul>
</li>
<li>preludeというのがデフォルトで読み込まれる型が存在する場所。</li>
<li><code>.expect()</code>により、Resultが評価済みになる</li>
<li>マクロがまだ慣れない</li>
<li><code>extern crate rand;</code>が<a href="https://doc.rust-lang.org/book/ch02-00-guessing-game-tutorial.html#generating-a-random-number">最新版だと要らなくなっている</a>。</li>
<li><code>rand::Rng</code>は<code>gen_range</code>のためにuseしている。CLionだとかってにuseを推測して追加してくれた。</li>
<li><code>match</code>はswitch文みたいな感じ。けど、defaultが必ず実行されるって感じではないな。
<ul>
<li>ただし、全て網羅しないと怒られるのが便利。</li>
<li>アームという呼び方が新鮮</li>
<li>単一の式のときは{}が省略できる</li>
<li>ブロック{}のときは、終わりにカンマを入力するとrustfmtが除去する(最後の条件かどうかは関係ない)。</li>
</ul>
</li>
<li>シャドーイングは面白い。
<ul>
<li>よく、<code>hoge_str</code>や<code>hoge_int</code>のような変数を書くので、ありがたい。</li>
<li>ただし、コードを読むときに少し混乱しそう?</li>
</ul>
</li>
<li><code>let ... match</code>で変数への束縛でmatchが使えるのは便利(これまで知らなかったので、変数宣言して条件つけて束縛する処理書いてた)。</li>
</ul>
<h3 id="シャドーイング">シャドーイング?</h3>
<pre><code>fn main() {
    let x = 5;

    let x = x + 1;

    let x = x * 2;

    println!(&quot;The value of x is: {}&quot;, x);
}
</code></pre><p>とか</p>
<pre><code>let spaces = &quot;   &quot;;
let spaces = spaces.len();
</code></pre><p>みたいに、同一変数名を使い回せること。再代入ではない</p>
<h2 id="3章">3章</h2>
<ul>
<li>constは型注釈が必須</li>
<li><code>100_000</code>のような記述が便利(<a href="https://docs.oracle.com/javase/jp/8/docs/technotes/guides/language/underscores-literals.html">Javaもできるって言われてびっくりしたw</a>)</li>
<li>タプルの中身を一部だけ書き換え可能。(mutを指定すれば)
<ul>
<li><code>tup.0 = 20;</code>のような感じで。</li>
</ul>
</li>
<li>配列は固定長でかつ、同一の型のものだけが入る</li>
<li>文末にセミコロンがない場合に四季になるというのはちょっと射にくいので辛いのでは。。。
<ul>
<li>自分は明示的に<code>return</code>を書きたくなる。が、returnだと動かない場合もある。。。</li>
</ul>
</li>
<li><code>let ... if</code>のような記述もできる。</li>
<li><code>(1..4)</code>はRange型</li>
</ul>
<h3 id="おまけ">おまけ</h3>
<p>フィボナッチ数列計算してみろというのがまとめにあったので。こんな感じでいいのかな?</p>
<pre><code>fn calc_fibonacci(n: usize) -&gt; usize {
    if n == 0 {
        return 0;
    } else if n==1 {
        return 1;
    } else {
        return calc_fibonacci(n-1) + calc_fibonacci(n-2);
    }
}
</code></pre><h2 id="その他">その他</h2>
<p>知り合いと読みすすめると、人が不思議に思ったところが、自分が理解が曖昧だったことなどに気づけて便利です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Visual Studio Codeのプラグインを作ってみた(Azure Search Analyze Client)</title>
      <link>https://blog.johtani.info/blog/2020/03/19/azure-search-analyze-plugin/</link>
      <pubDate>Thu, 19 Mar 2020 10:50:15 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/03/19/azure-search-analyze-plugin/</guid>
      <description>動機 Azure Cognitive Searchを検索エンジンに使っているお客さんを手伝っています。 そこで、検索の基本的な話をさせていただきました(もともとJJUGナイ</description>
      <content:encoded><h2 id="動機">動機</h2>
<p><a href="https://docs.microsoft.com/ja-jp/azure/search/search-what-is-azure-search">Azure Cognitive Search</a>を検索エンジンに使っているお客さんを手伝っています。
そこで、<a href="https://noti.st/johtani/halVGM">検索の基本的な話</a>をさせていただきました(もともとJJUGナイトセミナーでしゃべる予定だったスライドがベース)。</p>
<p>で、Elasticsearchなどの転置インデックスを利用している検索エンジンで検索の基本的な動作がどうなっているかを理解するのに、
個人的には一番重要だと思っているのがAnalysis(Analyze)の機能です。
転置インデックスの単語の切り出し方がどうなっているかによって、望んだ単語でうまく検索できているかいないかなどがわかります。</p>
<p><a href="https://docs.microsoft.com/ja-jp/rest/api/searchservice/test-analyzer">Azure Cognitive SearchもAnalyze Text</a>というAPIを提供してくれています(内部的にはElasticsearchだし)。
APIはあるのですが、返ってくる結果はJSONです。また、他のAnalyzerの設定との違いなどをみたくなったりもします。
やはり、普段使っているツールなどで簡単にどういう単語が出てくるかがわかるとうれしんじゃないかなぁ?と。</p>
<p>ということで、最初はPythonでちょっとAPI呼び出して、カンマ区切りで出力するものを作ってみたのですが、GUIとかあると便利かなぁという話になりました。
最近、ブログ書いたりするのにVisual Studio Codeを使い始めているので、これなら使いやすいかなと。
ということで、Visual Studio Codeの拡張機能(プラグイン?)としてインストールできる<a href="https://github.com/johtani/azure-search-analyze-client">ツール</a>を作ってみました。
Azure Cognitive Searchを使っている人向けなので、ニッチなツールですが。。。</p>
<h3 id="余談">余談</h3>
<p>Elastic Stack(Elasticsearch)向けにはKibanaのプラグインでAnalyze APIを可視化するツールを作ってます。
<a href="https://github.com/johtani/analyze-api-ui-plugin">analyze-api-ui-plugin</a>です。Elastic Stack、特にKibanaを必ず利用する方はこちらを使うと便利かもです。</p>
<h2 id="概要と機能">概要と機能</h2>
<ul>
<li><a href="https://github.com/johtani/azure-search-analyze-client">GitHub リポジトリ</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=johtani.azure-search-analyze-client&amp;ssr=false#overview">Visual Studio Marketplaceのページ</a></li>
</ul>
<p>Marketplaceに公開しているので、<code>johtani</code>や<code>Azure Search Analyze Client</code>などで検索してもらえれば出てきてインストールができます。</p>
<p>機能としては、以下の2つです。</p>
<ol>
<li>テンプレートから入力値設定用のドキュメントを作成(<code>Untitled-1</code>というドキュメントをエディタに新しく開く)</li>
<li>Azure Cognitive SearchのAnalyze Text APIを呼び出して、結果をHTMLのテーブル形式で表示</li>
</ol>
<h3 id="入力値設定用のドキュメント作成">入力値設定用のドキュメント作成</h3>
<p>APIの呼び出しに必要な情報を記入してもらうのに、いくつか案を考えました。</p>
<ul>
<li>プラグインの設定に記入してもらう</li>
<li>環境変数とかを読み出す</li>
<li>テキストとして保存したファイルを使う</li>
</ul>
<p>設定や環境変数だと、異なる環境に接続したりするときに、わざわざ設定し直すのがめんどくさいかもと。
で、愛用していた<a href="https://marketplace.visualstudio.com/items?itemName=humao.rest-client">REST API Client</a>を真似するのが良いかもという結論になり、<code>.analyze</code>という拡張子のファイルから必要な項目を読み出して、APIを呼び出す形にしてあります。</p>
<p><code>Command Palette</code>(左下の歯車マークもしくは、メニューのViewから開ける)から<code>Azure Search Analyze Client: Create Azure Search Analyze Request</code>というコマンドを選びます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200319/command_palette.png" />
    </div>
    <a href="/images/entries/20200319/command_palette.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>すると、以下のようなファイルがエディタに開きます。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200319/input_template.png" />
    </div>
    <a href="/images/entries/20200319/input_template.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>これらの項目をまずは埋めていきます。
それぞれの値がどういったものかは<a href="https://github.com/johtani/azure-search-analyze-client">GitHubのREADME</a>を御覧ください。</p>
<p>入力値エラー(存在チェックしかしていない)がある場合は、ダイアログが表示されます。</p>
<h3 id="結果表示">結果表示</h3>
<p>値を入力したら、設定値と<code>###</code>の間に表示されている<code>Analyze text with analyzers</code>というグレーの文字をクリックします。すると、APIにリクエストを送信し、結果が返ってきて、別のパネルとして表示されます。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200319/response.png" />
    </div>
    <a href="/images/entries/20200319/response.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>複数のAnalyzerを入力値に設定すると、それぞれがどのような区切り方をするかがわかります。
文字の下にある<code>[0:2]</code>は、その単語がもとの文章の何文字目から何文字目までに出現しているかというオフセットの表示になります。</p>
<p>もし、Analyzer名の設定ミスなどで指定されたAnalyzerがない場合は、結果画面にエラーが表示されるようにしています。</p>
<p>以上が簡単な機能の説明です。
簡単なと言いつつ、これだけしか機能がありませんが。</p>
<h2 id="visual-studio-codeのプラグインの作り方">Visual Studio Codeのプラグインの作り方</h2>
<p>プラグイン自体の作り方に関しては<a href="https://code.visualstudio.com/api/get-started/your-first-extension">Visual Studio CodeのGetting Started</a>がわかりやすかったです。
APIや機能が豊富なので、最初はちょっと戸惑いましたが、<a href="https://github.com/microsoft/vscode-extension-samples">サンプルもGitHubで多数公開</a>されています。</p>
<p>あとは、<a href="https://marketplace.visualstudio.com/items?itemName=humao.rest-client">REST API Client</a>を参考にさせていただきました。</p>
<p>Getting Startedを一通り読むことで、なんとなくプラグインの作成からMarketplaceへのリリースまでが完了できました。
(TypeScriptに慣れていないのがあるので、プログラミング自体は手間取りましたが。。。)</p>
<h2 id="今後の対応">今後の対応?</h2>
<p>いまのところ、こんなところを考えていますが、こんな機能がほしい、バグが有るなどあれば、GitHubにIssueを上げていただければと(使う人すくないだろうけど)。</p>
<ul>
<li>いろいろなエラーに関する対応</li>
<li>Readmeに画像をアップ</li>
<li>アイコン作成?</li>
<li>自動補完機能?</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>Visual Studio Codeの拡張機能を作ってみました。
Yeomanによるプロジェクトテンプレートが用意されているので、とりあえず、Hello worldを作るのは簡単でした。
試行錯誤しつつTypeScriptを書いたので、TypeScriptっぽくないところなどもあるかもですが、誰かの役に立つツールになってくれれば良いなぁと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>実践Rust入門の3章を読み終わった</title>
      <link>https://blog.johtani.info/blog/2020/03/02/finish-bicycle-book-chap3/</link>
      <pubDate>Mon, 02 Mar 2020 18:20:13 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/03/02/finish-bicycle-book-chap3/</guid>
      <description>これまで 実践Rust入門はじめました 実践Rust入門の3章を読んでるところ 3章終了 3章の終わりまで読み終えた。 いきなり実践的なプログラムで少</description>
      <content:encoded><p>これまで</p>
<ul>
<li><a href="/blog/2020/01/31/start-reading-bicycle-book">実践Rust入門はじめました</a></li>
<li><a href="/blog/2020/02/23/bicycle-book-chap3">実践Rust入門の3章を読んでるところ</a></li>
</ul>
<h2 id="3章終了">3章終了</h2>
<p>3章の終わりまで読み終えた。</p>
<p>いきなり実践的なプログラムで少し面食らっていたが、ステップを追って所有権周りの話まで来たので、
なんとなくRustのいいところが理解できたような気がする。</p>
<p>ただ、最後の<code>split_at_mut</code>が実際には内部でどういう形に変換することによって、コンパイルエラーにならずに、
借用がうまく行っているのかあたりは、まだきちんと理解できていない。</p>
<p>これは、どの言語にも言えるんだけど、リファレンスをうまく読み解きながら、
自分がやりたい処理ができるかどうかを考えるのって結構むずかしいなぁと思う。</p>
<p><code>benchmark.rs</code>はコピペして実行しただけなので、またあとで読み返してみるかな。</p>
<p>ということで、ここから先は、基本を勉強する感じで4章から読みつつ、なんかプログラムをまた書いてみるか。</p>
</content:encoded>
    </item>
    
    <item>
      <title>KuromojiのCLIコマンドとpicocliとGraalVM</title>
      <link>https://blog.johtani.info/blog/2020/02/28/kuromoji-cli/</link>
      <pubDate>Fri, 28 Feb 2020 11:49:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/28/kuromoji-cli/</guid>
      <description>Kuromoji-CLI Rust初心者がRust製の日本語形態素解析器の開発を引き継いでみたという記事にありますが、LinderaというRust製の日本語形態素解</description>
      <content:encoded><h2 id="kuromoji-cli">Kuromoji-CLI</h2>
<p><a href="https://qiita.com/mosuka/items/0fdaaf91f5530d427dc7">Rust初心者がRust製の日本語形態素解析器の開発を引き継いでみた</a>という記事にありますが、LinderaというRust製の日本語形態素解析が出てきました。KuromojiのRustクローンみたいなものです。
で、作者の人とのチャットの中で、「MeCabみたいなKuromojiのCLIないですかね？誰か知りませんか？」という話になりました。</p>
<p>自分は普段はKuromojiの確認をするのは、Elasticsearch関連もしくは、Azure Searchなどだったりするので、
当たり前のようにKibanaを立ち上げたり、REST APIを叩いてましたが、たしかにコマンドラインインタフェースあると便利かも?
と思い、作ってみるかということに。</p>
<h2 id="picocli">picocli</h2>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">いま、JavaでかんたんなCLI作るとなったら何がおすすめ？</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1232486871772459009?ref_src=twsrc%5Etfw">February 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>で、いつものツイートです。で、<a href="https://github.com/remkop/picocli">picocli</a>という返信がありました。
JavaでCLIのプログラムを書くのが簡単になりそうだと。
<a href="https://picocli.info/">ドキュメント</a>もしっかりしてます。サンプルも載ってるし。</p>
<p><a href="https://github.com/johtani/kuromoji-cli/blob/master/src/main/java/info/johtani/misc/cli/kuromoji/KuromojiCli.java#L38">Annotationで、コマンドの説明などパラメータ、オプションなどが記述でき、必須チェックなども可能です</a>。</p>
<p>また、オプションの説明文に関しては、テンプレートを用いることで、取りうる値をEnumの値で出力してくれたりといった便利機能まで備わっていました。</p>
<h2 id="graalvm">GraalVM</h2>
<p>また、<a href="https://remkop.github.io/presentations/20190906/">GRAALVMとPICOCLIでJAVAのネイティブコマンドラインアプリを作ろう</a>というスライドも教えてもらい、ネイティブコマンドまで作れるらしいと。
なんて便利なんでしょう!ってことで、こちらもチャレンジしてみました。
GraalVMがどんなものかはなんとなくは知ってたのですが、何者だろう?とツイートしたところ、これまた返信が。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">っ 資料はかなり古いので現行版と乖離はあるけど概念は変わってないので..<a href="https://t.co/KRQjhNdD9f">https://t.co/KRQjhNdD9f</a></p>&mdash; たむたむ🏫 (@tamtam180) <a href="https://twitter.com/tamtam180/status/1232654053458399233?ref_src=twsrc%5Etfw">February 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">「OCHaCafe2 #2 一体何モノなの？GraalVM入門編」の時の資料です。ご参考まで。<a href="https://t.co/NiKdIfRvTj">https://t.co/NiKdIfRvTj</a></p>&mdash; Takahiro Kitayama (@kitayama_t) <a href="https://twitter.com/kitayama_t/status/1232657370410389505?ref_src=twsrc%5Etfw">February 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>本当に、Twitter便利ですね。</p>
<p>GraalVMがどんなものかもいただいた資料でサクッと理解できました。
ただ、picocliが実はものすごく良くできており、自分でインストールする必要も実はありませんでした。</p>
<p><a href="https://github.com/palantir/gradle-graal">gradle-graal</a>というGradleのプラグインと、
<a href="https://github.com/remkop/picocli/tree/master/picocli-codegen">Picocli Code Generation</a>を利用することで、
以下のコマンドを実行することで、ネイティブコマンドアプリが作成できました。</p>
<pre><code>gradle nativeImage
</code></pre><p><code>build.gradle</code>ファイルに記述したのは、以下のような項目です。</p>
<ul>
<li><a href="https://github.com/johtani/kuromoji-cli/blob/master/build.gradle#L4">pluginの利用</a></li>
<li><a href="https://github.com/johtani/kuromoji-cli/blob/master/build.gradle#L28">picocli-codegenをannotationProcessorとして呼び出し</a></li>
<li><a href="https://github.com/johtani/kuromoji-cli/blob/master/build.gradle#L39">jarに関する記述(メインクラスと依存するJarを含める設定)</a></li>
<li><a href="https://github.com/johtani/kuromoji-cli/blob/master/build.gradle#L46">ネイティブコマンドの名前とか(gradle-graalの設定)</a></li>
</ul>
<h2 id="辞書が読み込めなかった">辞書が読み込めなかった</h2>
<p>で、native imageを作って実行しましたが、Kuromojiが内包している辞書ファイルが読み込めませんでした。
どうやら、picocli-codegenの自動生成では対応できない、リソース関連設定が必要らしいと。</p>
<p><a href="https://github.com/remkop/picocli/tree/master/picocli-codegen#other-options">picocli-godegenの設定</a>を見ていて見つけたのが、<code>other.resource.patterns</code>というオプションでした。</p>
<p><a href="https://github.com/johtani/kuromoji-cli/blob/master/build.gradle#L35">kuromoji-cliのbuild.gradleに</a>、その設定を加えたら辞書が読み込めるようになりました。</p>
<p>この設定をどうやって書くのか?というのを探すのにちょっと時間がかかりました。Picocli作者のRemko Popmaさんの<a href="https://github.com/remkop/picocli-native-image-demo/blob/master/build.gradle">サンプルリポジトリに設定を利用した<code>build.gradle</code></a>がありました。サンプル本当にありがたいですね。</p>
<h2 id="完成">完成</h2>
<p>出来上がったのは、<a href="https://github.com/johtani/kuromoji-cli">kuromoji-cli</a>というリポジトリになります。
内部で利用しているのはAtilikaのKuromojiです。LuceneのKuromojiではないのでご注意を(LuceneのKuromojiで最初やってみたのですが、リフレクションなどが多用されてて、Single Imageにするのが手間がかかりそうだった)。
コマンドのポータビリティはそれほど無いと理解してる(あってる?)ので、利用してみたい方は、リポジトリをクローンしてからビルドしてみてください。</p>
<h3 id="参考資料">参考資料</h3>
<ul>
<li><a href="https://www.atilika.com/ja/kuromoji/">kuromoji</a></li>
<li><a href="https://picocli.info/">Picocliドキュメント</a></li>
<li><a href="https://www.slideshare.net/tamrin69/getting-started-graalvm">Getting Started GraalVM / GraalVM超入門</a></li>
<li><a href="https://speakerdeck.com/hhiroshell/graalvm-for-beginners">一体何モノなの? GraalVM入門編 / GraalVM for beginners</a></li>
</ul>
<h2 id="まだ途中">まだ途中?</h2>
<p>ほかにもあると便利かも?という機能があればIssueやPRを上げてください。
とりあえず、頭にあるのは以下のとおりです。</p>
<ul>
<li>JSONの出力はまだ未対応</li>
<li>いろんな辞書をAtilikaのKuromojiはサポートしてるのでそのへんも対応してみる?</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>実践Rust入門の3章を読んでるところ</title>
      <link>https://blog.johtani.info/blog/2020/02/23/bicycle-book-chap3/</link>
      <pubDate>Sun, 23 Feb 2020 16:37:17 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/23/bicycle-book-chap3/</guid>
      <description>これまでのはこちら。読書メモなので、本と合わせて読んでいただくのが良いです。 実践Rust入門はじめました 3章のクイックツアーを読んでます。 バ</description>
      <content:encoded><p>これまでのはこちら。読書メモなので、本と合わせて読んでいただくのが良いです。</p>
<ul>
<li><a href="/blog/2020/01/31/start-reading-bicycle-book">実践Rust入門はじめました</a></li>
</ul>
<p>3章のクイックツアーを読んでます。
バイトニックソート自体の理解はちょっとおいておいて、読み進めています。
いくつか疑問に思ったことがあったので、またメモを。
まだ、3.5.7の手前ですが。</p>
<h2 id="疑問点">疑問点</h2>
<ol>
<li>
<p>3.4.1のジェネリクス対応のテストケースの部分で、既存のu32用のテストケースの入力のデータ列に<code>Vec&lt;u32&gt;</code>という型注釈をつけるのですが、
追加した文字列の入力データには注釈をここではつけないのはなんでなんだろう?<em>ちなみに、なくても動いた。バージョンの違いとかあるのかしら?</em></p>
</li>
<li>
<p>3.4.6のmatch文</p>
<ul>
<li><code>match</code>文の引数?が<code>*order</code>になっていたが、<code>order</code>でも実行できた。引数にくるのが参照だから<code>*</code>が付いてるんだとも運だが、なくても動くのはコンパイラがよしなに解釈してくれてるからかな?</li>
</ul>
</li>
</ol>
<h2 id="便利なツール">便利なツール</h2>
<p>Rust標準のツールの説明がいくつか3章で紹介されてて便利だったのでメモ。</p>
<ol>
<li>rustfmt</li>
</ol>
<p>フォーマッター。デフォルトでフォーマット機能が付いてるの便利ですね。言語として決まってると、プロジェクトごとに悩まなくていいってのがありますし。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">rustfmt ファイル名
</code></pre></div><p>という形で使えるみたい。
プロジェクトごとだと<code>cargo fmt</code>のほうが楽そうかな。</p>
<ol start="2">
<li>標準ライブラリAPIドキュメントをブラウザで閲覧</li>
</ol>
<pre><code>rustup doc --std
</code></pre><p>これでデフォルトブラウザでRustの公式ドキュメントが開きます。
しかもローカルファイルだからサクサク。検索バーもついてて便利です。</p>
<ol start="3">
<li>エラーのドキュメントを閲覧</li>
</ol>
<pre><code>rustc --explain 308
</code></pre><p>コンパイル時にエラーが出たときに、<code>error[E0308]</code>のようにコンソールに出てきます。
ヒントも出てくるのですが、詳細が上記のコマンドで読めるみたいです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>2019年に買ってよかったもの</title>
      <link>https://blog.johtani.info/blog/2020/02/21/best-buy-2019/</link>
      <pubDate>Fri, 21 Feb 2020 17:05:02 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/21/best-buy-2019/</guid>
      <description>もう2020年2月になってしまってますが、2019年に買って便利だったもの良かったものをいくつか記録に残しとこうかなと。 BRAVIA KJ-55X9500G Amazon | ソニー SONY 49</description>
      <content:encoded><p>もう2020年2月になってしまってますが、2019年に買って便利だったもの良かったものをいくつか記録に残しとこうかなと。</p>
<h2 id="bravia-kj-55x9500g">BRAVIA KJ-55X9500G</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07QLK1L4B/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07QLK1L4B&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07QLK1L4B/?tag=johtani-22">
      Amazon | ソニー SONY 49V型 液晶 テレビ ブラビア 4Kチューナー内蔵
      </a>
    </p>
  </div>
</div>
<p>テレビをやっと買い替えました。
前に使っていたのはプラズマディスプレイで、10年以上たったもので、パネルの一部が赤い残像が残ってたりしました。
もう流石にいいでしょうということで、買い替えたのがSonyのテレビです。
4Kのチューナーもついているし、アップコンバーターもついているので普通の地上波でもきれいです。
まぁ、10年前のテレビに比べればたくさんいいことがありますよねw</p>
<p>新しいTV買って一番活躍してるのはYouTubeボタンだったりします。
最近はミュージックビデオがYouTubeにたくさんあるので、King Gnuなどの音楽流したりするのに便利です(テレビなのに)。
今は、プライムビデオボタンがより活用される感じになってきてます(テレビなのにw)。</p>
<h2 id="sodastream">Sodastream</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07SJ71CS6/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07SJ71CS6&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07SJ71CS6/?tag=johtani-22">
      Amazon｜スピリット ワンタッチ ホワイト スターターキット
      </a>
    </p>
  </div>
</div>
<p>ここ数年、炭酸を普段飲んでます。甘いもの飲まないようにって意味でも。。。
で、これを買うまでは、サントリーの天然水スパークリングレモンとかを、箱買いしてました。
が、まぁ、ペットボトルのゴミが半端ないんですよ。場所も取るし。</p>
<p>ソーダストリームのことは知ってたのですが、自分で炭酸の量を考えながら、入れないといけないので避けていました。</p>
<p>が、今回購入したものは、水の入ったボトルを指して、ボタン一つで3段階の炭酸を入れることができるんです!買って本当に重宝してます。うちでは1Lのボトルを1本追加して、2本で運用してます。
家族はなっちゃんのオレンジとかを炭酸割りしたり、カルピスソーダ作ったりして楽しんでます。
ワンタッチということで、簡単に入れられるので家族も重宝してます。</p>
<h2 id="nature-remo--chromecast">Nature Remo + Chromecast</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07CWNLHJ8/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07CWNLHJ8&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07CWNLHJ8/?tag=johtani-22">
      Amazon | Nature スマートリモコン Nature Remo mini Remo-2W1 
      </a>
    </p>
  </div>
</div>
<p>Google Home miniがキャンペーンで手に入ったので、ベッドルームの照明やクーラーの操作もできそうなのでNature Remoを導入しました。
Chromecastは余っている液晶モニターにつないで、スマホからキャストできるようにしたり、音楽聞けるようにしてあります。</p>
<p>Nature Remoは温度計などもついているので、部屋の温度に合わせてエアコンのオンオフなども可能で、
暑い夏にとても便利でした。寝る時間にはクーラーが入っているのですんなり寝れたりと。
スマホから電源オンオフできるのも便利ですね。
Chromecastはモニターにつなぎましたが、今は、Google Music Playのプレーヤーになってます。
(最近、家にあるCDを読み込んでGoogle Play Musicにアップしてる)。</p>
<p>ただ、それ以上のことには使えてないので、もうちょっと面白いことしてみないとなぁ。</p>
<h2 id="モニターアーム">モニターアーム</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07C2DBDNC/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07C2DBDNC&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07C2DBDNC/?tag=johtani-22">
      Amazon | HUANUO PC モニター アーム 液晶ディスプレイ アーム ガススプリング式
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B072MYML17/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B072MYML17&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B072MYML17/?tag=johtani-22">
      Amazon | グリーンハウス 液晶ディスプレイアーム 4軸 クランプ式
      </a>
    </p>
  </div>
</div>
<p>ディスプレイがふるくなってたので、4Kディスプレイに買い替えた(27インチで4万切るとかで安い。。。)んですが、Mac Book Pro 15インチ(今は16インチ)を使っていて、Macの画面がディスプレイにかぶるので
モニターアームを買いました。
1つ目がガススプリング式のアームです。なのに、5000円を切ってるんですよ。おそるべし。
2つ目はクランプ式です。アームにしたものの特に動かすことはなく、高さを出したいだけってことがあるので、クランプ式のも使ってます(家ではディスプレイ2枚使ってる)。
コスパ最高です。</p>
<h2 id="anker-powercore-10000-pd">Anker PowerCore 10000 PD</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07L4M82CK/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07L4M82CK&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07L4M82CK/?tag=johtani-22">
      Amazon | Anker PowerCore 10000 PD（10000mAh PD対応最小最軽量 大容量 モバイルバッテリー）
      </a>
    </p>
  </div>
</div>
<p>Pixel 3にしてから、電池の持ちはよくはなってるんですが、もしもの時のためにモバイルバッテリーは話せないですよね。
PD対応なので、Pixel 3に急速充電ができるし、サイズもそれほどじゃまにならないサイズなので便利です。USB-Aもついているので、このあと紹介するイヤホンの充電ケーブルなんかも差し込めて大活躍です。</p>
<h2 id="サウンドピーツのイヤホン">サウンドピーツのイヤホン</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B083J83LBH/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B083J83LBH&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B083J83LBH/?tag=johtani-22">
      Amazon | SOUNDPEATS(サウンドピーツ) Q35HD ワイヤレス イヤホン 高音質・低遅延 IPX8防水 
      </a>
    </p>
  </div>
</div>
<p>私が持ってるのはこの前のバージョンで、micro-USBで充電充電するタイプですが、こちらもコスパ最高です。
冬は防寒も兼ねてヘッドフォンをしているのですが、春から秋にかけてはこれまで、優先のイヤホンを使ってました。もしくは、Bluetoothのレシーバーに優先のイヤホンつけてました。電池切れたら、イヤホンさして使う感じで。
が、Pixel 3はイヤホンジャックがありません。ということで、ワイヤレスイヤホンに移行するかということで選んだのがこちらです。
世の中的には、完全ワイヤレスイヤホンが流行っていますが、片方無くなってしまうイメージしかわかなくて。。。
これが壊れても、完全ワイヤレスではなくつながったワイヤレスイヤホンを使うと思います。</p>
<h2 id="ヘッドルーペ">ヘッドルーペ</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B072J8HLN6/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B072J8HLN6&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B072J8HLN6/?tag=johtani-22">
      Amazon | Tumao ヘッドルーペ ルーペ メガネ LED付拡大鏡 眼鏡の上装着 メガネゴムバンド両用 5レンズ交換調節 
      </a>
    </p>
  </div>
</div>
<p>プラモデルをちょいちょい作るんですが、もう年ですね。。。細かいところが見にくくて。
ガンダムのRG系のプラモデルだと、1/144なんですよ。小さいんですよ。見えないんですよ。。。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">完成しましたー！ <a href="https://t.co/S9QTS4kJDI">pic.twitter.com/S9QTS4kJDI</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1229033901139451907?ref_src=twsrc%5Etfw">February 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>ということで、ヘッドルーペ導入してみました。
最初は、卓上に置けるルーペにしてみたんですが、自分がルーペがあるところに動かないといけないの結構つらくて。
これのおかげで、プラモデル作成に問題はなくなりました。老眼とは関係なく、ちょっと猫背になっちゃうのが今は悩みの種ですが。。。こたつで作ってるからなぁ。</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、いくつかよかったものを紹介してみました。
今年はちゃんと年末に書こう。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ブログ移行日記(その5) - Jugemのブログを移行</title>
      <link>https://blog.johtani.info/blog/2020/02/21/import-jugem-posts/</link>
      <pubDate>Fri, 21 Feb 2020 12:04:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/21/import-jugem-posts/</guid>
      <description>ブログ移行日記(その5)です。前回まではこちら ブログ移行日記(その1) ブログ移行日記(その2) ブログ移行日記(その3) ブログ移行日記(その4</description>
      <content:encoded><p>ブログ移行日記(その5)です。前回まではこちら</p>
<ul>
<li><a href="/blog/2020/01/22/intro-hugo-and-theme/">ブログ移行日記(その1)</a></li>
<li><a href="/blog/2020/01/23/convert-md-from-octopress-to-hugo/">ブログ移行日記(その2)</a></li>
<li><a href="/blog/2020/01/24/setting-hugo/">ブログ移行日記(その3)</a></li>
<li><a href="/blog/2020/01/28/introduce-algolia/">ブログ移行日記(その4)</a></li>
</ul>
<p>今回はこれまでとは異なる特殊な話です。</p>
<p>最初にブログを書き始めたときに利用していたのが<a href="http://johtani.jugem.jp/">Jugem</a>のブログでした。
その後、Octopressに移行して、今年、Hugoに移行したという流れです。</p>
<p>で、よくよく考えると、Jugemのブログも移行できるんじゃないか?となりました。
じゃあ、やってみるかと。なので、今回のブログは自分の備忘録です(興味のない人が大多数じゃないかな)。
一応、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py">Pythonで書いたプログラムはGitHubに上がっています</a>。文字列置換と正規表現のオンパレードです。</p>
<h2 id="jugemからexport">JugemからExport</h2>
<p>まずは、移行元のデータが取り出せるかどうかを調べたところ、<a href="https://support.jugem.jp/hc/ja/articles/222313727-%E4%BD%9C%E6%88%90%E3%81%97%E3%81%9F%E8%A8%98%E4%BA%8B%E3%82%92%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AF%E5%87%BA%E6%9D%A5%E3%81%BE%E3%81%99%E3%81%8B-">text形式もしくはXML形式でエクスポートが可能でした</a>。</p>
<p>変換処理が必要なはずなので、XMLでダウンロードします。
独自のXMLですが、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py#L239">記事ごとにXMLのタグ(<code>&lt;entry&gt;</code>)でまとめられている</a>ので、処理が楽です。</p>
<h2 id="xmlをmarkdownに">XMLをMarkdownに</h2>
<p><code>&lt;entry&gt;</code>タグの下に次のような項目が入っているので、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py#L204">抜き出します</a>。</p>
<h3 id="ヘッダ部">ヘッダ部</h3>
<ul>
<li>title - 記事タイトル</li>
<li>author - 著者</li>
<li>category - タグ</li>
<li>date - 投稿日付</li>
<li>description : 本文(先頭部分)</li>
<li>sequel : 本文(つづき)</li>
</ul>
<p>titleからdateまでをHugoのMarkdownの<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py#L204">ヘッダ部分として出力します</a>。
<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py#L38">日付は形式が違うので合わせるように変換</a>し、
titleは<code>(Jugemより移植)</code>という文字列を追加しました。</p>
<p>また、Hugoの個別のコンテンツにするためにそれぞれをMarkdownのファイルに変換しています。
ファイル名は変換後の<code>date</code>の先頭10文字(<code>yyyy-MM-dd</code>)に<code>-</code>を付け加えて、
タイトルを追加しました。<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py#L18">ホントは英語のファイル名にしたかったんですが、ちょっと手抜き</a>。ファイル名に使用できないような文字は<code>-</code>に置換しています。</p>
<h3 id="本文">本文</h3>
<p>本文部分はもう少し複雑です。
まずは、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/from_jugem_to_hugo.py#L228">descriptionとsequelから抜き出した文字列を結合</a>します。</p>
<p>で、内部の文字列に次のようなものがあるので、それぞれMarkdownに変換したりという処理を書いてます。</p>
<ul>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L60">HTMLのheadingタグをMarkdown形式に</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L66">br、hr、del、strongなどのタグもMarkdown形式に</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L112">aタグの処理</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L93">Amazonのアフィリエイトタグの処理</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L161">ul、olタグの処理</a></li>
</ul>
<p>などです。
アフィリエイトタグは、数行に渡るの複数のHTMLタグで記述されているので、
<a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L146">ASINと商品タイトルだけを抜き出しています</a>。
Hugoでアフィリエイトのリンクを作るために、<a href="https://github.com/ikemo3/hugo-amazon-jp">hugo-amazon-jp</a>という公開されているshortcodeを元に、カスタマイズしたものを使っています。
これ用に、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L104">必要なASINとタイトルを別ファイルに出力</a>したりしています。</p>
<p>また、いくつか画像を使っている記事があったのですが、これが曲者でした。
XMLに入っているimgタグに画像へのURLがあるのですが、アクセスしても存在しないURL担っています。。。
<a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L130">ブログで公開している画像のファイル名に似たものがXMLに入っていたので、URLを組み直して、ダウンロードするという処理も書いています</a>。</p>
<p>あとは、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/65e29cc7e613495e0d2c75e3f617c6f778085b61/from_jugem_to_hugo.py#L81">昔ちょっと凝った書き方(spanタグで章みたいなことやってた)をしていた部分の処理</a>を加えて完成です。</p>
<h2 id="まとめ">まとめ</h2>
<p>元のXMLを見たり、抜き出したファイルを見ながら、トライアンドエラーでプログラムを書きました。
なんとなく変換できたなかっていうところで、取り込んで公開しました。
まだ、全部の記事をチェックしてないですが、なんとなく移植できたので一旦これでいいかなと。
昔の記事を見たときにおかしい場所があったら手で治すつもりでいます。</p>
<p>なんか、もうちょっとうまくプログラムかけた気もしますが、書き捨てのプログラムだと思うのでこんなもんかな。</p>
<p>1点気になっているところは、コメントの部分です。
ブログにコメントをいただいていたのですが、その部分は移植できてないです。</p>
<p>Octopresに移植していこうは、<a href="https://disqus.com/">Disqus</a>のサービスでコメント部分を提供しています。ここに移植するのも変な話だなぁと思っているので、本文にコメントを取り込む感じかなぁ？</p>
<p>もともとのJugemのサイトもそのまままだ残してあるので、そのうち気が向いたらで。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Azure Cognitive Searchでインデックスを作って検索</title>
      <link>https://blog.johtani.info/blog/2020/02/19/research-azure-cognitive-search/</link>
      <pubDate>Wed, 19 Feb 2020 11:48:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/19/research-azure-cognitive-search/</guid>
      <description>お手伝いしているお客さんがAzure Cognitive Searchを利用してます。 検索周り=Azure Cognitive Searchに関する手伝いをする形で入っており、 いく</description>
      <content:encoded><p>お手伝いしているお客さんがAzure Cognitive Searchを利用してます。
検索周り=Azure Cognitive Searchに関する手伝いをする形で入っており、
いくつか触った感触をブログにまとめてみようかと(お客さんからはOKいただいてます)。</p>
<h2 id="azure-cognitive-searchとは">Azure Cognitive Searchとは？</h2>
<ul>
<li><a href="https://azure.microsoft.com/ja-jp/services/search/">公式サイト</a></li>
</ul>
<p>「AIを活用した」クラウド検索サービスと紹介されています。</p>
<p>昔はAzure Searchと呼ばれていましたが、ここ最近はAzure Cognitive Searchと呼ばれているみたいです(<a href="https://techcommunity.microsoft.com/t5/azure-ai/knowledge-mining-with-azure-cognitive-search/ba-p/1020774">Microsoft Igniteで発表された話がまとまっているページもあります</a>)。
もともと、検索エンジンのSaaSサービス(キーワード検索、あいまい検索、オートサジェスト、スコアリングなどの機能)として作られていた部分に、データの登録パイプラインにCognitive Serviceの便利な機能を簡単に使えるようにしたものというイメージでしょうか。</p>
<p>バックエンドはElasticsearchのはずです。変わってなければ。
昔、Elastic社主催のユーザーカンファレンスで<a href="https://www.elastic.co/jp/elasticon/2015/sf/powering-real-time-search-at-microsoft">MSの方が公演された資料</a>が公開されていたりします。
ちなみに質問が多いのでしょうか、<a href="https://docs.microsoft.com/ja-jp/azure/search/search-faq-frequently-asked-questions#what-is-the-difference-between-azure-cognitive-search-and-elasticsearch">Azure Cognitive SearchとElasticsearchの違いはなんですか?</a>というページがよくある質問のページに用意されていました。参考までに。</p>
<p>今回はちょっとしたインデックスをつくって検索する部分を紹介してみようかと思います
(Cognitiveなところは機会があればまた)。</p>
<p><a href="https://docs.microsoft.com/ja-jp/azure/search/search-create-service-portal">普通の使い方</a>については、Azureのドキュメントなどを読んで頂く形にします。
ポータルと呼ばれるブラウザ上でAzureのサービスを触ることができる画面が用意されています。
ここで、簡単な操作(インデックス作成、フィールドの追加)</p>
<p>APIを使ってインデックス(特にAnalyzer)の設定をしたり、データをいれて、クエリしてみるというところをサクッと紹介しようと思います。</p>
<h2 id="インデックスの作り方">インデックスの作り方</h2>
<p><a href="https://docs.microsoft.com/ja-jp/azure/search/search-what-is-an-index#recommended-workflow">インデックス作成に関するドキュメント</a>も用意されています。最初はポータル(GUI)でインデックスを作成する方法が紹介されています。
ですが、今回はn-gram(n=2)のAnalyzerを利用したかったので、GUIではなくAPIでインデックスを作成しました。
<a href="https://docs.microsoft.com/ja-jp/azure/search/index-add-custom-analyzers">カスタムアナライザー</a>を利用する場合、REST APIを利用しなければ行けないということになっています。
n-gramのAnalyzerを含むインデックス生成のREST APIは以下のとおりです。こちらを実行することで、インデックスが作成されます(JSONの記述ミスなどがある場合はエラーが返ってきます)。</p>
<pre><code>@host = &lt;サーチのサービス名&gt;.search.windows.net
@api-key = &lt;APIキー&gt;

###

POST https://{{host}}/indexes/?api-version=2019-05-06
Content-Type: application/json
api-key: {{api-key}}

{
    &quot;name&quot;:&quot;ngram-test&quot;,
    &quot;fields&quot;:[
       {
          &quot;name&quot;:&quot;id&quot;,
          &quot;type&quot;:&quot;Edm.String&quot;,
          &quot;key&quot;:true,
          &quot;searchable&quot;:false
       },
       {
          &quot;name&quot;:&quot;ngram&quot;,
          &quot;type&quot;:&quot;Edm.String&quot;,
          &quot;searchable&quot;:true,
          &quot;analyzer&quot;:&quot;bi_gram_analyzer&quot;
       }
    ],
    &quot;analyzers&quot;:[
      {
         &quot;name&quot;:&quot;bi_gram_analyzer&quot;,
         &quot;@odata.type&quot;:&quot;#Microsoft.Azure.Search.CustomAnalyzer&quot;,
         &quot;tokenizer&quot;:&quot;bi_gram_tokenizer&quot;,
         &quot;tokenFilters&quot;:[
            &quot;lowercase&quot;
         ]
      }
    ],
    &quot;tokenizers&quot;:[
       {
          &quot;name&quot;:&quot;bi_gram_tokenizer&quot;,
          &quot;@odata.type&quot;:&quot;#Microsoft.Azure.Search.NGramTokenizer&quot;,
          &quot;minGram&quot;:2,
          &quot;maxGram&quot;:2
       }
    ]
 }
</code></pre><p>なんだかどこかで見たことのある記述のようなそうでないような。。。
Analyzerは、charFilters(0以上複数可)、tokenizer(1つ必須)、tokenFilters(0以上複数可)から構成されます。
フィールドで指定するのはAnalyzerなので、まずanalyzersに<code>CustomAnalyzer</code>の設定を行います。
名前は<code>bi_gram_analyzer</code>にしました(好きに付けてください)。
<code>tokenizer</code>にはこのあと設定するtokenizerの名前を設定します。ここでは、<code>bi_gram_tokenizer</code>という名前にしています。
また、大文字小文字を気にせずに検索したいため、<code>tokenFilters</code>に<code>lowercase</code>を指定しています。こちらは<a href="https://docs.microsoft.com/ja-jp/azure/search/index-add-custom-analyzers#token-filters-reference">すでに定義済み</a>のため、定義済みの名前で呼び出すだけで使用できます。</p>
<p>次が、<code>bi_gram_tokenizer</code>の設定です。
n=2としたいので、<code>tokenizers</code>配下にTokenizerの設定をします。
<code>@odata.type&quot;:&quot;#Microsoft.Azure.Search.NGramTokenizer</code>がTokenizerの名前です(ちょっと独特な名前ですね)。
Tokenizerごとにオプションがあり、NGramTokenizerの場合は、<code>minGram</code>、<code>maxGram</code>がオプションに相当します。
今回は2文字ごとにトークンを出力したいので、minとmaxをそれぞれ2としています。</p>
<p>これで、あとは、フィールドで<code>analyzer</code>という設定に<code>bi_gram_analyzer</code>を指定すればそのフィールドは<code>bi_gram_analyzer</code>を使用してアナライズされるようになります(このへんはElasticsearchといっしょですね)。
フィールドは文字列を扱うので、<code>Edm.String</code>というタイプにしてあります。データ型については、<a href="https://docs.microsoft.com/ja-jp/azure/search/search-what-is-an-index#fields-collection-and-field-attributes">フィールドコレクションとフィールド属性</a>というドキュメントを参考に設定しましょう。</p>
<h3 id="閑話休題---rest-client-exstension-for-visual-studio-code">閑話休題 - REST Client Exstension for Visual Studio Code</h3>
<p>なお、今回のサンプルは<a href="https://github.com/Huachao/vscode-restclient">REST Clinet Extention for Visual Studio Code</a>を利用する想定の記述になっています。</p>
<p>Visual Studio Codeで<code>.rest</code>もしくは<code>.http</code>というファイルに以下のAPIを記述すると、<code>Send Request</code>というリンクがURLの上部に出てくるような拡張機能です。REST APIにリクエストするときに便利なツールになっています。
変数も使えるので、APIのキーやURLの一部をこのように共通化して、他の環境でも使いやすくできるのは素晴らしいなと。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200220/rest_client_extension.png" />
    </div>
    <a href="/images/entries/20200220/rest_client_extension.png" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="アナライザーの挙動の確認">アナライザーの挙動の確認</h2>
<p>設定したAnalyzerがきちんと機能しているかというのを確認する必要があります。
入力した文字列がきちんと想定している単語として切り出されて、転置インデックスの見出し語に使われるかというのが重要になるからです。</p>
<p>Azure Cognitive Searchでもアナライザーのテスト用APIが用意されています。
<a href="https://docs.microsoft.com/ja-jp/azure/search/index-add-custom-analyzers#test-custom-analyzers">使い方はこちら</a>。
<a href="https://docs.microsoft.com/ja-jp/rest/api/searchservice/test-analyzer">APIの仕様のページもありました</a>。
「<code>アナライザーの挙動はどんな感じ？</code>」という文字列が、作成したインデックスの定義したアナライザー<code>bi_gram_analyzer</code>により、
どのように分割されるかを確認するAPIの呼び出しは以下のとおりです。</p>
<pre><code>###
POST https://{{host}}/indexes/ngram-test/analyze?api-version=2019-05-06
Content-Type: application/json
api-key: {{api-key}}

{
   &quot;analyzer&quot;:&quot;bi_gram_analyzer&quot;,
   &quot;text&quot;: &quot;アナライザーの挙動はどんな感じ？&quot;
}

</code></pre><p>レスポンスはこんな形です。ヘッダ部分は省略してあります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;@odata.context&#34;</span>: <span style="color:#e6db74">&#34;https://{{host}}.search.windows.net/$metadata#Microsoft.Azure.Search.V2019_05_06.AnalyzeResult&#34;</span>,
  <span style="color:#f92672">&#34;tokens&#34;</span>: [
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;アナ&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">0</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;ナラ&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">1</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">3</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">1</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;ライ&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">4</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">2</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;イザ&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">3</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">5</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">3</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;ザー&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">4</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">6</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">4</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;ーの&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">5</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">7</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">5</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;の挙&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">6</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">8</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">6</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;挙動&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">7</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">9</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">7</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;動は&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">8</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">10</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">8</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;はど&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">9</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">11</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">9</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;どん&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">10</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">12</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">10</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;んな&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">11</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">13</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">11</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;な感&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">12</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">14</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">12</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;感じ&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">13</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">15</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">13</span>
    },
    {
      <span style="color:#f92672">&#34;token&#34;</span>: <span style="color:#e6db74">&#34;じ？&#34;</span>,
      <span style="color:#f92672">&#34;startOffset&#34;</span>: <span style="color:#ae81ff">14</span>,
      <span style="color:#f92672">&#34;endOffset&#34;</span>: <span style="color:#ae81ff">16</span>,
      <span style="color:#f92672">&#34;position&#34;</span>: <span style="color:#ae81ff">14</span>
    }
  ]
}
</code></pre></div><p>このAzure SearchのAnalyze API、使用できるオプションはすくないですが、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html">ElasticsearchのAnalyze API</a>と似ています。</p>
<h2 id="データの登録の仕方">データの登録の仕方</h2>
<p>データ登録もAPIからできます(あたりまえですね)。
<a href="https://docs.microsoft.com/ja-jp/rest/api/searchservice/addupdate-or-delete-documents">APIは1件ずつではなく、バルクで登録できる形で提供されています</a>。</p>
<p>サンプルとしては、以下のような形です。
<code>@search.action</code>の部分(<code>search</code>があるとわかりにくい気がするけど。。。)が、ドキュメントの登録、更新、削除の命令を書き込むところになります。
今回は単純に登録するだけなので、<code>upload</code>を指定しました。
ほかにも<a href="https://docs.microsoft.com/ja-jp/rest/api/searchservice/addupdate-or-delete-documents#document-actions">いくつかアクションが用意されています。</a>用途に合わせて指定する感じになります。
<code>id</code>、<code>ngram</code>はそれぞれフィールド名です。ドキュメントに登録したい値を記述します。</p>
<pre><code>
POST https://{{host}}/indexes/ngram-test/docs/index?api-version=2019-05-06
Content-Type: application/json
api-key: {{api-key}}

{
  &quot;value&quot;: [
    {          
      &quot;@search.action&quot;: &quot;upload&quot;,  
      &quot;id&quot;: &quot;1&quot;,
      &quot;ngram&quot;: &quot;新しいAzure Searchの使い方&quot;
    },
    {          
      &quot;@search.action&quot;: &quot;upload&quot;,  
      &quot;id&quot;: &quot;2&quot;,
      &quot;ngram&quot;: &quot;Elasticsearchの紹介&quot;
    }
  ]
}

</code></pre><h2 id="検索クエリ">検索クエリ</h2>
<p>最後は検索クエリです。
<a href="https://docs.microsoft.com/ja-jp/azure/search/search-query-overview">検索クエリはいくつかのオプションがあります</a>。
ざっくりだと、<code>queryType</code>が<code>simple</code>と<code>full</code>という2種類が用意されており、ちょっとした検索を作る場合は<code>simple</code>で事足りそうという感じです。
入力された単語(スペース区切りで複数扱い)をフィールド(複数可)に対していずれかの単語を含むもしくは、すべての単語を含むという検索に行くというパターンですね。
このときの、「いずれか」か「すべて」の設定が<a href="https://docs.microsoft.com/ja-jp/azure/search/search-query-overview#tips-for-unexpected-results"><code>searchMode</code></a>というパラメータになります。
<code>any</code>の場合、Googleの検索と同様に、どれかの単語が入っているドキュメントが対象に、<code>all</code>の場合すべての単語が含まれるドキュメントだけがたいしょうになるといった形です。</p>
<p><code>queryType=full</code>の場合はLuceneの構文でクエリがかけます。Elasticsearchの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html">Query String Query</a>みたいな形です。</p>
<p>簡単なサンプルは次のような感じです。このサンプル</p>
<pre><code>
POST https://{{host}}/indexes/multi-field-test/docs/search?api-version=2019-05-06
Content-Type: application/json
api-key: {{api-key}}

{
   &quot;search&quot;: 'ngram:&quot;使い方&quot; ngram:&quot;紹介&quot;',
   &quot;queryType&quot;: &quot;full&quot;,
   &quot;searchMode&quot;: &quot;any&quot;
}
</code></pre><p>すこしだけクエリの補足を。
searchに入力された単語をダブルクォートで囲んでいます。これは、「使い方」という文字がbi_gram_analyzerにより「使い」「い方」に
分割されるのですが、必ずこの順序で出現したものだけを検索対象にしたい(フレーズ検索)という意味になります(*bi_*gramなので、「紹介」に関してはダブルクォートは厳密には必要ないです)。</p>
<p>あと、レスポンスは今回記載していませんが、<code>@search.score</code>という項目で、スコアが返ってきます。
デフォルトのスコア計算には何を使ってるんだろう?<a href="https://docs.microsoft.com/ja-jp/azure/search/index-add-scoring-profiles#what-is-default-scoring">ドキュメントにはTF-IDFとの記述があるのですが</a>。。。カスタマイズもできそうです。</p>
<p>少しオモシロイと思ったのは、スキーマ(インデックスの設定)に定義されているが、ドキュメントとしては登録していない項目についても、
Azure Cognitive Searchはドキュメントのフィールドが<code>null</code>という形で返ってくるようでした。
そもそもフィールドが存在しないドキュメントとフィールドの値が<code>null</code>のものの違いは無いようです。</p>
<p>簡単ですが、インデックスの設定、ドキュメントの登録、検索の方法の紹介でした。</p>
<h2 id="ちょっと触った感想">ちょっと触った感想</h2>
<p>一番売りである、Cognitiveの部分はまだ触っていないです、すみません(こっちが売りな気もするんですが)。
検索エンジンの部分としては、Elasticsearchを知っていると、「あー、そんな感じね」という気持ちになれるサービスです。
個々のAPIやデータの形式は異なるので、きちんとAPIのリファレンスなどを確認しつつという形になりますが、なんとなくこういうAPIなどがありそうだな?と予測しつつ使えるかなと。
内部的にはElasticsearchだと思いますが、独自のAPIでラップされているおかげで、バージョンの違いなどを意識せずに使えるのではないかと思います。</p>
<p>また、今回は紹介していませんが、マイクロソフト独自の各言語のアナライザー(日本語も含む)があります。
Luceneのアナライザーとマイクロソフトのアナライザーのどちらも利用できますので、ここの違いを見てみるのも面白そうだなと思いました。
緯度経度を利用した検索、フィルター検索(スコア計算対象にならない)、ファセット、スコア調整の機能なども備えているようです。</p>
<p>なんにせよ、利用する場合やドキュメントを読む場合に、全文検索の仕組みをなんとなく知っておいたほうが読みやすいんじゃないかなというのが感想です。</p>
<p>ここ数年はElasticsearchがメインでほかはほぼ触っていない状況だったので新しい製品に触れるのは面白いですね。
時間があったら、アナライザーの違いなども調べてみたいなと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>「検索システム探訪」したいな=&gt;やりました</title>
      <link>https://blog.johtani.info/blog/2020/02/10/explore-search-system/</link>
      <pubDate>Mon, 10 Feb 2020 23:20:27 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/10/explore-search-system/</guid>
      <description>また、ツイートから始まるお話です。 まだまだ、検索システムってどんなものかを勉強したいしということで、こんなのをツイートしてました。 「検索シス</description>
      <content:encoded><p>また、ツイートから始まるお話です。
まだまだ、検索システムってどんなものかを勉強したいしということで、こんなのをツイートしてました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">「検索システム探訪」みたいな名前がいいかな? <a href="https://t.co/sG7BDdGI9h">https://t.co/sG7BDdGI9h</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1225306152721739776?ref_src=twsrc%5Etfw">February 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>検索について知り合いとお茶をしながら話をしていて、こういう話を聞くの面白いなぁと思ったので。
どんなことを個人的には聞きたいかな?というのをまとめといたほうがいいかと思いブログを書いています。
こんなことも面白そうだよね?</p>
<h2 id="どんなこと聞きたいかな">どんなこと聞きたいかな?</h2>
<p>いまのところ思いついた内容はこんな感じかな?
こんなのも聞くと面白そうだよね?などありましたら、コメント欄に書いてもらえればと。</p>
<ul>
<li>利用しているシステム・サービスは?
<ul>
<li>構成やどこのサービスなのか？</li>
</ul>
</li>
<li>検索ユーザーはどんな人？
<ul>
<li>ユーザーの種類</li>
<li>ユーザーの検索ニーズ</li>
</ul>
</li>
<li>検索対象データはどんなもの？
<ul>
<li>データ件数はどのくらい？</li>
<li>属性、項目がどのくらいあるか？</li>
</ul>
</li>
<li>検索結果として出したいものは？
<ul>
<li>ソートはどんなものがあるの？</li>
<li>ランキングで重要なものは？</li>
</ul>
</li>
<li>UI/UXはどんな感じ?
<ul>
<li>ハイライト、ソート、ファセット(Aggs)</li>
<li>キーワードサジェスト、オートコンプリート</li>
</ul>
</li>
<li>検索システムの監視(オブザバビリティ??)
<ul>
<li>ビジネス的な指標</li>
<li>クエリログ、クリック(ユーザーに関する情報)</li>
<li>システムメトリクス</li>
</ul>
</li>
<li>困っていることは？
<ul>
<li>ランキング</li>
<li>クエリ</li>
<li>検索システム分析</li>
</ul>
</li>
</ul>
<h2 id="第1回やりました">第1回やりました。</h2>
<p>ちなみに、最初のツイートに対して、すぐに返事をくれた方がいました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">うちでよければご説明します！</p>&mdash; Kizashi （令和もRailsエンジニア募集中） (@kizashi1122) <a href="https://twitter.com/kizashi1122/status/1225293097329819648?ref_src=twsrc%5Etfw">February 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>Twitter本当にありがとうございますという感じです。</p>
<p>で、早速話を聞かせていただきました。大阪の方だったので、テレカンでやらせていただきました。
移動時間とか気にせずにできるので、テレカン形式いいですよね。</p>
<p>内容は、非常に面白かったです。
使ってるシステムについて、どんなところで、どのようなように検索エンジンをつかっているのか、
どんなことで困っているのかなどをお聞きすることができました。</p>
<h3 id="20200213-追記">2020/02/13 追記</h3>
<p>対応していただいた<a href="https://ingage.co.jp/">Ingage</a>の永田さんから、内容を公開しても良いという承諾をいただいたので、メモを公開します。</p>
<ul>
<li>サービス : <a href="https://ingage.jp/relation/">Re:lation</a>
<ul>
<li>問い合わせメールなどのチームで対応する対応</li>
</ul>
</li>
<li>利用している検索エンジン
<ul>
<li>Elasticsearch (AWS Es)
<ul>
<li>ノード数 8 - 3 master + 5 data</li>
</ul>
</li>
</ul>
</li>
<li>データ
<ul>
<li>データ数 : 約1億5千万</li>
<li>親子関係 : チケット - メール(=1ドキュメント) - コメント
<ul>
<li>画面としてはチケットの一覧=aggsでチケットIDの一覧を生成してリスト表示</li>
</ul>
</li>
<li>インデックス数 : 5 = テナントごとに振り分けしてる。
<ul>
<li>8シャード - スケールアウトした場合も対応できるので。</li>
</ul>
</li>
<li>PostgreSQLにマスターは保存</li>
<li>データの単位はメール。コメントはtextの配列でメールに保存されている。</li>
<li>EsはIDの一覧を返す。ただし、<em>チケットのIDの一覧</em></li>
</ul>
</li>
<li>機能
<ul>
<li>ハイライト
<ul>
<li>Esではなく独自実装。</li>
</ul>
</li>
<li>クエリ
<ul>
<li>n-gramでやってる = kuromoji使ってない</li>
<li>min=1, max=2</li>
</ul>
</li>
</ul>
</li>
<li>悩み事
<ul>
<li>EBS上だったものをローカルにしたので速くなった。</li>
<li>期間が長いと、検索結果一覧の表示が遅い
<ul>
<li>aggsが遅いのかな?</li>
<li>検索結果一覧はチケットが持っているメールの日付の降順。結果一覧はチケットの一覧。</li>
</ul>
</li>
<li>詳細検索はDB、左のステータスもファセットも現在はDBから生成
<ul>
<li>Es使ったほうが早いかも?</li>
</ul>
</li>
<li>マルチテナントのつらさ</li>
<li>検索ログ
<ul>
<li>独自ログはとっていない = アプリのログから判別</li>
</ul>
</li>
</ul>
</li>
<li>今後
<ul>
<li>アドレス帳検索</li>
</ul>
</li>
</ul>
<h2 id="お誘いお待ちしております">お誘いお待ちしております!</h2>
<p>今後もこれやっていきたいなぁ。話してもいいよ!という方がいらっしゃいましたら、<a href="https://twitter.com/johtani">@johtani</a>までDMもしくはメンションをください。お待ちしております!</p>
</content:encoded>
    </item>
    
    <item>
      <title>ボードゲーム紹介</title>
      <link>https://blog.johtani.info/blog/2020/02/01/intro-board-game/</link>
      <pubDate>Sat, 01 Feb 2020 21:04:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/02/01/intro-board-game/</guid>
      <description>年末、何種類かのボードゲームを家族で楽しみました。 家族の感想を交えて簡単に紹介しようかと思います。 家族や知人でボードゲームを購入するときの参</description>
      <content:encoded><p>年末、何種類かのボードゲームを家族で楽しみました。
家族の感想を交えて簡単に紹介しようかと思います。
家族や知人でボードゲームを購入するときの参考に参考にしてもらえたらと。
それぞれのゲームの詳細については、それぞれWikipediaなどのリンクを張っておくので、参考にしていただけたらと。
結構有名なボードゲームが多いのかな?</p>
<h2 id="カタン">カタン</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B017SB7QLO/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B017SB7QLO&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B017SB7QLO/?tag=johtani-22">
      Amazon | カタン スタンダード版 | ボードゲーム | おもちゃ
      </a>
    </p>
  </div>
</div>
<h3 id="簡単な紹介">簡単な紹介</h3>
<ul>
<li>プレーヤー数 : 3-4名 (拡張版を追加すると5-6名でもプレイできるみたい)</li>
<li>対象年齢 : 10歳以上</li>
<li>種類 : 対戦型</li>
<li>紹介 : <a href="https://ja.wikipedia.org/wiki/%E3%82%AB%E3%82%BF%E3%83%B3%E3%81%AE%E9%96%8B%E6%8B%93%E8%80%85%E3%81%9F%E3%81%A1">大航海時代に発見された無人島を複数の入植者たちが開拓していき、もっとも繁栄したプレイヤーが勝利するという内容のボードゲーム(Wikipediaより)</a>。</li>
<li>特徴 : 5種類の資源を元に、道を作ったり、開拓地や都市を作ったり、サイコロを使った運も必要ですが、プレーヤー同士で手持ちの資源を交渉することも可能なゲームです。</li>
</ul>
<h3 id="感想">感想</h3>
<ul>
<li>最初の配置が重要な気がします。資源の組み合わせによってできることが変わってくるので、何をやりたいかというのを元に考えて配置するのが重要そうです(私はまだうまくできてませんがw)。</li>
<li>プレーヤー同士の交渉も1つの手段なので、会話しながら、自然と交渉術も身についてきそうです。</li>
<li>戦略だけじゃなくサイコロを使った運もあるので、子供と大人が混ざっていても偏った結果にはならないです。</li>
<li>自分の道や町が発展していくのも楽しみの1つです。</li>
</ul>
<h2 id="お化け屋敷の宝石ハンター">お化け屋敷の宝石ハンター</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B01FLADSW0/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B01FLADSW0&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B01FLADSW0/?tag=johtani-22">
      Amazon | お化け屋敷の宝石ハンター FBH20 | おもちゃ | おもちゃ
      </a>
    </p>
  </div>
</div>
<h3 id="簡単な紹介-1">簡単な紹介</h3>
<ul>
<li>プレーヤー数 : 2-4名</li>
<li>対象年齢 : 8歳以上</li>
<li>種類 : 協力型</li>
<li>紹介 : <a href="http://mattel.co.jp/toys/mattel_games/">お化け屋敷がおばけでいっぱいになる前に、みんなで協力して８つの宝石をみつけて、屋敷から脱出しよう！(メーカーホームページより)</a>。</li>
<li>特徴 : 対戦ではなくプレーヤーみんなで協力してやるボードゲーム。おばけのフィギュアも可愛いです。サイコロを利用した運もありますが、みんなでどういった戦略で宝石を取り出すかなど会話も進むゲームです。上級編という難しめのルールも用意されているので、ハラハラ・ドキドキしながらプレイできます。</li>
</ul>
<h3 id="感想-1">感想</h3>
<ul>
<li>負けて悔しくなったり機嫌が悪くなることがないのがいいですね。</li>
<li>普通のルールだと少し物足りない感じですが、上級編のルールはより、みんなで協力する必要が出てきます。</li>
<li>また、上級編のルールは絶妙な難易度で、もう一度やりたいと思わせるのが良い仕組みです。</li>
</ul>
<h2 id="ラビリンス">ラビリンス</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B0002YM10Q/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B0002YM10Q&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B0002YM10Q/?tag=johtani-22">
      Amazon | ラビリンス (Labyrinth) ボードゲーム | ボードゲーム | おもちゃ
      </a>
    </p>
  </div>
</div>
<h3 id="簡単な紹介-2">簡単な紹介</h3>
<ul>
<li>プレーヤー数 : 1-4名</li>
<li>対象年齢 : 8歳以上</li>
<li>種類 : 対戦型</li>
<li>紹介 : <a href="http://ganguoroshi.jp/item/4005556264445.html">動く迷路で宝探し！通路が動く不思議な迷路の中に、「宝物」や「奇妙な生き物」が隠されています。プレイヤーはこれらを集めてこなければなりませんが、そのためには、巧みに迷路を動かさなければなりません。(メーカーホームページより)</a>。</li>
<li>特徴 : サイコロがないボードゲームです。先を予測しながら人のじゃまをしつつ、自分に有利に進めていくゲームです。プレーヤー個別に難易度を変えられる仕組みがあります(子供だけに有利なルール設定が可能)。</li>
</ul>
<h3 id="感想-2">感想</h3>
<ul>
<li>子供ルールがあるので、1度のゲームで別々の難易度が設定できるのがいいです。左右盲の人には子供ルールとかもありです。</li>
<li>ラビリンスが得意な人は地図を読むのが得意な人だと思います。</li>
<li>頭も使うけど、自分が望んだ道ができた時の快感があります。</li>
</ul>
<h2 id="モノポリー">モノポリー</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B07BB4FDLG/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B07BB4FDLG&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B07BB4FDLG/?tag=johtani-22">
      Amazon | ハズブロ ボードゲーム モノポリー クラシック C1009 正規品 | ボードゲーム | おもちゃ
      </a>
    </p>
  </div>
</div>
<h3 id="簡単な紹介-3">簡単な紹介</h3>
<ul>
<li>プレーヤー数 : 2-6名</li>
<li>対象年齢 : 8歳以上</li>
<li>種類 : 対戦型</li>
<li>紹介 : <a href="https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%8E%E3%83%9D%E3%83%AA%E3%83%BC">プレイヤーは双六の要領で盤上を周回しながら他プレイヤーと盤上の不動産を取引することにより同一グループを揃え、家やホテルを建設することで他のプレイヤーから高額なレンタル料を徴収して自らの資産を増やし、最終的に他のプレイヤーを全て破産させることを目的とする。モノポリーとは英語で「独占」を意味する。(Wikipediaより)</a>。</li>
<li>特徴 : 資本主義とは?みたいなのを体験できるゲーム。すごろく型ですが、ゴールがあるわけではなく、決着がつくまでぐるぐる回る感じです。</li>
</ul>
<h3 id="感想-3">感想</h3>
<ul>
<li>資本主義を象徴しているゲーム展開。真剣勝負間違いなしです。</li>
<li>負けると途中退場ってのがちょっとつらいです。。。</li>
<li>カタンとは違った感じの交渉術になります。</li>
<li>コマが色んな種類があってかわいいです。</li>
</ul>
<h2 id="パンデミック">パンデミック</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B00DKSX2TK/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B00DKSX2TK&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B00DKSX2TK/?tag=johtani-22">
      Amazon | パンデミック:新たなる試練 (Pandemic) 日本語版 ボードゲーム | ボードゲーム | おもちゃ
      </a>
    </p>
  </div>
</div>
<h3 id="簡単な紹介-4">簡単な紹介</h3>
<ul>
<li>プレーヤー数 : 2-4名</li>
<li>対象年齢 : 8歳以上</li>
<li>種類 : 協力型</li>
<li>紹介 : <a href="https://ja.wikipedia.org/wiki/%E3%83%91%E3%83%B3%E3%83%87%E3%83%9F%E3%83%83%E3%82%AF_(%E3%83%9C%E3%83%BC%E3%83%89%E3%82%B2%E3%83%BC%E3%83%A0)">『パンデミック』（Pandemic）は、プレイヤーが新型ウイルス対策チームの一員となり、協力しあい感染症の世界的流行（パンデミック）に立ち向かうというボードゲームである。(Wikipediaより)</a>。</li>
<li>特徴 : プレーヤーが個別のロール(役割)を持っており、よりみんなで協力をしないとウイルスに打ち勝てない感じです。</li>
</ul>
<h3 id="感想-4">感想</h3>
<ul>
<li>ルールが結構複雑で、理解しながらだと少し時間かかりました。</li>
<li>8歳以上になっていますが、少しむずかしい印象でした。</li>
<li>戦略、戦術が問われる形のゲームなのでしっかり理解すると楽しそう。</li>
<li>世界地図上で、ウイルスと戦っていく感じは臨場感があって面白いです。</li>
</ul>
<h2 id="ナインタイル">ナインタイル</h2>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B073CM2RVL/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B073CM2RVL&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B073CM2RVL/?tag=johtani-22">
      Amazon | ナインタイル | カードゲーム・トランプ | おもちゃ
      </a>
    </p>
  </div>
</div>
<h3 id="簡単な紹介-5">簡単な紹介</h3>
<ul>
<li>プレーヤー数 : 2-4名</li>
<li>対象年齢 : 6歳以上</li>
<li>種類 : スピード対戦型</li>
<li>紹介 : <a href="https://oinkgms.com/jp/nine-tiles">「ナインタイル」の基本ルールは、それぞれの手元にある、９枚のタイルを自由に動かしたりひっくり返したりして、誰よりも早くお題どおりに並べるだけ。(公式サイトより)</a>。</li>
<li>特徴 : ボードゲームというか、カードゲーム。ルールは簡単ですが、記憶力と素早さが重要です。</li>
</ul>
<h3 id="感想-5">感想</h3>
<ul>
<li>脳トレに良さそう(子供に勝てませんでした。。。)。</li>
<li>瞬発力+記憶力が重要。</li>
<li>デザインがきれい</li>
</ul>
<h2 id="じゃあ家族の人気は">じゃあ、家族の人気は?</h2>
<p>ラビリンス、カタン、モノポリーあたりが人気です。
ゲーム性もありますが、デザインとしてもきれいなものが多かったです。</p>
<p>テレビゲームも面白いですが、ワイワイ喋りながらボードゲームをやるのも楽しかったです。
今回紹介したボードゲームはどれもある程度ルールを理解するところから始めないといけないですが、理解力や説明力なども必要になってくるので色んな楽しみがあるかなと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>実践Rust入門はじめました</title>
      <link>https://blog.johtani.info/blog/2020/01/31/start-reading-bicycle-book/</link>
      <pubDate>Fri, 31 Jan 2020 21:58:12 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/31/start-reading-bicycle-book/</guid>
      <description>実践Rust入門という本を買っていた(去年の7月だ。。。)のですが、積んであったので、時間を作って読み始めようかと。 実践Rust入門[言語仕</description>
      <content:encoded><p>実践Rust入門という本を買っていた(去年の7月だ。。。)のですが、積んであったので、時間を作って読み始めようかと。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4297105594/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4297105594&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4297105594/?tag=johtani-22">
      実践Rust入門[言語仕様から開発手法まで] | κeen, 河野 達也, 小松 礼人 |本 | 通販 | Amazon
      </a>
    </p>
  </div>
</div>
<h2 id="経緯">経緯</h2>
<p>もともとは、<a href="/blog/2018/02/14/start-nlp100-with-rust/#undefined">言語処理100本ノックはじめました(Rust)</a>という感じで、触っていたのですが、場当たり的にやってても時間を持っていかれるだけだなということに気づいたのが最初です。</p>
<p>今年の目標は、覚えられなので、ちょっとずつでもアウトプットしていこうってのもあり、
読書記録をつけつつ、読んでいこうかなぁと。</p>
<h2 id="どこまで読んだ">どこまで読んだ?</h2>
<p>2章の<code>2-2-5</code>までです。
前回、Rustの環境はセットアップしていたのですが、新PCに切り替わったので、<code>rustup</code>からはじめました。</p>
<h3 id="rustup">rustup</h3>
<p><code>rustup</code>ではデフォルト設定のままではなく、<code>PATH</code>変数の書き換えだけはしない形でインストールを行いました。</p>
<p><code>PATH</code>変数は<code>.zshrc</code>ファイルで変更したかったためです(<code>rustup</code>コマンドに変更して貰う場合は<code>.profile</code>などのファイルが変更されそうだったため)。</p>
<p>インストールが終わったあとに<code>.zshrc</code>に以下の行を追加しました。</p>
<pre><code>### For Rust env
source $HOME/.cargo/env
</code></pre><h2 id="疑問点">疑問点</h2>
<p>ここまで読んだ疑問点です。</p>
<ol>
<li><code>cargo new hello</code>したあとに<code>main.rs</code>に以下の<code>main()</code>関数が出来上がっている!?</li>
</ol>
<pre><code>fn main() {
    println!(&quot;Hello, world!&quot;);
}
</code></pre><p>驚きましたが、<code>cargo new hoge</code>ってやっても、おなじ<code>main.rs</code>ができてました。デフォルトで出来上がるんですね。どんな超能力!?と思ってしまいましたw</p>
<ol start="2">
<li><code>cargo new hello</code>して出来上がった<code>Cargo.toml</code>に著者名が入力されていた。</li>
</ol>
<pre><code>authors = [&quot;Jun Ohtani &lt;メアド&gt;&quot;]
</code></pre><p>なんで?と思いました。まだ解明してないです。
本を読んでいけばわかるかな?</p>
<ul>
<li>予想:gitの設定(<code>~/.gitconfig</code>)に氏名とメアドが設定されているので、これを利用しているのかな?</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>フリーランスはじめました(仮)</title>
      <link>https://blog.johtani.info/blog/2020/01/31/start-freelance/</link>
      <pubDate>Fri, 31 Jan 2020 20:37:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/31/start-freelance/</guid>
      <description>12月に退職しますブログを書きましたが、その後どうなったのか?という話をしてないなと思ったので、一応現在の状況を書き留めておこううかと。 結論</description>
      <content:encoded><p>12月に<a href="/blog/2019/12/06/leaving-elastic/">退職します</a>ブログを書きましたが、その後どうなったのか?という話をしてないなと思ったので、一応現在の状況を書き留めておこううかと。</p>
<h2 id="結論">結論</h2>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">実際にはフリーランスとして働き始めてます。検索周りをお手伝いするお仕事を。読解いとこあれば所属するつもりでもいるし。</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1221819238811230208?ref_src=twsrc%5Etfw">January 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">フリーランスとして最初の仕事はElasticsearchではなく、Azure Cognitive Searchの支援だったりします。Esで困ってる方の手伝いもしますので、興味ある方はDMください。</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1223087120014725122?ref_src=twsrc%5Etfw">January 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>2020年1月24日が、正式なElasticの退職日でした(有給休暇消化してた)。
で、現在のステータスはこのツイートです。</p>
<h2 id="現在の心境">現在の心境</h2>
<p>どこかに就職してガッツリ検索もやりたいし、いろんな会社の検索やElastic Stackに関するお手伝いをするのにも興味があるし、どちらにも興味がある状況です。まぁ、検索に興味が尽きないというのだけは決まってますかね。</p>
<p>悩んでいるところに、幸いにもAzure Search周りで手伝ってほしいという話が舞い込んできたので、フリーランスとしてお手伝いをさせていただいてます(現時点で、半稼働で、が3月末まで)。</p>
<p>Elastic Stackなどの知見もあるのでのお手伝いをするというのもありかなぁと。お仕事発注してもいいな?という方があれば、TwitterのDMなど、連絡をいただければと。</p>
<p>現在の状況でした。</p>
<p>副業ができる会社に就職するのがどっちもできていいのかも?</p>
</content:encoded>
    </item>
    
    <item>
      <title>ブログ移行日記(その4) - 検索機能(Algolia)の導入</title>
      <link>https://blog.johtani.info/blog/2020/01/28/troduce-algolia/</link>
      <pubDate>Tue, 28 Jan 2020 13:40:04 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/28/troduce-algolia/</guid>
      <description>ブログ移行日記(その4)です。その他の記事はこちら。 ブログ移行日記(その1) ブログ移行日記(その2) ブログ移行日記(その3) ブログ移行日記(</description>
      <content:encoded><p>ブログ移行日記(その4)です。その他の記事はこちら。</p>
<ul>
<li><a href="/blog/2020/01/22/intro-hugo-and-theme/">ブログ移行日記(その1)</a></li>
<li><a href="/blog/2020/01/23/convert-md-from-octopress-to-hugo/">ブログ移行日記(その2)</a></li>
<li><a href="/blog/2020/01/24/setting-hugo/">ブログ移行日記(その3)</a></li>
<li><a href="/blog/2020/02/21/import-jugem-posts/">ブログ移行日記(その5)</a></li>
</ul>
<p>前回までで、なんとなく移行は終わってます。
今回はテーマで使えるようになっているブログの検索機能の導入の話です。</p>
<p>検索サービスは<a href="https://www.algolia.com/">Algolia</a>を利用します。
OctopressのころはElastic社のサービスである<a href="https://www.elastic.co/jp/site-search">Elastic Site Search</a>の機能を利用して、クローリングしてから検索できるようにしていましたが、Hugoで導入できるモジュール?があったので、今回からこちらに移行しました。</p>
<h2 id="参考記事">参考記事</h2>
<ul>
<li><a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite#site-search-with-algolia">Clean White ThemeのREADME(Algoliaの設定方法)</a></li>
<li>参考ブログ:<a href="https://blog.uni-3.app/2019/01/02/hugo-algolia-search/">hugoで作ったblogにalgoliaで全文検索機能を追加する</a></li>
</ul>
<h2 id="algoliaとは">Algoliaとは?</h2>
<p>検索のas-a-serviceをやっている会社です。<a href="https://en.wikipedia.org/wiki/Algolia">Wikipediaによると本社はサンフランシスコにあるみたいですね</a>(フランスの会社のイメージでした。起業された方がフランス出身だからかなぁ?)。
クラウドで検索インデックスを保持でき、API経由で検索したり登録したりできる感じのサービスです。内部で使われているのはOSSではない独自の検索エンジンです。</p>
<p>クラウドで提供されているサービスなのでサクッと検索を使い始めることができるのがいい点ですね。
また、<a href="https://www.algolia.com/pricing/">小さな非商用のプロジェクトにフリーで利用できるプラン</a>も提供されているようです(2020年1月現在)。</p>
<h2 id="algoliaのサービス登録からインデックス作成">Algoliaのサービス登録からインデックス作成</h2>
<p><a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite#site-search-with-algolia">私が利用しているテーマで設定する方法</a>の記載があったので手順の通りにやってみました。
大きく2つの作業(Algolia側とHugo側)が必要です。まずは、Algoliaで必要な作業から。作業の流れだけ記載しておきます。<a href="https://forestry.io/blog/search-with-algolia-in-hugo/#3-create-your-index-in-algolia">詳細は「Static site search with Hugo + Algolia」の3)</a>を確認してください。</p>
<ol>
<li>Algoliaのサインアップ(すでにアカウントがあれば不要)</li>
<li>New Applicationの作成(名前とプランの指定)</li>
<li>リージョンの指定</li>
<li>インデックス名の指定</li>
<li>APIキーを確認</li>
</ol>
<p>です。これで、Algolia側の準備は完了です。</p>
<p>今回は関係ないですが、Algoliaの管理画面で、利用状況(データ登録などの操作回数、クエリの回数、インデックスに保存されているレコード数)の確認が可能です。
ほかにも有料プランを利用すると<a href="https://www.algolia.com/doc/guides/getting-insights-and-analytics/search-analytics/out-of-the-box-analytics/">Analytics</a>などもできるようです。</p>
<h2 id="hugo側で必要な設定">Hugo側で必要な設定</h2>
<p>今度はHugo側です。Hugoのサイトのディレクトリに移動してからの作業です。</p>
<p>仕組みとしては、</p>
<ol>
<li>Hugoのoutput機能でAlgolia向けのJSONファイルを生成する</li>
<li>Node.jsのライブラリを使用してAlgoliaに1.で生成したJSONを登録、更新する</li>
<li>検索画面の作成</li>
</ol>
<p>という流れになります。
ですので、作業としては以下のとおりです。</p>
<ol>
<li>Output出力の設定(すでにテーマ側で設定されているので、特に作業は必要なし)</li>
<li>npm環境の構築(Hugoのconfig.tomlと同じディレクトリ階層)
<ol>
<li>Node.jsのインストール(必要であれば)</li>
<li>npm環境の初期化</li>
<li>npmでatomic-algoliaのインストール</li>
<li>atomic-algolia向けの設定(登録のためのAPI関連の設定)</li>
<li>Algolia向けJSONの出力設定</li>
</ol>
</li>
<li>検索関連の設定
<ol>
<li>content/search/placeholder.mdの作成</li>
<li>検索用のAPIキーなどを設定</li>
</ol>
</li>
</ol>
<h3 id="npm関連の作業">npm関連の作業</h3>
<p>以下、npm関連の作業です。</p>
<ol>
<li>Node.jsのインストール(必要であれば)
<ul>
<li>割愛します。環境に合わせてインストールしてください。私はnvm経由でインストールしています。</li>
</ul>
</li>
<li>npm環境の初期化
<ul>
<li>Hugoのディレクトリで<code>nvm init</code>を実行</li>
</ul>
</li>
<li>npmでatomic-algoliaのインストール
<ul>
<li>Hugoのディレクトリで<code>npm install atomic-algolia --save</code></li>
</ul>
</li>
<li>atomic-algolia向けの設定(登録のためのAPI関連の設定)
<ul>
<li>Hugoのディレクトリに<code>.env</code>ファイルを作成し、以下を設定します。</li>
</ul>
</li>
<li>Algolia向けJSONの出力設定
<ul>
<li>特に変更なし(<code>config.toml</code>に少し設定があります)。</li>
</ul>
</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ALGOLIA_APP_ID<span style="color:#f92672">=</span>AlgoliaのApplication ID
ALGOLIA_ADMIN_KEY<span style="color:#f92672">=</span>AlgoliaのAdmin API Key
ALGOLIA_INDEX_NAME<span style="color:#f92672">=</span>先程作ったインデックス名
ALGOLIA_INDEX_FILE<span style="color:#f92672">=</span>public/algolia.json
</code></pre></div><p>最後の<code>ALGOLIA_INDEX_FILE</code>は固定文字列でいいと思います。
<code>hugo</code>コマンドを実行すると<code>public</code>ディレクトリ配下に<code>algolia.json</code>というファイルが生成され、Algolia登録用のJSONが出力されています。</p>
<blockquote>
<p>余談 : algolia.jsonの出力の設定は、config.tomlに記載があります。また、JSONファイルのテンプレート自体は<code>themes/hugo-theme-cleanwhite/layouts/_default/list.algolia.json</code>にあります。Algoliaに登録するデータの構造など変更をする場合はこのテンプレートをカスタマイズすれば良さそうです。</p>
</blockquote>
<h3 id="検索関連の設定">検索関連の設定</h3>
<p>実際に検索の画面を表示するために、検索用の画面と、検索用のAPIの設定が必要です。</p>
<ol>
<li>content/search/placeholder.mdの作成
<ul>
<li><code>/search/</code>が検索用のページになります。空のファイルです。実際にはJavaScriptが検索用の窓を表示したりしてくれます(これが必要な理由がまだ不明だなぁ)。</li>
</ul>
</li>
<li>検索用のAPIキーなどを設定
<ul>
<li>検索のためのAPIキーなどの設定が必要となります。<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite/blob/master/exampleSite/config.toml#L32">テーマ作者の方のサンプルの<code>config.toml</code>にパラメータは用意されています</a>。</li>
</ul>
</li>
</ol>
<p>以下の値を設定します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">  <span style="color:#a6e22e">algolia_search</span> = <span style="color:#66d9ef">true</span>
  <span style="color:#a6e22e">algolia_appId</span> = <span style="color:#e6db74">&#34;AlgoliaのApplication ID&#34;</span>
  <span style="color:#a6e22e">algolia_indexName</span> = <span style="color:#e6db74">&#34;作成したインデックス名&#34;</span>
  <span style="color:#a6e22e">algolia_apiKey</span> = <span style="color:#e6db74">&#34;AlgoliaのSearch-Only API Key&#34;</span>
</code></pre></div><p>以上でAlgolia関連の設定などの作業が終了です。</p>
<h2 id="algoliaへのデータ登録方法">Algoliaへのデータ登録方法</h2>
<p>最後に、実際にデータを登録する必要があります。
手順は、以下のとおりです。</p>
<ol>
<li><code>hugo</code>コマンドの実行(htmlと一緒に登録データの<code>algolia.json</code>を生成)</li>
<li><code>npm run algolia</code>コマンドの実行(atomic-algoliaを利用してAlgoliaにデータを登録)</li>
</ol>
<p>設定などが問題なければ、Algoliaの管理画面で登録ができているはずです。
実際にブログのデプロイには<code>deploy.sh</code>というファイルをこちらを元に作成して使っています。このなかで、<code>hugo</code>コマンド実行後に<code>npm run algolia</code>を実行するようにしいます。</p>
<h2 id="今後の課題">今後の課題</h2>
<p>Hugoで生成された記事はそれぞれのブログポスト以外に、タグごとのページも生成されています。
こちらも実はAlgoliaのインデックスに登録されていて、タグを入力すると、タグ名のリンクが出てきます。</p>
<p>こちらは<code>elasticsearch</code>で検索したときの検索結果です。1件目はタグページです。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200128/tag.png" />
    </div>
    <a href="/images/entries/20200128/tag.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>これらのページはAlgoliaに登録しないようにするのが良さそうかな?と考えているところです(考えてるだけ)。</p>
<p><strong>2020/01/29更新</strong></p>
<p><code>list.algolia.json</code>を編集して、記事だけをインデックスするように修正しました。
テーマに存在する<code>layouts/_default/list.algolia.json</code>を、自分のところにコピーし、次のように変更しました。if文を1行追加して、<code>post</code>という種類のものだけを出力するようにしました。</p>
<pre><code>{{/* Generates a valid Algolia search index */}}
{{- $.Scratch.Add &quot;index&quot; slice -}}
{{- $section := $.Site.GetPage &quot;section&quot; .Section }}
{{- range .Site.AllPages -}}
  {{- if or (and (.IsDescendant $section) (and (not .Draft) (not .Params.private))) $section.IsHome -}}
    {{- if (and (eq .Section &quot;post&quot;) (ne .URL &quot;/post/&quot;)) -}}
      {{- $.Scratch.Add &quot;index&quot; (dict &quot;objectID&quot; .UniqueID &quot;date&quot; .Date.UTC.Unix &quot;description&quot; .Description &quot;dir&quot; .Dir &quot;expirydate&quot; .ExpiryDate.UTC.Unix &quot;fuzzywordcount&quot; .FuzzyWordCount &quot;keywords&quot; .Keywords &quot;kind&quot; .Kind &quot;lang&quot; .Lang &quot;lastmod&quot; .Lastmod.UTC.Unix &quot;permalink&quot; .Permalink &quot;publishdate&quot; .PublishDate &quot;readingtime&quot; .ReadingTime &quot;relpermalink&quot; .RelPermalink &quot;html&quot; .Params.Description &quot;title&quot; .Title &quot;type&quot; .Type &quot;url&quot; .URL &quot;weight&quot; .Weight &quot;wordcount&quot; .WordCount &quot;section&quot; .Section &quot;tags&quot; .Params.Tags &quot;categories&quot; .Params.Categories &quot;author&quot; .Params.authors &quot;content&quot; .Params.Description &quot;excerpt_html&quot; .Params.Description &quot;excerpt_text&quot; .Params.Description &quot;summary&quot; .Summary)}}
    {{- end -}}
  {{- end -}}
{{- end -}}
{{- $.Scratch.Get &quot;index&quot; | jsonify -}}
</code></pre><h2 id="まとめ">まとめ</h2>
<p>これで、ブログ内記事検索ができるようになります。
Algoliaは個人の非商用利用の場合、フリープランが用意されているのがありがたいですね。
まだ、Hugoと連携しただけで、Algolia自体でどんな機能があって、どんなことができそうかといったところは調べていませんが、簡単に利用できるのはとても助かります。</p>
<p>まぁ、個人ブログの検索機能ってそんなに使う人はいないんですが、自分としては便利かなぁと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ブログ移行日記(その3) - Hugoの設定と微調整(テーマに合わせた)</title>
      <link>https://blog.johtani.info/blog/2020/01/24/setting-hugo/</link>
      <pubDate>Fri, 24 Jan 2020 17:04:41 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/24/setting-hugo/</guid>
      <description>ブログ移行日記(その3)です。その他の記事はこちら。 ブログ移行日記(その1) ブログ移行日記(その2) ブログ移行日記(その4) ブログ移行日記(</description>
      <content:encoded><p>ブログ移行日記(その3)です。その他の記事はこちら。</p>
<ul>
<li><a href="/blog/2020/01/22/intro-hugo-and-theme/">ブログ移行日記(その1)</a></li>
<li><a href="/blog/2020/01/23/convert-md-from-octopress-to-hugo/">ブログ移行日記(その2)</a></li>
<li><a href="/blog/2020/01/28/introduce-algolia/">ブログ移行日記(その4)</a></li>
<li><a href="/blog/2020/02/21/import-jugem-posts/">ブログ移行日記(その5)</a></li>
</ul>
<p>今回は、Hugoの設定とテーマの一部変更した点について記載します。</p>
<h2 id="設定ファイルconfigtoml">設定ファイル(config.toml)</h2>
<p>Hugoの設定ファイルは<code>config.toml</code>になります。<code>hogo new site</code>コマンドで生成したディレクトリの中にデフォルトで含まれていますが、こちらではなく、テーマの作者が用意してくれた<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite/blob/master/exampleSite/config.toml"><code>config.toml</code></a>を元に変更を加えました。</p>
<p>ちなみにテーマの作者の方が<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite/tree/master/exampleSite">設定とコンテンツを含めたサンプル</a>も公開してくれています。設定やディレクトリの構成はこちらを参考にしました。</p>
<p>設定ファイルで変更した項目は以下のとおりです。</p>
<h3 id="サイトのタイトルや説明など">サイトのタイトルや説明など</h3>
<p>特記することはありません。好きなように変えました。</p>
<ul>
<li>baseurl</li>
<li>title</li>
<li>SEOTitle</li>
<li>description</li>
<li>keyword</li>
<li>slogan</li>
<li>sidebar_about_descrption</li>
</ul>
<h3 id="画像関連ヘッダーや著者近影">画像関連(ヘッダーや著者近影)</h3>
<p>画像の置き場は<code>static/images/</code>ディレクトリですが、設定ファイルには
<code>images/</code>から設定します。</p>
<ul>
<li>header_image : ブログのヘッダー背景</li>
<li>sidebar_avatar : 著者近影</li>
</ul>
<h3 id="言語周り特に多言語対応する予定は無いのですが">言語周り(特に多言語対応する予定は無いのですが)</h3>
<ul>
<li>languageCode = &ldquo;ja&rdquo;</li>
<li>defaultContentLanguage = &ldquo;ja&rdquo;</li>
</ul>
<h3 id="オフにした機能削除した項目">オフにした機能、削除した項目</h3>
<ul>
<li>freands = false</li>
<li>bookmarks = false</li>
</ul>
<p>上記の設定変更以外に、<code>[[params.bookmark_link]]</code>や<code>[[params.friend_link]]</code>も削除。</p>
<p>中国のサービスや、特化した設定など。</p>
<ul>
<li>Baidu Analytics関連</li>
<li>Disqus proxy関連</li>
<li>Reward(wechat pay &amp; alipay関連)</li>
</ul>
<h3 id="paramssocial関連">params.social関連</h3>
<ul>
<li>rss = true</li>
<li>twitter</li>
<li>linkedin</li>
<li>github</li>
</ul>
<p>ほかはすべてコメントアウト。</p>
<h3 id="外部サービス関連">外部サービス関連</h3>
<ul>
<li>disqusShortname : Disqusのショートネーム</li>
<li>googleAnalytics : Gooogle AnalyticsのトラッキングID</li>
<li><a href="https://www.algolia.com/">algolia関連</a> : <em>別のブログ記事として、設定方法などを書きます。</em></li>
</ul>
<h3 id="追加した設定昨日のブログで説明済みhttpsblogjohtaniinfoblog20200123convert-md-from-octopress-to-hugo">追加した設定(<a href="https://blog.johtani.info/blog/2020/01/23/convert-md-from-octopress-to-hugo/">昨日のブログで説明済み</a>)</h3>
<ul>
<li>Octopressと同じ形のパーマリンク</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[<span style="color:#a6e22e">permalinks</span>]
  <span style="color:#a6e22e">post</span> = <span style="color:#e6db74">&#34;/blog/:year/:month/:day/:slug&#34;</span>
</code></pre></div><ul>
<li>Markdownファイル中のHTMLタグ表示</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[<span style="color:#a6e22e">markup</span>.<span style="color:#a6e22e">goldmark</span>.<span style="color:#a6e22e">renderer</span>]
  <span style="color:#a6e22e">unsafe</span> = <span style="color:#66d9ef">true</span>
</code></pre></div><p>以上が設定ファイル関連です。</p>
<h2 id="テーマの変更点">テーマの変更点</h2>
<p>テーマそのままでは問題があったり、独自に変更したいという点があったので、いくつか変更をしています。</p>
<h3 id="フォントの変更">フォントの変更</h3>
<p>そのままテーマを適用するだけでうまく行けばよかったのですが、フォントの問題が発生しました。テーマの作者の方が中国の方だから?かどうかはわかりませんが、デフォルトのままだと中華フォントになってしまいました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">あー、中華フォントだな。 <a href="https://t.co/OvHpY4LSp1">pic.twitter.com/OvHpY4LSp1</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1217703951522353152?ref_src=twsrc%5Etfw">January 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><code>config.toml</code>に<code>custom_css</code>という設定があり、こちらで指定したCSSのファイルがテーマの<code>layouts/partials/head.html</code>から読み込まれる仕組みがあるようなので、フォントに関するCSSをこの機能を使用して指定するようにしました。</p>
<p><code>config.toml</code>の設定は次のとおりです(リスト)になっているので、複数のファイルに分割して、読み込ませることも可能なのかな?。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml"><span style="color:#a6e22e">custom_css</span> = [<span style="color:#e6db74">&#34;css/custom-font.css&#34;</span>]
</code></pre></div><p>cssファイルについては、<code>static/css/custom-font.css</code>というファイルを作成し、次のような記載になっています。
フォントの指定と右側サイドバーの自己紹介の部分の文字色を変更するためのものです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-css" data-lang="css"><span style="color:#f92672">body</span><span style="color:#f92672">,</span> <span style="color:#f92672">h1</span><span style="color:#f92672">,</span> <span style="color:#f92672">h2</span><span style="color:#f92672">,</span> <span style="color:#f92672">h3</span><span style="color:#f92672">,</span> <span style="color:#f92672">h4</span><span style="color:#f92672">,</span> <span style="color:#f92672">h5</span><span style="color:#f92672">,</span> <span style="color:#f92672">h6</span><span style="color:#f92672">,</span> .<span style="color:#a6e22e">navbar-custom</span> { 
    <span style="color:#66d9ef">font-family</span>: Helvetica,<span style="color:#e6db74">&#34;Sawarabi Gothic&#34;</span>,Meiryo,<span style="color:#e6db74">&#34;メイリオ&#34;</span>,<span style="color:#e6db74">&#34;Hiragino Kaku Gothic ProN&#34;</span>, <span style="color:#e6db74">&#34;ヒラギノ角ゴ ProN&#34;</span>,YuGothic,<span style="color:#e6db74">&#34;游ゴシック&#34;</span>,Arial,<span style="color:#66d9ef">sans-serif</span>; 
}
.<span style="color:#a6e22e">sidebar-container</span> {
    <span style="color:#66d9ef">color</span>: <span style="color:#ae81ff">#404040</span>;
    <span style="color:#66d9ef">font-size</span>: <span style="color:#ae81ff">14</span><span style="color:#66d9ef">px</span>;
}
</code></pre></div><h3 id="faviconの変更">faviconの変更</h3>
<p>テーマに<code>favicon.ico</code>が含まれていたのですが、せっかくなので独自のものに変えてみようかと。
ただ、残念ながら、こちらはパスおよびファイル名が<code>layouts/partials/head.html</code>に<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite/blob/c93ea6d2cfaf41f76bb49510c55643ea984e8990/layouts/partials/head.html#L46">直書き</a>されていました。</p>
<p>画像は<code>images</code>配下にと思っていたのですが、このパスだけを変更するために<code>head.html</code>を自分の配下にコピーしてカスタマイズするのもどうかと思った(テーマに変更やバグ修正が入るたびに手動でコピーするのはなぁと思った)ので、<code>static/img/favicon.ico</code>ファイルを作成しました。</p>
<p>テーマよりもHugoのプロジェクトにあるファイルを優先するようなので、ファイルだけをプロジェクトに作成しました。</p>
<h3 id="記事一覧のテンプレート">記事一覧のテンプレート</h3>
<p>記事の一覧で表示される、作成者と作成日時が英語表記でかつ、冗長な感じがしたので、スッキリさせるために、<code>layouts/partials/post_list.html</code>をテーマからコピーして、次のように変更しました。</p>
<ul>
<li>元の形式は : <code>Posted by author Monday, January 2, 2006</code></li>
<li>現在の形式 : <code>2006-01-02 by author</code></li>
</ul>
<h3 id="記事のテンプレート">記事のテンプレート</h3>
<p>今回採用したテーマでは、記事の先頭に記事のセクションを元に目次を生成してくれるものでした。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">目次をコンテンツから自動で作ってくれるの便利だな。 <a href="https://t.co/9bU3sLnUrm">pic.twitter.com/9bU3sLnUrm</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1217734384159117312?ref_src=twsrc%5Etfw">January 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>とても便利です。ただ、表示が「TOC」なんです。
英語でしかも「ToC」という表記ならまだ気にならなかったかもですが、<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite/blob/c93ea6d2cfaf41f76bb49510c55643ea984e8990/layouts/_default/single.html#L60">大文字だと</a>流石に気になったので、プロジェクトの<code>layouts/_default/single.html</code>にコピーして「目次」という日本語に書き換えました。
このHTMLにテーマで修正が入った場合はどうしようかなぁ。。。というのが目下の悩みです。。。</p>
<h3 id="archetypeテンプレートの追加">Archetypeテンプレートの追加</h3>
<p>最後は新規記事を書くときに生成されるMarkdownのメタデータの追加です。
Hugoには<a href="https://gohugo.io/content-management/archetypes/#what-are-archetypes">Archetypes</a>というのが存在します。</p>
<p>Hugoでは<code>hugo new 記事</code>としたときに、記事の種類(<code>content/ディレクトリ名=記事のタイプ</code>)によって、作成するmarkdownファイルをテンプレートから生成する機能があります。この生成時に使われるのが<code>archetypes</code>というディレクトリにあるファイルです。</p>
<p>私のブログサイトでは、今のところ<code>content/post</code>というブログの記事だけを書く予定ですので、<code>archetypes/post.md</code>というファイルを作って以下のようなメタデータを<code>hugo new</code>したときに自動で生成するようにしました(<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite/blob/c93ea6d2cfaf41f76bb49510c55643ea984e8990/archetypes/post.md">テーマにあった<code>post.md</code>ファイル</a>の代わり)。</p>
<pre><code>---
layout: post
title: &quot;{{ replace .Name &quot;-&quot; &quot; &quot; | title }}&quot;
slug: &quot;{{ substr .Name 11 }}&quot;
author: johtani
date: {{ .Date }}
comments: true
tags: []
draft: true
---
</code></pre><ol>
<li>タイトルはファイル名のハイフンを空白に変換したもの(実際にはファイル名は英語にしているので、使いませんが。。。)</li>
<li><code>slug</code>はファイル名の先頭から<code>YYYY-MM-DD-</code>という11文字を除いたもの。これは、OctopressのURLに合わせるために使用するURLの一部の文字列です。</li>
<li><code>author: johtani</code> : 著者は私だけだから固定文字列</li>
<li><code>comments: true</code> : ブログ記事にはDisqusのコメント機能を利用</li>
<li><code>tags: []</code> : 各内容によってタグを付けるが、生成時には空</li>
<li><code>draft: true</code> : 明示的にこの行を消すまではドラフト記事としたいため</li>
</ol>
<p>という感じです。ほかにどのよなメタデータがあるのかまではまだ調べていないので、今後また適宜変更していくと思います。</p>
<h2 id="まとめ">まとめ</h2>
<p>Hugoの設定や、テーマそのままではなく独自の変更を加えた部分を思い出して書き出してみました。</p>
<p>これで、<a href="https://www.algolia.com/">Algolia</a>に関する部分以外はだいたい思い出して書いたと思います。
次は、Algoliaの使い方と設定について書き残す予定です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ブログ移行日記(その2) - Markdownファイルの変換</title>
      <link>https://blog.johtani.info/blog/2020/01/23/convert-md-from-octopress-to-hugo/</link>
      <pubDate>Thu, 23 Jan 2020 19:31:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/23/convert-md-from-octopress-to-hugo/</guid>
      <description>その他の記事はこちら ブログ移行日記(その1) ブログ移行日記(その3) ブログ移行日記(その4) ブログ移行日記(その5) ブログ移行日記(その2)</description>
      <content:encoded><p>その他の記事はこちら</p>
<ul>
<li><a href="/blog/2020/01/22/intro-hugo-and-theme/">ブログ移行日記(その1)</a></li>
<li><a href="/blog/2020/01/24/setting-hugo/">ブログ移行日記(その3)</a></li>
<li><a href="/blog/2020/01/28/introduce-algolia/">ブログ移行日記(その4)</a></li>
<li><a href="/blog/2020/02/21/import-jugem-posts/">ブログ移行日記(その5)</a></li>
</ul>
<p>ブログ移行日記(その2)です。<a href="https://blog.johtani.info/blog/2020/01/22/intro-hugo-and-theme/">前回</a>はHugoとは?というのと、自分が選んだテーマについて記載しました。
本家の手順などを参考にすると、Hugoにテーマを適用し、でHTMLを生成して、表示するところまでできるはずです。</p>
<p>今回は、OctopressのmarkdownファイルをHugo用に変換する方法について紹介します。お手製ですが、Pythonスクリプトを作ったので、そちらも合わせて簡単に紹介する予定です。</p>
<h2 id="参考ブログ">参考ブログ</h2>
<p>「Octopress Hugo 移行」でググるといくつか出てきます。先人の知恵ありがたいですね。
ということで、私はこちらの2つのブログを参考にさせていただきました。ありがとうございます。</p>
<ul>
<li><a href="https://iriya-ufo.net/blog/2018/12/27/octopress-to-hugo/">Octopress から Hugo へ移行した - iriya-ufo&rsquo;s blog</a></li>
<li><a href="https://gam0022.net/blog/2016/09/25/migrated-from-octopress-to-hugo/">OctopressからHugoへ移行する方法 | gam0022.net</a></li>
</ul>
<h2 id="画像のコピー">画像のコピー</h2>
<p>画像はそのまま上記参考ブログを元に<code>source/images</code>から<code>static/images</code>にコピーしました。特にディレクトリ構造の変更とかもしませんでした。</p>
<h2 id="コンテンツのコピー">コンテンツのコピー</h2>
<p>こちらは、コピーのタイミングでいくつか変換などの処理を行いました。
ファイルの変換には<a href="https://github.com/johtani/from-octopress-to-hugo">Pythonのスクリプト</a>を書きました。
自分向けの移行ツールなんで、ディレクトリ名とか引数にすらしてないです。。。</p>
<p>参考ブログと同様の変換</p>
<ul>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L65">メタデータの日付変換</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L106">categoriesをtagsに変換</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L83">画像タグの変換</a>
<ul>
<li>タイトル、画像のサイズなどに合わせていくつか分岐があります。</li>
</ul>
</li>
</ul>
<p>参考ブログとは異なる変換、変更</p>
<ul>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L110">コードブロックは無変換</a>
<ul>
<li><a href="https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences">0.60.0からCode Fencesに対応</a>したみたいなので不要でした。</li>
</ul>
</li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L38">ディレクトリ構造の変更</a>
<ul>
<li>ファイルを年ごとのディレクトリに格納(これまでは、全てのファイルが同一ディレクトリにあった)</li>
</ul>
</li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L38">拡張子の変更 (<code>.markdown</code> -&gt; <code>.md</code>)</a></li>
<li><a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L76">メタデータに<code>author</code>の追加(自分しか書かないんですけどね)</a></li>
</ul>
<h2 id="urlをoctopressに合わせる">URLをOctopressに合わせる</h2>
<p>Google検索からの流入もあり、これまでのURLに変更はかけたくないなと。
こちらも参考ブログに記載があるので、<a href="https://gam0022.net/blog/2016/09/25/migrated-from-octopress-to-hugo/#octopress%E3%81%A8%E5%90%8C%E4%B8%80%E3%81%AE%E3%83%91%E3%83%BC%E3%83%9E%E3%83%AA%E3%83%B3%E3%82%AF%E3%81%AB%E3%81%99%E3%82%8B-%E3%82%AA%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3">そのまま参考に</a>させていただきました。</p>
<p><code>slug</code>については、<a href="https://github.com/johtani/from-octopress-to-hugo/blob/master/fileconverter.py#L80">移行ツール</a>でファイル名を元に追加する処理を書きました。</p>
<h2 id="htmlを含んだmarkdownの対応">HTMLを含んだMarkdownの対応</h2>
<p>TweetやAmazonのアフィリエイトのリンクがHTMLタグでいくつかの記事に含まれており、デフォルトの設定だと表示されません。
<code>config.toml</code>ファイルに以下の設定を追記することで、出力されるようになりました。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[<span style="color:#a6e22e">markup</span>.<span style="color:#a6e22e">goldmark</span>.<span style="color:#a6e22e">renderer</span>]
  <span style="color:#a6e22e">unsafe</span> = <span style="color:#66d9ef">true</span>
</code></pre></div><p>名前が<code>unsafe</code>なので、ちょっと気になりますが。。。
すべての記事を表示してチェックしてみたわけではないので、おかしな記事を見つけた方は連絡をいただけると助かります。</p>
<h2 id="まとめ">まとめ</h2>
<p>OctopressのファイルをHugo用にコピーや変換した方法を思い出しながら書いてみました。基本的には参考ブログに上げた2つのブログを真似したものになります。</p>
<p>次は、利用したテーマのサンプル設定を元に、自分用に変更した点などについて書き残しておこうかな?</p>
</content:encoded>
    </item>
    
    <item>
      <title>ブログ移行日記(その1) - Hugoとテーマ</title>
      <link>https://blog.johtani.info/blog/2020/01/22/intro-hugo-and-theme/</link>
      <pubDate>Wed, 22 Jan 2020 11:23:34 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/22/intro-hugo-and-theme/</guid>
      <description>その他の記事はこちら ブログ移行日記(その2) ブログ移行日記(その3) ブログ移行日記(その4) ブログ移行日記(その5) 起因 いつものツイートから</description>
      <content:encoded><p>その他の記事はこちら</p>
<ul>
<li><a href="/blog/2020/01/23/convert-md-from-octopress-to-hugo/">ブログ移行日記(その2)</a></li>
<li><a href="/blog/2020/01/24/setting-hugo/">ブログ移行日記(その3)</a></li>
<li><a href="/blog/2020/01/28/introduce-algolia/">ブログ移行日記(その4)</a></li>
<li><a href="/blog/2020/02/21/import-jugem-posts/">ブログ移行日記(その5)</a></li>
</ul>
<h2 id="起因">起因</h2>
<p>いつものツイートから始まる私の行動です。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">(もしかしたら前に一度おすすめしたかもですが) Goのtemplate構文に拒絶反応がなければHugo割と良いですよ〜。</p>&mdash; Nobuyuki Kubota (@nobu_k) <a href="https://twitter.com/nobu_k/status/1182345041051701248?ref_src=twsrc%5Etfw">October 10, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>ってことで、Hugo勧められたし、テーマが豊富だしということで、乗り換えた次第です。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">(元)同僚に教えてもらった記事を見てる。Gatsbyも気になるんだけど、デザインセンスないし、テーマが豊富なのがいいなぁ。/ Comparison of Gatsby vs Jekyll vs Hugo | GatsbyJS - <a href="https://t.co/yUbKiBmtMS">https://t.co/yUbKiBmtMS</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1215141907967311872?ref_src=twsrc%5Etfw">January 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>一応Gatsbyのサイトにあった比較も見たのですが、テーマの豊富さが勝ちました。デザインを自分でやれるほどではないので。</p>
<h2 id="理由">理由</h2>
<p>乗り換えるに至ったのは主に2つの理由です。</p>
<ol>
<li>Octopressが更新されていない</li>
<li>ページが増えてきてサイトの生成に時間がかかる</li>
</ol>
<p>前に使っていた<a href="http://octopress.org/">Octopress</a>も<a href="https://github.com/jekyll/jekyll">Jekyll</a>というものがベースになっていました。
Jekyllは今でも更新があるのですが、Octopressが更新されなくなってしまったのと、Rubyがベースになっているため?なのかはわかりませんが、
ブログのページ数が増えてきて、サイトのビルドに時間がかかってくるようになりました。</p>
<h2 id="結果">結果</h2>
<p>まだ、改良点があるかもですが、とりあえず公開できる感じになったと思ったんで切り替えました。</p>
<p>前のブログはこんな感じで、</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200122/octopress.png" />
    </div>
    <a href="/images/entries/20200122/octopress.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>移行後はこんな感じです。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20200122/hugo.png" />
    </div>
    <a href="/images/entries/20200122/hugo.png" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="hugoとは">Hugoとは?</h2>
<ul>
<li><a href="https://gohugo.io/">公式サイト</a></li>
</ul>
<p>こちらにあるように、Go言語で実装されているウェブサイト構築フレームワークです。
Go言語で実装されているのもあり、インストールが簡単でした。
Macを使っていますが、<a href="https://gohugo.io/getting-started/installing/#macos">Homebrewでインストール</a>ができてしまいます。
他の方法もあるようでしたが、Emacsをインストールするのにbrewを入れているので、brewでインストールしました。</p>
<p>使い方は色んな人が書いてるし、公式ドキュメントを見ていただけばいいかな。</p>
<h2 id="テーマとは">テーマとは?</h2>
<p>Hugoのサイトに<a href="https://themes.gohugo.io/">テーマの一覧</a>があります。
一応、個人のブログなので、それなりにデザインを入れつつ、他の人と違う感じにしたいなと。
テーマ一覧をざっと眺めて良さそうなのをピックアップしたら、最終的にこちらのテーマになりました。</p>
<ul>
<li><a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite">Clean White</a></li>
</ul>
<p>それなりに更新されてますし、DisqusとSearch(Algolia)が使えるのでこのテーマに決めました。
テーマのインストール方法などは<a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite">GitHubのREADMEの「Quick Start」</a>に記載があります。</p>
<p>私は、Hugo自体の設定などをGitHubで管理したかったので、<code>git submodule</code>を利用して、次のような構成になりました。</p>
<pre><code>hugo - main repository
├── archetypes
├── content
├── data
├── layouts
├── public - github.com/johtani/johtani.github.io
├── resources
├── static
└── themes
    └── hugo-theme-cleanwhite - github.com/zhaohuabing/hugo-theme-cleanwhite.git
</code></pre><ul>
<li><code>hugo</code> : <code>hugo new site</code>コマンドで作成されたディレクトリです。このディレクトリでまず<code>git init</code>しました(このリポジトリはプライベートで管理してます)。</li>
<li><code>themes/hugo-theme-cleanwhite</code> : テーマのQuick Startにある<code>git submodule add</code>コマンドでサブモジュールとしてテーマをインストールしました。</li>
<li><code>public</code> : hugoが生成するHTMLのトップのディレクトリがこちらです。私はGitHub pagesを利用してブログを公開しているので、<code>git submodule add</code>で<code>johtani.github.io</code>をサブモジュールにしました。</li>
</ul>
<p>Hugoで生成したHTMLなどをGitHub pagesで公開するときの手順などは<a href="https://gohugo.io/hosting-and-deployment/hosting-on-github/#types-of-github-pages">Hugoのドキュメント</a>に記載がありました。</p>
<h2 id="まとめ">まとめ</h2>
<p>Hugoに移行した理由や、Hugoとテーマの簡単な紹介でした。
テーマが豊富なのはデザイン力(りょく)がない身としてはありがたいですよね。
次はOctopressのmarkdownファイルをHugo用に変換したり、それに関する設定周りの話を書く予定です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>OctopressからHugoへ移行中(まだ途中)</title>
      <link>https://blog.johtani.info/blog/2020/01/16/moving-to-hugo/</link>
      <pubDate>Thu, 16 Jan 2020 18:23:37 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/01/16/moving-to-hugo/</guid>
      <description>5年くらい、Octopressを使用していましたが、更新されなくなっているのと、コンテンツの生成に時間がかかることもあり、 ほかのプラットフォ</description>
      <content:encoded><p>5年くらい、Octopressを使用していましたが、更新されなくなっているのと、コンテンツの生成に時間がかかることもあり、
ほかのプラットフォームを使用するように変更しました。</p>
<p>とりあえず、今回の移行で参考にした記事とかURLをリストアップしてみました。
詳細についてはまた明日以降で。</p>
<ul>
<li>参考記事
<ul>
<li>移行関連 : <a href="https://iriya-ufo.net/blog/2018/12/27/octopress-to-hugo/#github-pages-">https://iriya-ufo.net/blog/2018/12/27/octopress-to-hugo/#github-pages-</a></li>
<li>移行関連 : <a href="https://gam0022.net/blog/2016/09/25/migrated-from-octopress-to-hugo/">https://gam0022.net/blog/2016/09/25/migrated-from-octopress-to-hugo/</a></li>
<li>Hugo自体の日本語紹介記事 : <a href="https://knowledge.sakura.ad.jp/22908/">https://knowledge.sakura.ad.jp/22908/</a></li>
</ul>
</li>
<li>Hugo概要
<ul>
<li><a href="https://gohugo.io/">https://gohugo.io/</a></li>
</ul>
</li>
<li>テーマ
<ul>
<li>一覧 : <a href="https://themes.gohugo.io/">https://themes.gohugo.io/</a></li>
<li>利用したテーマ : <a href="https://github.com/zhaohuabing/hugo-theme-cleanwhite">https://github.com/zhaohuabing/hugo-theme-cleanwhite</a></li>
</ul>
</li>
<li>データ移行
<ul>
<li>作ったスクリプト : <a href="https://github.com/johtani/from-octopress-to-hugo">https://github.com/johtani/from-octopress-to-hugo</a></li>
</ul>
</li>
<li>favicon
<ul>
<li>作ったサイト : <a href="http://emblemmatic.org/markmaker/#/">http://emblemmatic.org/markmaker/#/</a></li>
</ul>
</li>
<li>Algoliaセッティング
<ul>
<li>参考 : <a href="https://blog.uni-3.app/2019/01/02/hugo-algolia-search/">https://blog.uni-3.app/2019/01/02/hugo-algolia-search/</a></li>
</ul>
</li>
<li>GitHub Pagesでの運用
<ul>
<li><a href="https://gohugo.io/hosting-and-deployment/hosting-on-github/#types-of-github-pages">https://gohugo.io/hosting-and-deployment/hosting-on-github/#types-of-github-pages</a></li>
</ul>
</li>
</ul>
<h2 id="残タスク">残タスク</h2>
<ul>
<li>Amazonのアフィリンクをきれいに表示するlayoutか何かを用意する?　</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2019）</title>
      <link>https://blog.johtani.info/blog/2019/12/31/looking-back-2019/</link>
      <pubDate>Tue, 31 Dec 2019 12:59:03 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/31/looking-back-2019/</guid>
      <description>今年も振り返りブログ書いてます。Sasukeがついてる。 振り返り（2018年に書いた抱負から） まずは去年の抱負を元に。 TOEIC 受けました。 お。TO</description>
      <content:encoded><p>今年も振り返りブログ書いてます。Sasukeがついてる。</p>
<!-- more -->
<h2 id="振り返り2018年に書いた抱負から">振り返り（2018年に書いた抱負から）</h2>
<p>まずは去年の抱負を元に。</p>
<h5 id="toeic">TOEIC</h5>
<p>受けました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">お。TOEICの点上がってた（まだまだだけど。。。）</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1186137471551041537?ref_src=twsrc%5Etfw">October 21, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>無事、上がってました。テストの点数を上げるのが目的ではなく、今の実力が
どんな感じかというのを確認するために受けました。
前回までが低かったのもあるんですが。
また、数年後に受けてみるのかな?</p>
<h5 id="cfp見つけて応募--いろんな場所に顔を出す">CfP見つけて応募 &amp; いろんな場所に顔を出す</h5>
<p>色んな所には顔を出しました。
CfPはそれほど出せていなく、とちぎRubyとDevRelJPで喋った程度かと。
12月に最終出社日を迎えたので、今後は顔を出したりCfP見つけて応募も、
一旦休憩に入る予定です。</p>
<h5 id="もっとブログ">もっとブログ！</h5>
<p>12月に入ってから、一気に増えてますねw
Elasticでは結局それほどかけなかったなぁ。</p>
<h5 id="rustの継続">Rustの継続</h5>
<p>こっちは、書籍も買ったのに手がついてないです。
来年に書籍を読みながら再入門する予定です。</p>
<h5 id="開発の継続">開発の継続</h5>
<p>一応ほそぼそと続けてますが、メンテナンスですね。
Analyzer向けの<a href="https://github.com/johtani/analyze-api-ui-plugin">Kibanaのプラグイン</a>のバージョンアップにだけ対応していたり。
年初にReact化には着手しました。
あとは、Luceneにちょっとしたパッチ(Lukeの画面とKuromojiのUniDic対応)を送った感じかな。もっとLucene周りやりたいかな。</p>
<h5 id="日本でエンジニア獲得">日本でエンジニア獲得!</h5>
<p>トレーナーやエンジニアが入社してくれました!
が、私が退職してしまいますが。。。
私自体が次のステップに移らないとなぁと。
Elasticに興味がある人いたら、中の人を紹介できるので声をかけてください!</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<p>さて、ここからは今年の出来事を。</p>
<ul>
<li>検索技術勉強会発足</li>
<li>勉強会運営座談会</li>
<li>初フロリダ</li>
<li>初カナダ&amp;初ナイアガラの滝</li>
<li>退職(予定)</li>
</ul>
<p>検索技術勉強会ってのをはじめました(<a href="http://blog.johtani.info/blog/2019/02/26/start-search-engineering-tech-talk/">はじめましたブログはこちら</a>)。
やっぱり検索の話を聞いたり考えたりするのが好きなので、もっといろんな話を聞く機会がほしいなぁと。ツイートすると何人かの知り合いが手を上げてくれたので、初めて見たのが2月でした。
所属会社もあったので、自分が全面に出ない形で運営に協力するという形式でやっています。なんだかんだで<a href="https://search-tech.connpass.com/">もう4回も開催できています</a>。スピーカーへの声掛けなど、複数人で運営するとそのあたりが多様性が出るし、楽になるのですごくいいなぁと。
来年も2月か3月にやりたいねと話をしているところです。興味のある方は、ぜひスピーカーを!</p>
<p>で、その流れで、他の勉強会の運営ってみんなどうしてるんだろう?
という疑問も出てきたので、<a href="http://blog.johtani.info/blog/2019/04/25/meetup-organizer-drinkup/">勉強会運営座談会</a>なるものもやってみました。少人数で集まって、勉強会の運営ってどんなやり方してます?どんなことに困ってます?みたいなのを共有してみました。
自分自身がそれほど、ほかの勉強会の運営に顔を出してたわけでもなく、やり方をみんなはどうやって共有してるのかなぁ?というのが気になったので。
思った以上に属人化してるのかもなぁというのが感想でした。
ドタバタしてたので、あれからやってませんが、共有する勉強会みたいなのも面白いのかも?</p>
<p>今年も2回ほどElasticの社内イベントで海外に行かせてもらいました。
5月にフロリダと11月にトロントでした。
フロリダでは、NASAにも行けたし、トロントでは足を伸ばしてナイアガラの滝を見に行くなどもしてみました。こういう機会が定期的にあるのは、いいですよねぇ。</p>
<p>最後は、<a href="http://blog.johtani.info/blog/2019/12/06/leaving-elastic/">退職イベント</a>ですね。5年5ヶ月勤めたElasticでの仕事が、12/6で最終出社日でした。実際の退職日は1/24となっています(現在有給休暇消化中)。長かったような、短かったような。日本で1人目で入社してさまざまなことをやらせてもらい、本当に色んな経験ができました。英語も上達したし。
ただ、外にいることでできることがありそうだなと感じたこともあってのこの決断でした。</p>
<p>最終出社日以降、色んな人とランチさせてもらったりしています。
次に何をやろうかな?ってのんびり考えながら休んでいるところなので。
やっぱり、検索周りのことをやりたいなぁというのが念頭にあるので、そのあたりで模索中という感じです。
が、せっかくの機会なので、少しのんびりしようかなとも思ってます。
なので、プラモデルの作成や配達業(デス・ストランディング)をやってみたり、16インチMacbookのセットアップしながらブログ書いたりしています。
年明けに入ったら、プログラミングもちょっとやりたいな。</p>
<p>あとこの場を借りて、ブログの欲しい物リストのものを送っていただいた皆様に感謝を述べさせていただきます。本当にありがとうございました!この感謝はまた何かの機会にでも!</p>
<h2 id="来年の抱負">来年の抱負</h2>
<p>ということで、来年の抱負です。</p>
<ul>
<li>職につく</li>
<li>ブログプラットフォームの移行</li>
<li>プログラミング</li>
<li>Rust再入門</li>
<li>検索の勉強</li>
<li>検索技術勉強会の継続</li>
</ul>
<p>まぁ、まずは職につかないとですね。どこかに在籍しつつ、副業で検索周りのこととかやるのもありかもなぁと考えたりしています(プラモデルつくりながらw)。</p>
<p>ブログのプラットフォームをOctopressからHugoあたりに移行しようかなと思ってます。Octopress自体がもう開発がされてないようですし。Hugoだとテンプレート?テーマ?がそれなりにありそうなので良さそうかなぁと。移行プログラムも書かないとかな。新しいものを調べるのって楽しいですよね。</p>
<p>プログラミングも復活させたいなと。どうしてもこれまでは、しゃべるのをメインにしていたので、後回しにしてしまっていたのもあります。まぁ、向いてないのかもと思ったりもしますが、下手の横好きなりに少しずつでも何かを改善したり作ったりしたいなと。
勉強会の運営周りですこし楽をするためのプログラム書いたりもしているので、この辺もブログに書こうかな?</p>
<p>Rustは<a href="https://github.com/tantivy-search/tantivy">Tantivy</a>というプロダクトも出てきているし、勉強しようと思って、<a href="https://gihyo.jp/book/2019/978-4-297-10559-4">自転車本</a>を買いつつ、Kindleの肥やしにしているので、手を動かそうとおもいます。
ブログ書いてサボった場合の可視化しないとなw</p>
<p>検索の勉強もやりたいなと。これまではElasticsearchに注力していましたが、ほかのプロダクトやサービスも調べてみたいなと。教えを請いに皆さんのところにお邪魔するかもしれないので、そのときは優しくしてください。</p>
<p>今年始めた検索技術勉強会を今後も継続できるようにしたいなと。
で、運営周りを楽にできるような仕組みをもう少し導入できればなぁと考えていきたいなと思います。</p>
<p>さて、嵐の曲も始まったし終わりです。</p>
<p>今年も様々な方々に助けていただきました。また、最終出社日以降にランチを一緒にさせていただいたり、遊びに行かせていただいたりとありがとうございました。年明けもひましてるので、ぜひランチしましょうw</p>
<p>来年はなにやってるんだろうなぁ?
今後も皆さんよろしくおねがいします!!</p>
</content:encoded>
    </item>
    
    <item>
      <title>macOS標準のターミナルのウィンドウグループが便利</title>
      <link>https://blog.johtani.info/blog/2019/12/23/whats-window-group-of-terminal/</link>
      <pubDate>Mon, 23 Dec 2019 10:14:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/23/whats-window-group-of-terminal/</guid>
      <description>このブログ書こうと思って、これまでMacのセットアップしてたんだった。。。 普段、macOSの標準ターミナルを使ってます。 で、ターミナルを使う</description>
      <content:encoded><p>このブログ書こうと思って、これまでMacのセットアップしてたんだった。。。</p>
<!-- more -->
<p>普段、macOSの標準ターミナルを使ってます。
で、ターミナルを使うシーンは次のような感じです。</p>
<ol>
<li>ブログのデプロイ（参考：<a href="http://blog.johtani.info/blog/2019/12/16/dockernize-octopress/">Octopress環境のDockerイメージ化</a>）</li>
<li>ElasticsearchとKibanaの動作確認
<ol>
<li>Elasticsearchの起動用ターミナル</li>
<li>Kibanaの起動用ターミナル</li>
</ol>
</li>
<li>プラグインの開発(<a href="https://github.com/johtani/elasticsearch-ingest-csv">elasticsearch-ingest-csv</a> と <a href="https://github.com/johtani/analyze-api-ui-plugin">analyze_api_ui</a>)
<ol>
<li>elasticsearchのリポジトリ</li>
<li>kibanaのリポジトリ</li>
<li>ingest-csvのリポジトリ</li>
<li>analyze_api_uiのリポジトリ</li>
</ol>
</li>
</ol>
<p>上記のように1.以外については複数のターミナルをそれぞれのディレクトリで起動して使っています。
これらの3つのパターンのときに、毎回ターミナル(シーンによっては複数のウィンドウ)を開いて、ディレクトリに移動って大変だなと。</p>
<h2 id="ウィンドウグループって">ウィンドウグループって？</h2>
<p>そこで、ターミナルに便利な機能がないのかな?と思い、メニューを見ていたところ「ウィンドウグループ」というのがあったので調べて、現在の使い方に至っています。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20191223/window_group_menu.png" />
    </div>
    <a href="/images/entries/20191223/window_group_menu.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p><a href="https://support.apple.com/ja-jp/guide/terminal/trml15652/mac">ウィンドウグループの使い方の説明はこちらです</a>。</p>
<p>使い方にあるようにウィンドウの場所、設定、タブなどを保存できます。
そして、「ウィンドウグループを開く」というメニューから保存しておいたグループを開くと、簡単に作業環境が再現可能になります。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20191223/open_window_group.png" />
    </div>
    <a href="/images/entries/20191223/open_window_group.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>開くときのメニューはこんな感じです。</p>
<h2 id="作り方">作り方</h2>
<p><a href="https://support.apple.com/ja-jp/guide/terminal/trml15652/mac">ウィンドウグループの使い方</a>に載ってます。
まずは、保存したいターミナルを開いて、開いたときに移動していたいディレクトリに移動します。
複数のタブやウィンドウを開いた状態にしたい場合は、複数ターミナルを起動して、それぞれ移動したいディレクトリに移動しておきます。</p>
<p>グループとしてまとめたいウィンドウが起動できたら、あとは、「ウィンドウ」メニューの「ウィンドウをグループとして保存」を選択します。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20191223/save_window_group.png" />
    </div>
    <a href="/images/entries/20191223/save_window_group.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>これだけです。あとは、使いたくなったときに開くだけです。
簡単ですよね?</p>
<p>作るときの注意点としては、複数のウィンドウグループを作りたいときは、
それぞれグループごとに作成しては、個別にやる必要があります。
現在開いているターミナルのうち、あれとこれをウィンドウにしたいというやり方はできないので、そこだけ注意点です。
環境ごとにプロファイル(バックグラウンドの色やテーマ)を覚えさせることができるので、1.のシーンだとIceberg、2.のシーンではBasic、3.のシーンではHomevrewで設定していたりします。
(ただ、Preztoのテーマを<code>agnoster</code>にしてしまったので、Basicだとプロンプトが見にくくてしょうがないのですが。。。)</p>
<h2 id="設定はどんな感じ">設定はどんな感じ？？</h2>
<p>ウィンドウグループの管理については、ターミナルの「環境設定」に管理用の画面があります。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20191223/config_window_group.png" />
    </div>
    <a href="/images/entries/20191223/config_window_group.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>この画面でいらないグループを削除したり、設定のエクスポート/インポートも可能です。</p>
<h2 id="私の使い方">私の使い方</h2>
<p>3つの利用シーンは説明しました。
すべてのウィンドウグループを変更するたびにインポート/エクスポートしてます。
上記1.と3.のユースケースはほぼ変更がないので、エクスポートしたものを移植用に管理していますが、2.のユースケースについては、頻繁に更新しています(ElasticsearchとKibanaのバージョンをアップしないといけないので)。</p>
<p>このとき、MacのGUIで毎回設定をしているわけではありません。
エクスポートしたファイルは、<code>.terminal</code>という名のXMLファイルになっています。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20191223/terminal_file.png" />
    </div>
    <a href="/images/entries/20191223/terminal_file.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>その中に、次のようなパスが記述されています。このバージョン番号の部分をアップデートのたびにemacsなどで変更し、ターミナルの管理画面で、削除してからインポートし直すだけで、バージョンアップに対応している感じです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">
					<span style="color:#f92672">&lt;key&gt;</span>Tab Working Directory URL<span style="color:#f92672">&lt;/key&gt;</span>
					<span style="color:#f92672">&lt;string&gt;</span>file://johtani-mac-15.local/Users/johtani/tmp/elastic_stack/v7.5/elasticsearch-7.5.1/<span style="color:#f92672">&lt;/string&gt;</span>
					<span style="color:#f92672">&lt;key&gt;</span>Tab Working Directory URL String<span style="color:#f92672">&lt;/key&gt;</span>
					<span style="color:#f92672">&lt;string&gt;</span>file://johtani-mac-15.local/Users/johtani/tmp/elastic_stack/v7.5/elasticsearch-7.5.1<span style="color:#f92672">&lt;/string&gt;</span>
</code></pre></div><h2 id="まとめ">まとめ</h2>
<p>ということで、標準ターミナルのウィンドウグループを使うと、よく開くディレクトリやウィンドウ、タブを覚えて置かせることができるので、それぞれの環境向けのターミナルを、好きなときに開くことができるようになります。
私は、ウィンドウは1つで複数のタブをウィンドウグループという形で覚えさせておくことで、各シーンの切り替え、もしくは併用を閉じたり開いたりを簡単にできるようにしています。</p>
<p>他のターミナルソフトを使うと似たようなことが簡単にできたりするのかな?
みなさんがどうやってこのような作業やってるのかな?ってのはちょっと気になるところです。</p>
<h2 id="余談いつものようにツイートとその反応">余談（いつものようにツイートとその反応）</h2>
<p>ちなみに、みんなどんなターミナル環境なんだろう？と思いツイートしてみた結果はこちらです。
(コメントRTってうまいことひろえないのかなぁ？)</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Mac標準のターミナル使ってる人ってどのくらいいるんだろ？iTermとか入れてるのかな？tmuxとか？</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1204686169260187648?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Iterm2使ってtmux使って今はIterm2だなー<br><br>command2回押すだけでターミナル出すくせが抜けん <a href="https://t.co/P8o4M1HSoj">https://t.co/P8o4M1HSoj</a></p>&mdash; GO☆ | TAFDATA (@_gogoponz) <a href="https://twitter.com/_gogoponz/status/1204736394800685057?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Alacritty + tmux やなー。標準はもう使ってない。 <a href="https://t.co/HI7hPCg3uo">https://t.co/HI7hPCg3uo</a></p>&mdash; Ryo HIGASHIGAWA (@biwakonbu) <a href="https://twitter.com/biwakonbu/status/1204729048137527296?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">標準です <a href="https://t.co/W27e2pcq3j">https://t.co/W27e2pcq3j</a></p>&mdash; shin higuchi @Acroquest (@shin0higuchi) <a href="https://twitter.com/shin0higuchi/status/1204727472937267200?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Macを買って以来、ずっと結局標準ターミナル以外使っていない感じ。珍しかったのか。。 <a href="https://t.co/Zjz32SgjC6">https://t.co/Zjz32SgjC6</a></p>&mdash; Kazuhiro Hara™ (@kara_d) <a href="https://twitter.com/kara_d/status/1204689857982779393?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</content:encoded>
    </item>
    
    <item>
      <title>homeshick導入</title>
      <link>https://blog.johtani.info/blog/2019/12/18/introduce-homeshick/</link>
      <pubDate>Wed, 18 Dec 2019 08:11:06 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/18/introduce-homeshick/</guid>
      <description>ドットファイル系（.emacsとか）をこれまでは、PCを引っ越すたびにコピーしてたんですが、いいかげん、GitHubとかで管理したいなと。 で</description>
      <content:encoded><p>ドットファイル系（<code>.emacs</code>とか）をこれまでは、PCを引っ越すたびにコピーしてたんですが、いいかげん、GitHubとかで管理したいなと。</p>
<!-- more -->
<p>で、ツイートしたところ（こればっかりだなｗ）、homeshick(homesick)が便利だよとの情報を得たので使ってみました。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">homesick、正確にいうとhomeshickを使ってます。悪くないです</p>&mdash; 🤓k.bigwheel🤓 (@k_bigwheel) <a href="https://twitter.com/k_bigwheel/status/1204263176566525952?ref_src=twsrc%5Etfw">December 10, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h3 id="使い方とか">使い方とか</h3>
<p>実物はGitHubで公開されていました。</p>
<p><a href="https://github.com/andsens/homeshick">https://github.com/andsens/homeshick</a></p>
<p>何者かというと、ホームディレクトリにあるドットファイル(<code>.zshrc</code>など)をgitコマンドで管理するのを楽にしてくれるシェルの関数群みたいです。
インストールは至ってかんたんで、git cloneで持ってくるだけです。<a href="https://github.com/andsens/homeshick">homeshick</a>のリポジトリのREADMEに記載があります。</p>
<p>使い方は<a href="https://github.com/andsens/homeshick/wiki/Tutorials">Tutorials</a>にあります。</p>
<ol>
<li><code>homeshick generate ほげほげ</code>で管理する単位（castle）を作ります。<code>ほげほげ</code>が名称です。今回私は<code>dotfiles</code>にしました。</li>
<li><code>homeshick track ほげほげ .zshrc</code>で管理したいファイルを指定します。すると、<code>homeshick</code>が対象のファイル（ここでは<code>.zshrc</code>）を<code>castle</code>の保存先ディレクトリにコピーしてから、シンボリックリンクをホームディレクトリ上に作ってくれます。</li>
<li><code>homeshick cd ほげほげ</code>で、<code>castle</code>の実際のディレクトリに移動します。実態は<code>.homeshick/repos/ほげほげ</code>です。</li>
<li><code>git remote</code>コマンドで<code>castle</code>とGitHubの関連をつけて、あとは、普通にgitコマンドでコミットしたりすればOKです。</li>
</ol>
<p>すごく簡単に導入できました。あとは、実際にtrackしたいファイルを追加していく感じです。
現状は、<code>.zshrc</code>、<code>.gitconfig</code>あたりを管理しています。
ついでに、homebrewでインストールしたものも<code>brew bundle dump</code>で出力して、homeshickで管理することで、brewでインストールしたものの管理もできそう（<a href="https://github.com/Homebrew/homebrew-bundle#dump">参考</a>）。</p>
<p>ってことで、至極かんたんでした。すばらしい。</p>
<h3 id="参考">参考</h3>
<ul>
<li><a href="https://github.com/andsens/homeshick">homeshick</a>
<ul>
<li><a href="https://github.com/andsens/homeshick/wiki/Tutorials">Tutorials</a></li>
</ul>
</li>
<li><a href="https://qiita.com/yu_suke1994/items/830c7ab8a6bf75044001">Qiita : homeshickを使ったdotfilesの管理</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>zshへの移行</title>
      <link>https://blog.johtani.info/blog/2019/12/17/move-to-zsh/</link>
      <pubDate>Tue, 17 Dec 2019 09:35:03 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/17/move-to-zsh/</guid>
      <description>Macbook Proが新しくなり、OSがCatalinaになっちゃいました。 これまで使ってた、.bash_profileをコピーしてターミナルを立ち上げ</description>
      <content:encoded><p>Macbook Proが新しくなり、OSがCatalinaになっちゃいました。
これまで使ってた、<code>.bash_profile</code>をコピーしてターミナルを立ち上げたところ、
なんか色々と設定が動いてないな？と思ったら、<code>zsh</code>がデフォルトに切り替わってました。</p>
<!-- more -->
<p>ということで、デフォルト変わったし、時間あるしzshってどんな感じなのか調べてみようと。</p>
<h2 id="準備前ツイート">準備前（ツイート）</h2>
<p>で、ツイートをしたところ、海外の方？より返信が来ました。</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I have a series of posts explaining the transition: <a href="https://t.co/dz8QNjxxGf">https://t.co/dz8QNjxxGf</a></p>&mdash; Scripting OS X (@scriptingosx) <a href="https://twitter.com/scriptingosx/status/1204279491331264513?ref_src=twsrc%5Etfw">December 10, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>おもしろそうなので、ざっくりと読んでみました。zshを利用されているTwitterの知り合いの何人かからは<code>zsh</code> + <code>oh-my-zsh</code>がいいよとコメント頂いてたんですが、</p>
<h2 id="moving-to-zsh">Moving to zsh</h2>
<p>Catalinaからzshがデフォルトになるからと、手元の環境を変更するための話をブログにまとめてくれてます。</p>
<p><a href="https://scriptingosx.com/2019/06/moving-to-zsh/">Moving to zsh</a></p>
<p>サラッと読んだので、いくつか抜粋すると</p>
<ol>
<li>bashでいくつか今設定しているものがあるのでその説明
<ol>
<li>alias - ファイルをそれぞれの拡張子ごとにアプリを切り替えてる（<a href="https://scriptingosx.com/2017/02/the-macos-open-command/"><code>open</code>コマンドの利用例があって参考になりました</a>）。</li>
<li>functions - <code>man</code>コマンドのときに新しいWindowひらいたりとか</li>
<li>shell settings - 大文字小文字関係なくファイル名をTabで候補を表示したり、historyをターミナル個別ではなく共有できるような設定にしたり</li>
<li>prompt - 現在のディレクトリがどこかとか表示したり</li>
</ol>
</li>
<li>zshの設定ファイル群の説明</li>
<li>shellオプションの抜粋</li>
</ol>
<p>といった感じでした（途中で読むのやめましたが。。。）</p>
<p>面白かったのは、<a href="https://scriptingosx.com/2019/07/moving-to-zsh-part-7-miscellanea/">Suffix Aliasesです</a>。
ファイル名だけをターミナルで指定すると、拡張子に応じて起動するコマンドを切り替えたりできると（結局まだ採用はしてないですがｗ）</p>
<h2 id="結局">結局？</h2>
<p>結局、上記ブログを読んでいろんなオプションあるんだなぁ。と思ったのですが、手っ取り早く試すために<a href="https://github.com/sorin-ionescu/prezto">Prezto</a>をインストールしました。
以下のような構成になってます。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">なるほど、これがzsh + prezto + powerline fontか。前までgit-promptを拾ってきてbashに組み込んでたけど、ブランチ表示されるし便利だな。 <a href="https://t.co/Z2ArBgRZey">pic.twitter.com/Z2ArBgRZey</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1205038841133854720?ref_src=twsrc%5Etfw">December 12, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<ul>
<li>Mac標準ターミナル</li>
<li><a href="http://cocopon.github.io/iceberg.vim/">Iceberg</a> プロファイル</li>
<li><a href="https://github.com/powerline/fonts">Powerline font</a></li>
<li><a href="https://github.com/sorin-ionescu/prezto">Prezto</a></li>
<li>agnoster theme - <code>prompt agnoster</code> or ~/.zpreztorc の変更</li>
<li>.zshrcにいくつかaliasなどを追加。<a href="https://gist.github.com/johtani/e669a78af70fb34728d4d3be72724526">参考:gist</a>
<ul>
<li>Preztoのデフォルトでlessなどからエディタを立ち上げるとnanoが起動したので、vimに変更</li>
<li>Preztoが作る設定と自分の設定を混ぜたくなかったので、.zshrcでinit.zshを呼び出し</li>
</ul>
</li>
</ul>
<p>になりました。フォントサイズだけはちょっと大きくしました（老眼ェ。。。）
ターミナルの参考にさせていただいたブログは「<a href="https://qiita.com/Angelan1720/items/60431c85592fe90fcdd5">preztoでzsh構築した時のメモ</a> 」です。</p>
<h2 id="ターミナルソフト余談">ターミナルソフト(余談？)</h2>
<p>terminalも人によっては、色んなものを使ってたなと思いツイートしました。
結構みなさん標準のterminalを使ってるみたいでした。ただ、私が聞いたこともないターミナルソフトもちらほらあったので、色々あるんだなぁと。</p>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Mac標準のターミナル使ってる人ってどのくらいいるんだろ？iTermとか入れてるのかな？tmuxとか？</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1204686169260187648?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</content:encoded>
    </item>
    
    <item>
      <title>Octopress環境のDockerイメージ化</title>
      <link>https://blog.johtani.info/blog/2019/12/16/dockernize-octopress/</link>
      <pubDate>Mon, 16 Dec 2019 10:01:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/16/dockernize-octopress/</guid>
      <description>最近使っていたPCが手元からなくなったので、16インチ Macbook Proをセットアップしているところです。 で、時間もあるのでこれまでほったらかしてきた</description>
      <content:encoded><p>最近使っていたPCが手元からなくなったので、16インチ Macbook Proをセットアップしているところです。
で、時間もあるのでこれまでほったらかしてきた、このブログの環境をちょっと変更しようかなと。</p>
<!-- more -->
<p>まずは、変更するにしても、Octopressのブログの環境自体は必要です。
実際に、移行する前のこの記事自体も書いているわけですし。</p>
<p>ただ、今後なくなる環境をローカル環境にセットアップするのもどうかと思い、Docker環境にしてみました。</p>
<h3 id="参考">参考</h3>
<p>&ldquo;Octopress Docker image&quot;でググって出てきたサイトを参考にしました。</p>
<p>参考ブログ：<a href="http://pappanyn.me/blog/2017/04/12/octopress-in-a-docker-container/">Octopress in a Docker Container</a></p>
<h3 id="セットアップからdockerイメージのビルドまで">セットアップからDockerイメージのビルドまで</h3>
<ol>
<li>Docker for Mac自体もインストールしていなかったので、インストールしておきます。</li>
<li>参考ブログの方がGitHubのリポジトリにDockerfileをアップしてくれているので、手順に従い、cloneします。
<ol>
<li>cloneしたDockerfileの3行目に<code>ENV LC_ALL C.UTF-8</code>を追加します（UTF-8でブログを書いており、previewした場合にエラーが出たため）</li>
<li>参考ブログにある「Build the docker image」の手順に従い、Gemfile、.gitconfigをコピーしてイメージをビルドします。</li>
</ol>
</li>
<li>参考ブログの「Rakefile」にあるように、自分の<code>octopress/Rakefile</code>の<code>Process.spawn(...)</code>にアドレスを追加します（Dockerコンテナの外からアクセスできるように）</li>
<li>自分のoctopressにある<code>Gemfile.lock</code>を削除しました（ビルドしたイメージにはいるGemと一部バージョンが異なる記載のものがあったため）</li>
</ol>
<p>ここまでで、</p>
<h3 id="docker-run">Docker run</h3>
<p>実際にコンテナを実行するためのスクリプトを書きました（書いたと言っても参考ブログにある起動コマンドを叩いてるだけですが。。。）</p>
<p><code>launch-octopress-docker.sh</code>というファイル名で以下のコマンドを実行してるだけです。</p>
<pre><code>!/bin/sh
docker run -p 4000:4000 --rm --volume /Users/johtani/projects/blog/octopress:/octopress --volume /Users/johtani/.ssh:/home/blogger/.ssh -ti blog/octopress /bin/bash
</code></pre><p>このシェルを起動すると、Dockerコンテナにbashで接続し、<code>/octopress</code>ディレクトリにログインしています。
あとは、いつものように<code>rake new_post[&quot;....&quot;]</code>で新規記事のテンプレートを作成したりすればOKです。</p>
<p><code>rake generate; rake preview</code>と実行してからローカルのブラウザで<code>http://localhost:4000</code>に接続すればプレビューも可能。</p>
<p>ということで、このDockerファイルとかを持っておけば、他の環境でもかんたんにOctopressの環境が再現できそうです。
先人の知恵有り難し。
これで、クリーンなまま他のブログ環境に移行できそう。</p>
<h2 id="追記20191217">追記(2019/12/17)</h2>
<p>Docker環境でいくつか問題があったので、追記しておきます。</p>
<h3 id="docker-for-macが再起動しない">Docker for Macが再起動しない</h3>
<p>Docker for MacがCatalina環境だと問題があるみたいでした。
CPUの数やメモリの数を変更してDockerを再起動したところ、ずっとStartingのまま。</p>
<p>解決方法としては、<a href="https://github.com/docker/for-mac/issues/3941#issuecomment-539101508">launchctlの設定に$PATHを追加するみたい</a>。これで、問題なく起動するようになった気がする</p>
<h3 id="sshの設定ファイルの問題">sshの設定ファイルの問題</h3>
<p>.ssh/configにMacのKeyChainを利用する設定を記載してるんですが、Ubuntu上だとこの設定のせいで、エラーが出てしまいます。
<a href="https://gotohayato.com/content/466/">IgnoreUnknownという設定で、知らないオプションがあったら無視するという設定になる模様</a>。ということで、Dockerコンテナ上で<code>rake deploy</code>のときにエラーが出てたのが解消できました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>退職します</title>
      <link>https://blog.johtani.info/blog/2019/12/06/leaving-elastic/</link>
      <pubDate>Fri, 06 Dec 2019 15:51:30 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/06/leaving-elastic/</guid>
      <description>本日、Elasticでの最終出社日でした。実際の退職日はまだ先ですが、有休消化という感じです。 思い返せばもう5年と5ヶ月も前ですが、Elas</description>
      <content:encoded><p>本日、Elasticでの最終出社日でした。実際の退職日はまだ先ですが、有休消化という感じです。</p>
<!-- more -->
<p>思い返せばもう5年と5ヶ月も前ですが、<a href="https://blog.johtani.info/blog/2014/07/01/join-elasticsearch/">Elastic（当時はElasticsearchって社名だった）に参加しました</a>。</p>
<p>社名も変わりましたし、プロダクトの数も増えたし、IPOもしました。
初の外資＋スタートアップということもあり、ものすごくエキサイティングな5年半でした。
Elasticを離れはしますが、検索自体にはまだまだ興味があるので、LuceneやElasticsearchになにかしら関わる感じのことをする予定です。
<a href="https://search-tech.connpass.com">検索技術勉強会</a>も今後もやっていきますし。</p>
<p>ただ、激動の5年半だったので、退職日までちょっとゆっくりしようと思っています。
次にどんなことをするのかはまた、別の機会にでも。</p>
<h2 id="おまけ">おまけ</h2>
<p>ということで、まぁお約束です。</p>
<p><a href="https://www.amazon.jp/hz/wishlist/ls/29EMX20UN9P16?ref_=wl_share">ほしい物リストはこちら</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>Apache LuceneのKuromojiのUniDicビルド対応パッチについて</title>
      <link>https://blog.johtani.info/blog/2019/12/04/about-lucene-4056/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/04/about-lucene-4056/</guid>
      <description>これは、情報検索・検索エンジン Advent Calendar 2019 の 4 日目の記事です。 1日目から、質の高いエントリーが続いていましたが、一旦休憩して頂く感じの記事になって</description>
      <content:encoded><p>これは、<a href="https://qiita.com/advent-calendar/2019/search">情報検索・検索エンジン Advent Calendar 2019</a> の 4 日目の記事です。</p>
<!-- more -->
<p>1日目から、質の高いエントリーが続いていましたが、一旦休憩して頂く感じの記事になってます。気軽に読んでくださいw。Advent Calendarつくらないの？と煽ったのもあり、穴を埋めようかなと。　</p>
<h2 id="発端">発端</h2>
<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ちょっと先ですがこういうのやります。実装寄りの話やOSS開発に興味がある方，きてください～ / Lucene 版 <a href="https://twitter.com/hashtag/Kuromoji?src=hash&amp;ref_src=twsrc%5Etfw">#Kuromoji</a> のコードを読む会（辞書ビルダー編） <a href="https://t.co/NgEmUohoPo">https://t.co/NgEmUohoPo</a> <a href="https://twitter.com/hashtag/kuromoji?src=hash&amp;ref_src=twsrc%5Etfw">#kuromoji</a></p>&mdash; Tomoko Uchida (@moco_beta) <a href="https://twitter.com/moco_beta/status/1169795202376073217?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><a href="https://search-tech.connpass.com/event/146365/">「Lucene 版 #Kuromoji のコードを読む会（辞書ビルダー編）」</a>という勉強会があり、参加したところ、UniDicの辞書のビルドがコケるという話を聞いたんで、ちょっとやってみるかと。</p>
<p>ちなみに、Kuromojiとは、Apache Luceneに入っている、日本語向けの形態素解析ライブラリです。IPAdicの辞書を内包しており、SolrやElasticsearchといった、Apache Luceneを利用している検索エンジンで手軽に使える形態素解析ライブラリになっています。が、対応している辞書がデフォルトだとIPAdicなのです。</p>
<h2 id="問題点">問題点</h2>
<p><a href="https://issues.apache.org/jira/browse/LUCENE-4056">LUCENE-4056</a>というIssueが上がっています。</p>
<p><code>build.xml</code>には記載はないけど、<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/DictionaryBuilder.java#L44">辞書のビルダーは対応していそうな雰囲気</a>を醸し出しているので、試してみたというのが発端？かと。で、実際に動かしてみると動かない点がありましたと。</p>
<p>また、Issueの会話で出ていたUniDicの辞書のライセンスの話もありました。
ただ、<a href="https://unidic.ninjal.ac.jp">UniDic</a>がライセンスを変更したので、このあたりはクリアできそうかなと。</p>
<h2 id="パッチ">パッチ</h2>
<p>ということで、動かしてみていくつか修正してパッチを作りました。</p>
<p><a href="https://github.com/apache/lucene-solr/pull/935">https://github.com/apache/lucene-solr/pull/935</a></p>
<p>最近のLuceneはGitHubでプルリク遅れるのが便利ですね。
そんなに大したことはやってないです。以下の点が問題だったので直しています。</p>
<ul>
<li>IPAdicとUniDicで語彙定義ファイルのCSVの形式（カラムの数）が異なる</li>
<li>unk.defのカラム数も異なる</li>
</ul>
<p>あとは、辞書のダウンロードの部分や<code>build.xml</code>での処理を追加した形です。
このプルリクを適用したlucene-solrのソースディレクトリを持ってきて、手元でjarをビルドすれば普通はIPAdicの辞書を内包したkuromojiのjarファイルが出来上がります。</p>
<p><code>lucene/analysis/kuromoji/build.xml</code>ファイルを、<a href="https://gist.github.com/johtani/91cfd2753aba2e001c1d39f47666ada7">このGist</a>にあるように変更して、<code>ant build-dict</code>とやれば辞書のビルドが可能です。
また、<code>cd lucene/;ant jar</code>とすれば、UniDicの辞書を内包したjarファイルもビルドできます(<code>lucene/build/analysis/kuromoji</code>の下にjarファイルができあがります)。</p>
<h2 id="確認">確認？</h2>
<p>一応、パッチは動くのですが、パッチ自体はUniDicの辞書をビルドする仕組みはオフのままです。なので、テストをどうやろう？というところでやなんで止まっています。。。</p>
<p>ただ、実際に作ったパッチできちんとIPAdicとUniDicがそれぞれビルドできているかの確認はしないとなと。</p>
<p>ということで、2つのjarファイルを読み込んで、それぞれトークナイズして、その出力を表示する<a href="https://github.com/johtani/check-dictionary">ツールを作ってみました</a>。</p>
<p>上記パッチを適用したlucene-solrのソースを持ってきて、IPAdicの辞書を内包したkuromojiのjarファイルと、UniDicの辞書を内包したjarファイルを用意し、ツールの支持に従って、ファイルをディレクトリに配置して、実行すれば以下のような出力がされるようになっています（とりあえず作ったものなので、Javaファイルにトークナイズしたいテキストを書かないといけないのですが）。</p>
<p>たとえば、「自転車と自動車の違いはなんでしょう？」という文字列を入力すると、以下のような出力になりました。</p>
<pre><code>+++ ipadic ++++++++++++++
token[0] is [自転車]
token[1] is [と]
token[2] is [自動車]
token[3] is [の]
token[4] is [違い]
token[5] is [は]
token[6] is [なん]
token[7] is [でしょ]
token[8] is [う]
+++ unidic ++++++++++++++
token[0] is [自転]
token[1] is [車]
token[2] is [と]
token[3] is [自動]
token[4] is [車]
token[5] is [の]
token[6] is [違い]
token[7] is [は]
token[8] is [なん]
token[9] is [でしょう]
</code></pre><p>UniDicは[短単位]で語彙が扱われるため、「自転車」や「自動車」がそれぞれ「自転」「車」、「自動」「車」という形でトークナイズされていることがわかります。</p>
<p>どちらがより便利なのか？というのは用途によっても変わってくるかと思いますが、検索の転置インデックスとしては、より短い単語で区切られている方が、より多くの文書にヒットする可能性が高くなるので、便利な可能性が高いです。</p>
<h3 id="まとめ">まとめ</h3>
<p>ということで、パッチを作ってみたものの、まだ取り込まれていない状況です。
着地点をどうするかって話かなと思っています。興味があれば遊んでみていただければと。</p>
<p>将来的には、<a href="https://issues.apache.org/jira/browse/LUCENE-8816">辞書をjarから切り離して別のディレクトリやjarとして使えるようにしよう</a>というIssueも作られています。こちらがすすめば、UniDicだけでなく、その他の辞書を切り替えながら使えるようになる日が来るのではないでしょうか？</p>
</content:encoded>
    </item>
    
    <item>
      <title>2019年のElastic StackとElastic</title>
      <link>https://blog.johtani.info/blog/2019/12/01/whats-happen-at-elastic-in-2019/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/12/01/whats-happen-at-elastic-in-2019/</guid>
      <description>Elastic stack (Elasticsearch) Advent Calendar 2019の1日目の記事になります。 まだ、1ヶ月を残してますが、簡単に今年起こったことを振り返ってみようと思います。毎年恒例ですね、</description>
      <content:encoded><p><a href="https://qiita.com/advent-calendar/2018/elasticsearch">Elastic stack (Elasticsearch) Advent Calendar 2019</a>の1日目の記事になります。</p>
<p>まだ、1ヶ月を残してますが、簡単に今年起こったことを振り返ってみようと思います。毎年恒例ですね、ここ数年。</p>
<!-- more -->
<h3 id="elastic-stack-660リリース1月">Elastic Stack 6.6.0リリース(1月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-6-6-0-released">リリース記事はこちら</a></p>
<p>Elastic APMが6.6のリリースと同時にElastic CloudでAPM Serverが無料で利用できるようになったのが地味に便利でした。APMのデモをやるために、それまでは手元にAPM Serverの起動が必要だったので。。。</p>
<p>ユーザーの方たちにはIndex Lifecycle Management（インデックスライフサイクル管理：ILM）がリリースされたのが便利だったと思います。
まだ、ベータでしたが、インデックスの世代管理を格段に便利にしてくれるツールになり、現在では必須アイテムとなっています。
もう一つ、地味に便利なのは、KibanaからElasticsearchへの接続を複数指定できるようになった点かと思います。</p>
<h3 id="elatic-common-schemaのベータリリース2月">Elatic Common Schemaのベータリリース(2月)</h3>
<p><a href="https://www.elastic.co/jp/blog/introducing-the-elastic-common-schema">リリース記事はこちら</a></p>
<p>Elastic Stackではメトリック、APM、ログなど、様々なデータを一元的に可視化することができるという利点があります。ただ、一元的にデータを可視化、検索するためには異なるデータセットに統一されたフィールド名が欠かせません。そのための手段としてElasticが公開したのが<a href="https://github.com/elastic/ecs">Elastic Common Schema</a>です。
各種データの項目名、型などを共通化する仕様をGitHub上で公開しています。最近のBeatsのモジュールはこのElastic Common Schemaに則ってデータが定義されるようになってきています。これにより、ログからメトリックへ、APMからログデータへというシームレスな移動ができるようになっています。</p>
<h3 id="elastic-stack-670リリース3月">Elastic Stack 6.7.0リリース(3月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-6-7-0-released">リリース記事はこちら</a></p>
<p>Elastic MapsやUptimeといった、これまでの可視化とは異なる便利なアプリが増え始めました。Mapsでは地図の表現が格段にアップしたので、コレまで以上に地理情報と合わせた可視化が楽しくなりました。</p>
<p>もちろん、基本的に必要な技術が着実にGAされていくのもElastic Stackの素晴らしい点です。</p>
<ul>
<li>Index Lifecycle Management(ILM)がGA</li>
<li>Cross Cluster Replication(CCR)がGA</li>
<li>CanvasがGA</li>
<li>Logs &amp; Infra UIがGA</li>
</ul>
<h3 id="elastic-stack-700リリース4月">Elastic Stack 7.0.0リリース(4月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-7-0-0-released">リリース記事はこちら</a></p>
<p>メジャーバージョンのリリースです。
KibanaのUIが刷新されたり、Elasticsearchのクラスター管理の機能が新規に構築されたり、様々な改善がこのリリースでも入っています。また、メジャーバージョンのリリースのタイミングが、さまざまな大きな仕様の変更や改善が入るタイミングでもあります。これまで以上にパフォーマンスが改善（Top-Nクエリ高速化など）されたり、新しい機能の追加（ナノ秒のサポート）されたりしました。</p>
<h3 id="elasticsearchのセキュリティの主要な機能が無料に5月">Elasticsearchのセキュリティの主要な機能が無料に(5月)</h3>
<p><a href="https://www.elastic.co/jp/blog/security-for-elasticsearch-is-now-free">リリース記事はこちら</a></p>
<p>6.8.0および7.1.0のリリースはこの機能の無償提供となりました。
結構衝撃的な話だったのではないかなぁと。これ以前は有償の機能だったセキュリティの以下の機能をElastic License配下で無料で提供する形に変わりました。まだ、ご存知でない方は、データの安全のためにもセキュリティ機能を利用することをおすすめします。</p>
<ul>
<li>TLSによる通信暗号化</li>
<li>ユーザー作成と管理にファイルおよびネイティブのレルム認証を使用可能</li>
<li>クラスターAPIとインデックスに対するユーザーアクセスの管理にロールベースのアクセス制御を使用可能、またSpaces機能でKibanaのマルチテナンシーの安全性を向上</li>
</ul>
<h3 id="elasticon19開催5月">Elastic{ON}19開催(5月)</h3>
<p>今年も東京で開催しました。ビデオなどはこちらで公開されています。
<a href="https://www.elastic.co/elasticon/tour/2019/tokyo">https://www.elastic.co/elasticon/tour/2019/tokyo</a></p>
<p>今回も偉そうにElatic Stackの新しくなった点を紹介するなどしてました。。。</p>
<h3 id="elastic-stack-720リリース6月">Elastic Stack 7.2.0リリース(6月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-7-2-0-released">リリース記事はこちら</a></p>
<p><a href="https://www.elastic.co/jp/blog/introducing-elastic-siem">Elastic SIEMがベータリリース</a>されたのがこのタイミングです。
2月の発表したElastic Common Schemaをフルに活用していると言ってもいいのがこの機能になります。まだ今後もどんどん改善が入るであろうきのうになります。</p>
<p>また、Elasticsearchをバックエンドにした検索ミドルウェアとして利用いただける<a href="https://www.elastic.co/blog/elastic-app-search-7-2-0-released">Elastic App Searchのセルフマネージド版</a>もこのタイミングでリリースされています。</p>
<p>さらに、このリリースの直前には<a href="https://www.elastic.co/jp/blog/announcing-elastic-cloud-on-kubernetes-eck-0-9-0-alpha-2">Elastic Cloud on Kuberunetes(ECK)というものベータリリース</a>されました。少しわかりにくいかもですが、ElasticsearchやKibanaをKubernetesのOperatorとして利用できるようになっています。こちらもElastic Licenseでリリースされているのでk8s上でKibanaやEsを管理しようとしている方は触ってみると面白いかもです。</p>
<h3 id="elastic-cloud-elasticsearch-serviceがgcp日本で利用可能に7月">Elastic Cloud Elasticsearch ServiceがGCP日本で利用可能に(7月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elasticsearch-service-is-now-available-on-google-cloud-platform-gcp-in-japan">リリース記事はこちら</a></p>
<p>Elastic Cloudもプラットフォームが拡大した年でした。
Google Cloud Platformの東京リージョンを選択できるようになりました。さらなる統合（支払いをGCP経由にまとめたり、GCPのコンソールから利用できたりなど）も進んでいます。</p>
<h3 id="elastic-stack-730リリース8月">Elastic Stack 7.3.0リリース(8月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-7-3-0-released">リリース記事はこちら</a></p>
<p>データフレームと呼ばれるデータ取り込み時のピボット機能が導入されました。また、MapsのGAリリース、Elastic APMの.NETエージェント正式リリースなど、細かいですが様々なものがリリースされています。</p>
<h3 id="elastic-cloud-elasticsearch-serviceがazureで利用可能に9月">Elastic Cloud Elasticsearch ServiceがAzureで利用可能に(9月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elasticsearch-service-on-elastic-cloud-now-available-on-microsoft-azure">リリース記事はこちら</a></p>
<p>Microsoft Azureへのデプロイも可能になりました。残念ながらまだ日本リージョンには来ていないですが、今後出てくるはずです！さまざまなクラウドベンダーのサポートにより、より多くの人に使っていただけるようになるのかと。</p>
<h3 id="elastic-stack-740リリース10月">Elastic Stack 7.4.0リリース(10月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-7-4-0-released">リリース記事はこちら</a></p>
<p>もう7.4.2まで出ていますが、いい感じの間隔で7.0、7.2、7.3と来ていますね。
7.4では、スナップショットリストアがKibanaから簡単に行えるようになりました。これまではKibanaのConsoleでJSONを見ながら管理されていたかもですが、GUIにより今どんなスナップショットがあるのか、どれをリストアするのかといった操作が簡単にできるようになっています。</p>
<p>KibanaについてはPKI認証のサポートなども始まり、様々な認証方式でより便利にKibanaが使えるようになっています。</p>
<h3 id="12月">12月？</h3>
<p>12月ですし、<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/events/266551992/">Elasticsearch勉強会では「LT＆忘年会」ということで、懇親会がメインの勉強会として12/6に開催</a>します。悪路クエストの緑川さん、吉岡さんに主体となっていただき、マイクロソフトさんを会場に借りて開催予定です。興味のある方はぜひご参加ください。
LTもおまちしています！</p>
<h3 id="まとめ">まとめ</h3>
<p>駆け足でしたが今年を振り返ってみました。
今年も色々ありました。残すところあと1ヶ月です！</p>
<p>さて、<a href="https://qiita.com/advent-calendar/2019/elasticsearch">Elastic Stack Advent Calendar 2019</a>は今日から25日まで続きます。今年は<a href="https://qiita.com/advent-calendar/2019/elasticsearch2">その2</a>もできています！こらからの記事を楽しみにしています！
本日は<a href="https://qiita.com/advent-calendar/2019/elasticsearch2">その2</a>で[kaibadash@github]さんが「5分でできるElastic stack環境構築」というのを書いてくれてるはずです！</p>
<p>ということで、次は<a href="https://qiita.com/KunihikoKido">KunihikoKido</a>さんの「
Elastic Cloud を使うようになって設計方針やら変わったことについて書きます。」になります。お楽しみに！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Bonfire Data &amp; Science #1に参加しました</title>
      <link>https://blog.johtani.info/blog/2019/10/25/bonefire-01/</link>
      <pubDate>Fri, 25 Oct 2019 19:40:02 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/10/25/bonefire-01/</guid>
      <description>Bonfire Data &amp;amp; Science #1にブログ枠で参加してきました。 ということで、メモです。 日時 : 2019/10/25 19:00 - 21:30 場所 : Yahoo! Japan サイト : https://yj-meetup.connpass.com/event/148121/ ハッシュタグ: #yjbonfire 概要 Data &amp;amp; Scienceと</description>
      <content:encoded><p>Bonfire Data &amp; Science #1にブログ枠で参加してきました。
ということで、メモです。</p>
<!-- more -->
<ul>
<li>日時 : 2019/10/25 19:00 - 21:30</li>
<li>場所 : Yahoo! Japan</li>
<li>サイト : <a href="https://yj-meetup.connpass.com/event/148121/">https://yj-meetup.connpass.com/event/148121/</a></li>
<li>ハッシュタグ: <a href="https://twitter.com/search?q=%23yjbonfire%20&amp;src=typed_query&amp;f=live">#yjbonfire</a></li>
</ul>
<h2 id="概要">概要</h2>
<p>Data &amp; Scienceとは？
データとサイエンスに関わる人達の情報共有のための勉強会/交流会</p>
<p>サイトから引用です。</p>
<pre><code>第1回のテーマは「画像検索」です！

最近EC系のサイトで類似画像検索が出来るようになったけどどうやってるの？
画像検索のモデルってどうしてるの？
画像検索のインフラはどうしてるの？
私たちの会社でも画像検索を用いたサービスを構築できるだろうか？
こういった疑問に答えたり、いま抱えている悩みを解決するヒントを得る場になればと思っています。

今回は、画像検索を行なっているヤフー, メルカリ, ZOZOテクノロジーズの3社に事例と基盤技術について登壇いただきます。
</code></pre><p>QAはこちら。https://app.sli.do/event/w3wayjmv/live/questions</p>
<h2 id="mercari画像検索について仮">Mercari画像検索について(仮)</h2>
<ul>
<li>
<p>発表者：荒瀬晃介(株式会社メルカリ / AIエンジニアリングチーム)</p>
</li>
<li>
<p>AIエンジニアチーム</p>
</li>
<li>
<p>写真検索プロジェクトのTech Lead</p>
</li>
<li>
<p>iOSのみで提供の写真検索</p>
</li>
<li>
<p>システムオーバービュー</p>
<ul>
<li>MobileNet v2で特徴抽出</li>
<li>ANNのインデックスを使ってDBに入れる</li>
</ul>
</li>
<li>
<p>論文も発表済み（3本）</p>
<ul>
<li>今日の発表はこの内の2本を元に話をします。</li>
</ul>
</li>
</ul>
<h4 id="c2cでの問題点">C2Cでの問題点</h4>
<ul>
<li>商品は床やテーブル上で撮影</li>
<li>クエリは着用（着ている）画像が使われやすい。
<ul>
<li>人が写ってる写真が結果に多いと業者が出展しているように見えてしまう。</li>
</ul>
</li>
</ul>
<h4 id="提案手法">提案手法</h4>
<ul>
<li>人が写ってるものから人の代表ベクトルを抜き取る (クエリ時にのみ処理を実施しているので変更が容易)</li>
<li>特徴変換ベクトル
<ul>
<li>トップスなど、分類ごとに学習させてからベクトルを構成</li>
</ul>
</li>
<li>なんでMobileNet v2?
<ul>
<li>エッジデバイスでの処理を見据えて選択</li>
</ul>
</li>
</ul>
<h4 id="インフラ">インフラ</h4>
<ul>
<li>
<p>Docker + k8s</p>
</li>
<li>
<p>CRDを使ってる？</p>
</li>
<li>
<p>training</p>
<ul>
<li>コンテナベースパイプライン
<ul>
<li>いくつかのバッチ処理を工程ごとにパイプライン化</li>
<li>バッチ実行情報をカスタムリソースとしている＝再実行が簡単（復旧作業が容易）</li>
</ul>
</li>
<li>画像を扱う＝ダウンロードが時間がかかる -&gt; 復旧しやすいようにPVにある程度キャッシュさせている</li>
</ul>
</li>
<li>
<p>Serving&hellip;</p>
<ul>
<li>GCP側</li>
</ul>
</li>
<li>
<p>なぜ2つに別れてるんだろう？実際のサービスも2つに分かれてたりするんだろうか？</p>
</li>
</ul>
<h4 id="将来の展望">将来の展望</h4>
<ul>
<li>Realtime Image Search
<ul>
<li>カメラでものを写している状態でそれが何かを検索できる。</li>
<li>物体検出＋特徴抽出をエッジで行うためできる</li>
</ul>
</li>
<li>エッジの性能により違いが出てきたりするっぽい</li>
</ul>
<h4 id="qa">QA</h4>
<ul>
<li>Q: 業者が想定されると、購入意欲を下げるというのは実験した結果？それとも想像？
<ul>
<li>A: 実験はしていないが、e-commerceにおいての研究がある</li>
</ul>
</li>
<li>Q: どういう理由でマルチクラウドにしたんでしょう?
<ul>
<li>A: 画像のマスターがAWS。メルカリのマイクロサービスはGKEなので、サービング環境がGCP</li>
</ul>
</li>
<li>Q: 画像の内、服のエリアが大部分で体の面積が少ない場合と、メガネや帽子のように、アイテムの方の面積が少ない場合で、トレーニングに必要なデータ数は変わりましたか？
<ul>
<li>A: カテゴリによる性質の違うはある。ので、改善は必要。</li>
</ul>
</li>
</ul>
<h2 id="zozo画像検索について仮">ZOZO画像検索について(仮)</h2>
<ul>
<li>
<p>発表者：平田拓也(株式会社ZOZO / AIエンジニアリングチーム)</p>
</li>
<li>
<p>(聞き逃した。。。)</p>
</li>
<li>
<p>WEAR</p>
</li>
<li>
<p>マルチサイズ</p>
</li>
</ul>
<h4 id="チーム構成">チーム構成</h4>
<ul>
<li>研究所＋ML Opsチーム</li>
</ul>
<h4 id="使用しているアルゴリズム">使用しているアルゴリズム</h4>
<ul>
<li>物体検出アルゴリズム</li>
<li>特徴量抽出アルゴリズム</li>
<li>近似最近傍探索（approximate nearest neighbor, ANN）</li>
</ul>
<h4 id="インフラ-1">インフラ</h4>
<ul>
<li>GCPを採用。
<ul>
<li>BigQuery上にデータ基盤がある</li>
<li>Managed GPUが必要</li>
</ul>
</li>
<li>なんでk8s?
<ul>
<li>コンテナ</li>
<li>Cloud Runがなかった</li>
</ul>
</li>
</ul>
<h4 id="アーキテクチャ">アーキテクチャ</h4>
<ul>
<li>マイクロサービス化されている</li>
<li><a href="https://techblog.zozo.com/entry/cloudnext19tokyo-imagesearch">Google Cloud Next 2019 Tokyoでsonotsさんの発表があるよ</a></li>
</ul>
<h4 id="監視項目">監視項目</h4>
<ul>
<li>CPUなどは見ていない</li>
<li>レスポンスタイムとステータス監視</li>
<li>リクエスト数</li>
<li>APM</li>
<li>使ってるものStackdriver + Datadog + Sentry
<ul>
<li>Warningが30分で続けたら通知などができるのがStackdriver</li>
</ul>
</li>
</ul>
<h4 id="画像検索の改善のためにやっていること">画像検索の改善のためにやっていること</h4>
<ul>
<li>課題
<ul>
<li>レイテンシーが大きい
<ul>
<li>推論が大変？</li>
</ul>
</li>
<li>急激なトラフィックの増加に対応できない
<ul>
<li>GPUのスケールアウトが問題 -&gt; 先行投資が必要</li>
</ul>
</li>
</ul>
</li>
<li>流れ
<ol>
<li>キャッシュありなし</li>
<li>物体検出 (GPU)</li>
<li>特徴量抽出 (GPU)</li>
<li>近似最近傍探索</li>
<li>DBから取得</li>
</ol>
</li>
</ul>
<p>2と3が問題</p>
<ul>
<li>2と3を特徴量DBという形でデータが登録された時点で特徴量などを計算してしまうことでGPUへの依存をなくした。</li>
<li>Apache AirFlowが便利？</li>
</ul>
<h4 id="cloud-composerとapache-airflow">Cloud ComposerとApache AirFlow</h4>
<p>Cloud Composerに関するいくつかのTipsがありました。</p>
<h4 id="qa-1">QA</h4>
<ul>
<li>Q: 推論をCPUでやったらどれぐらい遅いんだろう
<ul>
<li>A: GPUインスタンス代金 &lt; それと同等の速度を出すためのCPUインスタンス代金</li>
</ul>
</li>
<li>Q: k8sスケールアウト時のリソース割当を最適化する為、resource limit / request標準化 or 時系列分析などから自動調整するなどはされていますか？
<ul>
<li>A: &hellip;聞き逃した</li>
</ul>
</li>
<li>Q: (TCOを考慮したクラスタ構成) Cloud Composerの値段が高いようなパプリッククラウド利用の課題対策としてプライベートクラウドとのハイブリッド構成にされているのでしょうか？もしハイブリッド構成でしたら、パブリック or クラウドを切り分ける基準はございますか。
<ul>
<li>A: GKEオンリー</li>
</ul>
</li>
<li>Q: cloud composerでairflowを使う辛い点は？
<ul>
<li>A: 不安定さ。。。</li>
</ul>
</li>
</ul>
<h2 id="yahooショッピングにおける画像検索仮">Yahoo!ショッピングにおける画像検索(仮)</h2>
<ul>
<li>発表者：佐藤 純一(ヤフー株式会社)</li>
<li>商品検索APIの開発とか検索エンジンの保守とか</li>
<li>類似画像検索システムの開発が直近の仕事</li>
</ul>
<h4 id="類似画像検索">類似画像検索</h4>
<ul>
<li>ヤフーショッピング
<ul>
<li>3億の商品</li>
<li>ファッション系はビジュアルが重要＝言語による表現が難しい</li>
</ul>
</li>
<li>iOSとAndroid
<ul>
<li>Androidだとカメラで撮影してから検索みたいなことも可能</li>
</ul>
</li>
</ul>
<h4 id="システム概要">システム概要</h4>
<ol>
<li>物体検出</li>
</ol>
<ul>
<li>ノイズ除去</li>
</ul>
<ol start="2">
<li>特徴抽出</li>
</ol>
<ul>
<li></li>
</ul>
<ol start="3">
<li>インデックス(NGT) -&gt; <a href="https://github.com/yahoojapan/NGT">https://github.com/yahoojapan/NGT</a></li>
</ol>
<ul>
<li>1000万件を超える</li>
</ul>
<ol start="4">
<li>検索</li>
</ol>
<ul>
<li>アプリの画像をAPIに投げてベクトルから、近いものn件を取得</li>
</ul>
<h4 id="ベクトル化インデックス更新">ベクトル化/インデックス更新</h4>
<ul>
<li>GPUマシン</li>
<li>Kafka使ってる</li>
<li>クラウドストレージに日次？バックアップみたいに保存</li>
<li>差分更新の仕組みがある</li>
<li>メンズとレディースは分けている
<ul>
<li>絞り込み検索のために分離（インデックスのメタデータとしてタグがある）</li>
</ul>
</li>
</ul>
<h4 id="システム構成">システム構成</h4>
<ul>
<li>Python</li>
<li>Kafka</li>
<li>TensolflowServing</li>
<li>可視化？監視？はGrafana＋Prometheus</li>
</ul>
<h4 id="開発を通しての学び">開発を通しての学び</h4>
<ul>
<li>自動デプロイとかテスト</li>
<li>検索精度の確認ツールを作る
<ul>
<li>コレ重要だよね。何が変更してるかとか、何が正しいかってのが必要だし。。。</li>
</ul>
</li>
<li>復旧可能、早期復旧の仕組みを容易</li>
</ul>
<h4 id="今後の方針">今後の方針</h4>
<ul>
<li>対象商品の拡大</li>
<li>物体検出特徴抽出モデルの性能改善</li>
<li>NGTの検索システムをValdに移行
<ul>
<li><a href="https://github.com/vdaas/vald">Vald</a>
<ul>
<li>k8s上で動作、分散検索、分散インデキシングなどの機能を提供予定</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="qa-2">QA</h4>
<ul>
<li>Q: iOS(既存の画像)とAndroid(カメラで撮る)で画像検索の方式が違うのはなぜ？
<ul>
<li>A: アプリの違い</li>
</ul>
</li>
<li>Q: 1枚の写真に沢山写っている中で、対象の商品をどう識別しているんでしょう。靴もトップスもボトムスも写っていたら、かなり難しそう
<ul>
<li>A: Yahooブラウザの場合は1番大きな領域のものを選択。Yahoo shoppingだと選択可能</li>
</ul>
</li>
</ul>
<h2 id="ngtについて仮">NGTについて(仮)</h2>
<ul>
<li>発表者：岩崎 雅二郎(ヤフー株式会社)</li>
<li>類似画像検索を20年くらいやってる</li>
</ul>
<h4 id="近傍検索ライブラリ">近傍検索ライブラリ</h4>
<ul>
<li>
<p><a href="https://github.com/yahoojapan/NGT">https://github.com/yahoojapan/NGT</a></p>
<ul>
<li>高次元ベクトルの近傍検索</li>
<li>ツリーとグラフによるインデックス</li>
</ul>
</li>
<li>
<p>近傍検索とは?</p>
<ul>
<li>距離空間上でのクエリの近傍のオブジェクトを取得
<ul>
<li>k最近傍検索（通常はこっち）</li>
<li>範囲検索（あんまり使われない）</li>
</ul>
</li>
</ul>
</li>
<li>
<p>NGTの特徴</p>
<ul>
<li>世界トップレベルの高速高精度な近似近傍検索</li>
<li>OSS</li>
<li>追加削除が可能（削除がとくに難しいらしい）</li>
<li>多様な利用形態（Python、C++、C、Go、コマンドライン）</li>
<li>サーバ版NGT（ngtd、vald）を提供</li>
<li>共有メモリ版でメモリサイズ以上のデータ登録可能</li>
<li>量子化版NGT（NGTQ）により。。。</li>
</ul>
</li>
<li>
<p>ANNベンチマークによりテスト</p>
<ul>
<li>実行環境が決められているらしい。誰が実行しても比較可能</li>
<li>グラフベースの検索の仕組みのほうが性能がいいというのがベンチマーク結果からわかる</li>
</ul>
</li>
</ul>
<h4 id="なんではやいの">なんではやいの？</h4>
<ul>
<li>インデックス生成
<ul>
<li>ツリー（グラフの探索起点の取得に利用する。DVP-tree）</li>
<li>グラフ（ANNG）
<ul>
<li>ノードを逐次追加しつつ、近傍ノードを検索してから接続するというのを繰り返している</li>
</ul>
</li>
</ul>
</li>
<li>検索
<ul>
<li>ツリーから絞り込みつつ、グラフを検索する</li>
</ul>
</li>
<li>ANNGに課題があるのでONNG（Optimized Nearest Neighbors Graph）に
<ul>
<li>ノード単位の次数（入出）を最適化</li>
<li>データセットによって有効な次数が違う。。。</li>
</ul>
</li>
</ul>
<h4 id="ngtを利用した深層学習で">NGTを利用した深層学習で。。。</h4>
<ul>
<li>Yahoo!ラボ FavNavi</li>
<li>特徴量の構成（低次特徴量（300次元）、カテゴリ特徴量（128次元）、領域アスペクト比（1次元））
<ul>
<li>個別の特徴量だけだとイマイチな結果になるが、組み合わせるといい感じになる</li>
</ul>
</li>
</ul>
<h4 id="モデル性西洋学習データ">モデル性西洋学習データ</h4>
<p>スライドがあればいいなぁ（表書くの大変だし。。。）</p>
<h4 id="qa-3">QA</h4>
<ul>
<li>Q: 実際のお客さまの利用を考えると、近似近隣の密度が異なるので、近いものばかりの検索結果や遠いものも含んでしまった検索結果が出ると思うのですが、遠いものが含まれてしまうと、閾値でフィルタしたりするのでしょうか?
<ul>
<li>A: NGTは近いものしかないので、外れ値ってなんでしょう。。。
<ul>
<li>検索のフィルタリングをある程度している（カテゴリとか）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>慣れない分野の話を聞きながらざっとメモを取ったものなので役に立つかはわかりませんが。。。</p>
<p>アルゴリズムとかまでは得意ではないんですが、画像「検索」ということで参加してみました。
実際に画像検索の仕組みがどんな感じでできているのか、どんな技術がつかわれているのか？ってのがわかったのは
面白かったです。確かに検索のためのキーワードって出てこないことあるしなぁと。
あー、こんなかばんほしいとか、これなんだろ？みたいなのあるからなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch NEXT STEP 2を頂きました</title>
      <link>https://blog.johtani.info/blog/2019/10/10/review-es-next-step-2/</link>
      <pubDate>Thu, 10 Oct 2019 15:56:29 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/10/10/review-es-next-step-2/</guid>
      <description>この間のElasticsearch勉強会でAcroquest Technologyの人たちが技術書展7で販売されていた書籍を頂いてしまいました</description>
      <content:encoded><p>この間の<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/events/264954133/">Elasticsearch勉強会</a>でAcroquest Technologyの人たちが<a href="http://acro-engineer.hatenablog.com/entry/2019/09/19/120000">技術書展7で販売されていた書籍</a>を頂いてしまいました。なので、軽く読んでの感想と宣伝です（これ、電子版とかで買えないのかな？）。</p>
<!-- more -->
<p>Elasticのパートナーとしても活躍していただいてますが、こういう感じでさまざまなところでElastic Stackを広めていただいていて感謝しかありません。</p>
<p>章立てとしては以下の通りです。</p>
<ol>
<li>Elasticsearchでエンタープライズサーチを実現する</li>
</ol>
<ul>
<li>同僚が作成、メンテナンスしている<a href="https://fscrawler.readthedocs.io/en/latest/#">FSCrawler</a>を使った例も書かれています。ローカルのファイル（PDFやJSONとか）をサクッとElasticsearchにインストールしたりするのには便利です。</li>
</ul>
<ol start="2">
<li>Kibana Canvasによる柔軟な可視化</li>
</ol>
<ul>
<li>Canvasについてどんなものなのか？というのと<a href="https://github.com/elastic/examples/tree/master/canvas/elasticoffee">elasticcofee</a>というサンプルを元に、その変更の仕方などが説明されています。</li>
</ul>
<ol start="3">
<li>Elastic APMによるアプリケーションパフォーマンス監視</li>
</ol>
<ul>
<li>Go AgentとAPM自体の使い方の説明です。最近強化されている他の機能との連携（AlertingやMachine Learning）についても触れてくれています。</li>
</ul>
<ol start="4">
<li>Kubernetesクラスタのメトリクス・ログ・性能情報の可視化</li>
</ol>
<ul>
<li>Azure上でAzure Kubernetes Serviceの上で動いているアプリなどをBeats、APMでデータを取りつつ、他のAzure上に<a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/elastic.elasticsearch?tab=Overview">Azure Marketplaceのテンプレート</a>を用いて起動したElasticsearchとKibanaの環境を用いて可視化する方法が説明されています。</li>
</ul>
<p>ということで、薄い本ですが、手順をおって説明されていたり、7.3と新しいバージョンで書かれているのですばらしいなぁと。
前作の<a href="https://amzn.to/322j99v">「Elasticsearch NEXT STEP」</a>に引き続きインプレスさんから出たりするのかなぁ？</p>
<p>ちなみに、Elastic APMのRubyに関して興味がある方は、私が前に発表したときの資料がありますので、<a href="https://noti.st/johtani/eJPLbZ/elastic">こちらを参考に</a>してみてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>勉強会の受付自作？アプリ？HTMLについて</title>
      <link>https://blog.johtani.info/blog/2019/06/19/qr-code-with-meetup-dot-com/</link>
      <pubDate>Wed, 19 Jun 2019 19:10:44 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/06/19/qr-code-with-meetup-dot-com/</guid>
      <description>Twitterで「Meetup.comに切り替えたらー」みたいな話があったので、 受付用アプリとかなくてという話になったので、普段Elasti</description>
      <content:encoded><p>Twitterで「Meetup.comに切り替えたらー」みたいな話があったので、
受付用アプリとかなくてという話になったので、普段Elasticsearch勉強会で使用している
QRコード生成の仕組みを紹介してみようかと。</p>
<!-- more -->
<p>HTMLだけで済むようにQRコードを生成する<code>jquery.qrcode.min.js</code>ってのを使用してます。</p>
<p><a href="https://gist.github.com/johtani/657df4dcfa7c31a86575760e23b64f7d">HTMLコードはこちら</a>。</p>
<h2 id="仕組み">仕組み</h2>
<ul>
<li>1枚だけのHTMLを生成</li>
<li>Meetupの<a href="https://www.meetup.com/ja-JP/meetup_api/auth/#oauth2">OAuth2</a>の仕組みを利用(consumer keyを発行する必要あり。)</li>
<li>Meetup APIを利用してユーザー情報取得</li>
<li>取得したユーザー情報をGoogle Formに埋め込んだ形で表示するURLを組み立てる</li>
<li>組み立てたURLをQRコードにしてHTMLに表示</li>
<li>受付で、QRコードリーダーを使って、QRコードを読み込むとGoogle Formが開くので、「送信」ボタンを押す</li>
</ul>
<h3 id="1-htmlの作成">1. HTMLの作成</h3>
<p>QRコードを表示するためのHTMLを作成します。Gistに貼り付けてあるので、参考にしてもらえれば。
いくつか埋め込まないといけないものがあるので、個別にそれは説明します。</p>
<h3 id="2-google-formの準備">2. Google Formの準備</h3>
<p>Elasticsearch勉強会では、出席者の情報として、IDと氏名をリストとして保存しています。
目的としては、何人実際に参加したかを計測するのが目的です。
ですので、参加者リストのGoogle Formを作成します。
で、作成した後に、プレビューを表示する。こんな感じ。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20190619/google_form.png" />
    </div>
    <a href="/images/entries/20190619/google_form.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>この時のプレビューのURLをQRコードのURLとして使いたいので、このURLをQRコード表示のHTMLに埋め込みます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">function</span> <span style="color:#a6e22e">createGoogleFormURL</span>(<span style="color:#a6e22e">data</span>) {
  <span style="color:#66d9ef">var</span> <span style="color:#a6e22e">obj</span> <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;entry.1744035444&#34;</span> <span style="color:#f92672">:</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">id</span>,
    <span style="color:#e6db74">&#34;entry.2031666715&#34;</span> <span style="color:#f92672">:</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">name</span>
  };
  <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;https://docs.google.com/forms/d/e/&lt;GOOGLE_FORM_ID&gt;/viewform?&#34;</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">$</span>.<span style="color:#a6e22e">param</span>(<span style="color:#a6e22e">obj</span>);
}
</code></pre></div><p>ここら辺です。
<code>return</code>に書いてある<code>https</code>で始まる文字列をまず、先ほどのURLで置き換えます。
次に、プレビューのHTMLの中から<code>&lt;input&gt;</code>タグを探して、<code>name</code>の値を抜き出します。
それを<code>obj</code>のキーに利用します。<code>entry.</code>で始まる文字列が該当します。</p>
<p>これで、このURLをQRコードにすれば、値（ここだとIDと氏名）が埋め込まれた形のGoogle Formがスマホのブラウザで起動します。</p>
<h3 id="3-meetup-apiの準備">3. Meetup APIの準備</h3>
<p>MeetupのAPIを利用できるようにします。Meetup.comにログインするとみれるAPIのページがあります。
<a href="https://secure.meetup.com/meetup_api">https://secure.meetup.com/meetup_api</a></p>
<p>まず、OAuthを利用するためのConsumer Keyを発行します。</p>
<p>メニューのOAuth Consumersをクリックして、</p>


<div class="box" style="max-width:400">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20190619/meetup_oauth_consumers.png" />
    </div>
    <a href="/images/entries/20190619/meetup_oauth_consumers.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>&ldquo;Create New Consumer&quot;をクリックします。
すると、次のような画面が開きます。</p>


<div class="box" style="max-width:400">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20190619/meetup_oauth_create_consumer.png" />
    </div>
    <a href="/images/entries/20190619/meetup_oauth_create_consumer.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p><code>Consumer name</code>と<code>Redirect URI</code>が重要です。</p>
<p><code>Consumer name</code>はユーザーがMeetup経由で認証するときに、その認証画面で表示される名前になります。
わかりやすい名前を表示してあげると良いかと。</p>
<p><code>Redirect URI</code>が一番重要です。実際にOAuthで認証が通った後に表示するHTMLを提供しているURLを指定します。
Elasticsearch勉強会の場合は、私のドメインにwwwをつけた&quot;https://www.johtani.info&quot;を指定しています。
実際にQRコード表示用のHTMLを配置するHTTPサーバーのトップのURLを指定します。
（ちなみに、私のウェブサーバーはS3で運用してます。ですので、HTMLをS3のバケットにアップロードしてあるだけです）</p>
<p>必須項目を入力したあと、最下部にある<code>Register Consumer</code>ボタンを押せばキーが生成されます。
生成されたキーがOAuth2のURLのパラメータに必要になります。</p>
<p>これで、リダイレクトの準備が整いました。</p>
<p>OAuth2のURLには、<a href="https://www.meetup.com/ja-JP/meetup_api/auth/#oauth2implicit">Implicit Flow</a>を利用します。
これで、リダイレクト先のHTMLにトークンがわたるので、Meetup APIにこのトークンが使えるようになります。</p>
<p>認証用のURLはこちらです。このURLを参加者宛のメールに入れて毎回送信しています。</p>
<pre><code>https://secure.meetup.com/ja-JP/oauth2/authorize?response_type=token&amp;redirect_uri=&lt;QRコード表示HTMLのURL&gt;&amp;client_id=&lt;コンシューマーキー&gt;
</code></pre><p>参加者はこのリンクをクリックすることで、次のような画面が出てきます。
ログインしていない場合はLog in画面が表示され、まずはログインを促されます。</p>


<div class="box" style="max-width:400">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20190619/meetup_oauth_page.png" />
    </div>
    <a href="/images/entries/20190619/meetup_oauth_page.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>で、Allowをクリックすれば、<code>redirect_uri</code>に指定されているページが表示されるわけです。</p>
<p>QRコード表示用のHTMLでは、次のAPIを使用して、ログインしているユーザーの情報を取得してきています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">function</span> <span style="color:#a6e22e">getMemberId</span>() {
  <span style="color:#a6e22e">$</span>.<span style="color:#a6e22e">getJSON</span>(<span style="color:#e6db74">&#39;https://api.meetup.com/2/member/self/?only=id,name&amp;access_token=&#39;</span><span style="color:#f92672">+</span> <span style="color:#a6e22e">getToken</span>(), <span style="color:#a6e22e">displayQRCode</span>);
}
</code></pre></div><p>ここで取れた値が、先ほどの「2. Google Formの準備」で説明したコードの<code>data</code>の部分に渡ってくるわけです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">function</span> <span style="color:#a6e22e">createGoogleFormURL</span>(<span style="color:#a6e22e">data</span>) {
  <span style="color:#66d9ef">var</span> <span style="color:#a6e22e">obj</span> <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;entry.1744035444&#34;</span> <span style="color:#f92672">:</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">id</span>,
    <span style="color:#e6db74">&#34;entry.2031666715&#34;</span> <span style="color:#f92672">:</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">name</span>
  };
  <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;https://docs.google.com/forms/d/e/&lt;GOOGLE_FORM_ID&gt;/viewform?&#34;</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">$</span>.<span style="color:#a6e22e">param</span>(<span style="color:#a6e22e">obj</span>);
}
</code></pre></div><h3 id="4-qrコードの表示">4. QRコードの表示</h3>
<p>あとは、<a href="https://github.com/jeromeetienne/jquery-qrcode"><code>jquery.qrcode.min.js</code></a>を使用してURLを表示するだけです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">function</span> <span style="color:#a6e22e">displayQRCode</span>(<span style="color:#a6e22e">data</span>) {
  <span style="color:#a6e22e">$</span>(<span style="color:#e6db74">&#39;#qrcode&#39;</span>).<span style="color:#a6e22e">qrcode</span>({<span style="color:#a6e22e">width</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">128</span>,<span style="color:#a6e22e">height</span><span style="color:#f92672">:</span> <span style="color:#ae81ff">128</span>, <span style="color:#a6e22e">text</span><span style="color:#f92672">:</span><span style="color:#a6e22e">createGoogleFormURL</span>(<span style="color:#a6e22e">data</span>)});
}
</code></pre></div><p>サイズとURLを指定するだけですね。</p>
<h2 id="まとめ">まとめ</h2>
<p>あとは、受付で、参加者の方が表示してくれたQRコードをスマホのQRコードリーダーやカメラで読み込めばGoogle Formが開いて必要な情報は入っているので、「送信ボタン」を押せば参加者リストが出来上がっていくという形になります。</p>
<p>毎回勉強会の前日に、前回のGoogle Formを「コピー」してから、各回の勉強会の登録者フォームを作成しています。
コピーすることにより、Google Formの<code>&lt;input&gt;</code>タグに使用される<code>name</code>もそのままコピーされるので、
QRコード生成用のHTMLを書き換える部分の手間が減る形になっています（気づくまで数回かかったw）。</p>
<p>ですので、QRコードのHTMLの中身としては、「勉強会のページへのリンク」、「Google Formへのリンク」の2つを書き換えてから毎回アップロードしているだけとなっています。</p>
<p>このやり方が、スマートかどうかはわからないですが、受付アプリがない中、Meetup.comから取得した参加者リストのExcelやプリントアウトしたリストを元に参加者をチェックするよりは、手間が省けてるんじゃないかなぁと。</p>
<p>残念ながら、QRコードの存在を知らないでそのまま勉強会にくる人がいるので、受付で最低一人はMeetup.comの参加者リストから、
名前を検索してチェックするという作業もやってもらってます。
QRコードを持ってきた方がすんなり受付を通過できるようになってますので、ぜひQRコードを持って勉強会にきてもらえればと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>勉強会運営座談会ってのをやってみた</title>
      <link>https://blog.johtani.info/blog/2019/04/26/meetup-organizer-drinkup/</link>
      <pubDate>Fri, 26 Apr 2019 00:02:06 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/04/26/meetup-organizer-drinkup/</guid>
      <description>Elasticsearch勉強会や検索技術勉強会やってるんだけど、独学で勉強会やってるなーと思い、みんなはどうやってるんだろうとツイートして</description>
      <content:encoded><p><a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/">Elasticsearch勉強会</a>や<a href="https://search-tech.connpass.com">検索技術勉強会</a>やってるんだけど、独学で勉強会やってるなーと思い、みんなはどうやってるんだろうとツイートしてみたところ。</p>
<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">あー、勉強会運営座談会したい。</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1113015829195481088?ref_src=twsrc%5Etfw">2019年4月2日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="ja"><p lang="ja" dir="ltr">面白そう</p>&mdash; 機械の体を手に入れるのよ鉄郎 (@tetsuroito) <a href="https://twitter.com/tetsuroito/status/1113017239937081344?ref_src=twsrc%5Etfw">2019年4月2日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="ja"><p lang="ja" dir="ltr">やってみるか。</p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1113017372653215744?ref_src=twsrc%5Etfw">2019年4月2日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="ja"><p lang="ja" dir="ltr">やりましょう</p>&mdash; 機械の体を手に入れるのよ鉄郎 (@tetsuroito) <a href="https://twitter.com/tetsuroito/status/1113017409928015872?ref_src=twsrc%5Etfw">2019年4月2日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>ってことで、何人か釣れたので、<a href="https://connpass.com/event/126699/">勉強会運営座談会</a>をやってみました。
会場提供していただいた、<a href="https://classi.jp">Classiさん</a>ありがとうございました！</p>
<!-- more -->
<p>参加してくれた人たちはまぁ、勉強会のサイトを見ていただければなんとなくわかるかなと。
自己紹介とどんな勉強会、読書会運営してますってとこから始めて、次のようなトピックについてピザ食いながら3時間くらい話しました。</p>
<ul>
<li>スピーカーの募集方法
<ul>
<li>運営側から声かけてる？それとも公募してる？</li>
<li>登壇時間とかどうしてる？</li>
<li>スピーカーになってもらうためになんかやってる？</li>
</ul>
</li>
<li>スケジュール管理
<ul>
<li>会場探し、スピーカー探し、イベントページ作るタイミングは？</li>
<li>複数拠点開催したことある？</li>
<li>ハンズオンとかはどうしてる？</li>
</ul>
</li>
<li>土日開催？平日開催？</li>
<li>アンケートとってますか？</li>
<li>使用してるイベントサイトは？</li>
<li>ドタキャン対応どうしてる？出欠とったりしてる？懇親会の規模とかどうやって見積もってる？</li>
<li>運営費用関連はどうしてる？
<ul>
<li>グッズとか、ツールとか、スピーカーへの謝礼とか</li>
</ul>
</li>
<li>運営って複数でやってる？
<ul>
<li>その時のツールは？</li>
<li>運営メンバーにはどうやってなってもらう？</li>
<li>録画配信とかまで手が回る？</li>
</ul>
</li>
<li>会場探すの大変じゃない？
<ul>
<li>会場の部屋の使い方はどうしてる？</li>
</ul>
</li>
<li>行動規範とかどうしてる？</li>
<li>読書会の場合に人が減ってったりしない？</li>
</ul>
<p>まぁ、こんな感じです。色々話してメモしてたんですが、まぁトピックくらいで。</p>
<p>そもそもは、勉強会の運営って本業ではない（はずな）ので、いかに楽をしつつうまく運営できるかってのを知りたいのと、
他の人どうやってるかってのからアイデアもらえるといいなーと思ったんでやってみました。
他のツールがどんなものかとか知れたし、あー、そういうやり方すればいいのねーみたいなのも知れたので、次回以降の勉強会に活かせればなーと。
自分がやってるやり方とかもブログ書くといいのかなぁ？</p>
<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">残念です！次回もやるかもしれないので、 <a href="https://twitter.com/johtani?ref_src=twsrc%5Etfw">@johtani</a> さんに期待してください！</p>&mdash; 機械の体を手に入れるのよ鉄郎 (@tetsuroito) <a href="https://twitter.com/tetsuroito/status/1121413483331895296?ref_src=twsrc%5Etfw">2019年4月25日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>次回やるのかなぁ？興味ある人いるのかなぁ？</p>
</content:encoded>
    </item>
    
    <item>
      <title>Search Engineering Tech Talk(検索技術勉強会)の運営として参加して始めてみました。</title>
      <link>https://blog.johtani.info/blog/2019/02/26/start-search-engineering-tech-talk/</link>
      <pubDate>Tue, 26 Feb 2019 23:38:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2019/02/26/start-search-engineering-tech-talk/</guid>
      <description>どーも、johtaniです。 Search Engineering Tech Talkという勉強会に運営として参加して、第1回の勉強会を開始しました。 本日（2/26）は第1回目だったの</description>
      <content:encoded><p>どーも、johtaniです。</p>
<p><a href="https://search-tech.connpass.com/event/112866/">Search Engineering Tech Talk</a>という勉強会に運営として参加して、第1回の勉強会を開始しました。
本日（2/26）は第1回目だったので、ブログを残しておこうかと。</p>
<!-- more -->
<p>勉強会自体の資料については<a href="https://search-tech.connpass.com/event/112866/">第1回の勉強会のページ</a>にあるし、勉強会の感想とかブログはツイートやみんながブログを書いてくれると思うので、勉強会開催の経緯などについてブログを残しておこうかと。</p>
<h2 id="なんで始めたの">なんで始めたの？</h2>
<p>私自身が古くはFAST Searchに始まり、何か縁があって、
検索のシステムに長く携わってきたこと（Apache Solrの本書いたり、Elasticsearch勉強会始めたり）もあり、
検索が面白いなと日々思ってます（思ってるだけかもしれないが）。</p>
<p>で、これまでElasticsearch勉強会をやっているのですが、検索エンジン固有の話ではない、
いわゆる検索の共通の課題というのがあるなぁと。
そういう課題やノウハウって、製品に限らず共有できれば面白いことがもっとできるんじゃないだろうか？
と感じることが多々ありまして。
オープンソースのコミュニティをソースコードをベースではなく、共通の課題・話題を中心としたコミュニティが
あってもいいんじゃないかなぁと。</p>
<p>まぁ、要は、私がみんなの検索で困ってることとか、どうやって検索システム考えてるのかが聞きたかったわけですよ。</p>
<p>ということで、一人でやっても面白くないので、興味ありそうな人を募ってやってみようということを始めたのが2018年12月くらいです。</p>
<h2 id="運営とかどうしてるの">運営とかどうしてるの？</h2>
<p>まずは、共同主催者（コアメンバー）を募集してみようということで、Googleフォーム作って、
興味ありそうな人がいる場所に投稿してみました。（TwitterとかFBとか）
で集まったのが今回紹介したメンバー（<a href="https://noti.st/johtani/ZsQG5A/search-engineering-tech-talk">スライド参照</a>）です。
ユーザー企業の人もいれば、私みたいな検索エンジンの人もいるので面白い感じにできたかなぁと。
で、スピーカーを運営や知り合いに声をかけて第1回をやってみたという感じです。</p>
<h2 id="今後どうするの">今後どうするの？</h2>
<p>残念ながら次回はまだ未定です。
2ヶ月に1回くらいのペースで開催できればなーと思ってますが、スピーカーが集まるかなどによるかなぁと。
ということで、<a href="https://search-tech.connpass.com">勉強会のグループのページ</a>にスピーカー応募フォームのリンクがありますので、スピーカーに興味がある方は入力していただければと。</p>
<p>もちろん第2回はやりたいので、勉強会のページからの連絡をお待ちください！！</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2018）</title>
      <link>https://blog.johtani.info/blog/2018/12/31/looking-back-2018/</link>
      <pubDate>Mon, 31 Dec 2018 16:14:45 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/12/31/looking-back-2018/</guid>
      <description>今年も振り返りブログをかけてます。よかったw 振り返り（2017年に書いた抱負から） まずは去年の抱負を元に。 もっと英語の継続＆TOEIC まぁ、</description>
      <content:encoded><p>今年も振り返りブログをかけてます。よかったw</p>
<!-- more -->
<h2 id="振り返り2017年に書いた抱負から">振り返り（2017年に書いた抱負から）</h2>
<p>まずは去年の抱負を元に。</p>
<h5 id="もっと英語の継続toeic">もっと英語の継続＆TOEIC</h5>
<p>まぁ、継続してます。英会話も続けてますし、海外TVドラマや映画見てます。
ただ、昨年書いたTOEICはイベントが被りまくってて受けれてないです。。。
来年は受けれればいいが。
ちなみに見たドラマはこの辺。あんまり見てないなぁ。
「はじまりのうた」は飛行機の中で見たんですが、よかったです。
最近は音楽系の映画が好きなのかなぁ。グレーテストマンショー（ミュージカル風）とかもハマったし。</p>
<ul>
<li>Game of Thrones</li>
<li>はじまりのうた BEGIN AGAIN (映画)</li>
<li>24 シーズン4まで</li>
</ul>
<p>今は、ボキャブラリのなさに苦しんでる感じです。キクタンとかするべきなのかもなぁ。
基本勉強が下手だからなー。</p>
<h5 id="継続的にイベントに登壇--cfpもっと出すぞ">継続的にイベントに登壇 ＆ CfPもっと出すぞ！</h5>
<p>OSCには出てました。あとは、いくつかに呼ばれて出たりでしょうか。
CfPはJJUGしか出せてない気がするんで、もっと出さないとですね。。。
春のJJUGでは20分で短すぎた「オープンソースとビジネスモデル」の話（同僚のネタ）が
今年面白かった内容かなぁと。
もっと話を作るのをうまくしないとだろうなぁ。
ユースケースが増えてるんで、もっといろんなところに出ていかないとなぁと。
ブースの出し方とかもちょっと考えないといけないかもなーと思ってたり。
マンネリになってきてる気がするんで、なんか取り入れないとなぁ。</p>
<h5 id="もっとブログ">もっとブログ！</h5>
<p>出だしはよかったんですが、途中でRustネタのブログも止まってしまいましたね。。。
業務が忙しくなったのを言い訳にして時間が取れなくなってるんで、
もっと習慣つけないとなぁ。</p>
<h5 id="雑誌やweb系雑誌で記事を">雑誌やWeb系雑誌で記事を。</h5>
<p>できてないです。。。どうすっかなぁ。
重い腰上げないのが問題なんですけどね。。。
自社のウェビナーとかはそこそこやってましたが。</p>
<h5 id="コミュニティを別の方法で盛り上げ">コミュニティを別の方法で盛り上げ</h5>
<p>フォーラムは皆さんのおかげで盛り上がってきてる気がしてます。
別の方法ではなく、勉強会を9月から毎月開催にして、ユースケースごとに切り替えてみました。
あとは、<a href="http://bit.ly/SpeakerElasticTokyoMeetup">スピーカー登録用のフォーム</a>の用意とかして、継続的に楽にスピーカーの人たちが見つけられるようにと。
私は「全然」関わってないんですが、技術書典でいろんな方に書籍を書いていただきました。
また、<a href="https://elasticlover.hatenablog.jp/archive">Elasticlover</a>という毎週いろんな記事をまとめていただけ助かりました。
少しでも感謝をということで、コミュニティランチというイベントで、フォーラムで回答していただいてる方や、書籍を出していただいた方を招いてCEOのShayとランチするというイベントもやってみました。
少しは盛り上げられたかなぁ。
次は、ハッカソンみたいなのとかやってみるのもありなのかなぁ？（サポートしきれない気がするんだよなぁ）</p>
<h5 id="elasticsearchなど検索系の開発にも参加">Elasticsearchなど検索系の開発にも参加</h5>
<p>開発。。。
Analyzer向けの<a href="https://github.com/johtani/analyze-api-ui-plugin">Kibanaのプラグイン</a>の開発は継続してますが、あんまり開発してないですねぇ（<a href="https://github.com/johtani">GitHubの草</a>をみながら)。
ちょっとしたPRはやったりしてますが、もっとやりたい。。。
とりあえず、来年しょっぱなは、検索系じゃないですが、KibanaのプラグインのReact化をやらないとなぁと。</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<p>さて、反省が多かったですが、ここからは今年の出来事を。</p>
<ul>
<li>初スノーシュー in US</li>
<li>オフィス引越し</li>
<li>パリで料理w</li>
<li>K8sとde:code</li>
<li>QNAPとか</li>
<li>IPO!</li>
<li>初のオンライン登壇</li>
<li>初アイルランド!</li>
<li>Pixel3 XL</li>
</ul>
<p>今年も初モノがちらほら。</p>
<p>アメリカでスノーシューやりました。昨年は釣りでしたが、今年はスノーシューでした。
スノーシューはすごくよかったんですが、サンフランシスコから社内ミーティングのある場所までの
移動に10時間バスに缶詰という変な初モノもありました。。。
スノーリゾートのある山の上にバスで移動だったんですが、前日までの天候の生でチェーン規制が出てしまい、登りかけた山を降りて、違う経路で登り直すという長旅でした。。。
途中からずっとゲームオブスローンズみてました。。。</p>
<p>オフィスが引っ越しました。人数が増えてきたのもあり、銀座のWeWorkに引っ越しました（私はあんまり行かなかったりしますがw）。
ただ、すでに手狭になってきて、来年はまた引っ越してるかもなぁ。
今年の終わりで日本のメンバーが22名に増えました！すごい！4年半前の22倍！</p>
<p>パリで料理もしましたwセールスのキックオフミーティングがパリで開催され、なぜか参加してきました。
そこで、DevRelチームのミーティング＆ディナーがあったんですが、ディナーがアクティビティ付きで、チームのみんなとパリで料理やカクテル作ってきました。
まさか、パリで手巻き寿司作るとは思わなかった（怖くて食べてないですw）</p>
<p>今年はイベントが連続することが多かったです。パリから帰ってすぐに<a href="https://thinkit.co.jp/article/13087">de:codeでMSの川崎さんと登壇</a>したりしました。Kubernetesを触る機会ができたんでよかったですが、時差ボケは辛かったw今年は他にもカンファレンス直後にトレーニングだったりと、夏に喉やられちゃいました。喉に負担をかけない話し方の練習すべきなんだろうなぁ。</p>
<p>自宅の環境もちょっと整えました。10年前から使ってた、TeraStationを<a href="https://amzn.to/2QZFxPe">QNAPに刷新</a>。快適に検索できるわ、クラウドにバックアップとれるわで、すごく快適です。
あとは、自室のディスプレイをディスプレイアームにつけたり、壊れたモニタースピーカーを<a href="https://amzn.to/2EX5qsB">Yamahaのパワードスピーカー</a>に買い替えたりと。
ちょこちょこ自宅で作業したりするんで、快適です。</p>
<p>今年はめでたいことに会社がIPOしました。<a href="https://www.elastic.co/blog/ze-bell-has-rung-thank-you-users-customers-and-partners">ニューヨーク証券取引所で株式公開しました</a>。
転職して4年、初体験なんで何がどうってのはいまいち実感わかないんですが、順調にきてるのは嬉しい限りです。今後も頑張りますよ！</p>
<p>自社のウェビナーでは、オンライン登壇してたんですが、<a href="https://wp.infra-workshop.tech/event/elastic-stack-%E5%85%A5%E9%96%80/">インフラ勉強会でオンライン登壇</a>させていただきました。
Elastic Stackの入門的な話をさせていただいたので、興味があればみていただければと。
こういうのにはマイクが非常に重要だなというのが結論です。ウェビナーで使ってる<a href="https://amzn.to/2EZvvHo">Shureのマイク</a>で話したので聞き取りやすかったです。
来年はもっと手軽にちょっとしたウェビナーとかポッドキャストやろうかな、ということで、持ち運び用に<a href="https://amzn.to/2EZs6sm">SAM SONGo Mic</a>を購入してみたのでどっかで試してみたいなー。</p>
<p>アイルランド（ダブリン）にも行きました。これまた、社内のミーティングなんですけどね。半年に1度エンジニアが集まる社内イベントがあるので、いろんなところに旅をさせてもらっていて、すごく楽しいです。
みんなにも会えるし。ただ、アイルランドの英語はきつかった。。。</p>
<p>最後はPixel3です。SonyのXperiaをここ数年は使ってたんですが、電池の持ちなどが悪くなったんで、気になってたPixel3に変えました。すごくいい。いらないものが入ってないのもいい。あと<a href="https://twitter.com/johtani/status/1073185795870085120">カメラがすごくいい</a>。他の機能をまだちゃんと調べてないんで、誰か便利機能知ってたら教えてくださいw</p>
<p>とまぁ、こんな感じでした。</p>
<h2 id="来年の抱負">来年の抱負</h2>
<p>最後は来年の抱負を。</p>
<ul>
<li>TOEIC</li>
<li>CfP見つけて応募 &amp; いろんな場所に顔を出す</li>
<li>もっとブログ！</li>
<li>Rustの継続</li>
<li>開発の継続</li>
<li>日本でエンジニア獲得!</li>
</ul>
<p>まぁ、英語ですね。これは地道にやるしかないんだろうなと思いつつ、コツコツが苦手でw
とりあえず、今年はどこかでTOEIC受けないとな。</p>
<p>いつも行かないけど、弊社プロダクトに関係のあるカンファレンスのCfPを見つけ出して、
応募しまくって、少しでもしゃべらないとなぁ。喋れなくても、Elasticsearchがらみで登壇していただいてる方がいるカンファレンスには顔を出していきたいなぁと。
登壇される方いたら、ぜひ連絡ください！あとは、CfPのサイトを探す仕組みを作らないとなぁ。
検索サイト作るかなぁ。
あとは、カンファレンスではなく、いろんな会社に遊びにいきたいと思ってます。
入門的な話をしてほしいような方、こんな使い方してますっていう話をしていただける方、大募集です。
これもGoogle Formでも作ってみるか。</p>
<p>ブログなぁ。<a href="https://amzn.to/2EZhzwf">小さな習慣</a>を読んだのに、習慣化できてないので、なんとかしないとという意味で。。。が、頑張ります。。。</p>
<p>Rustの継続＆開発の継続もですね。Java歴が長いので、他の言語を勉強しようと思ってRustやってますがなかなか身についてないというか、時間を取れてない。これもコツコツやらないとなー
検索関連の開発もちょっとずつやりたいなと思ってるんで、<a href="https://github.com/DmitryKey/luke/issues/134">LukeへContribute</a>しようかな。</p>
<p>切実なんですが、日本に弊社のエンジニアを増やしたいなと。特に人前で喋ってもらえる人が増えると助かるんです。弊社最近、ユースケースが増えてきてて、追いつかなくてwダレカタスケテーw</p>
<p>さーて、そろそろ紅白のサザンが始まるんで終わりです。</p>
<p>今年も様々な方々に様々な面で助けていただきました。本当にお世話になりました。
この場を借りてお礼申し上げます。</p>
<p>来年ももちろん、助けてもらうんで、よろしくお願いいたします！
あと、話聞きたい方、声かけてくださーい。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elastic Stack 7.0で入ってくる新機能をちょっと紹介</title>
      <link>https://blog.johtani.info/blog/2018/12/25/whats-new-in-elastic-stack-7/</link>
      <pubDate>Tue, 25 Dec 2018 00:00:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/12/25/whats-new-in-elastic-stack-7/</guid>
      <description>Elastic stack (Elasticsearch) Advent Calendar 2018の25日目の記事になります。 今年最後のAdvent Calendarです！来年も忘れてなければやるはず！ 今日は、すでにalp</description>
      <content:encoded><p><a href="https://qiita.com/advent-calendar/2018/elasticsearch">Elastic stack (Elasticsearch) Advent Calendar 2018</a>の25日目の記事になります。
今年最後のAdvent Calendarです！来年も忘れてなければやるはず！</p>
<p>今日は、すでにalpha2までリリースされた7系でどんな変更が入るのかをちょっとだけ紹介します。
ほんとにちょっとだけですよ。</p>
<!-- more -->
<h3 id="kibanaのk7-design-kibana">Kibanaのk7 design (kibana)</h3>
<p><a href="https://www.elastic.co/guide/en/kibana/master/release-notes-7.0.0-alpha1.html#K7-design-7.0.0">Kibanaの新デザインです。K7って呼ぶのかな？</a></p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20181225/k7dashboard.png" />
    </div>
    <a href="/images/entries/20181225/k7dashboard.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>まだ、メニューと一部が実装されているだけですが、7.0.0でガラッと変わりそうです。
そのほかの画面の<a href="https://github.com/elastic/kibana/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+k7">Issueはこちら</a>です。
&ldquo;k7&quot;で検索しただけですが。メタIssueが見つからなかったんで。<a href="https://github.com/elastic/kibana/issues/25736">例えば、こんな感じでアプリとかのスイッチとかがこんな感じになるよというデザイン案が観れたりします</a>。</p>
<h3 id="zen2-elasticsearch">Zen2 (elasticsearch)</h3>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-discovery.html">Elasticsearchの新しいクラスター管理機構アルゴリズム</a>になります。
Zenと呼ばれる独自実装のものを6系までは使っていましたが、7系向けに変更がかかりました。
実際には、Nodeを探す仕組み、Masterの選出アルゴリズム、クラスター状態の管理などを行います。
上記のリンクにあるようにドキュメントも詳しくなりました。
信頼性をさらに向上し、設定ミスを起こしにくくして、より使いやすくという目的で様々な変更が加えられています。
これが、<a href="https://github.com/elastic/elasticsearch/issues/32006">メタIssue</a>かな？
アルゴリズムの変更や、クラスターの状態の管理の方法などの変更に関するIssueやPRにリンクが貼ってあります。</p>
<h3 id="新しいデータタイプ-elasticsearch">新しいデータタイプ (elasticsearch)</h3>
<h4 id="featurefeature-vector-datatype">Feature/Feature vector datatype</h4>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/feature.html">ドキュメントはこちら</a>と<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/feature-vector.html">こちら</a></p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-feature-query.html">feature query</a>と合わせて使用するためのフィールドで、しかもクエリのスコア計算「のみ」に使用するフィールドになります。
検索条件やソート、Aggregationの対象ではなく、クエリのスコアに影響させたい値を入れておくためのフィールドです。
6から追加された機能の「<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/search-uri-request.html">track_total_hits</a>」をfalseにした時と合わせると、function_scoreなどで計算をしていた場合よりも、検索性能が上がるという利点まであります。
ちなみに、「track_total_hits」は検索ヒット数を計算しないで、上位のデータを取得する時にクエリを早くするといったことができる機能になります。
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules-index-sorting.html">Index Sorting</a>と組み合わせることで威力が発揮できる仕組みになるはずです。</p>
<h3 id="filebeat-supports-netflow-beats">Filebeat supports NetFlow (beats)</h3>
<p><a href="https://www.elastic.co/guide/en/beats/filebeat/master/filebeat-input-netflow.html">NetFlowが入力</a>として追加されます。
Filebeatと言いつつ、File以外の入力が徐々に増えてきてますね（UDPやTCPにも対応しましたし）。
ネットワーク機器などの監視を行う方などにはさらに便利になってくるのではないでしょうか？
（私はこの辺りは不得手なので、誰か使ってみてもらえればと！）</p>
<h2 id="まとめ">まとめ</h2>
<p>まだ、序の口って感じですが、今年はこの辺で。7系ではここであげた以外にも様々な機能が追加されています（もしくは予定です）。
Elasticのドキュメントの良いところは、masterブランチのドキュメントも公開されていることです。
ドキュメントのバージョンを<code>7.0.0-alpha2</code>にすれば、masterブランチで追加されたページが見れるので、
興味のある方は眺めてみていただければと。物によって、リリースノートが書かれていなかったりするので注意は必要ですが。</p>
<p>今年もあと数日になりましたが、Advent Calendarへの参加ありがとうございました！
来年ももちろんやりますので、年始からネタを考えてくださいね。</p>
<p>来年一発目は、<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/events/257301177/">第28回Elasticsearch勉強会 - 6.5機能紹介 -</a>になります。ウェビナーでも紹介しましたが、6.5で入った様々な新機能をデモありで紹介する予定です。
興味のある方はぜひご参加ください。</p>
<p>では、来年もよろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>2018年のElastic StackとElastic</title>
      <link>https://blog.johtani.info/blog/2018/12/01/whats-happen-at-elastic-in-2018/</link>
      <pubDate>Sat, 01 Dec 2018 10:48:38 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/12/01/whats-happen-at-elastic-in-2018/</guid>
      <description>Elastic stack (Elasticsearch) Advent Calendar 2018の1日目の記事になります。 ちょっと遅れちゃいました。。。 まだ、1ヶ月を残してますが、簡単に今年起こったことを振り返ってみよ</description>
      <content:encoded><p><a href="https://qiita.com/advent-calendar/2018/elasticsearch">Elastic stack (Elasticsearch) Advent Calendar 2018</a>の1日目の記事になります。</p>
<p>ちょっと遅れちゃいました。。。
まだ、1ヶ月を残してますが、簡単に今年起こったことを振り返ってみようと思います。毎年恒例ですね、ここ数年。</p>
<!-- more -->
<h3 id="elastic-stack-620リリース2月">Elastic Stack 6.2.0リリース(2月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-6-2-0-released">リリース記事はこちら</a></p>
<p>APMがGAリリースされ、Beats monitoring UIも追加されました。Stackとしての統一度がちょっとずつ上がってきた感じですね。
Kibanaのホーム画面（左メニューのKibanaアイコンをクリックした時）にデータ登録のチュートリアル的な画面が追加されています。
特にBeatsを利用する時の流れが簡単にわかるのがいい感じです。Metricsなどはローカルでちょっと試すのにも簡単な流れですので、ぜひ一度やってみてもらいたいなと。
個人的にはtermsを使ったパイチャートで、<a href="https://github.com/elastic/kibana/pull/15525">その他の数値がどのくらいあるかといった表示ができるよう</a>になって、やっと帰ってきた！（Kibana 3の頃にはあった機能）という印象でした。</p>
<h3 id="elasticon18開催2月">Elastic{ON}18開催(2月)</h3>
<p>第4回目のユーザーカンファレンスがSFで開催されました。
今年のキーノートが今年最大のニュースですね。
<a href="https://www.elastic.co/jp/blog/doubling-down-on-open">X-Packのコードの公開が発表された</a>のがこの時でした。
個人的に今後もオープンソースに携わっていきたいと思いながら日々働いていますが、
Elasticのオープンソースへのこだわりと、シンプルな考え方を再確認して素晴らしい会社で働けてるなーと。
商用のソースコードを公開してユーザーや顧客の皆さんとより良いものを作っていきたいという形ですので、今後もよろしくお願いします！
<a href="https://www.elastic.co/elasticon/conf/2018/sf/opening-keynote">キーノートの動画はこちら</a>からご覧いただけます。</p>
<p>そのほかにも次のような発表が行われました。</p>
<ul>
<li>SQL for elasticsearch</li>
<li>Canvas</li>
<li>Elastic App Search(旧Swiftype)</li>
</ul>
<h3 id="elastic-stack-630リリース6月">Elastic Stack 6.3.0リリース(6月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-6-3-0-released">リリース記事はこちら</a></p>
<p>2月末のElastic{ON}で発表されたX-Packのコードの公開にはやはり時間がかかりました。
有償コードのリポジトリとの統合やライセンスの変更、テスト環境などなど、色々大変だったみたいです。
ようやく公開され、ベーシックのライセンスの扱いなども変わり、より使いやすくなったのがこのタイミングです。</p>
<ul>
<li>X-Packをプラグインとしてのインストールが不要に</li>
<li>ベーシックライセンスがデフォルトでONに。6.3から登録などが不要に。</li>
<li>Apache 2.0ライセンスの部分のみのディストリビューションも別途ダウンロードできるようになどなど</li>
</ul>
<p><a href="https://www.elastic.co/jp/webinars/elastic-stack-6-3">日本でもリリースウェビナーをやりました</a>。ご覧いただけましたかね？</p>
<h3 id="elastic-cloud-elasticsearch-serviceがより使いやすく8月">Elastic Cloud Elasticsearch Serviceがより使いやすく(8月)</h3>
<p><a href="https://www.elastic.co/jp/blog/the-next-generation-elasticsearch-service-hot-warm-clusters-machine-learning-more-hardware-choices-and-new-pricing">リリース記事はこちら</a></p>
<p>これまでは、メモリとストレージの比率だけしか指定できなかったのですが、
このリリースで様々なユースケースに応じた組み合わせが可能になりました。
CPUやメモリリソースよりもストレージを大きくしたりなどです。
専用マスターノードを追加できたり、待望の機械学習（Machine Learning）が提供されたりと色々と変更があり使いやすくなったかと。
昔からよく聞かれる、Kuromojiなどのカスタム辞書を登録する機能もあるので、Elastic Cloud便利です。
<a href="https://www.elastic.co/jp/cloud/elasticsearch-service">ご存知ない方は、14日間のトライアル</a>もありますので試していただければと！</p>
<h3 id="elastic-stack-640リリース8月">Elastic Stack 6.4.0リリース(8月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-6-4-0-released">リリース記事はこちら</a></p>
<p>フィールドエイリアスや韓国語のアナライザーがElasticsearchに追加されました。
Kibanaはデザインがここからさらに少しずつ変更が入ってたりします。
<a href="https://elastic.github.io/eui/#/">Elastic UIフレームワークと呼ばれるデザイン用のライブラリ</a>が、ElasticのプロダクトのUIに取り込まれていってる感じです。統一感が取れてきてますよね。私が開発している<a href="https://github.com/johtani/analyze-api-ui-plugin/issues/25">Analyze UIのプラグインにも取り込みました</a>。
あとは、マイクロソフトのde:codeで話をさせていただいた、<a href="https://www.elastic.co/blog/logstash-6-4-0-released">Logstash向けのAzure Moduleがリリース</a>されたのもこのバージョンでした。AzureのEvent Hubからデータを取り込んで、SQLデータベースのモニタリングや、ユーザーの認証などをとってKibanaで可視化するものです。</p>
<p>もっとも気に入っているのはサンプルデータの登録が簡単になったことです。これまでは、KibanaとElasticsearchを用意した後に、データを入れるためにFilebeatなどを使ってから、ようやくKibanaで遊べるという形でした。
6.4からは、ElasticsearchとKibanaを立ち上げて、Kibanaのホーム画面の「Sample Data」のリンクを押した後に、「Sample flght data」の「Add」ボタンを押せばKibanaからデータが登録されます（<a href="https://www.elastic.co/guide/en/kibana/current/tutorial-sample-data.html">サンプルデータについてはこちら</a>）。とりあえず触ってみたいという方への敷居がさらに下がったのではないかなぁと。</p>
<h3 id="elastic認定エンジニア第1号8月">Elastic認定エンジニア第1号(8月)</h3>
<p><a href="https://www.elastic.co/jp/blog/celebrating-the-first-elastic-certified-engineer">ブログ記事はこちら</a></p>
<p>認定制度も始まりました。Elasticsearchの知識、経験を問われるテストを受けていただき、合格すると認定されるというやつです。
なんと、社外で世界初の認定エンジニアがアクロクエストの吉岡さんでした（上記ブログ参照）。
私もトレーナーやってるのもあり、慌てて認定をとったりしましたw。
認定テストは筆記ではなく、実際に作業をするテストなので実践的です。
トレーニングの受講が必須ではないのも面白いなぁと思いました。
トレーニングや認定エンジニアに興味がある方は、<a href="https://training.elastic.co">Elasticのトレーニングのサイト</a>をご覧ください。
1月末にはまた、日本語でElasticsearchのトレーニングも開催されます！</p>
<h3 id="elastic-cloud-enterprise-20リリース9月">Elastic Cloud Enterprise 2.0リリース(9月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-cloud-enterprise-2-0-0-released">リリース記事はこちら</a></p>
<p><a href="https://www.elastic.co/jp/products/ece">Elastic Cloud Enterprise</a>をご存知ない方もいらっしゃるかもしれません。
Elastic Cloudの裏側で利用しているクラスターの起動などの仕組みを製品として提供しているのがこちらになります。
Elastic Cloudで機械学習や様々な構成ができるようになったものがリリースされたのがこの2.0です。
複数のElasticsearchクラスターを管理したい場合には、こちらが便利なツールになってるんじゃないかなぁと。
部署ごとにクラスターを提供するといったことが可能になるので、乱立する前に利用するのも便利かなーと。</p>
<h3 id="ニューヨーク証券取引所で株式を公開10月">ニューヨーク証券取引所で株式を公開(10月)</h3>
<p><a href="https://www.elastic.co/jp/blog/ze-bell-has-rung-thank-you-users-customers-and-partners">ブログ記事</a></p>
<h3 id="日本語でブログ10月">日本語でブログ(10月)</h3>
<p><a href="https://www.elastic.co/jp/blog/how-to-configure-elasticsearch-cluster-better">Elasticsearchの運用に関する典型的な4つの誤解</a>というブログを書きました。4年も働いてるのに、会社のブログに翻訳以外で書いたことなかったので。。。
Twitterや勉強会、ブログ記事などで見かけるよくある誤解に関する記事を書いてみました。
Elasticは英語のブログも活発に書かれているのですが、今後もこのような形で日本語でのブログも頑張りますので、
読んでみたいものなどあればコメントいただければと。</p>
<p>まぁ、<a href="https://www.elastic.co/jp/blog/author/kosho-owa">弊社の大輪は色々書いてるんで</a>、私がもっと頑張れって話ですかね。。。</p>
<h3 id="elastic-stack-650リリース11月">Elastic Stack 6.5.0リリース(11月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-6-5-0-released">リリース記事はこちら</a></p>
<p>昨日（11/30）のウェビナーでも話をさせていただきましたが、Elastic Stack 6.5は「本当にマイナーリリース？？？」と思うほど盛りだくさんの機能がリリースされました。</p>
<ul>
<li>インフラUI、ログUI</li>
<li>Elastic APMの分散トレーシング対応</li>
<li>Java &amp; Go APM Agent GAリリース</li>
<li>Cross Cluster Replication</li>
<li>ODBCドライバー</li>
<li>Kibana Canvas</li>
<li>Kibana Spaces</li>
<li>Data Visualizer for files</li>
<li>Functionbeat</li>
<li>LogstashのApp Search output</li>
</ul>
<p>リストアップしただけでもこれです。45分のウェビナーでは伝えきれてないなぁとも思ってますので、何か検討しようと思います！</p>
<h3 id="elasticsearch勉強会3月から12月">Elasticsearch勉強会(3月から12月)</h3>
<p><a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/">Elasticsearch勉強会ページ</a></p>
<p>今年は、6回の勉強会を開催（1つは12月19日開催）しました。
9月からは、ユースケースなど、もっと参加者の皆さんの興味があることにフォーカスしながら開催をしてみ始めました。
参加しやすくなってればいいのですが。。。
そろそろまたアンケートをとったりして、参加しやすいか、どんな改善がしてほしいかなどを聞きたいなと思っています。</p>
<p>12月はもっと皆さんと喋りたいなということで、<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/events/256619262/">スピーカーなしの「LT&amp;忘年会」</a>にしてみました。
私やElasticのものも参加するので、ぜひ色々聞いたり、他のコミュニティの方達の使い方を聞き出して、
新しい発見をしていただければなーと思います。LTでスピーカーの練習をするってものありですよ！（まだ誰も応募してくれてない。。。）
発表することで、フィードバックがもらえて、自分の使い方に自信が持てたり、その他の視点を得ることができると思いますので、
ぜひ発表してみていただければと。</p>
<h3 id="12月のjohtani出没イベント">12月のjohtani出没イベント</h3>
<p>12月は以下のイベントにブースを出してます。イベントに参加される方ははぜひブースにお立ち寄りください！！</p>
<ul>
<li>12/4-5 : <a href="https://containerdays.jp">Japan Container Days</a> - 東京</li>
<li>12/8 : <a href="https://www.ospn.jp/osc2018-fukuoka/">OSC福岡</a> - 福岡</li>
<li>12/15 : <a href="http://www.java-users.jp/ccc2018fall/#/">JJUG CCC 2018 Fall</a> - 東京</li>
<li>12/19 : <a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/events/256619262/">第27回Elasticsearch勉強会</a> - 東京</li>
</ul>
<p>なんか、忙しそうだな。。。</p>
<h3 id="まとめ">まとめ</h3>
<p>駆け足でしたが今年を振り返ってみました。
今年も色々ありましたが、今後もよろしくお願いいたします。</p>
<p>さて、<a href="https://qiita.com/advent-calendar/2018/elasticsearch">Elastic Stack Advent Calendar 2018</a>は今日から25日まで続きます。こらからの記事を楽しみにしています！</p>
<p>ということで、次は<a href="https://qiita.com/kaibadash@github">kaibadash@github</a>さんの「ぼくの考えた最強のElasticsearch index設定を最強にわかりやすく書くぞ！！！」になります。お楽しみに！</p>
</content:encoded>
    </item>
    
    <item>
      <title>新規導入した長期出張用のアイテムをいくつか紹介</title>
      <link>https://blog.johtani.info/blog/2018/10/18/items-for-long-businesstrip/</link>
      <pubDate>Thu, 18 Oct 2018 01:34:55 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/10/18/items-for-long-businesstrip/</guid>
      <description>（転職する少し前までパスポートすら持ってなかったのに）今の会社に転職してから、半年に1度の割合で海外への出張が発生する生活を送っています。 4</description>
      <content:encoded><p>（転職する少し前までパスポートすら持ってなかったのに）今の会社に転職してから、半年に1度の割合で海外への出張が発生する生活を送っています。
4年目になりますが、今回いくつか新しいガジェット？アイテム？を出張用に導入したので感想を書いておこうかなと。</p>
<!-- more -->
<p>とりあえず、以下の4つです。</p>
<h2 id="felimoa-ネック-ピロー">Felimoa ネック ピロー</h2>
<p>これまでは、無印で購入した「<a href="https://www.muji.net/store/cmdty/detail/4550002125325">フィットするネックピロー</a>」を使ってました。
特に使ってる間は問題ないのですが、どうしてもかさばります。。。
そんな時、FBで見かけたのがこちら。</p>
<iframe style="width:240px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=johtani-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=B077GQR2QC&linkId=68ae5d69e21ae8138332beb9d301a542&bc1=000000&lt1=_blank&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr">
    </iframe>
<p>厚みがないので、持ち運びが便利でした（商品紹介の中にありますが、リュックにつけると邪魔にならない）。
飛行機の中で使うので、それほど暑くない場所なので、ムレる感じでもなく。
ただ、ヘッドフォン＋真横にフレームがくる形になるとヘッドフォンに干渉するので、
私はフレームを顎の下の方に寄せた形で使ってました。
軽さと場所を取らないのがよかったです。</p>
<h2 id="ノイズキャンセリングヘッドフォン">ノイズキャンセリングヘッドフォン</h2>
<p>これまでは、<a href="https://www.sony.jp/walkman/products/NW-A840_series/">SONYのNW-A847</a>という
ノイズキャンセリング機能付きのウォークマン＋アクセサリーで外部入力を取り込む形で、飛行機で映画を見ていました。
これ自体が8年前の製品ですね。。。
これとは別に、冬の寒い時期は防寒も兼ねてヘッドフォンを使用しています。
<a href="https://www.sony.jp/headphone/products/MDR-10R/">MDR-10R</a>を使ってたんですが、昨年こんな感じになりまして。。。</p>
<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">応急処置 <a href="https://t.co/uDErZWpPVC">pic.twitter.com/uDErZWpPVC</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/928850255222681601?ref_src=twsrc%5Etfw">2017年11月10日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>で、ヘッドフォンの買い替えを検討してたんです。
そこへ飛び込んできたのがSonyの新しいノイズキャンセリングヘッドフォンでしたと。</p>
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&language=ja_JP&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=B07GZ8DZC8&linkId=0b91ad46d6f630a45d53c65eba62725a"></iframe>
<p>せっかく買い換えるし、今度はいいものを長くということでフライト前日に購入してしまいました。
結論としては高いけど買ってよかったと。よかった点はこんな感じです。</p>
<ul>
<li>軽い</li>
<li>イヤーパッドが耳を抑えることがないので長時間でも気にならない</li>
<li>人の声だけノイズキャンセリングをオフにする（騒音などはキャンセルしてくれる）</li>
<li>バッテリーが長時間</li>
<li>ヘッドフォンを外さないでも案内を聞くことができる「クイックアテンションモード」</li>
</ul>
<p>などなど。行きのフライトで5時間も遅れが出ましたが、空港、飛行機内で非常に快適に過ごせました。
大事に使おう。。。</p>
<h2 id="cw-x">cw-x</h2>
<p>知り合いがFBにアップしてて、立ちっぱなしの仕事だったり、長時間のフライトでも足が疲れないという噂を聞いて購入しました。</p>
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&language=ja_JP&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=B01BKDX2QY&linkId=5af7d1ce20febf92530820e8e6b66284"></iframe>
<p>長時間のフライトの後に、ホテルで1泊しても足の疲れって結構残ってるんです。
今回、12時間のフライトでズボンの下に履いて行きましたが、足の疲れがほぼありませんでした。
デブになりつつあり、足太くなって来てるので、履くのはちょっと苦労しますが。。。もともときつめに作ってるんじゃないかなぁ（希望的観測）。
帰りも使用しましたが、帰りはフライト前に1万歩ほどダブリンの街中を歩き回ってたので、流石に疲れが出てます。。。
今度は、カンファレンスのブースなどで立つことが多いシーンでも使ってみて、疲れがどうなるかを試してみようと思います。</p>
<h2 id="携帯ウォシュレット">携帯ウォシュレット</h2>
<p>初めて購入しました。海外のホテルで1週間とか連泊するんですが、トイレットペーパーがやはり硬めなのが気になって。。。</p>
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&language=ja_JP&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=B005FDJ8SM&linkId=e959d19209ac105a9f0e30ab00e2c4be"></iframe>
<p>世界に誇る日本の技術の一つだと確信してますw
ドイツに長期出張してた友人が便利だよと言ってたので、思い切って購入して持って行きました。
硬めのトイレットペーパーも気にならなくなるのでとても幸せな気分ですw</p>
<h2 id="まとめ">まとめ</h2>
<p>とりあえず、今回はこんな感じです。
総じて導入してよかったなという感じでした。
他にもおすすめ出張グッズとかあればコメントいただけると嬉しいです。</p>
<p>普段使ってるものとか、どういうものを持っていってるとか興味ある人いるかなぁ？
ではでは。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第25回Elasticsearch勉強会を開催しました。</title>
      <link>https://blog.johtani.info/blog/2018/10/17/26th-elasticsearch-tokyo-meetup/</link>
      <pubDate>Wed, 17 Oct 2018 17:33:50 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/10/17/26th-elasticsearch-tokyo-meetup/</guid>
      <description>毎月開催の2回目になります。 今回は日経さんの会場をお借りしての開催となりました。 前回から、スピーカーの募集をhttp://bit.ly/Sp</description>
      <content:encoded><p>毎月開催の2回目になります。
今回は日経さんの会場をお借りしての開催となりました。</p>
<p>前回から、スピーカーの募集をhttp://bit.ly/SpeakerElasticTokyoMeetup で行なっております。
ぜひ皆さんのノウハウを共有していただけると助かります。
また、次回もすでにスケジュール済みです。次回は<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/events/254646236/">「ログ/メトリック分析」回</a>になります。</p>
<p>以下は、個人的なメモになります。</p>
<!-- more -->
<h3 id="メディアコンテンツ向け記事検索dbとして使うelasticsearch--future-architect-株式会社-村田-靖拓さん-twitter-famipapamart">メディアコンテンツ向け記事検索DBとして使うElasticsearch / Future Architect 株式会社 村田 靖拓さん (twitter: @famipapamart)</h3>
<ul>
<li>メディア記事コンテンツ検索</li>
<li>全ての情報が1indexに入っているようにすること。</li>
<li>typeは少し悩んだ。</li>
<li>範囲検索にはならない場合がある。（文字列で登録してWildcard検索できるようにした）</li>
<li>kuromojiで基本対応</li>
<li>異体字についてはchar filterでマッピング</li>
<li>細かな設定とかもスライドにて公開予定。
<ul>
<li>基本的なプラグインだけで対応した</li>
</ul>
</li>
<li>Dynamic Field mappingを有効にしたまま対応</li>
<li>パフォーマンス検証</li>
<li>初回のインデックスのロードは<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html#_warm_up_the_filesystem_cache">この辺かなぁ？</a>。</li>
<li>自力でQueryのoffset-limitを構築するのかぁ。
<ul>
<li>ソート条件が固定らしいのでできる方法</li>
</ul>
</li>
</ul>
<h3 id="minne-での検索運用仮--_shiro16-さん">minne での検索運用(仮) / @_shiro16 さん</h3>
<ul>
<li>ハンドメイドなものをマーケットプレイスがminne</li>
<li>SolrからElasticsearchに切り替えた話 2016/02以降はEs
<ul>
<li>昔は、DBからSolrへ同期</li>
<li>Es版ではDBからの同期ではなく、Workerに対してリクエストを入れる</li>
</ul>
</li>
<li>現状は独自にEC2で運用中</li>
<li>ユーザーが求めているものがきちんとでているかを計測している
<ul>
<li>行動ログはどんな感じ？</li>
<li>TDにログを入れて、CTRとかを計算してre:dashで可視化</li>
</ul>
</li>
<li>A/Bテストも実施
<ul>
<li>指標はキャンペーンなどが実施されている場合にブレる場合もある</li>
</ul>
</li>
<li>トレンドをログから知ることができる</li>
<li>Function Scoreでスコアを変更してる
<ul>
<li>季節的な単語でスコアを変更したりする</li>
</ul>
</li>
<li>ドリンクの対応などをして聴けてないところが。。。</li>
</ul>
<h3 id="query_stringのはなし--加藤遼さん日本経済新聞社">query_stringのはなし / 加藤遼さん　(日本経済新聞社)</h3>
<p>電池が切れそう＋ピザとかの手配をしていたらメモが取れず。</p>
<ul>
<li>苦労が滲み出る感じのセッションでした。</li>
<li>query_string queryが実際にどんなクエリになっているかの説明を交えて説明してもらえたのはすごくよかったんじゃないかと。</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>検索は話してくれる人が多いし話題に事欠かないなぁという印象でした。
今回も、スピーカーの皆さん、会場提供をしていただいた日経さんありがとうございました。</p>
<p>他のユースケースのスピーカーも募集してます。ぜひMeetup.comの概要に記載してあるリンクからスピーカーの応募をお願いします！</p>
</content:encoded>
    </item>
    
    <item>
      <title>JJUGナイトセミナーで話しました</title>
      <link>https://blog.johtani.info/blog/2018/07/29/apm-java-at-jjug/</link>
      <pubDate>Sun, 29 Jul 2018 14:45:30 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/07/29/apm-java-at-jjug/</guid>
      <description>7/25にJJUGナイトセミナーでElastic Stackの紹介とAPMのJava Agentの紹介をしたので、補足のブログです。 （久々に書く</description>
      <content:encoded><p><a href="https://jjug.doorkeeper.jp/events/77485">7/25にJJUGナイトセミナー</a>でElastic Stackの紹介とAPMのJava Agentの紹介をしたので、補足のブログです。
（久々に書くな。。。）</p>
<!-- more -->
<p>スライドとサンプルアプリのリポジトリはこちら。</p>
<ul>
<li>スライド：https://speakerdeck.com/johtani/intro-elastic-stack-and-elastic-apm-java</li>
<li>リポジトリ：https://github.com/johtani/apm-beats-kubernetes-demo/tree/master</li>
</ul>
<h3 id="サンプルアプリ">サンプルアプリ</h3>
<p>勉強会の頭でサンプルアプリへのアクセス用QRコードを用意して質問してもらう感じにしました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180729/APM___Beats_demo_app.jpg" />
    </div>
    <a href="/images/entries/20180729/APM___Beats_demo_app.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>アプリ自体が質問の受付と、そこにある質問に対して聞きたいかどうかのVoteができる仕組みになっています。
セッション最後にこの画面をみながら答えました。
手を挙げていただくよりも、匿名（名前入れるようになってますが、実名である必要はない）で登録できるし、
みんなが聞きたいかどうかもわかるので便利だなぁと。</p>
<p>元は、Elastic{ON} 2018であった「<a href="https://www.elastic.co/webinars/elasticsearch-log-collection-with-kubernetes-docker-and-containers">Docker &amp; Kubernetes Log Collection and Monitoring with Beats and Elasticsearch</a>」のサンプルアプリです。
これをSpring Bootに移植して、ちょっとだけサンプルコードを追加したものになります。</p>
<h3 id="構成とかの補足">構成とかの補足</h3>
<p>スライド、GitHubのリポジトリのREADMEにある図は、k8s上のサンプルアプリケーションの構成だけでした。
ElasticsearchとKibanaを含めた図はこんな感じです。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180729/system-overview.jpg" />
    </div>
    <a href="/images/entries/20180729/system-overview.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>k8s上の各種Beats、APM Serverは一旦Elastic CLoud Elasticsearch Service（AWS Tokyoリージョンにデプロイ）に対してデータを投げます。
で、KibanaはGKE(k8s on GCP)で動かして、実際に勉強会ではkubectl proxyで接続してから表示していました。
この構成にしている理由は次の理由です。</p>
<ul>
<li>Elastic CloudのElasticsearchクラスターにデータを投げている理由
<ul>
<li>クラスターの起動が簡単。</li>
<li>データを永続化したい。k8sのアプリは必要がなくなったらデータを削除したいから。</li>
<li>k8sのdeploymentを書く手間を省く</li>
<li>普段使ってるので。。。</li>
</ul>
</li>
<li>KibanaをGKEで動かした理由
<ul>
<li>Elastic Cloudでは*「現時点(7/29現在)」*で、KibanaでAPM専用のUIを起動できない</li>
<li>ローカルである必要はない</li>
</ul>
</li>
</ul>
<p>といったところです。
Elastic Cloudのインスタンスを起動したままにしておけば、デモをしなくても、このタイミングのログ、メトリクスを
再利用して話をすることも可能です。</p>
<h3 id="まとめ">まとめ</h3>
<p>ということで、簡単ですが、構成の補足でした。
勉強会では告知したのですが、今後の勉強会はトピックスごとでやりたいなと。ということで、どんなトピックスに興味があるのか、スピーカーの応募にはどんなツールが話しやすいかなどといったことのアンケートを集めております。ぜひご協力ください！
また、アンケート、このブログに関する質問がある場合は、<a href="https://twitter.com/johtani">@johtani</a>、もしくはブログへのコメントでお願い致します。</p>
<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">今後の勉強会のやり方などについてアンケートを実施中です。<a href="https://t.co/XU15CHro0n">https://t.co/XU15CHro0n</a> 皆さん是非ご協力ください。ここにない項目については返信をお願いします。 <a href="https://twitter.com/hashtag/elasticsearchjp?src=hash&amp;ref_src=twsrc%5Etfw">#elasticsearchjp</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/1021388358730170368?ref_src=twsrc%5Etfw">2018年7月23日</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</content:encoded>
    </item>
    
    <item>
      <title>Analyze UIとKibanaのプラグインの作成方法（第3回）</title>
      <link>https://blog.johtani.info/blog/2018/04/20/directory-layout-and-architecture/</link>
      <pubDate>Fri, 20 Apr 2018 15:30:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/04/20/directory-layout-and-architecture/</guid>
      <description>第2回から少し間が空いてしまいましたが、templateで作成したプラグインのディレクトリ構成とどういう流れでデータがやり取りされるかについ</description>
      <content:encoded><p><a href="http://blog.johtani.info/blog/2018/02/09/getting-started-template-kibana-plugin/">第2回</a>から少し間が空いてしまいましたが、templateで作成したプラグインのディレクトリ構成とどういう流れでデータがやり取りされるかについてみていきます。
（2018/02月時点で作成したディレクトリ構成にしたがって説明します）
ちなみに、JavaScriptの優れた開発者ではないので、誤解している点や、効率の悪い書き方などがあるかもしれません。見つけた場合は、連絡をいただければと思います。</p>
<!-- more -->
<p>では、まずは作成したディレクトリ構成についてみていきましょう。</p>
<h2 id="ディレクトリ構成">ディレクトリ構成</h2>
<p><code>simple-sample-kibana-plugin</code>がプラグインのプロジェクトのトップディレクトリになります。このディレクトリに次のような構成でサブディレクトリが存在します(なお、画像はIntelliJに取り込んだ後のディレクトリになっているので、<code>.iml</code>など、不要なファイル/ディレクトリが存在しています)。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:400">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180420/directories.jpg" />
    </div>
    <a href="/images/entries/20180420/directories.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>主要なディレクトリ、ファイルについて簡単に一覧で説明します(順不同)。</p>
<table>
<thead>
<tr>
<th>ファイル/ディレクトリ名</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td>index.js</td>
<td>プラグインの本体。Kibanaはこのファイルのオブジェクトを読み込みプラグインを起動。設定などの読み込みもこちら。</td>
</tr>
<tr>
<td>package.json</td>
<td>npm/yarnのパッケージに関する情報を定義するファイル</td>
</tr>
<tr>
<td>README.md</td>
<td>README。プラグインの説明などを記載する。インストール方法なども記載すると便利</td>
</tr>
<tr>
<td>public</td>
<td>ブラウザ側に配布されるプログラムや画像一式</td>
</tr>
<tr>
<td>public/less/main.less</td>
<td>LESS用のファイル。アプリ固有のスタイルなどを記載</td>
</tr>
<tr>
<td>public/app.js</td>
<td>ブラウザ側で読み込まれるプラグインのモジュールなど。</td>
</tr>
<tr>
<td>public/template/index.html</td>
<td>HTMLのテンプレート。ブラウザ上での描画に利用</td>
</tr>
<tr>
<td>server/routes</td>
<td>Kibanaサーバー側で動作するプラグイン。hapi.jsを利用してREST APIを実装する</td>
</tr>
</tbody>
</table>
<p>重要なファイルについて少しだけ説明します。</p>
<h4 id="packagejson">package.json</h4>
<p>npmやyarnでビルドなどをするときに使用するパッケージ情報を記載するためのファイルです。
プラグインの名前、バージョン、説明などを記載します。
Kibanaのバージョンについてもこちらで管理します。この情報を
また、ライブラリなどの依存関係についてもこちらで記載しています。
以下、抜粋。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;simple-sample-kibana-plugin&#34;</span>,
  <span style="color:#f92672">&#34;version&#34;</span>: <span style="color:#e6db74">&#34;0.0.0&#34;</span>,
  <span style="color:#f92672">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;Sample plugin for explaining how to make kibana app&#34;</span>,
  <span style="color:#f92672">&#34;main&#34;</span>: <span style="color:#e6db74">&#34;index.js&#34;</span>,
  <span style="color:#f92672">&#34;kibana&#34;</span>: {
    <span style="color:#f92672">&#34;version&#34;</span>: <span style="color:#e6db74">&#34;6.2.1&#34;</span>,
    <span style="color:#f92672">&#34;templateVersion&#34;</span>: <span style="color:#e6db74">&#34;7.2.4&#34;</span>
  },
  <span style="color:#f92672">&#34;scripts&#34;</span>: {
    <span style="color:#f92672">&#34;lint&#34;</span>: <span style="color:#e6db74">&#34;eslint **/*.js&#34;</span>,
<span style="color:#960050;background-color:#1e0010">...</span>
  },
  <span style="color:#f92672">&#34;devDependencies&#34;</span>: {
    <span style="color:#f92672">&#34;@elastic/eslint-config-kibana&#34;</span>: <span style="color:#e6db74">&#34;^0.14.0&#34;</span>,
    <span style="color:#f92672">&#34;@elastic/eslint-import-resolver-kibana&#34;</span>: <span style="color:#e6db74">&#34;^0.9.0&#34;</span>,
    <span style="color:#f92672">&#34;@elastic/plugin-helpers&#34;</span>: <span style="color:#e6db74">&#34;^7.1.3&#34;</span>,
<span style="color:#960050;background-color:#1e0010">...</span>
    <span style="color:#f92672">&#34;expect.js&#34;</span>: <span style="color:#e6db74">&#34;^0.3.1&#34;</span>
  }
}
</code></pre></div><p>ちなみに私は、<code>version</code>などをリリースするたびに変更しています。</p>
<h4 id="indexjs">index.js</h4>
<p>最初にKibanaに読み込まれるオブジェクトになります。
Kibanaのアプリの名前や、必要なモジュールなどを記載します。</p>
<p>また、<code>kibana.yml</code>から設定など読み込む処理なども書くことができます。</p>
<p>2行目の<code>exampleRoute</code>はサーバー側のAPIとして利用するhapi.js用のファイルのパスになります。</p>
<p><code>uiExports</code>はこのアプリの画面に関する設定などの記載になります。
<code>app</code>の部分が実際にアプリの情報で、
<code>main</code>があとで説明するこのプラグインのUIのためのJavaScriptファイル(public/app.js)になります。mainですので、最初に読み込まれる処理が記載されているものを指定します。<code>app.js</code>というファイル名を変更する場合は、こちらの<code>app</code>の部分を変更したファイルに合わせましょう。</p>
<p><code>config(Joi)</code>の関数が設定ファイルの読み込みなどの処理を記載する場所です。</p>
<p><code>init(server, options)</code>の関数が初期化処理を記載する場所になります。
このサンプルアプリでは、2行目の<code>import</code>で読み込んだhapi.js用のファイルの関数を呼び出しています。引数で渡している<code>server</code>がhapi.jsの<code>server</code>オブジェクトになります。
<code>route</code>メソッドを使用して作成しているプラグイン用のREST APIを追加しています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">import</span> { <span style="color:#a6e22e">resolve</span> } <span style="color:#a6e22e">from</span> <span style="color:#e6db74">&#39;path&#39;</span>;
<span style="color:#66d9ef">import</span> <span style="color:#a6e22e">exampleRoute</span> <span style="color:#a6e22e">from</span> <span style="color:#e6db74">&#39;./server/routes/example&#39;</span>;

<span style="color:#66d9ef">export</span> <span style="color:#66d9ef">default</span> <span style="color:#66d9ef">function</span> (<span style="color:#a6e22e">kibana</span>) {
  <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">new</span> <span style="color:#a6e22e">kibana</span>.<span style="color:#a6e22e">Plugin</span>({
    <span style="color:#a6e22e">require</span><span style="color:#f92672">:</span> [<span style="color:#e6db74">&#39;elasticsearch&#39;</span>],
    <span style="color:#a6e22e">name</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;simple-sample-kibana-plugin&#39;</span>,
    <span style="color:#a6e22e">uiExports</span><span style="color:#f92672">:</span> {

      <span style="color:#a6e22e">app</span><span style="color:#f92672">:</span> {
        <span style="color:#a6e22e">title</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;Simple Sample Kibana Plugin&#39;</span>,
        <span style="color:#a6e22e">description</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;Sample plugin for explaining how to make kibana app&#39;</span>,
        <span style="color:#a6e22e">main</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;plugins/simple-sample-kibana-plugin/app&#39;</span>
      },

...
    },

    <span style="color:#a6e22e">config</span>(<span style="color:#a6e22e">Joi</span>) {
      <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">Joi</span>.<span style="color:#a6e22e">object</span>({
        <span style="color:#a6e22e">enabled</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">Joi</span>.<span style="color:#66d9ef">boolean</span>().<span style="color:#66d9ef">default</span>(<span style="color:#66d9ef">true</span>),
      }).<span style="color:#66d9ef">default</span>();
    },

    <span style="color:#a6e22e">init</span>(<span style="color:#a6e22e">server</span>, <span style="color:#a6e22e">options</span>) {
      <span style="color:#75715e">// Add server routes and initialize the plugin here
</span><span style="color:#75715e"></span>      <span style="color:#a6e22e">exampleRoute</span>(<span style="color:#a6e22e">server</span>);
    }
  });
};

</code></pre></div><h4 id="publicappjs">public/app.js</h4>
<p>画面用のモジュールです。
<code>uiRoutes</code>という機能を使用して、アプリの呼び出しURLを定義します。テンプレートで作成したばかりの場合は、<code>/</code>というURLが追加されるのみです。</p>
<p>実際に画面を表示する際に動くコントローラーの部分はその下の
<code>uiModules.controller</code>に指定してあるfunctionが画面描画の
処理を書く部分になります。
templateで作成したプラグインでは、&ldquo;title&quot;など表示に必要なデータを<code>$scope</code>というオブジェクトに詰め込んでいます。
これはAngularJS(1系)でのモデルオブジェクトになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">import</span> <span style="color:#a6e22e">moment</span> <span style="color:#a6e22e">from</span> <span style="color:#e6db74">&#39;moment&#39;</span>;
<span style="color:#66d9ef">import</span> { <span style="color:#a6e22e">uiModules</span> } <span style="color:#a6e22e">from</span> <span style="color:#e6db74">&#39;ui/modules&#39;</span>;
<span style="color:#66d9ef">import</span> <span style="color:#a6e22e">uiRoutes</span> <span style="color:#a6e22e">from</span> <span style="color:#e6db74">&#39;ui/routes&#39;</span>;

<span style="color:#66d9ef">import</span> <span style="color:#e6db74">&#39;ui/autoload/styles&#39;</span>;
<span style="color:#66d9ef">import</span> <span style="color:#e6db74">&#39;./less/main.less&#39;</span>;
<span style="color:#66d9ef">import</span> <span style="color:#a6e22e">template</span> <span style="color:#a6e22e">from</span> <span style="color:#e6db74">&#39;./templates/index.html&#39;</span>;

<span style="color:#a6e22e">uiRoutes</span>.<span style="color:#a6e22e">enable</span>();
<span style="color:#a6e22e">uiRoutes</span>
  .<span style="color:#a6e22e">when</span>(<span style="color:#e6db74">&#39;/&#39;</span>, {
    <span style="color:#a6e22e">template</span>,
    <span style="color:#a6e22e">resolve</span><span style="color:#f92672">:</span> {
...
    }
  });

<span style="color:#a6e22e">uiModules</span>
  .<span style="color:#a6e22e">get</span>(<span style="color:#e6db74">&#39;app/simple-sample-kibana-plugin&#39;</span>, [])
  .<span style="color:#a6e22e">controller</span>(<span style="color:#e6db74">&#39;simpleSampleKibanaPluginHelloWorld&#39;</span>, <span style="color:#66d9ef">function</span> (<span style="color:#a6e22e">$scope</span>, <span style="color:#a6e22e">$route</span>, <span style="color:#a6e22e">$interval</span>) {
    <span style="color:#a6e22e">$scope</span>.<span style="color:#a6e22e">title</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Simple Sample Kibana Plugin&#39;</span>;
    <span style="color:#a6e22e">$scope</span>.<span style="color:#a6e22e">description</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Sample plugin for explaining how to make kibana app&#39;</span>;
...
    <span style="color:#a6e22e">$scope</span>.<span style="color:#a6e22e">$watch</span>(<span style="color:#e6db74">&#39;$destroy&#39;</span>, <span style="color:#a6e22e">unsubscribe</span>);
  });

</code></pre></div><h4 id="serverroutesexamplejs">server/routes/example.js</h4>
<p><a href="https://hapijs.com">hapi.js</a>というNode.jsのためのサーバーフレームワークです。
このフレームワークをKibanaは使っており、Kibanaのサーバーとブラウザとのやり取りに使用するREST APIを記述するために使用しています。
例えば、Elasticsearchとのやり取りを実際に行うAPIなどをこのREST API内部で記述します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">export</span> <span style="color:#66d9ef">default</span> <span style="color:#66d9ef">function</span> (<span style="color:#a6e22e">server</span>) {

  <span style="color:#a6e22e">server</span>.<span style="color:#a6e22e">route</span>({
    <span style="color:#a6e22e">path</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;/api/simple-sample-kibana-plugin/example&#39;</span>,
    <span style="color:#a6e22e">method</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;GET&#39;</span>,
    <span style="color:#a6e22e">handler</span>(<span style="color:#a6e22e">req</span>, <span style="color:#a6e22e">reply</span>) {
      <span style="color:#a6e22e">reply</span>({ <span style="color:#a6e22e">time</span><span style="color:#f92672">:</span> (<span style="color:#66d9ef">new</span> Date()).<span style="color:#a6e22e">toISOString</span>() });
    }
  });

}
</code></pre></div><p><code>path</code>の部分がブラウザ側からアクセスするURLになります。
実際にElasticsearchとやり取りする処理の書き方については、次回の記事で説明します。</p>
<h2 id="アーキテクチャ簡易版">アーキテクチャ（簡易版）</h2>
<p>ざっくりですが、ファイルやディレクトリについて説明しました。
簡単なデータのやり取りについての流れを説明します。</p>
<p>Kibana自体はNode.jsで実装されサーバーとして動作していますが、ブラウザでアクセスすることで画面を描画しています。
簡単なコンポーネントを並べるとデータのやり取りはこのような形です。</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180420/architecture.jpg" />
    </div>
    <a href="/images/entries/20180420/architecture.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>すごく簡易で大雑把な絵ですが。。。</p>
<p>実際のプラグインとしては大きく、2つの処理があります。</p>
<ul>
<li>ブラウザ上の処理
<ul>
<li>クリックなどのイベント処理</li>
<li>HTMLなどのレンダリング処理</li>
</ul>
</li>
<li>Kibanaサーバー上の処理(Elasticsearchなどとの通信が必要な場合)
<ul>
<li>外部との通信処理</li>
<li>ブラウザ上では重い処理</li>
</ul>
</li>
</ul>
<p>絵に記載しましたが、ブラウザ上の処理についてはAngularJSが主なフレームワークで、サーバー上の処理についてはhapi.jsがフレームワークとなっています。</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、今回はディレクトリ構造とファイルの説明、どういったフレームワークが使われ、データのやり取りがどのように行われているか説明しました。</p>
<p>次回からは、実際に私が作成した<a href="https://github.com/johtani/analyze-api-ui-plugin">Analyze UI</a>を元にElasticsearchとのデータのやり取りなどについて紹介していきます。</p>
</content:encoded>
    </item>
    
    <item>
      <title>カンファレンス情報の探し方（CfP、スポンサー応募、開催期間など）？</title>
      <link>https://blog.johtani.info/blog/2018/03/24/how-to-find-conferences/</link>
      <pubDate>Sat, 24 Mar 2018 13:50:12 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/03/24/how-to-find-conferences/</guid>
      <description>#MANABIYA のブースでセッション時間の合間はお客さん少ないので、ブログを書いてみたり。 ここ数年、スポンサーとして色々なカンファレンスに参加してるんです</description>
      <content:encoded><p>#MANABIYA のブースでセッション時間の合間はお客さん少ないので、ブログを書いてみたり。
ここ数年、スポンサーとして色々なカンファレンスに参加してるんですが、それについてちょっと気なることがあったので。</p>
<!-- more -->
<h2 id="スポンサー情報ってどうやって探してます">スポンサー情報ってどうやって探してます？</h2>
<p>職業柄、カンファレンスにCfP出してセッションしてみたり（落ちること多いけど）、スポンサーとしてブースを出したりしています。</p>
<p>で、疑問があるんですが、みなさんどうやってカンファレンスの情報をゲットしてます？</p>
<p>数年やって来て、検索したりして見つけてスポンサーした結果、
これまでスポンサーしていたので、情報が流れてくるという感じになりました。
それ以外だと、@yusuke さんに教えてもらったりと言うのがあったんですが。。。</p>
<p>自分が知らない、自分のTwitterで流れてこないという情報をどうやったら集められるかな？と言うのが今の課題になってます。
と言うことで、ブログを書いてみました。</p>
<h2 id="カンファレンスを検索できるサイトとか便利">カンファレンスを検索できるサイトとか便利？</h2>
<p>で、私の会社は検索エンジンの会社なんで、検索できると便利では？と考えるんです。</p>
<p>CfPの応募期間で絞り込みできたり、場所で検索できたり、スポンサー情報を取得する方法が載ってたりすると便利なのではないかなと。
便利に思うのが自分だけでは？と言うのがあるんですが。。。</p>
<p>あると嬉しい人、データを掲載したいなと言う人いますでしょうか？
Google Formなどで入力してもらえるようにして何か作るのもありかなぁと考えているところです。
それとも、スポンサーを募っているカンファレンスのスタッフや企業の人は、こういう情報は特定の人にだけ知ってもらったりしたいでしょうか？</p>
<p>自分だけではわからないことが多いので、懸念事項や、それダメでしょ？
あると便利！などのフィードバックをいただけると助かります。</p>
<p>それ以外に、カンファレンス自体の場所、開催日程、サイトへのURLなどが検索できると便利だったりするかも？というのもあります。</p>
<p>とりあえず、自分が知ってるものだけ、個人的に検索できるようにしたりするのがいいかなぁ。。。
コメントお待ちしてます！</p>
</content:encoded>
    </item>
    
    <item>
      <title>第1章の05から06までやってみた（言語処理100本ノック）</title>
      <link>https://blog.johtani.info/blog/2018/03/20/nlp100-ch01-05to06/</link>
      <pubDate>Tue, 20 Mar 2018 21:34:45 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/03/20/nlp100-ch01-05to06/</guid>
      <description>Rustで言語処理100本ノックの続きで、05と06です。 05. n-gram 問題はこちら。 みんな大好きn-gramです。単語と文字があるので、それぞれ別関</description>
      <content:encoded><p>Rustで言語処理100本ノックの続きで、05と06です。</p>
<!-- more -->
<h3 id="05-n-gram">05. n-gram</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec05">こちら</a>。</p>
<p>みんな大好きn-gramです。単語と文字があるので、それぞれ別関数として実装しました。問題はbi-gramと<code>n=2</code>だったのですが、一応、<code>n</code>を引数に取る形にして実装しました。</p>
<p>まずは、単語です。</p>
<script src="http://gist-it.appspot.com/https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs?slice=128:139"></script>
<p>前に実装した時は、自分で頑張って、先頭から数えたりしてたんですが、Rustには<a href="https://rust-lang-ja.github.io/the-rust-programming-language-ja/1.6/std/primitive.slice.html#method.windows">windows(n)</a>という便利なメソッドがsliceにあり、これを利用したらこんな簡単になりました。
sliceは特定のシーケンス（配列）に対してある特定のサイズのViewを作ってくれます（説明あってる？）。
ということで、文字列から、単語の配列（スペース区切りで単語にしている）を作り出して、<code>windows(n)</code>メソッドを通すと、
<code>n</code>で指定した数字の個数だけの単語の配列を先頭から、1単語ずつずらして作ってくれます。まさに、n-gram!
戻り値は配列の配列です。
1点だけ疑問点があるのは、「空白で区切ったものが単語」という考え方で良いかどうか？という点です。特に問題文にはそれが明示されていなかったので、このような前提を置いてあります。</p>
<p><code>invalid_n(text, n)</code>は<code>n</code>の値や入力された文字列をチェックする関数です。入力チェックですね。<code>n</code>が1よりも小さい場合、入力文字列が空文字の場合は、warningでメッセージを出して、空の配列を返す仕組みになっています。</p>
<script src="http://gist-it.appspot.com/https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs?slice=115:126"></script>
<p>次は、文字です。</p>
<script src="http://gist-it.appspot.com/https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs?slice=128:139"></script>
<p>単語とほぼ一緒ですが、入力文字列を、1文字ずつの配列にしているところが異なります。
また、<code>windows</code>メソッドで取り出された、1文字ずつの<code>n</code>個の配列を文字列に修正してから、結果の配列に入れています。
ここでも疑問は空白をどう扱うか？になります。
現時点では、空白も1文字とカウントして扱うことにしてあります。
どっちがいいのかなぁ？</p>
<h3 id="06-集合">06. 集合</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec06">こちら</a>。</p>
<p>まずは、文字n-gramで出てきた文字列をSetに入れる関数から。</p>
<script src="http://gist-it.appspot.com/https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs?slice=159:174"></script>
<p>n-gramの問題で実装した文字n-gramの関数の戻り値を配列ではなく、BTreeSetに変えたものになります。比較などがしやすいように？と思い、BTreeSetを利用していますが、実装としてはHashSetでも問題ないかと。
この関数の集合（Set）を元に、和集合、積集合、差集合を求める関数を実装しました。</p>
<script src="http://gist-it.appspot.com/https://github.com/johtani/nlp100-rust/blob/5439f96e8521bffe34689c23aa076eb3a2fe817b/src/chapter01/answer.rs?slice=176:187"></script>
<p>Setのメソッドとして、それぞれ、<code>union</code>＝和集合、<code>intersection</code>＝積集合、<code>difference</code>＝差集合のメソッドが用意されているので、特に困ることはなかったです。
差集合については、1-2と2-1で結果が異なるはずなので、それぞれをテストケース、main.rsで出力するようにしてあります。</p>
<h3 id="所感">所感</h3>
<p>今回は、Rustがすでに実装してくれているメソッドがあったので楽ができました。
やりたいことに相当するメソッドがあるかどうかを調べるためにリファレンスを探さないといけないのがちょっと苦労しましたが。。。
ということで、今日はこの辺りまで。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第1章の03から04までやってみた（言語処理100本ノック）</title>
      <link>https://blog.johtani.info/blog/2018/02/19/nlp100-ch01-03to04/</link>
      <pubDate>Mon, 19 Feb 2018 18:34:08 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/02/19/nlp100-ch01-03to04/</guid>
      <description>Rustで言語処理100本ノックの続きで、03と04です。 03. 円周率 問題はこちら。 入力文字列を.split_whitespace()で分割して</description>
      <content:encoded><p>Rustで言語処理100本ノックの続きで、03と04です。</p>
<!-- more -->
<h3 id="03-円周率">03. 円周率</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec03">こちら</a>。</p>
<script src="http://gist-it.appspot.com/http://github.com/johtani/nlp100-rust/blob/8ca508cc16fbc7a11ac04bbc43687dfec4f25483/src/chapter01/answer/mod.rs?slice=60:71"></script>
<p>入力文字列を<code>.split_whitespace()</code>で分割しておいて、単語ごとのベクタを作り出し、そこに対して文字を数えました。「アルファベットの」という注意書きがあるので&rdquo;,&ldquo;や&rdquo;.&ldquo;は含めずに数えるのかなということで、
charの<code>.is_alphabetic()</code>で<code>A-z</code>までの判定をしつつ、文字のベクタを作ってから、そのベクタの長さを詰め込むという感じでやりました。</p>
<p>これ、ひょっとして、collectでベクタにしなくても、i32とかの変数でカウントするとベクタ作らなくてもいいなじゃにか？というのに書きながら気づいた。。。
必要じゃないオブジェクトを作ってるよなぁ。</p>
<p><code>.filter().map</code>とかかな？この辺りの操作がイマイチ苦手。Javaでもまだ馴染めてないところなんだよなぁ。頭固すぎ。</p>
<h3 id="04-元素記号">04. 元素記号</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec04">こちら</a>。</p>
<script src="http://gist-it.appspot.com/http://github.com/johtani/nlp100-rust/blob/8ca508cc16fbc7a11ac04bbc43687dfec4f25483/src/chapter01/answer/mod.rs?slice=73:112"></script>
<p>大作ですね。何だろう、大作。。。
最終的に連想配列（辞書型もしくはマップ型）」ということだったので、BTreeMapに詰め込んでます。
HashMapでもいいんですが、文字列で出力した時にキーが並んで見やすいからという理由で、BTreeMap使いました。それ以上の理由はないです。普通にやるなら、HashMapかな？</p>
<p>入力として、1文字だけの出力をする場所（インデックス番号）の配列を受け取ってます。1点だけ、チェックしていない、けど入力値の想定をしていて、<code>idx_one_symbols</code>がソートされていて、小さいものから順番に出てくるものとしてます。関数作って、チェックすべきかな？</p>
<p>で、指定された場所の最後のものが入力文字列よりも大きいかどうかというチェックもしています。（あー、テストケース書いてないな）この辺りのせいでちょっと長めになってます。</p>
<p>単語の配列を作るのは03の時と同じやり方です。
回しかたがちょっと違って、<code>.iter().enumerate()</code>で回して、添字と値をタプル？でとりだしてます。添字を見ながら1文字取り出すのか、2文字取り出すのかの判断が必要だからです。あとは一緒ですね。1文字取り出すときは、<code>.first()</code>を使って見ました。
実は、2文字取り出す時と、1文字の時と同じロジック使った方が共通化できて、短くなった？？？</p>
<p>ということで、こんな感じでした。いつものようにツッコミお待ちしてます。</p>
<h3 id="所感">所感</h3>
<p>問題それぞれについてではなく、
やってて思ったのですが、問題に対して想定される結果が記載されていると嬉しいなと思いました。
ロジックについては、各自実装者に寄ったり、言語によって違いが出たりするし、議論するベースになっていいかなと思うんですが、
問題で想定されている結果（出力）があると、自分の実装にケアが足りないところがないのか？とか、ケアしなくていい点とかがわかるのかもなぁと。
ユニットテスト相当のものがあると楽かなぁと。</p>
<p>このケースどうするんだろ？みたいなのが、ところどころコメントに残ったりしてます。
出題の意図としては、その部分も議論の対象ということなのかな？</p>
</content:encoded>
    </item>
    
    <item>
      <title>第1章の00から02までやってみた（言語処理100本ノック）</title>
      <link>https://blog.johtani.info/blog/2018/02/15/nlp100-ch01-00to02/</link>
      <pubDate>Thu, 15 Feb 2018 21:59:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/02/15/nlp100-ch01-00to02/</guid>
      <description>「鉄は熱いうちに打て」ということで、言語処理100本ノックの第1章の00から02を実装してみました。 さて、これが効率がいいのかどうかはさてお</description>
      <content:encoded><p>「鉄は熱いうちに打て」ということで、言語処理100本ノックの第1章の00から02を実装してみました。</p>
<!-- more -->
<p>さて、これが効率がいいのかどうかはさておき。</p>
<h3 id="00-文字列の逆順">00. 文字列の逆順</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec00">こちら</a>。</p>
<p>最初、<a href="https://github.com/johtani/nlp100-rust/blob/48567d107511541d0401af4ef58c6abb98a6083b/src/chapter01/answer.rs">Vecのreverse()で逆順にして0からlen()まで回してた</a>んですが、pop()がいい感じに後ろから取れることがわかったんで、切り替えました。
シンプルかな？</p>
<script src="http://gist-it.appspot.com/http://github.com/johtani/nlp100-rust/blob/ccbdf272adbb5aa738dc73e78e3667e4300a49c1/src/chapter01/answer.rs?slice=2:12"></script>
<h3 id="01-パタトクカシーー">01. 「パタトクカシーー」</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec01">こちら</a>。</p>
<script src="http://gist-it.appspot.com/http://github.com/johtani/nlp100-rust/blob/ccbdf272adbb5aa738dc73e78e3667e4300a49c1/src/chapter01/answer.rs?slice=14:24"></script>
<p>1文字ずつ取り出して、インデックスの番号が2で割ってあまりが0なら文字列に追加していくってのでやってみました。
（ブログ書いてるところで、<code>i in 0..char_array.len()</code>じゃなくて、<code>(i, x) in char_array.iter().enumerate()</code>に切り替えました。）
matchとか使って綺麗に書けたりするのかなぁ？</p>
<h3 id="02-パトカータクシーパタトクカシーー">02. 「パトカー」＋「タクシー」＝「パタトクカシーー」</h3>
<p>問題は<a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec02">こちら</a>。</p>
<script src="http://gist-it.appspot.com/http://github.com/johtani/nlp100-rust/blob/ccbdf272adbb5aa738dc73e78e3667e4300a49c1/src/chapter01/answer.rs?slice=26:56"></script>
<p>だいぶ思考錯誤してる感じがソースに現れてます。
とりあえず、両方の文字列をcharsの配列にして個々のイテレータを回しながら、next()の戻り値があれば追加していく感じにして、
終了条件が両方Noneを通ったらにしてるけど、、、
なんか、もっと綺麗にできないのかなぁ。。。
next()のタプル返す関数作って、とかでなんかできたりするかなぁ？</p>
<h3 id="gist-it">gist-it</h3>
<p>関係ないですが、GitHubのコードを貼り付けるのに便利なサービスがあるみたいです。</p>
<p><a href="http://gist-it.appspot.com">http://gist-it.appspot.com</a></p>
<p>これほんと便利だな。行数指定もできるし。
説明するのが簡単だ。</p>
<p>とりあえず、今日はこの辺まで。なんか、いい知恵あれば教えてください！</p>
</content:encoded>
    </item>
    
    <item>
      <title>言語処理100本ノックはじめました(Rust)</title>
      <link>https://blog.johtani.info/blog/2018/02/14/start-nlp100-with-rust/</link>
      <pubDate>Wed, 14 Feb 2018 19:52:31 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/02/14/start-nlp100-with-rust/</guid>
      <description>ども。新しいもの始めないと頭が退化する。。。ということで、こちら（ happy new year and new language - katsyoshiのめもみたいなもの）のブログに触発されて、</description>
      <content:encoded><p>ども。新しいもの始めないと頭が退化する。。。ということで、こちら（
<a href="http://blog.katsyoshi.org/blog/2018/01/16/happy-new-year-and-new-language/">happy new year and new language - katsyoshiのめもみたいなもの</a>）のブログに触発されて、言語処理100本ノックをはじめてみました。</p>
<!-- more -->
<p><a href="http://www.cl.ecei.tohoku.ac.jp/nlp100/#ch1">言語処理100本ノック</a>とは、自然言語処理になるのかな、東北大学の研究室の先生が公開している言語処理に関する実践的な課題をベースにプログラミングなどのスキルを学んでいくための問題集です。
元々はPythonを対象とされているようですが、Rustでやってみようかと。
まぁ、先ほどあげたブログの二番煎じです。。。
ちなみに、インスパイアされた元のブログの方はRust book 2nd editionを読み終えたらしいですが、私はかじった程度です（ダメかも？）。</p>
<p>NLPもRustもかじった程度なので、苦戦しそうですが、ちょっとずつやっていこうかなと。
ということで、準備運動の第1章から始めようかと。
GitHubにちょっとずつあげていく予定です。
<a href="https://github.com/johtani/nlp100-rust">https://github.com/johtani/nlp100-rust</a></p>
<p>まぁ、まずは宣言のブログを書いてみただけです。
続いてなかったら、叱咤激励してください。叱咤だけかも？</p>
</content:encoded>
    </item>
    
    <item>
      <title>Analyze UIとKibanaのプラグインの作成方法（第2回）</title>
      <link>https://blog.johtani.info/blog/2018/02/09/getting-started-template-kibana-plugin/</link>
      <pubDate>Fri, 09 Feb 2018 18:17:37 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/02/09/getting-started-template-kibana-plugin/</guid>
      <description>第1回では、Analyze UIというプラグインの紹介をしました、ごく簡単にですが。 第2回では、Kibanaのプラグインの作成方法を順を追って</description>
      <content:encoded><p>第1回では、<a href="https://github.com/johtani/analyze-api-ui-plugin">Analyze UIというプラグイン</a>の紹介をしました、ごく簡単にですが。</p>
<p>第2回では、Kibanaのプラグインの作成方法を順を追って見ていこうと思います。今回は、プラグインのプロジェクトの作り方を説明します。
どんなファイルがあるのかなどについては第3回で説明します（2018/02月現在の方法になります。残念ながら、Kibanaのプラグイン作成自体はまだExperimentalな話になっていますので、変更がある可能性があります）。</p>
<!-- more -->
<p>実はそれほど難しいというわけではありません。Kibanaのプラグインを作成するためのテンプレートが用意されています。<a href="https://github.com/elastic/template-kibana-plugin/">template-kiban-plugin</a>です。
テンプレートのリポジトリのREADMEに作業手順の記載があります。</p>
<ol>
<li>KibanaのリポジトリをClone、Checkout</li>
<li>Node.jsの環境を用意する</li>
<li>Kibanaを起動できるようにする</li>
<li>SAOのインストール</li>
<li>テンプレートによるプロジェクトファイルの生成</li>
</ol>
<p>順を追って説明します。
<code>PLUGIN_DEV_DIR</code>というディレクトリ配下で作業をしている想定になります。</p>
<h2 id="1-kibanaのリポジトリをclonecheckout">1. KibanaのリポジトリをClone、Checkout</h2>
<p>開発環境として、Kibanaが必要です。Kibanaのプラグインを作るので。
手順などは<a href="https://github.com/elastic/kibana/blob/6.2/CONTRIBUTING.md#contributing-code">KibanaのCONTRIBUTING.md</a>に記載があります。
ということで、まずはKibanaのリポジトリをCloneします。</p>
<pre><code>cd PLUGIN_DEV_DIR
git clone git@github.com:elastic/kibana.git
</code></pre><p>このままだと、masterブランチなので、開発したい対象のKibanaのバージョンのブランチもしくはタグをcloneします。今回は6.2.1向けということで、次のようになります。</p>
<pre><code>git checkout v6.2.1
</code></pre><p>これで、ソースが6.2.1向けになりました。</p>
<h2 id="2-nodejsの環境を用意する">2. Node.jsの環境を用意する</h2>
<p>Node.jsをインストールします。
Kibanaのリポジトリに<code>.node-version</code>というファイルがあります。
こちらにNode.jsのバージョンが記載されています。
Kibanaが使用しているNode.jsを利用できるようにします。ローカルでは<a href="https://github.com/creationix/nvm#installation">nvm</a>利用してインストールしました。後から、切り替えが可能だからです。
nvm自体のインストールについては<a href="https://github.com/creationix/nvm">nvmのサイト</a>をご覧ください。
nvmがインストールできたら、次のコマンドで、Kibanaが使用しているバージョンをインストールします。</p>
<pre><code>cd kibana
nvm install &quot;$(cat .node-version)&quot;
</code></pre><p>すでにnvmを利用している場合などは、Kibana起動時にKibanaのバージョンに合わせたNode.jsに切り替えるようにしてください。</p>
<h2 id="3-kibanaを起動できるようにする">3. Kibanaを起動できるようにする</h2>
<p>Kibanaでは<a href="https://yarnpkg.com/lang/en/">yarn</a>というjavascript向けのパッケージマネージャーを利用して起動やビルドなどを行います。まずはyarnをインストールします。<a href="https://github.com/elastic/kibana/pull/15485">最近npmからyarnに切り替えた</a>ようです。
私はMacだったので、<a href="https://yarnpkg.com/en/docs/install">brewでインストール</a>しました。
インストールできたら、次のコマンドを実行します。</p>
<pre><code>yarn
</code></pre><p>これにより、package.jsonから必要なライブラリなどをダウンロードして来てくれます。
問題なければ「✨  Done in 439.30s.」というような表示がされます（結構時間かかりますね）。
では、Kibanaを起動できるか確認してみましょう。
さらに、Elasticsearchも起動してみます。
Kibanaのpackage.jsonの中にはElasticsearchを起動するためのスクリプトも用意されています。実際には<a href="https://gruntjs.com">grunt</a>を利用してタスクを実行しているようです。Elasticsearchの起動にはJavaが必要になります。
今回は6.2.1なので、JDK 8以降がインストールされている必要があります。
こちらはインストールされているものとします。</p>
<pre><code>yarn elasticsearch
</code></pre><p>で起動できます。</p>
<pre><code>&gt;&gt; Started 1 Elasticsearch nodes.
</code></pre><p>という表示が出てればOKです。
次にKibanaです。別のTerminalを起動して、以下のコマンドで起動できます。</p>
<pre><code>yarn start
</code></pre><p>これだけです。</p>
<pre><code>server    log   [06:58:56.930] [info][listening] Server running at http://localhost:5603
</code></pre><p>この辺りが出てればKibanaのServerは起動済みです。また、Elasticsearchに接続できていれば、次のログが出ているはずです。</p>
<pre><code>server    log   [07:02:18.010] [info][status][plugin:elasticsearch@6.2.1] Status changed from red to green - Ready
</code></pre><p>Elasticsearch接続用のKibanaのプラグインの状態になります。
これで、Kibanaの環境が整ったことが確認できました。
もちろん、Elasticsearchに関しては、yarnで起動せずに、tar.gzなどでダウンロードして来たElasticsearchを起動しておき、アクセスするといったことも可能です。プラグインなどをElasticsearchにもいれてテストしたい場合などはそちらの方が便利かもしれません。</p>
<h2 id="4-saoのインストール">4. SAOのインストール</h2>
<p>では、一度、ElasticsearchとKibanaを停止しましょう。フォワグラウンドで起動しているので、それぞれのTerminalでCtrl+Cで停止できます。
Kibanaのプラグイン作成むけに、テンプレートが作られています。<a href="https://sao.js.org">sao.js</a>というGitHubのリポジトリやnpmのパッケージをテンプレートとして使うことができるツールを利用してプラグインのプロジェクト（リポジトリ）を作成します。
実際にテンプレートとなるリポジトリは<a href="https://github.com/elastic/template-kibana-plugin">template-kibana-plugin</a>になります。
まずはSaoのインストールです。</p>
<pre><code>npm install -g sao
</code></pre><p>プラグインのテンプレートのページには上記のようにnpmを利用したインストール方法になっていますが、次のようにyarnでも可能です。</p>
<pre><code>yarn global add sao
</code></pre><p>これで、saoがインストールできました。</p>
<h2 id="5-テンプレートによるプロジェクトファイルの生成">5. テンプレートによるプロジェクトファイルの生成</h2>
<p>あとは、テンプレートを元にプロジェクトを作成します。
<code>PLUGIN_DEV_DIR</code>ディレクトリ配下に、kibanaと同じ階層で作成するプラグイン用のディレクトリを作成します。</p>
<pre><code>mkdir simple-sample-kibana-plugin
</code></pre><p>以下のような構成になります。</p>
<pre><code>kibana                      simple-sample-kibana-plugin
</code></pre><p>次にテンプレートを適用していきます。</p>
<pre><code>cd simple-sample-kibana-plugin
sao kibana-plugin@7.2.4
</code></pre><p>2行目がsaoを利用してプロジェクトを作成しているコマンドになります。
すると、次のような質問が出て来ます。
これらに答えるとプロジェクトに必要なファイル（package.jsonやREADME.mdなど）に入力した情報を適用したものを作ってくれます。</p>
<pre><code>? Name of your plugin?
? Provide a short description
? What Kibana version are you targeting?
? Should an app component be generated?
? Should translation files be generated?
? Should an hack component be generated?
? Should a server API be generated?
</code></pre><p>実際に答えた内容はこちら。</p>
<pre><code>? Name of your plugin? simple-sample-kibana-plugin
? Provide a short description Sample plugin for explaining how to make kibana app
? What Kibana version are you targeting? 6.2.1
? Should an app component be generated? Yes
? Should translation files be generated? Yes
? Should an hack component be generated? Yes
? Should a server API be generated? Yes
</code></pre><p>プラグインの名前などは、ディレクトリ名と同じものを入力補完してくれているので、そのままEnterでもOKです。
Descriptionについてはわかりやすいものを入力しましょう。
バージョンは、先ほどのKibanaのリポジトリに合わせて、<code>6.2.1</code>にしてあります。
あとは、作るプラグインの種類に応じて、必要なコンポーネントを作るかどうかの質問にYes/Noで答えます。
今回はサンプルの説明ということもあるので、全てYesで答えました。
ちなみに、私が実際に作成した<a href="https://github.com/johtani/analyze-api-ui-plugin/">analyze-api-ui-plugin</a>では、<code>app</code>と<code>translation</code>と<code>server</code>の3つを作成しました。
ただし、<code>translation</code>については現在はテンプレートで作成したままのファイルが入っており、実際には利用してないです。</p>
<p>完了したら、プラグインのサンプル入りのプロジェクトが完成です。
もう一度、Elasticsearchを立ち上げて、プラグインのプロジェクトからKibanaを起動してアクセスしてみます。まずは、<code>PLUGIN_DEV_DIR/kibana</code>ディレクトリの下で、Elasticsearchを起動します。</p>
<pre><code>yarn elasticsearch
</code></pre><p>次に、<code>PLUGIN_DEV_DIR/simple-sample-kibana-plugin</code>ディレクトリの下で、以下のコマンドを実行し、プラグインが入った状態のKibanaを起動します。</p>
<pre><code>yarn start
</code></pre><p>問題なく起動すれば、ブラウザでアクセスすると次のような画面が表示されるはずです。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:800">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180214/sample_start.jpg" />
    </div>
    <a href="/images/entries/20180214/sample_start.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>左側にメニューが1つ増えています。
クリックすると、上記画像のような画面が表示されるはずです。</p>
<p>これで、カスタムプラグインの開発ができる環境ができました！
次回は、プロジェクトのディレクトリ構成や、どんなツールが内部で使用されてデータのやり取りが行われているかについて説明します。お楽しみに。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Analyze UIとKibanaのプラグインの作成方法（第1回）</title>
      <link>https://blog.johtani.info/blog/2018/01/19/how-to-make-kibana-plugin-example-analysis-ui/</link>
      <pubDate>Fri, 19 Jan 2018 15:36:46 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2018/01/19/how-to-make-kibana-plugin-example-analysis-ui/</guid>
      <description>あけましておめでとうございます。今年はサボりがちだったブログをちょっとずつ復活させようかと。 ということで、第1弾として、昨年少し作っていたK</description>
      <content:encoded><p>あけましておめでとうございます。今年はサボりがちだったブログをちょっとずつ復活させようかと。
ということで、第1弾として、昨年少し作っていたKibanaのプラグインを何度かに分けて紹介したいと思います。</p>
<p>今回は<a href="https://github.com/johtani/analyze-api-ui-plugin">Analyze UIというプラグイン</a>の紹介です。</p>
<!-- more -->
<p>今回はインストール方法と簡単な機能紹介です。
細かな紹介は個別にやりたいと思います。</p>
<h3 id="analyze-ui-pluginとは">Analyze UI pluginとは？</h3>
<p>Elasticsearchの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html"><code>_analyze</code>というAPI</a>(個人的に好きなAPIです)をご存知でしょうか？</p>
<p>Elasticsearchは全文検索エンジンで、データの検索には<a href="https://ja.wikipedia.org/wiki/%E8%BB%A2%E7%BD%AE%E3%82%A4%E3%83%B3%E3%83%87%E3%83%83%E3%82%AF%E3%82%B9">転置インデックス</a>というものを使用します。
Elasticsearchにデータを登録する際に、<code>text</code>型のデータの場合、この転置インデックスのキーとなる単語を決める処理のことを<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html">Analysis</a>と呼びます（Analysisの詳細については割愛します。後日説明するかも？）。
このAnalysisの処理が、入力されたデータの文字列に対してどのように行われて、結果としてどんな単語がキーとして用いられているかを確認できる機能が<code>_analyze</code> APIです。検索で単語がうまくヒットしないな？とか、なんで、こんなので検索結果に出てくるんだ？といった場合、このAPIを利用すると、どのような単語で転置インデックスが作られているかがわかるので、検索にヒットしない/する理由を見つけることができます。</p>
<p>Elasticsearchの便利な点はRESTfulなAPI＋JSONでやりとりができる点なのですが、<code>_analyze</code> APIの結果をJSONで受け取っても、見るのにちょっと苦労します。。。こんな感じ。</p>
<p>リクエスト：</p>
<pre><code>POST _analyze
{
  &quot;analyzer&quot;: &quot;kuromoji&quot;,
  &quot;text&quot;: &quot;今年はブログをいっぱい書きますよ！&quot;
}
</code></pre><p>レスポンス：</p>
<pre><code>{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;今年&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;word&quot;,
      &quot;position&quot;: 0
    },
    {
      &quot;token&quot;: &quot;ブログ&quot;,
      &quot;start_offset&quot;: 3,
      &quot;end_offset&quot;: 6,
      &quot;type&quot;: &quot;word&quot;,
      &quot;position&quot;: 2
    },
    {
      &quot;token&quot;: &quot;いっぱい&quot;,
      &quot;start_offset&quot;: 7,
      &quot;end_offset&quot;: 11,
      &quot;type&quot;: &quot;word&quot;,
      &quot;position&quot;: 4
    },
    {
      &quot;token&quot;: &quot;書く&quot;,
      &quot;start_offset&quot;: 11,
      &quot;end_offset&quot;: 13,
      &quot;type&quot;: &quot;word&quot;,
      &quot;position&quot;: 5
    }
  ]
}
</code></pre><p>このくらいの量であればまだなんとかなりますが、文章が長くなると辛いですよね。</p>
<p>ということで、GUIがあると便利だろうなぁと。で、作ってみましたというのが今日紹介するKibana用のAnalyze UIプラグインです。
こんな感じで、Kibanaのアプリの一部として動作しブラウザ上で、入力テキストの文字列がどのようにanalyzeされて、単語になるかがわかります。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:800">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180119/analyze_ui_sample_1.jpg" />
    </div>
    <a href="/images/entries/20180119/analyze_ui_sample_1.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>（先ほどのAPIのサンプルと同じものを画面で入力した結果になります）。</p>
<h3 id="インストール方法">インストール方法</h3>
<p>現時点の最新版Kibana（6.1.2）に対応しています。
Kibanaのディレクトリで<code>kibana-plugin</code>コマンドを利用してインストールします。</p>
<pre><code>./bin/kibana-plugin install https://github.com/johtani/analyze-api-ui-plugin/releases/download/6.1.2/analyze-api-ui-plugin-6.1.2.zip
</code></pre><p>これだけです。
で、Kibanaを起動していただくと、左のメニューに「Analyze UI」という項目が増えています。</p>


<div class="box" style="max-width:400">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180119/kibana_menu.jpg" />
    </div>
    <a href="/images/entries/20180119/kibana_menu.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>クリックすると、Analyze UIが表示されます。</p>
<p>初期画面は入力された文字を特定のAnalyzerで処理した場合の結果を見るための画面です。綱目の説明は画像をご覧ください。</p>


<div class="box" style="max-width:800">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20180119/intro_analyze_ui_1.jpg" />
    </div>
    <a href="/images/entries/20180119/intro_analyze_ui_1.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>先ほどのJSONよりは見やすくなったかと思います。
そのほかにもいくつか画面や機能があるのですが、今日はこの辺りで。
「<code>_analyze</code> API便利なんだけど、JSONは。。。」とか「検索うまくできないなぁなんでだろう？」と思っている方は、ぜひ試して見ていただければと。
問題点などありましたら、GitHubのIssueを登録してください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2017）</title>
      <link>https://blog.johtani.info/blog/2017/12/31/looking-back-2017/</link>
      <pubDate>Sun, 31 Dec 2017 21:30:43 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2017/12/31/looking-back-2017/</guid>
      <description>今年は紅白を見ながら書いてます。 今年はいろんなところに出張に行ったなぁ。 振り返り（2016年に書いた抱負から） まずは去年の抱負を元に。 もちろ</description>
      <content:encoded><p>今年は紅白を見ながら書いてます。
今年はいろんなところに出張に行ったなぁ。</p>
<!-- more -->
<h2 id="振り返り2016年に書いた抱負から">振り返り（2016年に書いた抱負から）</h2>
<p>まずは去年の抱負を元に。</p>
<h5 id="もちろん英語の継続">もちろん英語の継続</h5>
<p>もちろん継続してます。英会話しながら海外TVドラマや映画を見てます。
ちょっとずつ英語字幕でも見ようとしてますが、まだまだだなぁ。
あと、TOEICを受けてないんで、それは受けないと。
見たドラマはこの辺です。</p>
<ul>
<li>ER</li>
<li>ウォーキング・デッド</li>
<li>Agent of Shield</li>
<li>SUITS</li>
<li>Mr.Robot(いまいち話の展開についていけなかった)</li>
</ul>
<h5 id="継続的にイベントに登壇">継続的にイベントに登壇</h5>
<p>OSCに今年もブース出してセッション持ってました。
勉強会ももちろんやりました。
あと、外部のカンファレンス（FOSS4G、BigData Analytics、db tech showcaseなど）や
勉強会(monitoring勉強会やCA.ioなど)にも登壇させていただきました。
もっと外部から読んでいただいたり、CfPに応募して通過できるようにしないとなぁ。</p>
<h5 id="もっと開発ブログ">もっと開発＆ブログ</h5>
<p>ブログはすみません。。。来年頑張ります。。。
ブログの代わりに<a href="https://www.amazon.co.jp/dp/477419218X?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=477419218X&amp;adid=1MB8R50WWEJ3JC8VXN60&amp;">書籍</a>を出せました。データ分析基盤系の書籍になるので、興味のある方はのぞいて見ていただければ。</p>
<p>開発はちょっとやってます。今年後半はKibanaのプラグインとかをちょっと書いてました。
Ingest-CSVもちょこちょこバージョンアップに追従してたりします。</p>
<h5 id="サポートエンジニア獲得">サポートエンジニア獲得！</h5>
<p>獲得しました！（自分の成果かどうかはわかりませんが。。。）
今は3名体制でサポートしてもらってます。おかげさまでだいぶサポートする機会が減ってきました。
開発は<a href="https://www.elastic.co/about/careers">世界各地どこでも募集中</a>だったりしますので、ぜひ連絡いただければと。</p>
<h5 id="個人的な検索勉強会の再開">個人的な検索勉強会の再開</h5>
<p>再開しました。<a href="http://amzn.to/2CmYcw9">検索エンジン自作入門</a>を知人と隔週くらいで読んでます。読んでるだけではあれなんで、実装もしてたり。全然違う言語でトライして見ようと思ったのでRustで書き始めてます。まずは、単純なところからと思ったんですが、
Rustの書き方の違いにだいぶ戸惑ってます。。。</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<p>ここからは今年の出来事を。</p>
<ul>
<li>初アメリカで釣り</li>
<li>Nintendo Switch!</li>
<li>初Japanチームオフサイト</li>
<li>チーム変更</li>
<li>初Berlin Buzzword!</li>
<li>母校（大学）訪問</li>
<li>Kibanaプラグイン開発</li>
</ul>
<p>今年も初モノがちらほらとありますね。
今年もカレンダー振り返ってたら「え？これ今年だったの？」ってのがちらほらありました。。。</p>
<p>アメリカで釣りやりました。自社のカンファレンスElastic{ON}のあとに、エンジニアが集まるミーティングがあるんですが、そこで1日Fun Dayがあって、そこで釣りをやりました。
魚群探知機とかで魚探すんですね、最近のは。2、3匹釣ったので非常に楽しかったです。
来年はスノーシューにチャレンジする予定。</p>
<p>Nintendo Switch買って、ゼルダやりました。今はイカやってます。
イカ欲はちょっと減ってるかもなぁ、1に比べると。
あとは、子供に買ったマリオやったりしてるかな。</p>
<p>ここ10年くらい行きたいと思っていたカンファレンス、<a href="https://berlinbuzzwords.de">Berlin Buzzword</a>に行ってきました！
これが今年一番嬉しかった出来事です。Luceneコミッターの方々にも会えたし。特にPoliceman JenkinsのUweさんに会えたのが本当に感激でした。。。
来年も行けるといいなぁ。。。</p>
<p>今年も出張が多めだったのであちこち飛び回ってたなぁ。
そんな飛び回っている中で、故郷である広島（出身地？）に出張で行ったので、母校に顔を出してきました。
変わってなかった。。。自分が一期生なので、自分が入学した時はまだクレーンとか立ってるような新校舎だったんだけど、それが20年くらい経て、だいぶ古くなってました。。。
広島市内は港近くに高速道路とか出ててだいぶ様変わりしてました。
ただ、小学校も高校も変わってなかったなぁ。久々に高校の同級生にも会えたし。
また、機会があればぜひ行きたいなと。帰省する場所ではなくなったので、行く機会がないんですよね。。。</p>
<p>開発もちょっとやってます。Kibanaのプラグインに挑戦して見てます。
いまだにJavaScriptは苦手なんですが。。。
ちょっとずつ改良して、ひょっとすると本体とかに取り込めるかな？
あとは、今年始めたRustをちょっとずつ継続して行きたいなと。</p>
<h2 id="来年の抱負">来年の抱負</h2>
<p>最後は来年の抱負を。</p>
<ul>
<li>もっと英語の継続＆TOEIC</li>
<li>継続的にイベントに登壇</li>
<li>CfPもっと出すぞ！</li>
<li>もっとブログ！</li>
<li>雑誌やWeb系雑誌で記事を。</li>
<li>コミュニティを別の方法で盛り上げ</li>
<li>Elasticsearchなど検索系の開発にも参加</li>
</ul>
<p>英語はもちろんですね。今年は12月の自社イベントに合わせて多くのAPJチームの人が来て、
チームで会話できたし、もっと英語やらないとなぁと。前よりもちょっとは喋れるようになった気がしてます（気がしてるだけで、喋れてるかどうかは不明）。あとは、TOEIC受けて、実力チェックしてみないとなぁ。</p>
<p>イベント登壇はまぁ、DevRelですから、一応。
来年もとりあえず、3月までは毎月どこかでブース出してます。
Ask Me Anything的なブースの出し方を今年はしてみようかなと思ってるので、
ちょっと使い始めたけど、この辺よくわからないと思ってる方、ぜひ質問しに来てください。
ブース出すだけじゃなくて、スピーカーとしていろんな場所で喋れるようにしないとなぁと。
もっと幅広く知ってもらえるように雑誌とかで連載できるようにしたいなぁと思ってます。
また、日本語の入り口の情報を増やすためにブログも書かないとなぁ。</p>
<p>勉強会も継続しますが、別の方法でも盛り上げて行きたいなぁと。
勉強会はどうしても聞く人が多くて、ユーザーの間での交流や情報交換とまでは行ってない気がするし、<a href="http://discuss.elastic.co">discuss.elastic.co</a>というフォーラムで質問は増えて来てるけど、もう少し交流しやすい場があると使ってもらえるかなぁ？と思っていたり。
何かおすすめとかあれば、連絡ください。
meetup.comの勉強会のページにも掲示板はついてるんだけど交流しにくそうだし。</p>
<p>来年は自分が最も興味のある検索系の開発にも参加したいなぁと。
Elasticsearchをちょっとずつ時間見つけてやってたりはしますが、もう少し首を突っ込んで行きたいなぁと。<a href="http://blog.johtani.info/blog/2017/11/10/welcome-swiftype/">Swiftype</a>もジョインしたし。もっと検索やって行きたい気持ちが強いので。</p>
<p>さて、ということで、今年もあと1時間なくなりました。
今年も様々な面で色々な方々に助けていただけました。本当にお世話になりました。
この場を借りてお礼申し上げます。</p>
<p>来年ももちろん、色々な方に助けてもらうと思いますが、よろしくお願いいたします！</p>
</content:encoded>
    </item>
    
    <item>
      <title>2018年のElasticは？</title>
      <link>https://blog.johtani.info/blog/2017/12/24/elastic-2018/</link>
      <pubDate>Sun, 24 Dec 2017 23:10:16 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2017/12/24/elastic-2018/</guid>
      <description>Merry Christmas! Elastic Stack Advent Calendar 2017の24日目の記事になります。 ちょっとですが、2018年のElasticについて書いてみようかと思います。 イベント いくつか</description>
      <content:encoded><p>Merry Christmas!
<a href="https://qiita.com/advent-calendar/2017/elasticsearch">Elastic Stack Advent Calendar 2017</a>の24日目の記事になります。</p>
<p>ちょっとですが、2018年のElasticについて書いてみようかと思います。</p>
<!-- more -->
<h2 id="イベント">イベント</h2>
<p>いくつか出展が決まっているイベントがあるのでまずは宣伝を。</p>
<h4 id="osc-osaka-2018">OSC Osaka 2018</h4>
<p>まずは、1月25日、26日に<a href="https://www.ospn.jp/osc2018-osaka/">OSC Osaka 2018</a>に出展し、話をします。参加者がいそうであれば、25日や26日の夜に勉強会もありかなぁと思っています。
セッションでは入門的な話をする予定です。ブースにいますので、色々質問がある関西の方はぜひご参加ください。</p>
<h4 id="developer-summit-2018">Developer Summit 2018</h4>
<p>2月15日、16日は<a href="http://event.shoeisha.jp/devsumi/20180215">Developer Summit 2018</a>に出展します。
こちらでもブースにいますので、AMA（Ask Me Anything）的に使っていただくのもいいかなと。
デブサミに参加される方はぜひお立ち寄りください。</p>
<h4 id="manabiya">Manabiya</h4>
<p>3月23日、24日は<a href="https://manabiya.tech">Manabiya</a>に出展します。
こちらでもAMAのつもりでブースを出す予定です。質問がある方はぜひお立ち寄りください。
こちらのイベントは初めての開催になるようなので、どんなイベントになるか楽しみにしています。</p>
<p>イベント回りはこの辺りで。
また、1月末に勉強会を予定しています。決まり次第また<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/">Meetup.com</a>の方々にメールを出す予定です。</p>
<h2 id="elastic-stackの2018年は">Elastic Stackの2018年は？</h2>
<h4 id="canvas">Canvas!</h4>
<p>昨年のElastic{ON}でみなさんをCanvasがリリースされるかなぁと。
現在、みなさんにテストしてもらえるように<a href="http://canvas.elastic.co">canvas.elastic.co</a>というサイトを公開中で、実際にインストールして試すことができるようになっています。
ぜひ、触って、全く新しいUIを体験して見てください。</p>
<h4 id="sql">SQL?</h4>
<p>こちらも昨年のElastic{ON}でみなさんから反響があったものです。
もう直ぐでてくるのではないかなぁと。</p>
<h4 id="その他は">その他は？</h4>
<p>いくつか面白そうで、取り込み済みのものをピックアップしておきます。</p>
<ul>
<li><a href="https://github.com/elastic/elasticsearch/pull/27478">Add ranking evaluation API</a>
<ul>
<li>検索クエリなどに対して検索結果のランクのクオリティを評価するためのAPIの追加（7.0予定）</li>
</ul>
</li>
<li>JDK9サポート？
<ul>
<li>6.2でサポートされそうです。が、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/setup.html#jvm-version">LTSのJDK8が推奨のままの予定</a>です。</li>
</ul>
</li>
<li>High-level REST ClientでいくつかAPIが追加
<ul>
<li><a href="https://github.com/elastic/elasticsearch/pull/27574">https://github.com/elastic/elasticsearch/pull/27574</a>、https://github.com/elastic/elasticsearch/pull/27351</li>
</ul>
</li>
<li>APM正式リリース？
<ul>
<li><a href="https://www.elastic.co/jp/solutions/apm">ベータ版</a>がリリースされているので、秒読み段階？</li>
</ul>
</li>
</ul>
<h2 id="ということで">ということで</h2>
<p>より詳しく知りたい方は、サンフランシスコで開催される<a href="https://www.elastic.co/elasticon/conf/2018/sf">Elastic{ON} 2018</a>に参加するのが一番です！（ステマ）
私も参加予定ですので、ぜひ、現地でお会いし、色々な情報をゲットしましょう。</p>
<p>明日で、今年のAdvent Calendarも最後です。micci184さんの記事を楽しみにしましょう！</p>
</content:encoded>
    </item>
    
    <item>
      <title>2017年のElastic StackとElastic</title>
      <link>https://blog.johtani.info/blog/2017/12/01/whats-happen-at-elastic-in-2017/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2017/12/01/whats-happen-at-elastic-in-2017/</guid>
      <description>Elastic stack (Elasticsearch) Advent Calendar 2017の1日目の記事になります。 まだ、1ヶ月を残していますが、簡単に今年起こったことを振り返ってみようかと思います。思った以上に</description>
      <content:encoded><p><a href="https://qiita.com/advent-calendar/2017/elasticsearch">Elastic stack (Elasticsearch) Advent Calendar 2017</a>の1日目の記事になります。</p>
<p>まだ、1ヶ月を残していますが、簡単に今年起こったことを振り返ってみようかと思います。思った以上に色々ありましたね。。。</p>
<!-- more -->
<h3 id="elastic-stack-520リリース-1月">Elastic Stack 5.2.0リリース (1月)</h3>
<p><a href="https://www.elastic.co/jp/blog/elastic-stack-5-2-0-released">リリース記事はこちら</a></p>
<p>HeatmapがKibanaで追加されたり、Heartbeatがベータですが追加されました。
個人的には、Terms aggregationの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.2/search-aggregations-bucket-terms-aggregation.html#_filtering_values_with_partitions">Filtering Valuesによるパーティション</a>が便利になったと思います。Terms Aggsでページングに似たことができるようになりました。</p>
<h3 id="elasticon17開催-3月">Elastic{ON}17開催 (3月)</h3>
<p>第3回目のユーザカンファレンスが開催されました。
バレーダンサーの踊りから始まったキーノート、様々なユーザ企業によるユースケース発表などいろいろありました。
<a href="https://www.elastic.co/elasticon/conf/2017/sf">セッションはこちらのサイトから</a>から録画を見ることができます。
<a href="https://www.elastic.co/elasticon/conf/2017/sf/elasticsearch-sql">SQL for Elasticsearch</a>やAwardが目を引いたと思います。SQL対応まだ出てきていないですが、もうすぐじゃないかと！</p>
<h3 id="elastic-stack-530リリース-3月">Elastic Stack 5.3.0リリース (3月)</h3>
<p><a href="https://www.elastic.co/blog/elastic-stack-5-3-0-released">リリース記事はこちら</a></p>
<p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules-overview.html">Filebeat module</a>が導入され、ログファイルを取り込んでKibanaで可視化するまでの手順がより簡単になりました。Elasticの目指しているものの一つに、シンプルな使い方、簡単にはじめられることといったものがあります。KibanaのTimepickerもより便利になったのも、このバージョンからです。</p>
<h3 id="shayがceoに-5月">ShayがCEOに (5月)</h3>
<p>2月にすでに発表されていましたが、Elasticsearchの生みの親のShay BanonがCEOに正式に就任しました。CEOになってさらに、精力的に様々なことをやっていて、本当にすごいなと思います。</p>
<h3 id="elastic-stack-540600-alpha1リリース-5月">Elastic Stack 5.4.0、6.0.0-alpha1リリース (5月)</h3>
<p><a href="https://www.elastic.co/blog/elastic-stack-5-4-0-released">5.4.0リリース記事はこちら</a>、<a href="https://www.elastic.co/blog/elastic-stack-6-0-0-alpha1-released">6.0.0-alpha1リリース記事はこちら</a></p>
<p>5.4.0は節目になるリリースでした。Machine Learningがベータリリースとして、X-Packに追加されましたし、ElasticsearchではCross Cluster Searchの改善が進みました。LogstashではPersistent QueueがGAになりましたし、KibanaにはTime Series Visual BuilderやEvent Contextなどが追加されますます使いやすくなりました。</p>
<p>また、6.0.0のalpha版も同時にリリースされ、様々な方からのフィードバックが集まり始めました。</p>
<h3 id="elastic-cloud-enterprise-gaリリース-5月">Elastic Cloud Enterprise GAリリース (5月)</h3>
<p>Elastic Cloudのバックエンドの技術を製品に採用したものになります。
多くのElasticsearchクラスタを管理しないといけない方には朗報でした。</p>
<h3 id="opbeatがjoin-6月">OpbeatがJoin (6月)</h3>
<p><a href="https://www.elastic.co/blog/welcome-opbeat-to-the-elastic-family">Opbeatチームがジョイン</a>したのが6月です。Elastic StackがAPM(Application Performance Monitoring)でも活躍することになりそうです。APMの仕組みとしては、APM Agentをアプリ側に配置し、APM Serverへデータを送信し、Elasticsearchに保存、Kibanaで可視化するという流れになります。</p>
<h3 id="elastic-stack-550リリース-7月">Elastic Stack 5.5.0リリース (7月)</h3>
<p><a href="https://www.elastic.co/blog/kibana-5-5-0-released">リリース記事はこちら</a></p>
<p>Machime LearningがGAリリースになったのが5.5.0です。色々な方から質問を受けました。それ以外にも、ElasticsearchのWindows MSI InstallerやKibanaのFilter editorなどが追加されました。Filter editorはこれまで検索条件を記述するのが難しいと感じていたKibanaユーザにとても喜んでもらえたものじゃないかなと。
GrokDebuggerが導入されたのもこのタイミングです。</p>
<h3 id="elastic-stack-560リリース-9月">Elastic Stack 5.6.0リリース (9月)</h3>
<p><a href="https://www.elastic.co/blog/elastic-stack-5-6-0-released">リリース記事はこちら</a></p>
<p>5系最後のマイナーリリースであり、6へのアップグレードが楽になる様々な仕組みが用意されたのがこのバージョンです。ElasticsearchのJava High level REST clientが導入されたのもこのバージョンです。本当に様々な機能が次のメジャーバージョンとの互換性のために組み込まれています。。。</p>
<h3 id="elastic-cloud-on-gcp-9月">Elastic Cloud on GCP (9月)</h3>
<p><a href="https://www.elastic.co/blog/announcing-the-ga-of-elastic-cloud-hosted-elasticsearch-on-google-cloud-platform-gcp">リリース記事はこちら</a></p>
<p>これまで、AWS上のみで展開していたElastic CloudがGCP上でも展開されることになりました。残念ながら、日本リージョンはまだありませんが、問い合わせなどが増えれば今後サポートされる可能性が高くなると思います！</p>
<h3 id="elastic-apm-alpha-9月">Elastic APM alpha (9月)</h3>
<p><a href="https://www.elastic.co/guide/en/apm/get-started/current/overview.html">リリース記事はこちら</a></p>
<p>OpbeatチームによりElastic StackのAPMがAlphaですがリリースされました。APMがOpen Sourceで利用できるんです！Agentがもっと増えてくると色々なことに使えるようになると思います。ぜひAgentを作成してみてください！</p>
<h3 id="elastic-stack-600リリース-11月">Elastic Stack 6.0.0リリース (11月)</h3>
<p><a href="https://www.elastic.co/blog/elastic-stack-6-0-0-released">リリース記事はこちら</a></p>
<p>待ちに待った6.0.0のリリースです。<a href="https://events.elastic.co/6-0-elastic-stack-jp">新機能については本日(12/1)の昼に行われるウェビナーをご覧ください！</a></p>
<h3 id="swiftypeがjoin-11月">SwiftypeがJoin (11月)</h3>
<p><a href="https://www.elastic.co/jp/about/press/elastic-acquires-saas-site-search-leader-swiftype">ニュースリリースはこちら</a></p>
<p>Swiftypeと呼ばれる検索のSaaSを提供している会社がジョインしました。
個人的には今年一番嬉しいニュースです。やはり、検索が好きなので。
簡単に<a href="http://blog.johtani.info/blog/2017/11/10/welcome-swiftype/">Site Searchを構築できる仕組み</a>は非常に面白いものです。
興味のある方は、ぜひ触ってみてください。日本語固有の機能などはまだないので、今後関わっていければなーと。</p>
<h3 id="elasticon-tour-tokyo開催-12月">Elastic{ON} Tour Tokyo開催 (12月)</h3>
<p>まだ開催前ですが、今年も<a href="https://www.elastic.co/elasticon/tour/2017/tokyo">東京で1dayイベントを開催</a>します。
残念ながら、もうSold outなので、キャンセル待ちになってしまっているみたいですが。私もスピーカーとして喋りますし、AMAブースにも立っています。
参加される方はぜひ声をかけていただければと思います。</p>
<h4 id="osc-2017enterprise-12月">OSC 2017.Enterprise (12月)</h4>
<p>オープンソースカンファレンスで今年も様々な都市に出張しました。
今年の締めくくりということで、<a href="https://www.ospn.jp/osc2017.enterprise/">12/8に渋谷で開催されるカンファレンス</a>に出展します。時間のある方は、ぜひブースに遊びに来てください。入門者向けのセッションもあるので、こちらもお待ちしております。</p>
<h3 id="まとめ">まとめ</h3>
<p>超駆け足ですが、今年を振り返ってみました。
今年もいろんなことがありました。書くのが大変だったw。
OSCなどイベントで声をかけていただいた皆様、ありがとうございました。</p>
<p>さて、<a href="https://qiita.com/advent-calendar/2017/elasticsearch">Elastic Stack Advent Calendar 2017</a>は始まったばかりです。これからの記事を楽しみにしています！</p>
<h4 id="ということで">ということで、</h4>
<p>次は<a href="https://qiita.com/aeroastro">aeroastroさん</a>の「Elasticsearchのインデックスを本当の意味で無停止再構築する方法」（<a href="https://qiita.com/advent-calendar/2017/elasticsearch">Advent Calendarのページはこちら</a>）になります、お楽しみに！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Swiftypeがジョインしたので触ってみました</title>
      <link>https://blog.johtani.info/blog/2017/11/10/welcome-swiftype/</link>
      <pubDate>Fri, 10 Nov 2017 11:14:49 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2017/11/10/welcome-swiftype/</guid>
      <description>Swiftypeでサイト内検索？ プレスリリース：Elasticがサイト内検索サービス最大手のSwiftypeを買収が出ましたが、Swifty</description>
      <content:encoded><h1 id="swiftypeでサイト内検索">Swiftypeでサイト内検索？</h1>
<p>プレスリリース：<a href="https://www.elastic.co/jp/about/press/elastic-acquires-saas-site-search-leader-swiftype">Elasticがサイト内検索サービス最大手のSwiftypeを買収</a>が出ましたが、SwiftypeチームがElasticにジョインしました！</p>
<h2 id="自分のブログ検索に導入してみる">自分のブログ検索に導入してみる？</h2>
<p><a href="https://swiftype.com/">Swiftype</a>はSite Searchをサービスとして提供しています。
実際に指定したサイトのデータをクロールして、インデキシングし、検索できるようになります。
ものは試しということで、自分のブログを登録してみました。検索窓はつけてないですが。
14日間のFree trialがあるのでそちらで試してみましょう。
検索窓はデモとか作れたらかな。</p>
<!-- more -->
<h2 id="ということでfree-trialへ">ということで、Free trialへ</h2>
<p>まずはユーザ登録です。Googleのアカウントと連携するか、Swiftypeにメルアドを登録するかが選択できます。
登録できたら、次にどちらのプロダクトを使うかという選択画面が出てきます。
今回はサイト検索して見たいので、「Site Search」の「START FREE TRIAL」をクリックします。</p>
<h2 id="engineの指定">Engineの指定</h2>
<p>Web Crawlerを利用するか、独自にSwiftypeのAPIを利用してデータを登録するかを選択します。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/01_Create_Search_Engine___Swiftype.jpg" />
    </div>
    <a href="/images/entries/20171110/01_Create_Search_Engine___Swiftype.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>今回は、お手軽な方のCrawlerを選択します。
すると次に、クロールしたいURLの指定画面が出てきます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/02_Create_Crawler-based_Search_Engine___Swiftype.jpg" />
    </div>
    <a href="/images/entries/20171110/02_Create_Crawler-based_Search_Engine___Swiftype.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>今回は自分のブログなので、&lsquo;<a href="http://blog.johtani.info">http://blog.johtani.info</a>'を指定します。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/03_Create_Crawler-based_Search_Engine___Swiftype.jpg" />
    </div>
    <a href="/images/entries/20171110/03_Create_Crawler-based_Search_Engine___Swiftype.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>で少し待つと、URLのチェック（存在チェックとか、接続チェックとか）が終わり、今回作成するEngineに名前をつける画面が出てきます。
「My Blog search」という名前にしてみました。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/04_https___app_swiftype_com_engines_my-blog-search_precrawls_166127_settings.jpg" />
    </div>
    <a href="/images/entries/20171110/04_https___app_swiftype_com_engines_my-blog-search_precrawls_166127_settings.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>sitemapがあるかどうかをCrawlerがチェックしています。
私のBlogはOctopressでできていて、sitemap.xmlも作ってくれているので、これを元にCrawlerはクロールをしてくれます。そのほかにも<a href="https://swiftype.com/documentation/crawler">RSSやAtomサポートもあるみたいですね</a>。
「COMPLETE SETUP」を押すと、Engineの管理画面が出てきます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/05_My_Blog_search_Overview___Swiftype.jpg" />
    </div>
    <a href="/images/entries/20171110/05_My_Blog_search_Overview___Swiftype.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>「CRAWLING」という表示が出ています。クロールしている最中です。これが終われば、「PREVIEW」に代わり、検索できる準備ができたことになります。
実際にPREVIEWして見ると、次のような検索ができるようになります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/06_My_Blog_search_Overview___Swiftype_PREVIEW.jpg" />
    </div>
    <a href="/images/entries/20171110/06_My_Blog_search_Overview___Swiftype_PREVIEW.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>そのほかにはWeightの調整画面などもありますね。フィールド毎に重み付けを変えて見たりもできます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20171110/07_Weights___Swiftype.jpg" />
    </div>
    <a href="/images/entries/20171110/07_Weights___Swiftype.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>日本語独自のAnalyzerなどはまだないので、そのあたりができてくるともっと便利になりそうです。
とりあえず、気になる方は触って見てはいかがでしょうか？
個人的は検索に関するサービスが出てきたのですごく楽しみにしています！</p>
</content:encoded>
    </item>
    
    <item>
      <title>データ分析基盤構築入門 を一部執筆しました。</title>
      <link>https://blog.johtani.info/blog/2017/09/21/release-intro-logging-analysis-system/</link>
      <pubDate>Thu, 21 Sep 2017 10:02:30 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2017/09/21/release-intro-logging-analysis-system/</guid>
      <description>久々に執筆しました。といっても、以前の書籍の更新版です。 まぁ、更新版といっても、私以外の方々は結構な量を書き直しor新規書き起こしされてます</description>
      <content:encoded><p>久々に執筆しました。といっても、<a href="http://blog.johtani.info/blog/2014/08/04/release-magazine-book-of-log-aggs-and-viz/">以前の書籍</a>の更新版です。
まぁ、更新版といっても、私以外の方々は結構な量を書き直しor新規書き起こしされてますが。。。</p>
<p>ということで、みなさん「買って」から感想をいただけるとうれしいです！（以下の画像でAmazonにジャンプできます！Kindle版も発売中です。）</p>
<p><a target="_blank"  href="https://www.amazon.co.jp/gp/product/477419218X/ref=as_li_tl?ie=UTF8&camp=247&creative=1211&creativeASIN=477419218X&linkCode=as2&tag=johtani-22&linkId=f7cbe8130343ea340b18b53eba20d4b7"><img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=477419218X&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=johtani-22" ></a><img src="//ir-jp.amazon-adsystem.com/e/ir?t=johtani-22&l=am2&o=9&a=477419218X" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /></p>
<!-- more -->
<p>今回もElasticsearchの章を担当しました。
5.4ベースで書きましたが、ちょっとずつ6でどう変わるかなども記載してあります。
また、付録ではLogstashやBeatsにもちょっと触れています。
また、自分が一番好きなKibanaの機能であるDev ToolsのConsoleについても記載してあります。こちらも合わせて目を通していただければと。</p>
<p>みなさんのフィードバック（ツイート、ブログ、Amazonのコメントなどなど）をお待ちしております！</p>
</content:encoded>
    </item>
    
    <item>
      <title>検索座談会ってのをやってみた</title>
      <link>https://blog.johtani.info/blog/2017/05/25/search-meetup/</link>
      <pubDate>Thu, 25 May 2017 00:24:13 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2017/05/25/search-meetup/</guid>
      <description>久々のブログ。 Elasticsearch勉強会を主催してますが、 プロダクトを超えて、検索で共通してある課題とか、悩みとか、あるかなぁと。 その</description>
      <content:encoded><p>久々のブログ。
Elasticsearch勉強会を主催してますが、
プロダクトを超えて、検索で共通してある課題とか、悩みとか、あるかなぁと。
その辺りを話す場所を考えてみるのいいかもと思い、検索座談会ってのを5人でやってみた。</p>
<!-- more -->
<p>とりあえず、初めてなので、5名ほどで。プロダクトはかぶってない感じでした。</p>
<h3 id="話した議題はこの辺り">話した議題はこの辺り。</h3>
<ul>
<li>形態素解析の辞書は何を使ってる？有償のもの？
<ul>
<li>そもそも形態素解析？</li>
</ul>
</li>
<li>辞書の更新とかは？
<ul>
<li>シノニムってどうしてる？</li>
</ul>
</li>
<li>サジェストとかのデータはどう作ってる？どうしてる？
<ul>
<li>検索ログとかから作ってる？</li>
</ul>
</li>
<li>検索結果のランキングって検索結果だけで決めるの？
<ul>
<li>検索漏れとかキーワードのミスマッチとかどうしてる？</li>
</ul>
</li>
</ul>
<h3 id="今後話したい内容はこの辺かな">今後話したい内容はこの辺かな？</h3>
<ul>
<li>緯度経度系の検索</li>
<li>画像検索、音声検索</li>
<li>検索のキーワードの調査とかしてる人とかいるかな？</li>
<li>どんな機能が欲しい？</li>
<li>前処理としてはどんなことをしてる？</li>
</ul>
<p>このあとどうしていくかはまだ考え中。知り合いを捕まえて、どんなことしてるかを聞きにいくのもいいし、議題みたいなことを決めて、それを話してくれそうな人にコンタクトして議論するのもありかなぁ。
普通の勉強会みたいなのにするかもしれないし。
とりあえず、どのくらい興味を持ってる人がいるのかがわからないので。。。</p>
<p>もちろん、Elasticsearch勉強会は別でやっていきますよ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2016）</title>
      <link>https://blog.johtani.info/blog/2016/12/31/looking-back-2016/</link>
      <pubDate>Sat, 31 Dec 2016 22:22:14 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/12/31/looking-back-2016/</guid>
      <description>今年はRioのプレイバック見ながら書いてます。 今年はなんか長い一年だった気が。 振り返り（2015年に書いた抱負から） まずは去年の抱負を元に</description>
      <content:encoded><p>今年はRioのプレイバック見ながら書いてます。
今年はなんか長い一年だった気が。</p>
<!-- more -->
<h2 id="振り返り2015年に書いた抱負から">振り返り（2015年に書いた抱負から）</h2>
<p>まずは去年の抱負を元に。</p>
<h5 id="英語の継続">英語の継続</h5>
<p>継続してます。まぁ当たり前ですが。もう少し話す機会増やさないとだなぁと思いつつ。英会話しながら、海外TVドラマ見ながら。
見たドラマはこの辺かな？</p>
<ul>
<li>ER</li>
<li>SUITS</li>
<li>メンタリスト</li>
<li>Numb3rs</li>
<li>エレメンタリー</li>
<li>Agent of Shield</li>
</ul>
<p>あと、映画も見てます。とりあえず、耳を慣らすために字幕ありで見てるので、
英語だけでとは行きませんが。。。
そろそろ同じ映画を英語字幕で見るとかしたほうがいいのかもなぁ。</p>
<h5 id="もっとelasticsearchの開発に参加">もっとElasticsearchの開発に参加</h5>
<p>一応参加してます。ちょっとずつですが。
イベントが重なるとできなくなるので、小さなPRとかを細々とって感じですが。。。</p>
<h5 id="人員の倍増">人員の倍増？</h5>
<p>倍増したかな？現時点で日本は8名になりました。そのうち1人は完全リモートです。
来年も倍増とは行かないだろうなぁ。取り急ぎ、サポートエンジニアが足りてないのでそこを埋めないと。。。</p>
<h5 id="日本語情報発信">日本語情報発信</h5>
<p>これはちょっと足踏みしてるかも。ブログの翻訳とかはやってるんですが、もう少しなんとかしないとなぁ。ブログも書かないと。。。</p>
<h5 id="splatoon-s">Splatoon S+?</h5>
<p>なりました！とりあえずS+とSを行ったり来たりしてる感じです。
メインはノヴァネオですが、Sに落ちたら違う武器でS+に上がるまでチャレンジするってのをやってます。わかば、バケスロソーダ、プライム、ワサビくらいはやったかな？
そろそろまた違う武器かなぁ。S+99は無理そうなんで。。。</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<p>その他の今年の出来事。</p>
<ul>
<li>初香港</li>
<li>初プラハ</li>
<li>OSCなど、東京以外でのカンファレンス参加</li>
<li>厄年</li>
<li>東京オフィス！</li>
<li>BBLとかトレーニング</li>
<li>lucene-gosenをちょっとメンテ</li>
<li>Podcastデビュー？</li>
</ul>
<p>初モノ今年も多いかな。
今年は、昨年よりは長く感じた一年でした。
月1くらいで出張してたのもあるのかなぁ？
香港、プラハはもちろん会社のイベントです。
香港はジャッキーチェンの映画を彷彿とさせるところがちょくちょくあって面白かったです。
ビルとかの補修の足場が竹竿とか。
プラハも面白かったです。やっぱりヨーロッパの古い町は歴史のある建物が多くて楽しいです。</p>
<p>OSCなどのイベントも色々行きました。福岡、名古屋、大阪、京都、札幌と回りました。
それとは別に大阪、福岡も行ったかな。
来年は沖縄と広島に行きたいなぁ。。。
出張のついで？じゃないですが、御朱印集めし始めました。神社仏閣もいいですよねぇ。</p>
<p><a href="http://blog.johtani.info/blog/2016/06/30/broken-something-in-this-year/">厄年</a>はきつかった。。。たまたまだとは思うけど、お祓いに行ったのが効いたのかなぁ。後半は特に問題なかったかな。腕時計無くしたと思ったら見つかったりしたし。</p>
<p>弊社東京オフィスができました。（あんまり行ってないけど）
ときどきアクロクエストさんがセミナーを開いたりしてます。
昨年のTour Tokyo後のMeetupでもイベントスペースを使って、Drinkup（勉強会の懇親会だけバージョン）もやりました。
今後もちょこちょことイベントをやってみようかな？</p>
<p>BBLも始めました。あまりきちんと宣伝してないからそれほど依頼は来てないですが。
あとは、今年も2回日本語でトレーニングしました。
前回はほぼ満席にまでなりました。
来年も頑張りたいかな。ショートカットするためにもいいと思うんで、トレーニングを受けて欲しいんです。。。</p>
<p>lucene-gosenのメンテもしました。
といってもプログラムではなく、ビルドシステムをAntからGradleに変更したんですが。
来年もちょっとずつ触りたいな。機能面で拡充するかはわからないですが。。。</p>
<p>Podcastにもデビューさせていただきました。
wyukawaさんのPodcastです。「<a href="http://wyukawa.tumblr.com/post/153257053753/johtani%E3%81%95%E3%82%93%E3%81%A8elasticsearch%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E8%A9%B1%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-guest">johtaniさんとElasticsearchについて話しました</a>」楽しかったです。
また出たいなぁというか、喋るの面白かった！</p>
<h2 id="来年の抱負">来年の抱負</h2>
<ul>
<li>もちろん英語の継続</li>
<li>継続的にイベントに登壇</li>
<li>もっと開発＆ブログ</li>
<li>サポートエンジニア獲得！</li>
<li>個人的な検索勉強会の再開</li>
</ul>
<p>英語はまぁ、当たり前ですね。会社の人つかまえてもっと喋る時間増やさないとだな。</p>
<p>来年もいろんなカンファレンスなどに出てブース出したり、セッション持ったりして
もっともっと広めていかないとなぁ。
あとは、勉強会とMeetupもやっていかないと。入門編と上級編みたいに分けてみるのもありかなぁと考えて見たり。
スピーカー、ご意見募集しております。</p>
<p>開発に時間をさくための時間の活用をしていかないとなぁと。
どうも時間の使い方がまだ上手くないので。ブログも一緒ですね。。。
月1くらいのペースではかきたいな。。。
ブログもそうだけど書籍もかな？？？？
あと、外資系の他の会社の人ともまた飲みたいなー。</p>
<p>開発の時間を確保という面ではサポートエンジニアも募集中です。
最近、クライアントが増えて来てチケットが増えて来てるんで、サポートしたりもしてるんです。。。
自分の時間を増やすためにもサポートエンジニアを確保しないと。誰かいい人いないかなぁ。</p>
<p>個人的にやってた勉強会を忙しいって言って中断してるので、再開しないと。。。</p>
<p>ということで、今年もあと數十分になってしまいました。
今年も色々な方に助けていただけました。お世話になりました。
この場を借りてお礼申し上げます。</p>
<p>来年もいろんな方々に助けてもらうと思いますが、よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>2016年のElastic Stack</title>
      <link>https://blog.johtani.info/blog/2016/12/25/elasticsearch-6-features/</link>
      <pubDate>Sun, 25 Dec 2016 00:03:50 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/12/25/elasticsearch-6-features/</guid>
      <description>Merry Christmas! Elastic stack Advent Calendar 2016 最終日の記事になります。 簡単に今年の変遷を振り返ってみます。 Elasticsearch 2.2 (2月) Elasticsearch 2.2.0、2.1.2、1.7.5リリース クエリプロ</description>
      <content:encoded><p>Merry Christmas!
<a href="http://qiita.com/advent-calendar/2016/elastic">Elastic stack Advent Calendar 2016</a> 最終日の記事になります。</p>
<p>簡単に今年の変遷を振り返ってみます。</p>
<!-- more -->
<h2 id="elasticsearch-22-2月">Elasticsearch 2.2 (2月)</h2>
<ul>
<li><a href="https://www.elastic.co/jp/blog/elasticsearch-2-2-0-and-2-1-2-and-1-7-5-released">Elasticsearch 2.2.0、2.1.2、1.7.5リリース</a></li>
</ul>
<p>クエリプロファイラやGeo系の性能改善などが取り込まれました。
また、同時期にリリースされたKibana 4.4ではColor pickerやShare用のURLの短縮化機能なども追加されました。</p>
<h2 id="第2回目のユーザカンファレンスelasticon開催2月">第2回目のユーザカンファレンス、Elastic{ON}開催（2月）</h2>
<p>サンフランシスコで、弊社第2回目のカンファレンスが開催されました。
2015年の会場よりも大きくなり、多数の方に参加いただきました。
ここで、以下の発表がありました。</p>
<h3 id="elastic-stackとx-packの紹介">Elastic StackとX-Packの紹介</h3>
<p>これまで、ELK stackと呼ばれて意味明日が、Beatsチームの参加により、ELKだけではなくなったこともあり、Elastic Stackと呼び名を変える事になりました。
また、Marvel、Shield、Watcherなどの商用の拡張機能についても、
単体の名称ではなく、X（Extension）-Packと1つの名前になる事に。
詳細については<a href="https://www.elastic.co/jp/blog/heya-elastic-stack-and-x-pack">公式のブログ</a>をご覧ください。</p>
<h3 id="elastic-cloudとelastic-cloud-enterpriseの発表">Elastic CloudとElastic Cloud Enterpriseの発表</h3>
<p>2015年にElasticにジョインし、
これまでFound.no（Found）と呼ばれていた弊社のElasticsearch as a Serviceが<a href="https://www.elastic.co/jp/blog/introducing-elastic-cloud-and-elastic-cloud-enterprise">Elastic Cloudと名称変更</a>しました。
また、Elastic Cloudで培っているノウハウを詰め込んだElastic Cloud Enterpriseも発表しました。実際に利用可能になるまでには
まだもう少しかかってしまいますが、アルファ版が公開されていますので、興味のある方は触ってみてください。</p>
<p>Elastic{ON}2016で撮影された、<a href="https://www.elastic.co/blog/life-without-elasticsearch-elasticon16">「Elasticsearchがないあなたの人生はどうなりますか？」</a> といった面白い動画も公開されています。</p>
<h2 id="elasticsearch-23リリース3月">Elasticsearch 2.3リリース（3月）</h2>
<ul>
<li><a href="https://www.elastic.co/jp/blog/elasticsearch-2-3-0-and-2-2-2-released">Elasticsearch 2.3.0および2.2.2をリリース</a></li>
</ul>
<p>Reindex APIが登場し、Mappingの変更やShard数の変更など、色々とデータの更新などがやりやすくなりました。
また、Task Managementの機能も追加され、長時間かかる処理を間違った場合などの対処が楽になりました。
個人的には、Deprecation Loggingの機能が導入されたことが嬉しいこととなります。次期メジャーバージョンで廃止される機能についてログに出力されるようになりました。
実際に運用されているアプリで利用している機能が今後なくなるかどうかをログを見るとわかるという仕組みです。</p>
<h2 id="rally登場4月">Rally登場（4月）</h2>
<ul>
<li><a href="https://www.elastic.co/jp/blog/announcing-rally-benchmarking-for-elasticsearch">Rally登場：Elasticsearchのベンチマークツール</a></li>
</ul>
<p>Elasticsearchのベンチマークツールがリリースされました。
定期的にElasticsearchの性能を計測することは問題点を見つける事に役に立ちます。そういった手助けをしてくれるツールが公開されることは非常に便利なことかと。</p>
<h2 id="elastic-stack-5-alpha1-リリース4月">Elastic Stack 5 alpha1 リリース（4月）</h2>
<ul>
<li><a href="https://www.elastic.co/jp/blog/elastic-stack-release-5-0-0-alpha-1">Elastic Stack 5.0.0 alpha 1 リリース</a></li>
</ul>
<p>Ingest NodeやLucene 6、新しいKibanaのUIなど多くのものが詰まっていました。ここから多くのユーザにテストしてもらい、5.0の正式リリースを迎えることができました。</p>
<h2 id="elasticsearch-240リリース8月">Elasticsearch 2.4.0リリース（8月）</h2>
<ul>
<li><a href="https://www.elastic.co/jp/blog/release-bonanza-elasticsearch-graph-shield-watcher-marvel-reporting-logstash-2-4-beats-1-3-and-kibana-4-6-are-now-available"></a></li>
</ul>
<p>2.xの最後のマイナーバージョンリリースです。
Reportingなどの追加とドットつきフィールド名の復活がありました。</p>
<h2 id="elastic-stack-500-beta1-リリース9月">Elastic Stack 5.0.0 beta1 リリース（9月）</h2>
<ul>
<li><a href="https://www.elastic.co/jp/blog/elastic-stack-release-5-0-0-beta1">Elastic Stack Release - 5.0.0-beta1</a></li>
</ul>
<p>ついにベータです。Painlessがスクリプトのデフォルトになったり、TimelionがKibanaに取り込まれるなど、正式リリースまであと少し！</p>
<h2 id="prelertチームジョイン9月">Prelertチームジョイン（9月）</h2>
<ul>
<li><a href="https://www.elastic.co/blog/welcome-prelert-to-the-elastic-team">Welcome Prelert to the Elastic Team</a></li>
</ul>
<p>Machine Learningエンジンを開発し、Elasticsearch,Kibanaとの組み合わせの製品をリリースしていたPrelertという会社がジョインしました。
Elasticsearchに保存された多くのデータをより活用していただくことができるかと思います。
Elastic{ON} Tour 2016 Tokyoで弊社SAの大輪の発表も人気があるものでした。まだベータ段階ですが、利用して見ることも可能です。
ビデオなどが公開されたらまたツイートしようと思います。</p>
<h2 id="elasticon-tour-tokyo-2016開催12月">Elastic{ON} Tour Tokyo 2016開催（12月）</h2>
<p>今年で2回目のTokyoローカルの1日イベントでした。
ブログは「まだ」書いてませんが、、、今回も盛りだくさんのイベントになりました。
早朝のトレーニング（ハンズオンではない）にも80名近くの方に参加していただけましたし、私はKibanaのキーノート＋デモという大役をもらいましたし、ちょっと大変でした。
今年もAMA（Ask Me Anything）ブースが大盛況でした。
色々な方から、弊社のサポート、開発者が色々な質問を受け、それに答えるという形です。楽しんでいただけたかと思います。
来年もぜひ開催したいなと思っています。</p>
<p>また、<a href="https://www.elastic.co/blog/first-wave-of-elasticon-17-sessions-revealed">Elastic{ON}17のセッションも</a>いくつか発表されています。
ぜひ、サンフランシスコで行われる本場のカンファレンスにもご参加ください！</p>
<h2 id="来年は">来年は？</h2>
<p>1月後半か2月に<a href="https://www.meetup.com/ja-JP/Tokyo-Elastic-Fantastics/">Elasticsearch勉強会</a>を検討しようと思っています。スピーカーに興味のある方は連絡いただければと。</p>
<p>会社としては、<a href="https://www.elastic.co/elasticon/conf/2017/sf">Elastic{ON}2017</a>が3月にまた開催されます。これで3回目となります。もちろん私も参加予定なので、参加される方は、現地で会いましょう！</p>
<p>そのほかにも<a href="http://www.bigdatacon.jp/ja/">BIG DATA ANALYTICS TOKYO</a>やオープンソースカンファレンス（<a href="http://www.ospn.jp/osc2017-osaka/">大阪</a>）、<a href="http://event.shoeisha.jp/devsumi/20170216/">デブサミ</a>といったカンファレンスに参加（登壇・ブースなど）予定です。
参加される方は、ぜひブースまでお越しください。</p>
<p>では、また来年のAdvent Calendarでお会いしましょう！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 5.0の便利機能紹介？</title>
      <link>https://blog.johtani.info/blog/2016/12/01/elasticsearch-5-dot-0-highlight/</link>
      <pubDate>Thu, 01 Dec 2016 17:34:30 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/12/01/elasticsearch-5-dot-0-highlight/</guid>
      <description>Elastic stack Advent Calendar 1日目の記事になります。 Elasticsearch 5.0が10月末にリリースされました。 リリースのブログでいくつか紹介されているのですが、そこでは紹介されて</description>
      <content:encoded><p>Elastic stack Advent Calendar 1日目の記事になります。</p>
<p><a href="https://www.elastic.co/jp/blog/elasticsearch-5-0-0-released">Elasticsearch 5.0</a>が10月末にリリースされました。
リリースのブログでいくつか紹介されているのですが、そこでは紹介されていない機能について2、3紹介しようと思います。</p>
<!-- more -->
<p>その前に、5.0、あれ？その前は2.xじゃなかったっけ？？と困惑されている方もいるかと思うので、簡単に5となった経緯の紹介をしようかと。</p>
<h3 id="バージョン番号">バージョン番号</h3>
<p>なぜ2から5に飛んだのかという話ですが、このスライドがその紹介になっています。</p>
<p><a href="https://speakerdeck.com/johtani/elastic-stack-5-dot-0-alpha1-alpha5?slide=5">https://speakerdeck.com/johtani/elastic-stack-5-dot-0-alpha1-alpha5?slide=5</a></p>
<p><a href="https://www.elastic.co/elasticon/conf/2016/sf/opening-keynote">Elastic{ON} 2016のキーノート</a>でも紹介がありましたが、KibanaやLogstashとElasticsearchを組み合わせて使うときにバージョンのミスマッチで動かないというユーザの声が上がっていました。
2.xのリリースから、同じ日にKibana、Logstash、Beatsもリリースするようになったのですが、
やはり、バージョン番号が異なるため、ミスマッチで動かないというユーザが時々いました。</p>
<p>Elastic Stackという名称にもなったため、バージョン番号をそろえようという事になり、
Elasticsearch、Kibana、Logstash、Beats全てが5.0.0としてリリースされ、
今後は同じバージョン番号になります。</p>
<p>ちなみに、「5」になった理由はKibanaのメジャーバージョンが「4」だったためです。</p>
<p>さて、では、いくつか機能の紹介を。</p>
<h3 id="reindex-from-remote-cluster">Reindex from remote cluster</h3>
<p>Reindexが2.3から導入されました。データの再登録ができるようになり、マッピングの変更や
Shardの数の変更などが柔軟に行えるようになりました。
便利でしたが、あくまでも同一のクラスタでデータを登録し直す形でした。</p>
<p>5.0からはこの機能に加えて、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html#reindex-from-remote">異なるクラスタからデータを取得してReindex</a>を行うことができるようになりました。
こんな形になります。</p>
<pre><code>POST _reindex
{
  &quot;source&quot;: {
    &quot;remote&quot;: {
      &quot;host&quot;: &quot;http://otherhost:9200&quot;,
      &quot;username&quot;: &quot;user&quot;,
      &quot;password&quot;: &quot;pass&quot;
    },
    &quot;index&quot;: &quot;source&quot;,
    &quot;query&quot;: {
      &quot;match&quot;: {
        &quot;test&quot;: &quot;data&quot;
      }
    }
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;dest&quot;
  }
}
</code></pre><p><code>username</code>と<code>password</code>はリモートのクラスタに認証の気候が存在する場合に利用できるオプションです。
また、ReindexのAPIはクエリを使用して、必要なデータだけを取得することが可能です。
この機能により、1.xや2.xのクラスタからデータを移行することが可能になります。</p>
<h3 id="custom-analyzer-test-using-analyze-api">Custom analyzer test using Analyze API</h3>
<p>もう一つ、ちょっとだけ便利な機能を紹介します。
独自にAnalyzerを定義（TokenizerとToken Filterなどを個別に設定）して、その挙動を確認するとき、2.xまでは、インデックスを作成してそのインデックスに対して<code>_analyze</code> APIを呼び出す必要がありました。</p>
<p>5.xからは<code>_analyze</code> APIの読み出しのパラメータで指定できるようになりました。
こんな感じです。ここでは、<code>lowercase</code>フィルタのあとに、<code>{...}</code>で<code>stop</code>フィルタを
パラメータの中で、指定しています。</p>
<pre><code>curl -XGET 'localhost:9200/_analyze' -d '
{
  &quot;tokenizer&quot; : &quot;whitespace&quot;,
  &quot;filter&quot; : [&quot;lowercase&quot;, {&quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [&quot;a&quot;, &quot;is&quot;, &quot;this&quot;]}],
  &quot;text&quot; : &quot;this is a test&quot;
}'
</code></pre><p>ちょっとだけですが、Analyzerなどを試すのが楽になるのではないでしょうか？</p>
<p>ということで、以上が1日目の記事でした。
Logstashなど、他の5.0.0に関する記事もAdvent Calendarに空きがあるようなので、個別にかこうかなと思います。お楽しみに！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene Kuromoji for NEologdで指定した品詞の単語を抜き出すIngest Pluginを書いてみた #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2016/07/22/making-ingest-processor-plugin-with-cookiecutter/</link>
      <pubDate>Fri, 22 Jul 2016 13:26:56 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/07/22/making-ingest-processor-plugin-with-cookiecutter/</guid>
      <description>久しぶりに、技術的なブログ書いてます。 Ingest Processorのプラグインを作ってみたくなったので、書いてみました。 ただ書いてみるんじゃ3番煎じ</description>
      <content:encoded><p>久しぶりに、技術的なブログ書いてます。</p>
<p>Ingest Processorのプラグインを作ってみたくなったので、書いてみました。
ただ書いてみるんじゃ3番煎じになりそうなので、<a href="https://cookiecutter.readthedocs.io/en/latest/readme.html">cookiecutter</a>を使ってみました。</p>
<!-- more -->
<p>と言っても、同僚のAlexが<a href="https://discuss.elastic.co/t/cookiecutter-template-for-writing-ingest-processors/52985">cookiecutter-elasticsearch-ingest-processor</a>と言うテンプレートを作ってくれているのを使っただけですが。（https://discuss.elastic.co に投稿された記事で、使い方がアニメgifで説明されててわかりやすいです）</p>
<p><a href="https://cookiecutter.readthedocs.io/en/latest/readme.html">cookiecutter</a>とは、コマンドラインで質問に答えると、テンプレートからプロジェクトが生成できるツールです。
Elasticでは、カスタムBeatを作る時に利用する例がいつかの日本語ブログや発表資料で話題になっていました。
これのIngest Processorのプラグインバージョンです。</p>
<p>今回は、NEologdも使ってみたかったので、Lucene Kuromoji for NEologdを利用して
指定した品詞の単語だけを抽出するProcessorを作ってみました。</p>
<p>GitHubのプロジェクト：https://github.com/johtani/elasticsearch-ingest-kuromoji-pos-extract</p>
<h3 id="cookiecutterの使い方">Cookiecutterの使い方</h3>
<p><a href="https://cookiecutter.readthedocs.io/en/latest/readme.html">Cookiecutterのインストールはサイト</a>をご覧ください。</p>
<pre><code>cookiecutter gh:spinscale/cookiecutter-elasticsearch-ingest-processor
</code></pre><p>あとは、出てくる以下の項目を指定するだけです。</p>
<ul>
<li><code>processor_type</code> : Ingest Processorのタイプ名です。<code>kuromoji_part_of_speech_extract</code>としました。（Alexのだと<code>_</code>を使うとちょっと問題があるので後述）</li>
<li><code>description</code> : readme.mdに利用されます。</li>
<li><code>developer_name</code> : 名前を記載。Javaのファイルのヘッダに利用</li>
<li><code>elasticsearch_version</code> : デフォルトで<code>5.0.0-alpha4</code>が指定されているので、特に指定せず</li>
</ul>
<p>以上の質問に答えたら、プロジェクトのディレクトリ構造が出来上がってます。
プロジェクトのビルドなどにはGradleを利用します。</p>
<h3 id="プロジェクトのintellij-idea用のファイルを生成">プロジェクトのIntelliJ IDEA用のファイルを生成</h3>
<p>build.gradleファイルでGradleのideaプラグインがapplyされているので、以下のコマンドを叩けばIntelliJ IDEAのプロジェクトファイル（？）が生成され、IntelliJで開けばすぐに開発ができる状態にできます。</p>
<pre><code>gradle idea
</code></pre><h3 id="コーディング">コーディング</h3>
<p>あとは、必要処理をコーディングします。
実際にコーディングするクラスは<code>org.elasticsearch.plugin.ingest.kuromoji_part_of_speech_extract</code>のパッケージにある以下の2つです。（パッケージ名にはprocessor_typeの名前が指定されている）</p>
<ul>
<li>IngestKuromojiPartOfSpeechExtractPlugin</li>
<li>KuromojiPartOfSpeechExtractProcessor</li>
</ul>
<h4 id="ingestkuromojipartofspeechextractplugin">IngestKuromojiPartOfSpeechExtractPlugin</h4>
<p>Pluginというクラスは、プラグインをNodeのModuleとして登録する処理を書くクラスとなります。
生成してすぐは、次のような形になっています。（※importやクラス定義の部分は省略しています。）</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Java" data-lang="Java"><span style="color:#f92672">...</span>
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">final</span> Setting<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">&gt;</span> YOUR_SETTING <span style="color:#f92672">=</span>
            <span style="color:#66d9ef">new</span> Setting<span style="color:#f92672">&lt;&gt;(</span><span style="color:#e6db74">&#34;ingest.kuromoji_part_of_speech_extract.setting&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;foo&#34;</span><span style="color:#f92672">,</span> <span style="color:#f92672">(</span>value<span style="color:#f92672">)</span> <span style="color:#f92672">-&gt;</span> value<span style="color:#f92672">,</span> Setting<span style="color:#f92672">.</span><span style="color:#a6e22e">Property</span><span style="color:#f92672">.</span><span style="color:#a6e22e">NodeScope</span><span style="color:#f92672">);</span>

    <span style="color:#a6e22e">@Override</span>
    <span style="color:#66d9ef">public</span> List<span style="color:#f92672">&lt;</span>Setting<span style="color:#f92672">&lt;?&gt;&gt;</span> getSettings<span style="color:#f92672">()</span> <span style="color:#f92672">{</span>
        <span style="color:#66d9ef">return</span> Arrays<span style="color:#f92672">.</span><span style="color:#a6e22e">asList</span><span style="color:#f92672">(</span>YOUR_SETTING<span style="color:#f92672">);</span>
    <span style="color:#f92672">}</span>

    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">onModule</span><span style="color:#f92672">(</span>NodeModule nodeModule<span style="color:#f92672">)</span> <span style="color:#66d9ef">throws</span> IOException <span style="color:#f92672">{</span>
        nodeModule<span style="color:#f92672">.</span><span style="color:#a6e22e">registerProcessor</span><span style="color:#f92672">(</span>KuromojiPartOfSpeechExtractProcessor<span style="color:#f92672">.</span><span style="color:#a6e22e">TYPE</span><span style="color:#f92672">,</span>
                <span style="color:#f92672">(</span>registry<span style="color:#f92672">)</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">new</span> KuromojiPartOfSpeechExtractProcessor<span style="color:#f92672">.</span><span style="color:#a6e22e">Factory</span><span style="color:#f92672">());</span>
    <span style="color:#f92672">}</span>
<span style="color:#f92672">...</span>
</code></pre></div><p><code>YOUR_SETTING</code>プロパティと<code>getSettings()</code>メソッドは<code>elasticsearch.yml</code>で指定したい設定を記述する場合の例になります。今回は特に必要ないので両方削除しました。
最終系は<a href="https://github.com/johtani/elasticsearch-ingest-kuromoji-pos-extract">GitHubのコード</a>をご覧ください。</p>
<h4 id="kuromojipartofspeechextractprocessor">KuromojiPartOfSpeechExtractProcessor</h4>
<p>Processorは実際にIngest Nodeで行う処理を書くところです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Java" data-lang="Java">
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">final</span> String TYPE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;kuromoji_part_of_speech_extract&#34;</span><span style="color:#f92672">;</span>

    <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">final</span> String field<span style="color:#f92672">;</span>
    <span style="color:#66d9ef">private</span> <span style="color:#66d9ef">final</span> String targetField<span style="color:#f92672">;</span>

    <span style="color:#66d9ef">public</span> <span style="color:#a6e22e">KuromojiPartOfSpeechExtractProcessor</span><span style="color:#f92672">(</span>String tag<span style="color:#f92672">,</span> String field<span style="color:#f92672">,</span> String targetField<span style="color:#f92672">)</span> <span style="color:#66d9ef">throws</span> IOException <span style="color:#f92672">{</span>
        <span style="color:#66d9ef">super</span><span style="color:#f92672">(</span>tag<span style="color:#f92672">);</span>
        <span style="color:#66d9ef">this</span><span style="color:#f92672">.</span><span style="color:#a6e22e">field</span> <span style="color:#f92672">=</span> field<span style="color:#f92672">;</span>
        <span style="color:#66d9ef">this</span><span style="color:#f92672">.</span><span style="color:#a6e22e">targetField</span> <span style="color:#f92672">=</span> targetField<span style="color:#f92672">;</span>
    <span style="color:#f92672">}</span>

    <span style="color:#a6e22e">@Override</span>
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">execute</span><span style="color:#f92672">(</span>IngestDocument ingestDocument<span style="color:#f92672">)</span> <span style="color:#66d9ef">throws</span> Exception <span style="color:#f92672">{</span>
        String content <span style="color:#f92672">=</span> ingestDocument<span style="color:#f92672">.</span><span style="color:#a6e22e">getFieldValue</span><span style="color:#f92672">(</span>field<span style="color:#f92672">,</span> String<span style="color:#f92672">.</span><span style="color:#a6e22e">class</span><span style="color:#f92672">);</span>
        <span style="color:#75715e">// TODO implement me!
</span><span style="color:#75715e"></span>        ingestDocument<span style="color:#f92672">.</span><span style="color:#a6e22e">setFieldValue</span><span style="color:#f92672">(</span>targetField<span style="color:#f92672">,</span> content<span style="color:#f92672">);</span>
    <span style="color:#f92672">}</span>

    <span style="color:#a6e22e">@Override</span>
    <span style="color:#66d9ef">public</span> String <span style="color:#a6e22e">getType</span><span style="color:#f92672">()</span> <span style="color:#f92672">{</span>
        <span style="color:#66d9ef">return</span> TYPE<span style="color:#f92672">;</span>
    <span style="color:#f92672">}</span>

    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">static</span> <span style="color:#66d9ef">final</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Factory</span> <span style="color:#66d9ef">extends</span> AbstractProcessorFactory<span style="color:#f92672">&lt;</span>KuromojiPartOfSpeechExtractProcessor<span style="color:#f92672">&gt;</span> <span style="color:#f92672">{</span>

        <span style="color:#a6e22e">@Override</span>
        <span style="color:#66d9ef">public</span> KuromojiPartOfSpeechExtractProcessor <span style="color:#a6e22e">doCreate</span><span style="color:#f92672">(</span>String processorTag<span style="color:#f92672">,</span> Map<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">,</span> Object<span style="color:#f92672">&gt;</span> config<span style="color:#f92672">)</span> <span style="color:#66d9ef">throws</span> Exception <span style="color:#f92672">{</span>
            String field <span style="color:#f92672">=</span> readStringProperty<span style="color:#f92672">(</span>TYPE<span style="color:#f92672">,</span> processorTag<span style="color:#f92672">,</span> config<span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;field&#34;</span><span style="color:#f92672">);</span>
            String targetField <span style="color:#f92672">=</span> readStringProperty<span style="color:#f92672">(</span>TYPE<span style="color:#f92672">,</span> processorTag<span style="color:#f92672">,</span> config<span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;target_field&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;default_field_name&#34;</span><span style="color:#f92672">);</span>

            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">new</span> KuromojiPartOfSpeechExtractProcessor<span style="color:#f92672">(</span>processorTag<span style="color:#f92672">,</span> field<span style="color:#f92672">,</span> targetField<span style="color:#f92672">);</span>
        <span style="color:#f92672">}</span>
    <span style="color:#f92672">}</span>

</code></pre></div><p><code>TYPE</code>が<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-pipeline-api.html">Ingest APIのPipelineでProcessorを指定するときに使う名前</a>になります。ここは、cookiecutterの時にprocessor_typeに入力した文字列になっています。
<code>kuromoji_part_of_speech_extract</code>だと長いので、<code>kuromoji_pos_extract</code>に変えました。</p>
<p><code>execute()</code>メソッドに<code>// TODO implement me!</code>とあります。
この部分に実際の処理を記述していきます。</p>
<p>あとは、<code>Factory</code>クラスでIngest APIで指定された設定項目を読み込みます。
今回作成した<code>elasticsearch-ingest-kuromoji-pos-extract</code>では品詞を指定する必要があるので、<code>pos_tags</code>を指定できるように処理を追加しました。</p>
<p>私が実装したものの説明をするとちょっと長くなりそうなので、<a href="https://github.com/johtani/elasticsearch-ingest-kuromoji-pos-extract">GitHubのコード</a>をご覧ください。</p>
<h3 id="テストのコーディング">テストのコーディング</h3>
<p>テストのクラスもテンプレートで生成されています。</p>
<ul>
<li>KuromojiPartOfSpeechExtractProcessorTests</li>
<li>KuromojiPartOfSpeechExtractRestIT</li>
</ul>
<h4 id="kuromojipartofspeechextractprocessortests">KuromojiPartOfSpeechExtractProcessorTests</h4>
<p>Processorクラスのテストになります。生成直後は次のような感じです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Java" data-lang="Java"><span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">testThatProcessorWorks</span><span style="color:#f92672">()</span> <span style="color:#66d9ef">throws</span> Exception <span style="color:#f92672">{</span>
    Map<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">,</span> Object<span style="color:#f92672">&gt;</span> document <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> HashMap<span style="color:#f92672">&lt;&gt;();</span>
    document<span style="color:#f92672">.</span><span style="color:#a6e22e">put</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;source_field&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;fancy source field content&#34;</span><span style="color:#f92672">);</span>
    IngestDocument ingestDocument <span style="color:#f92672">=</span> RandomDocumentPicks<span style="color:#f92672">.</span><span style="color:#a6e22e">randomIngestDocument</span><span style="color:#f92672">(</span>random<span style="color:#f92672">(),</span> document<span style="color:#f92672">);</span>

    KuromojiPartOfSpeechExtractProcessor processor <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> KuromojiPartOfSpeechExtractProcessor<span style="color:#f92672">(</span>randomAsciiOfLength<span style="color:#f92672">(</span>10<span style="color:#f92672">),</span> <span style="color:#e6db74">&#34;source_field&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;target_field&#34;</span><span style="color:#f92672">);</span>
    processor<span style="color:#f92672">.</span><span style="color:#a6e22e">execute</span><span style="color:#f92672">(</span>ingestDocument<span style="color:#f92672">);</span>
    Map<span style="color:#f92672">&lt;</span>String<span style="color:#f92672">,</span> Object<span style="color:#f92672">&gt;</span> data <span style="color:#f92672">=</span> ingestDocument<span style="color:#f92672">.</span><span style="color:#a6e22e">getSourceAndMetadata</span><span style="color:#f92672">();</span>

    assertThat<span style="color:#f92672">(</span>data<span style="color:#f92672">,</span> hasKey<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;target_field&#34;</span><span style="color:#f92672">));</span>
    assertThat<span style="color:#f92672">(</span>data<span style="color:#f92672">.</span><span style="color:#a6e22e">get</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;target_field&#34;</span><span style="color:#f92672">),</span> is<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;fancy source field content&#34;</span><span style="color:#f92672">));</span>
    <span style="color:#75715e">// TODO add fancy assertions here
</span><span style="color:#75715e"></span><span style="color:#f92672">}</span>
</code></pre></div><p>テストメソッドも実装されていますが、パラメータの追加の設定処理やアサーションが書かれてません。
実装に合わせて、アサーションや設定処理を追加しましょう。</p>
<h4 id="kuromojipartofspeechextractrestit">KuromojiPartOfSpeechExtractRestIT</h4>
<p>こちらはIntegration Testになります。
実際にElasticsearchに対して外部からAPIを叩くような感じです。
APIを叩くときに利用するJSONの設定やアサーションは<code>src/test/resources</code>にyamlファイルがあります。</p>
<ul>
<li>10_basic.yaml</li>
<li>20_kuromoji_part_of_speech_extract_processor.yaml</li>
</ul>
<p><code>10_basic.yaml</code>はプラグインがインストールされているかの確認のテストです。特に変更する必要はないです。</p>
<p><code>20_kuromoji_part_of_speech_extract_processor.yaml</code>は実際にコーディングしたProcessorが動くかどうかのテストです。</p>
<p>テストの内容については、<a href="https://github.com/johtani/elasticsearch-ingest-kuromoji-pos-extract">GitHubのコード</a>をご覧ください。</p>
<h3 id="テストの実行とzipの生成">テストの実行とZipの生成</h3>
<p>テストの実行とZipの生成は次のコマンドを実行すればOKです。</p>
<pre><code>gradle check
</code></pre><p>テストに問題があった場合は、コケますし、問題なければ<code>SUCCESS</code>と表示が出ます。
成功した場合は<code>build/distributions/</code>というディレクトリにzipファイルができています。
これをElasticsearchのpluginコマンドでインストールすれば動きます。</p>
<pre><code>bin/plugin install file:///path/to/elasticsearch-ingest-kuromoji-pos-extract/build/distribution/ingest-kuromoji_part_of_speech_extract-0.0.1-SNAPSHOT.zip
</code></pre><h3 id="kuromoji_pos_extractの利用方法">kuromoji_pos_extractの利用方法</h3>
<p>Ingest APIには便利な<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html">Simulate Pipeline API</a>があります。</p>
<p>ということで、<a href="https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md">mecab-ipadic-NEologd</a>にあったサンプルの文章を使って、使い方の説明です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON"><span style="color:#960050;background-color:#1e0010">POST</span> <span style="color:#960050;background-color:#1e0010">_ingest/pipeline/_simulate</span>
{
  <span style="color:#f92672">&#34;pipeline&#34;</span> : {
    <span style="color:#f92672">&#34;description&#34;</span> : <span style="color:#e6db74">&#34;kuromoji neologd extract test&#34;</span>,
    <span style="color:#f92672">&#34;processors&#34;</span> : [
      {
        <span style="color:#f92672">&#34;kuromoji_pos_extract&#34;</span> : {
        <span style="color:#f92672">&#34;field&#34;</span> : <span style="color:#e6db74">&#34;body&#34;</span>,
        <span style="color:#f92672">&#34;target_field&#34;</span> : <span style="color:#e6db74">&#34;noun_field&#34;</span>,
        <span style="color:#f92672">&#34;pos_tags&#34;</span> : [
          <span style="color:#e6db74">&#34;名詞-固有名詞-組織&#34;</span>,
          <span style="color:#e6db74">&#34;名詞-固有名詞-一般&#34;</span>,
          <span style="color:#e6db74">&#34;名詞-固有名詞-人名-一般&#34;</span>,
          <span style="color:#e6db74">&#34;名詞-固有名詞-地域-一般&#34;</span>,
          <span style="color:#e6db74">&#34;名詞-固有名詞-地域-国&#34;</span>
          ]
        }
      }
      ]
  },
  <span style="color:#f92672">&#34;docs&#34;</span> : [
    {
      <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;index&#34;</span>,
      <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;type&#34;</span>,
      <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;id&#34;</span>,
      <span style="color:#f92672">&#34;_source&#34;</span>: {
        <span style="color:#f92672">&#34;body&#34;</span> : <span style="color:#e6db74">&#34;10日放送の「中居正広のミになる図書館」（テレビ朝日系）で、SMAPの中居正広が、篠原信一の過去の勘違いを明かす一幕があった。&#34;</span>
      }
    }
    ]
}
</code></pre></div><p>結果はこちら。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">{
  <span style="color:#f92672">&#34;docs&#34;</span>: [
    {
      <span style="color:#f92672">&#34;doc&#34;</span>: {
        <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;index&#34;</span>,
        <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;id&#34;</span>,
        <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;type&#34;</span>,
        <span style="color:#f92672">&#34;_source&#34;</span>: {
          <span style="color:#f92672">&#34;noun_field&#34;</span>: [
            <span style="color:#e6db74">&#34;10日&#34;</span>,
            <span style="color:#e6db74">&#34;中居正広のミになる図書館&#34;</span>,
            <span style="color:#e6db74">&#34;テレビ朝日&#34;</span>,
            <span style="color:#e6db74">&#34;SMAP&#34;</span>,
            <span style="color:#e6db74">&#34;中居正広&#34;</span>,
            <span style="color:#e6db74">&#34;篠原信一&#34;</span>
          ],
          <span style="color:#f92672">&#34;body&#34;</span>: <span style="color:#e6db74">&#34;10日放送の「中居正広のミになる図書館」（テレビ朝日系）で、SMAPの中居正広が、篠原信一の過去の勘違いを明かす一幕があった。&#34;</span>
        },
        <span style="color:#f92672">&#34;_ingest&#34;</span>: {
          <span style="color:#f92672">&#34;timestamp&#34;</span>: <span style="color:#e6db74">&#34;2016-07-22T06:18:49.007+0000&#34;</span>
        }
      }
    }
  ]
}
</code></pre></div><p><code>noun_field</code>に固有名詞の単語が抜き出せているのがわかるかと思います。</p>
<h3 id="alexのテンプレートで困った点">Alexのテンプレートで困った点</h3>
<p>テンプレートは便利だったのですが、<code>processor_type</code>に<code>_</code>を使用したタイプ名を指定すると次のような問題（？）が発生しました。</p>
<ul>
<li>クラス名が<code>Kuromoji_part_of_speech_extractProcessor</code>となってしまう</li>
</ul>
<p>深刻な問題ではないのですが、JavaだとCamel Caseが普通なのでちょっと気になって。
ということで、<a href="https://github.com/spinscale/cookiecutter-elasticsearch-ingest-processor/pull/1">プルリク</a>作って出してみました。まだ取り込まれてないかな。</p>
<p>取り込み前に使いたい方は以下のコマンドを実行してください。
<code>processor_class_name</code>という項目が増えています。
デフォルトだと<code>processor_type</code>の<code>_</code>の部分を取り除きつつCamel Caseにしたものが入ります。</p>
<pre><code>cookiecutter gh:johtani/cookiecutter-elasticsearch-ingest-processor
</code></pre><h3 id="まとめ">まとめ</h3>
<p>ということで、とりあえず作ってみましたというものになります。
特徴的な単語（固有名詞だけ）を抜き出して、別のフィールドにできるので、タグみたいなものをこれを使って前処理で作れるようになるかなぁと。</p>
<h2 id="参考ブログ元ネタ">参考ブログ（元ネタ？）</h2>
<p>インスパイア元となったブログです。</p>
<ul>
<li><a href="http://dev.classmethod.jp/server-side/elasticsearch/ingest-plugin-useragent/">User Agentを解析するIngest Pluginを書いてみた</a></li>
<li><a href="http://blog.cybozu.io/entry/2016/07/05/080000">Elasticsearch 5.0.0のIngest Node用プラグインを書いた話</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>よく物が壊れる年</title>
      <link>https://blog.johtani.info/blog/2016/06/30/broken-something-in-this-year/</link>
      <pubDate>Thu, 30 Jun 2016 10:57:38 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/06/30/broken-something-in-this-year/</guid>
      <description>なんか、今年はよく物が壊れる、厄年だからかなぁ？ ということで、記念に何が壊れたかをブログに残しておこうかと。 腕時計（壊れてないかな） 電波腕時</description>
      <content:encoded><p>なんか、今年はよく物が壊れる、厄年だからかなぁ？</p>
<p>ということで、記念に何が壊れたかをブログに残しておこうかと。</p>
<!-- more -->
<ol>
<li>腕時計（壊れてないかな）
<ul>
<li>電波腕時計を使ってたんだけど、電池がへたってたのでオーバーホールしてもらった。</li>
</ul>
</li>
<li>Moto 360（壊れたというか。。。）
<ul>
<li>何度かバージョンアップしてるんだけど、数ヶ月前から、ちょっと触っただけで電源が落ちる現象が。ファームウェアのせいかAndroid Wearのバージョンアップのせいかわからないけど。。。挙げ句の果てに、電源を入れるために充電器にセットしないといけないと言う酷い仕様。。。</li>
</ul>
</li>
<li>自転車前輪
<ul>
<li>自転車で車道を横切る側溝の蓋の間に前輪がはまってすっ転んでしまい、タイヤのスポークがちょっと曲がってしまう</li>
</ul>
</li>
<li>ジーンズその1
<ul>
<li>自転車で転んだ時に、膝とかこすって破れる</li>
</ul>
</li>
<li>ジーンズその2
<ul>
<li>福岡出張中に雨で滑って派手に転んで、膝に大きな穴が。。。</li>
</ul>
</li>
<li>リュック
<ul>
<li>
<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">あー、怪しいなと思ってたんだけど、やっぱりこわれた、、、 <a href="https://t.co/CD2Y7t6AOD">pic.twitter.com/CD2Y7t6AOD</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/743409932981014528">2016年6月16日</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</li>
</ul>
</li>
</ol>
<p>怪しいとは思ってたんだけど。。。
ということで、リュックを新調しつつ修理に出してるところ。
5. Xperia Z3
* 先週金曜日にカメラのレンズが結露してるなぁと思ったら、電源が入らなくなった。補償サービスに入ってたんで、新しいZ3を次の日に送付してもらい復旧。いろいろなアプリがログインしないといけないのでかなり大変だった。。。</p>
<p>まだ、半年残ってるので他にも壊れるのかなぁ。。。
お祓いしてもらったので、何もなければいいんだけど。</p>
<p>ということで、なんとなく<a href="http://www.amazon.co.jp/registry/wishlist/29EMX20UN9P16/ref=cm_sw_r_tw_ws_NhjDxb61H9RBS">欲しいものリスト</a>を貼っておきますね。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第16回Elasticsearch勉強会を開催しました。 #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2016/06/28/16th-elasticsearch-meetup/</link>
      <pubDate>Tue, 28 Jun 2016 13:55:55 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/06/28/16th-elasticsearch-meetup/</guid>
      <description>第16回Elasticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、会場提供していただいたリクルートテクノロジーズ</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/46539">第16回Elasticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、会場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
今回は、司会だけに注力してみました（）。</p>
<!-- more -->
<h3 id="チェックイン数など">チェックイン数など</h3>
<ul>
<li>チェックインした人：141名</li>
<li>キャンセルしなかった人：67名</li>
</ul>
<p>でした。</p>
<p>今回は、参加希望者が多くて、当日にも100名近いキャンセル待ちの方がいたので、
240名まで、参加者枠を増枠（会場キャパ190名程度）して対応しました。
まぁ、読み通り、1/3の方はキャンセルしない形でした。
天気も良く電車の遅延などもなさそうだったので、ちょっとドキドキしてたのですが。</p>
<p>以下は簡単なメモです。</p>
<h2 id="logstashとelasticsearchで作るenterprise-search-platform-elastic-kosho-owa">「LogstashとElasticsearchで作るEnterprise Search Platform」/ Elastic Kosho Owa</h2>
<p>スライド：https://speakerdeck.com/kosho/enabling-enterprise-search-platform-with-elastic-stack</p>
<ul>
<li>使ってるLogstashの設定ファイルを elastic-japan at elastic dot co に送るとTシャツがもらえるらしい。</li>
<li>Logstashの<a href="https://github.com/logstash-plugins/logstash-filter-ruby/blob/master/lib/logstash/filters/ruby.rb">filter-ruby</a>はここで、evalしてcallしてるから、特にforkとかしてないかと。</li>
</ul>
<h2 id="企業業界情報プラットフォームspeedaにおけるelasticsearchの活用--株式会社ユーザベース-北内-啓さん">「企業・業界情報プラットフォームSPEEDAにおけるElasticsearchの活用」 / 株式会社ユーザベース 北内 啓さん</h2>
<p>スライド：http://www.slideshare.net/tau3000/speedaelasticsearch-63510388</p>
<ul>
<li>アルゴリズム関連の開発担当</li>
<li>企業データをいろんな軸で検索したい
<ul>
<li>データ数が約70億レコードになりそう（通貨 x MySQL）</li>
</ul>
</li>
<li>300万企業データ＋Nestedとかで持ってる。
<ul>
<li>11万フィールド？？？</li>
<li>10台の物理サーバに24仮想マシン</li>
</ul>
</li>
<li>企業名の検索
<ul>
<li>recall重視</li>
</ul>
</li>
<li>NewsPicksの検索機能
<ul>
<li>「日本 化粧品 売上高」業界のデータとかも観れるのかな？有料会員向け機能</li>
<li>登録済みキーワードかどうかをRDB＋Esに検索して、ID化するっぽい
<ul>
<li>ID（Analyze必要ない）検索だから、termクエリだった、サンプルが。</li>
</ul>
</li>
</ul>
</li>
<li>ノードの役割分担
<ul>
<li>更新はMasterNode経由でDataNodeへ。</li>
<li>検索はClientNode経由でDataNodeへ。</li>
</ul>
</li>
<li>1.xかぁ。。。</li>
</ul>
<h2 id="elasticsearchベースの全文検索システムfess--株式会社エヌツーエスエム-菅谷信介さん">「Elasticsearchベースの全文検索システムFess」 / 株式会社エヌツーエスエム 菅谷信介さん</h2>
<p>スライド：http://www.slideshare.net/shinsuke/elasticsearchfess</p>
<ul>
<li>10.xからSolrをやめてElasticsearchへ。</li>
<li>日本語検索
<ul>
<li>bigram＋形態素（1文字検索とかに対応するため）</li>
</ul>
</li>
<li>NeologDに対応したkuromojiを利用</li>
<li>DBFluteをESFluteとしてEs対応</li>
<li>KOPFを組み込んで使ってる</li>
<li>configをREST API経由で更新できるプラグインあり</li>
</ul>
<h2 id="lt">LT</h2>
<h3 id="elasticsearchとgcpのネットワークでハマった話-株式会社サイバーエージェント-平田大地-さん-daichild">「ElasticsearchとGCPのネットワークでハマった話」 株式会社サイバーエージェント 平田大地 さん @daichild</h3>
<p>スライド：https://speakerdeck.com/daic_h/gcpfalsenetutowakudehamatutahua</p>
<ul>
<li>hhkb2 2刀流！</li>
<li>networkのKeep-alive周りで困ったよというお話。</li>
<li>後で聞いたけど、<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-gce.html">GCE Cloud Plugin</a>は使ってるそうです。</li>
</ul>
<p><strong>06/28 17:00追記</strong></p>
<ul>
<li>Pingを定期的に実行させることで回避も出来るようです。</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-transport.html#_tcp_transport">transport.ping_schedule</a>に時間を指定します。通常のNode（Transport以外）は&rsquo;-1'が指定してあり、動作してません。</li>
</ul>
<h3 id="スクリプトフィールドで作るランキングみたいな何かiwag-さん">「スクリプトフィールドで作るランキングみたいな何か」iwag さん</h3>
<p>スライド：https://speakerdeck.com/iwag/elasticsearch-dezuo-rurankingu</p>
<ul>
<li>1.xかぁ。。。</li>
<li>あとは、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html">function_score</a>とかも面白いですよ！</li>
</ul>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://tsuyoshi-nakamura.hatenablog.com/entry/2016/06/28/115244">第16回elasticsearch勉強会に参加してきた</a></li>
<li><a href="https://masutaka.net/chalow/2016-06-28-1.html">第16回Elasticsearch勉強会に参加してきた </a></li>
</ul>
<h2 id="まとめ宣伝">まとめ＋宣伝？</h2>
<p>1.xがまだまだいますねぇ、早く2.xにアップしましょう！（5.0ももう直ぐだし）。懇親会でも色々と話しましたが、https://discuss.elastic.co というフォーラムあるので、ぜひ活用してください。</p>
<p>次回は8月末か9月頭かでしょうか。
<a href="http://www.ospn.jp/osc2016-kyoto/">7月末にOSC京都</a>に出没するので、京都で勉強会やりたいと思ってます！
会場とかスピーカーとか興味ある人連絡ください。</p>
<p>東京の勉強会のスピーカーも随時募集中ですので、連絡ください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第15回Elasticsearch勉強会を開催しました。 #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2016/03/17/15th-elasticsearch-jp/</link>
      <pubDate>Thu, 17 Mar 2016 12:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/03/17/15th-elasticsearch-jp/</guid>
      <description>第15回Elasticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、会場提供していただいたリクルートテクノロジーズ</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/40444">第15回Elasticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、会場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
今回は、Elastic{on} 2016開催直後ということで、大半はElastic{on}に関する話でした。</p>
<!-- more -->
<h3 id="チェックイン数など">チェックイン数など</h3>
<ul>
<li>チェックインした人：114名</li>
<li>キャンセルしなかった人：62名</li>
</ul>
<p>でした。
今回は、少しおそめで1時間前にキャンセル待ちがいない状態にしました。
まぁ、いつもの感じでしょうか。数値も安定してきた感じですかね。</p>
<h2 id="elasticsearchと機械学習を実際に連携させる--preferred-networks-america-inchttpswwwpreferred-networksjp-cto-久保田展行kubota-nobuyuki-さん">&ldquo;Elasticsearchと機械学習を実際に連携させる&rdquo; / <a href="https://www.preferred-networks.jp">Preferred Networks America, Inc.</a> CTO 久保田展行(Kubota Nobuyuki) さん</h2>
<p>スライド：<a href="http://www.slideshare.net/nobu_k/elasticsearch-59627321">Elasticsearchと機械学習を実際に連携させる</a></p>
<p><a href="http://blog.johtani.info/blog/2016/01/08/14th-elasticcsearch-jp/">前回の続き</a>の話で、今回が本題でした。</p>
<p>勉強会直前に発表された<a href="http://sensorbee.io">SensorBee</a>をElasticsearchと一緒に使うとどんなことができるかというお話です。
まぁ、前処理重要ですよねというのが、いつものことですが、印象的でした。
いつものようにわかりやすい説明だったので、使ってブログを書いて欲しいなと。</p>
<p>発表の中で、説明に出てきたデモとか。</p>
<ul>
<li><a href="https://research.preferred.jp/2016/01/ces2016/">CES2016でロボットカーのデモを展示してきました</a></li>
</ul>
<h2 id="elasticon-2016レポート--elastic-jun-ohtani">&ldquo;Elastic{ON} 2016レポート&rdquo; / Elastic Jun Ohtani</h2>
<p>スライド：<a href="https://speakerdeck.com/johtani/elastic-on-2016-repoto">elastic{on} 2016 レポート</a></p>
<p>写真多めで、キーノートをメインに話をしました。</p>
<p>簡単なまとめとしては</p>
<ul>
<li>プロダクトロゴができました。<a href="https://www.elastic.co/brand">ロゴ画像などはこちら</a></li>
<li>次のメインバージョンは全て5.0。（<a href="https://www.elastic.co/v5">5.0に関する通知が欲しい人はこちらで登録できます</a>）</li>
<li><a href="https://www.elastic.co/elasticon/conf/2016/sf">elastic{on} 2016のビデオなどはこちら</a></li>
<li>BBL始めます。連絡ください</li>
</ul>
<h2 id="elasticonの過ごし方--クラスメソッド株式会社-藤本-真司-さん">&ldquo;Elastic{ON}の過ごし方&rdquo; / クラスメソッド株式会社 藤本 真司 さん</h2>
<p>スライド：<a href="http://dev.classmethod.jp/server-side/elasticsearch/elasticsearch-study-15-lt/">Elastic{ON}の過ごし方</a></p>
<p>印象に残ったのは</p>
<ul>
<li>「自他共に認めるブログの会社」</li>
<li>4/12にSAPさんに会場を借りてElastic＆クラスメソッドでイベントやります。</li>
</ul>
<p>やっぱりご飯が美味しいんですねぇ。
早速<a href="http://dev.classmethod.jp/server-side/elasticsearch/elasticsearch-study-15-lt/">ブログ</a>が書かれてました。</p>
<h2 id="elasticon-2016-見るべきセッション資料-7選--acroquest-technology株式会社-谷本-心-さん">&ldquo;Elastic{ON} 2016 見るべきセッション資料 7選&rdquo; / Acroquest Technology株式会社 谷本 心 さん</h2>
<p>スライド：<a href="https://speakerdeck.com/shintanimoto/elastic-on-2016-jian-rubekisetusiyonzi-liao-7xuan-number-elasticsearchjp">Elastic{ON} 2016 見るべきセッション資料 7選 #elasticsearchjp</a></p>
<p>印象に残ったのは</p>
<ul>
<li>東京でハンズオンやる会場提供者募集中！</li>
<li>Ingest Node（参考：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html">Ingest Nodeのドキュメントは公開中。</a>）</li>
<li>Reindex API（参考：<a href="https://github.com/elastic/elasticsearch/pull/17060">Backport reindex to 2.x </a>）</li>
</ul>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://dev.classmethod.jp/server-side/elasticsearch/elasticsearch-study-15-lt/">第15回elasticsearch勉強会にLTで登壇しました #elasticsearch #elasticsearchjp</a></li>
<li><a href="http://d.hatena.ne.jp/Kazuhira/20160316/1458142636">第15回elasticsearch勉強会に参加してきました #elasticsearch #elasticsearchjp</a></li>
</ul>
<h2 id="まとめ宣伝">まとめ＋宣伝</h2>
<p>来年のElastic{ON}に参加したいと思っていただけたらよかったなと。</p>
<p>4/12にクラスメソッドさんとイベントを行います。また、ツイートすると思います。</p>
<p>次回はいつも通りだと5月中旬になるかと思います（大丈夫かな？<a href="http://www.ospn.jp/osc2016-nagoya/">OSC 2016 Nagoya</a>でしゃべったり、ブース出したりとかするけど）。
5末に名古屋に出没します。名古屋で勉強会できればやりたいと思ってます。会場とかスピーカーとか興味がある方は連絡ください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>「自然言語処理の基本と技術」を読んでる</title>
      <link>https://blog.johtani.info/blog/2016/03/14/review-basics-and-tech-of-nlp/</link>
      <pubDate>Mon, 14 Mar 2016 22:34:12 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/03/14/review-basics-and-tech-of-nlp/</guid>
      <description>久々のポスト。。。 久々に、技術書読んでます。「自然言語処理の基本と技術」という本です。 監修の方のツイートを見て気になったので、買ってみました</description>
      <content:encoded><p>久々のポスト。。。</p>
<p>久々に、技術書読んでます。<a href="http://amzn.to/1QSplr1">「自然言語処理の基本と技術」</a>という本です。</p>
<p>監修の方のツイートを見て気になったので、買ってみました。
書籍のサイトの説明はこんな感じでした。</p>
<!-- more -->
<blockquote>
<p>本書は、この未来に不可欠となるに違いない自然言語処理の、技術的、ビジネス的基礎知識をくまなくコンパクトに図解した一冊です。
著者陣もそれぞれの分野の第一線で活躍するエキスパート揃い！</p>
</blockquote>
<p>確かに著者陣がすごいです。</p>
<p>まだ、「はじめに」と自分に関係のある「情報検索」の章を流し読みしただけなんですが、次のような特徴がある本です。</p>
<ul>
<li>平易な単語で説明してある（難しい専門用語が少ない）</li>
<li>数式が出てこない（多分。少なくとも読んだ部分では見てない）</li>
<li>説明には例と図解がある</li>
</ul>
<p>情報検索の章で言うと、全文検索でよく使われる転置インデックス（索引という単語が使われてる）がなぜ必要なのか、どういう感じで作られるのか、
転置インデックスに利用する索引の単語をどうやって作るのか（文字N-Gramや形態素解析）、単語の正規化（ステミングやストップワード）などの説明が
本当にわかりやすく書かれています。
スコアリングについても触れられています。</p>
<p>Elasticsearchも転置インデックスを用いた検索を行っており、
MappingでAnalyzerの指定をしている理由などの理解に役に立つと思います。</p>
<p>全文検索システムがどのように検索を処理しているかをざっくり理解するのにはもってこいじゃないかと。
1点残念だなと思ったのは、書籍に「索引」がありませんでした（本の索引を思い浮かべてくださいっていう説明あったんだけど）。。。
Kindle版を購入すれば「検索」できるのかな？</p>
<p>ということで、まだ、流し読みしただけなんですが、「すごく」オススメです。
購入はこちらから！</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=479812852X&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
</content:encoded>
    </item>
    
    <item>
      <title>第14回Elasticsearch勉強会を開催しました。 #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2016/01/08/14th-elasticcsearch-jp/</link>
      <pubDate>Fri, 08 Jan 2016 11:34:56 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2016/01/08/14th-elasticcsearch-jp/</guid>
      <description>あけましておめでとうございます、johtaniです。 第14回Elasticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆</description>
      <content:encoded><p>あけましておめでとうございます、johtaniです。</p>
<p><a href="https://elasticsearch.doorkeeper.jp/events/36330">第14回Elasticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、会場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
今年もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<!-- more -->
<h3 id="チェックイン数など">チェックイン数など</h3>
<ul>
<li>チェックインした人：122名</li>
<li>キャンセルしなかった人：58名</li>
</ul>
<p>でした。
今回も当日の昼の時点でキャンセル待ちがない状態にしていました。
いくつか電車が止まっていたという話を聞いていたので、開始を5分遅らせ、
受付は45分くらいまで開けておくという対応をしてもらいました。</p>
<h3 id="ココが辛いよelasticsearch--株式会社リクルートテクノロジー-tatakaba-さん">&ldquo;ココが辛いよelasticsearch&rdquo; / 株式会社リクルートテクノロジー @tatakaba さん</h3>
<p>スライド：<a href="http://www.slideshare.net/takahitotakabayashi/elasticsearch-56936397">ココが辛いよelasticsearch</a></p>
<p>実際にいくつかのサービスで運用されている内容とどういった機能を利用しているか、
どういったものを独自に作っているかという話をしていただきました。</p>
<ul>
<li>独自PluginでA/Bテストしてる</li>
<li>Snapshotの活用</li>
<li>Index作成は環境に合わせて行っている。</li>
<li>バージョンは混在</li>
<li>PusnaRSのバージョンアップの話。
<ul>
<li>2つのバージョンのクラスタを用意してリアルタイムに切り替え。</li>
</ul>
</li>
<li>Elasticsearchの活用
<ul>
<li>QueryのRewrite：</li>
<li>SolrのリクエストをEsで受け付けたり。</li>
</ul>
</li>
<li>辛い話。
<ul>
<li>バージョンアップが辛い</li>
<li>Riverなくなるのつらい</li>
<li>データずれるのつらい</li>
</ul>
</li>
</ul>
<p>補足：</p>
<h4 id="バージョンアップについて">バージョンアップについて</h4>
<p>1.x系から2.x系にアップされるのであれば、こちらを必ず試してください。</p>
<p><a href="https://github.com/elastic/elasticsearch-migration">https://github.com/elastic/elasticsearch-migration</a></p>
<p>「.」が使えなくなるという話は、Solrとの大きな違いになるのかもなぁと。
ネスト構造のデータの表記を「.」で行うというのを厳密に行えるように、
「.」を使えなくしたというのがあるかと。</p>
<h4 id="riverについて">Riverについて</h4>
<p>Riverがなくなった理由については、https://www.elastic.co/blog/deprecating-rivers で記載があります。
便利なのですが、負荷が偏ったり、スケールしないとかいう問題点があるかなと。</p>
<p>良いサンプルとしては、JDBC Riverなどは、Javaのプログラムとして起動できるように変更されていたりします。</p>
<p><a href="https://github.com/jprante/elasticsearch-jdbc/wiki/jdbc-plugin-feeder-mode-as-an-alternative-to-the-deprecated-elasticsearch-river-api">https://github.com/jprante/elasticsearch-jdbc/wiki/jdbc-plugin-feeder-mode-as-an-alternative-to-the-deprecated-elasticsearch-river-api</a></p>
<p>（<strong>個人的</strong> には、SolrのDIHもRiverもあんまり好きではなかったです。データの変換処理と、ロード処理は別々にしたい人だったので。）</p>
<h4 id="データのズレなど">データのズレなど</h4>
<p>耐障害性とか信頼性に関しては、どういった問題点があるのか、どういった対応をしているのかというのがまとめられたページが用意されています。</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html">https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html</a></p>
<h3 id="機械学習を利用したちょっとリッチな検索--preferred-networks-america-inc-cto-久保田展行kubota-nobuyuki-さん">「機械学習を利用したちょっとリッチな検索」 / Preferred Networks America, Inc. CTO 久保田展行(Kubota Nobuyuki) さん</h3>
<p>スライド：<a href="http://www.slideshare.net/nobu_k/ss-56810268">機械学習を利用したちょっとリッチな検索</a></p>
<p>来日していただき、機械学習と検索の話をしてもらいました。
本編は次回の発表かもw</p>
<ul>
<li>機械学習を元に、検索対象の情報を元の情報から増やしてあげる。</li>
<li>増えた情報を検索できるようにする</li>
</ul>
<h4 id="今日のゴール">今日のゴール：</h4>
<ul>
<li>機械学習とはどういうものか？</li>
<li>データの集め方とか、アノテーションとか</li>
<li>学習の方法（ツールやライブラリに依存）</li>
</ul>
<h4 id="esでの活用方法">Esでの活用方法</h4>
<ul>
<li>オフラインで学習させて、情報を付与した後に、Elasticsearchに入れる</li>
<li>Jubatus＋fluentdで</li>
<li>ChainerサポートのOSSのツールを公開予定</li>
</ul>
<p>「ここからが本当の地獄だ。。。」ってのが聴きたいw</p>
<h3 id="lucene-query-再考---domain-specific-query-実装----supership株式会社-インフラ事業開発本部検索グループ-大川真吾-さん">「Lucene Query 再考 - Domain Specific Query 実装 -」 / Supership株式会社 インフラ事業開発本部検索グループ 大川真吾 さん</h3>
<p>スライド：<a href="http://www.slideshare.net/ShingoOKAWA/elasticsearch-20150107-56772462">Lucene Query 再考 - Domain Specific Query 実装 -</a></p>
<p>Luceneのクエリに関する話と、クエリパーサーに関する話でした。
こういった濃い話も勉強会でしてもらえると、色々な参加者に楽しんでいただけるかなぁと。
次回も続きを話してもらう予定です。</p>
<p>補足：</p>
<p>参考までにですが、Elasticsearchに入門したての人向けに、
Analyzerとか転置インデックスとかの話をした時のスライドになります。
<a href="https://speakerdeck.com/johtani/lucenetori-ben-yu-falsejian-suo">https://speakerdeck.com/johtani/lucenetori-ben-yu-falsejian-suo</a></p>
<h3 id="lt">LT:</h3>
<h4 id="fluentd-meets-beats--repeatedly-さん">Fluentd meets Beats / @repeatedly さん</h4>
<p>スライド：http://www.slideshare.net/repeatedly/fluentpluginbeats-at-elasticsearch-meetup-14</p>
<p>参考Qiita：http://qiita.com/repeatedly/items/77af41788f0b3ccdefd2</p>
<p>Beatsの説明をTDの人からしてもらうなどw
FluentdにBeatsからのデータを流し込めるようにしたプラグインが出たという話でした。</p>
<p>filebeatの性能の件は社内で聞いてみようかと。</p>
<h4 id="elasticsearchインデクシングのパフォーマンスを測ってみた--日本ibm黒澤亮二さん">Elasticsearchインデクシングのパフォーマンスを測ってみた / 日本IBM　黒澤亮二さん</h4>
<p>スライド：<a href="http://www.slideshare.net/kuron99/elasticsearch-56784623">Elasticsearchインデクシングのパフォーマンスを測ってみた</a></p>
<p>参考Qiita：http://qiita.com/rjkuro/items/e79eec7ffb0511b7c678</p>
<p>細かな性能測定の結果を駆け足で話してもらいました。
皆さんもこの指標をもとに、手元の環境を計測してみたりしてみてもらえればと。</p>
<p>あとは、2.x系になってるので、同じ方法で計測してもらってまた
発表してもらえると嬉しいなー（棒）</p>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://acro-engineer.hatenablog.com/entry/2016/01/08/123857">Elasticsearch勉強会　第14回フィードバック</a></li>
</ul>
<h2 id="まとめ宣伝">まとめ＋宣伝</h2>
<p>久々に（初めてかな？）、ゲストがいないのに自分が喋りませんでした。
次回は3月中旬を予定してます。
次回は、Elastic{ON}16の報告をする予定です。いろいろと発表あるだろうし。</p>
<p>あと、今月末の1/29に<a href="http://www.ospn.jp/osc2016.enterprise-osaka/">オープンソースカンファレンス 2016.enterprise@Osaka</a>にブース出展します。
セミナー枠でも弊社OSSプロダクトの概要を話しする予定です。
関西の方は、ぜひ参加していただければと。ブースでお待ちしております。</p>
<p>また、スピーカーや場所が用意できたら、出張勉強会もまたやりたいなと思っています。
興味ある方は、連絡ください！</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2015）</title>
      <link>https://blog.johtani.info/blog/2015/12/31/looking-back-2015/</link>
      <pubDate>Thu, 31 Dec 2015 21:33:02 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/12/31/looking-back-2015/</guid>
      <description>今年は紅白見ながら書いてます。 早い一年だったなぁ。 振り返り(2014年に書いた抱負から) まずは去年の抱負を元に振り返りをば。 英語の継続 海外の</description>
      <content:encoded><p>今年は紅白見ながら書いてます。
早い一年だったなぁ。</p>
<!-- more -->
<h2 id="振り返り2014年に書いた抱負から">振り返り(2014年に書いた抱負から)</h2>
<p>まずは去年の抱負を元に振り返りをば。</p>
<ul>
<li>英語の継続</li>
<li>海外のイベントへの参加</li>
<li>多岐にわたるイベントでのスピーカー</li>
<li>日本の人員の倍増！？</li>
<li>Elasticsearchに関する日本語の情報発信</li>
<li>Elasticsearch座談会みたいなものの開催</li>
</ul>
<p>英語はまぁ、継続してます。亀の歩みですが。。。</p>
<p>海外のイベントへの参加は、<a href="http://blog.johtani.info/blog/2015/03/11/attend-elasticon/">自社のイベント</a>だけでした。<a href="https://www.elastic.co/elasticon/conf/2016/sf">来年のElastic{ON}</a>ももちろん参加です。来年こそBerlin Buzzwords行きたいな。。。</p>
<p>多岐にわたるイベントとまではいってないですかね。。。来年はOSC(<a href="http://www.ospn.jp/osc2016.enterprise-osaka/">大阪</a>や<a href="http://www.ospn.jp/osc2016-spring/">東京</a>)に出没予定です。参加される方は、声をかけていただければと。</p>
<p>人員は倍増しました！営業の方が参画されたのがかなり助かってます。
今現在で4名です！来年も倍増できるかなぁ？</p>
<p>日本語の情報発信はリリースブログの翻訳が多めでした。来年は独自コンテンツも増やさないとなぁ。
あとやっぱ書籍かなぁ。。。座談会は来年の課題に。。。</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<p>その他の今年の出来事。</p>
<ul>
<li>初渡米</li>
<li>初バルセロナ</li>
<li>東京以外で勉強会</li>
<li>初自社イベント（日本で）</li>
<li>初トレーナー</li>
<li>不惑</li>
</ul>
<p>今年も初モノ多いですね。今年も楽しい一年でした。
自社イベントでアメリカやバルセロナなどに行きました。40年近く、海外に行ったことないせいか、海外に行って経験することが面白いです。</p>
<p>東京以外での勉強会もやりました。名古屋、大阪、京都、北海道と。
来年もOSCで訪れた場所で勉強会を開催したいなぁと。</p>
<p>自社のイベント（Elastic{ON} Tour Tokyo）も東京でやりました。
どうなることかと思っていましたが、朝から大勢の方に来ていただけてホッとしました。
自分のトークがどうだったのかという反応も気になりますが。。。
せっかくShayたちが来ていたので、もっとブースに話しに来てもらえるともっと嬉しかったかもなぁと。</p>
<p>Tour Tokyoを挟んで自社の公式トレーニングのトレーナーもやりました。
日本で3回目にして、初の日本語でのトレーニングでした。前の2回は逐次通訳だったんですが。
トレーニングって大変だなぁと思ったのと、やはり日本語でトレーニングを受けられるとだいぶ違うだろうなという実感がありました。
来年もできればいいなと。トレーニング自体があったことを知らない人もいたのかなぁ？</p>
<h2 id="来年の抱負">来年の抱負</h2>
<ul>
<li>英語の継続</li>
<li>もっとElasticsearchの開発に参加</li>
<li>人員の倍増？</li>
<li>日本語情報発信</li>
<li>Splatoon S+?</li>
</ul>
<p>英語あいかわらず出来てないので、頑張らないと。
もっと喋ったりしないとなんだろうなぁ。</p>
<p>来年は開発をもっとやりたいなと。
来年1月にはセールスのエンジニアが加入するので、開発にもっと時間を割きたいなと。</p>
<p>人員は倍増できるように頑張りたいです。少なくとも、2人が1月に入ることは決まってるんで、後二人！</p>
<p>ブログももっと書かないとですね。新機能とかこんな使い方できるよとか。
どんな記事が欲しいかとかコメントもらえると楽なので、突っ込みくださいw</p>
<p>最後は、完全に私用ですね。Splatoonにはまってますw
SとA+を行ったり来たりで。。。</p>
<p>ということで、今年はあと、数時間ですが、色々とお世話になりました。 この場を借りてお礼申し上げます。</p>
<p>来年も、いろんな方に絡んでいくとは思いますが、よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>2015年のElasticsearch</title>
      <link>https://blog.johtani.info/blog/2015/12/25/about-elasticsearch-in-2015/</link>
      <pubDate>Fri, 25 Dec 2015 13:29:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/12/25/about-elasticsearch-in-2015/</guid>
      <description>今年最後のAdvent Calendarとなります。 この記事はElasticsearch Advent Calendar 2015の最終日のエントリです。 簡単に今年の変遷を</description>
      <content:encoded><p>今年最後のAdvent Calendarとなります。</p>
<p>この記事は<a href="http://qiita.com/advent-calendar/2015/elasticsearch">Elasticsearch Advent Calendar 2015</a>の最終日のエントリです。</p>
<p>簡単に今年の変遷を、Elasticsearchをベースに振り返ってみようかと思います。</p>
<!-- more -->
<h2 id="kibana-4リリース2月">Kibana 4リリース（2月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/02/20/kibana-4-literally-ja/">Kibana 4（日本語訳）</a></li>
</ul>
<p>いきなり、Elasticsearchではない話題ですが。
AggregationベースのKibanaがリリースされました。
画面が黒くないというので、話題になりましたw
12月末時点では、4.3.1になっています。
Sub Aggregationによる強力なグラフ表示や異なるインデックスに対するグラフを
一つのダッシュボードに表示できるといったことができるようになりました。</p>
<h2 id="セキュリティ向けプラグインshieldのリリース2月">セキュリティ向けプラグインShieldのリリース（2月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/02/27/you-know-for-security-shield-goes-ga-ja/">セキュリティ向けプラグインShieldのリリース（日本語訳）</a></li>
</ul>
<p>商用向けのプラグインの第2弾です。
セキュリティ強化のためのプラグインで、いろいろなところで引き合いがあったりします。</p>
<h2 id="初のユーザカンファレンスelasticon開催3月">初のユーザカンファレンス、Elastic{ON}開催（3月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/03/11/attend-elasticon/">#elasticon に参加中</a></li>
</ul>
<p>サンフランシスコで、弊社初のカンファレンスが開催されました。（来年（2016年）もサンフランシスコで開催されます。）
また、ここで、以下の2点の発表がありました。</p>
<ul>
<li>ロゴ及びドメイン名などの変更</li>
<li>Foundのジョイン</li>
</ul>
<p>約1300名が参加する大イベントでした。
初の渡米で楽しんできましたが、ドメインの切り替えなどは大変でした。。。
まだ、ロゴを変えて1年経ってないということが実感できてないです。</p>
<p><a href="https://www.elastic.co/found">Found</a>のジョインはまだまだ、日本で知名度が出てないかもなぁと。
もっと広めないと。
利点としては以下の通りです。</p>
<ul>
<li>新バージョンがすぐに利用可能に。また、バージョンアップも画面で指定可能</li>
<li>公式プラグイン＋その他いくつかのプラグインが利用可能</li>
</ul>
<h2 id="elasticsearch-15-リリース4月">Elasticsearch 1.5 リリース（4月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/04/01/elasticsearch-1-5-0-released-ja/">Elasticsearch 1.5.0リリース（日本語訳）</a></li>
</ul>
<p>主に、resiliencyに関する改良になります。
毎リリースで信頼性向上につながる改良が含まれる形になっています。
このリリースの近くで初の東京の外での勉強会を<a href="http://blog.johtani.info/blog/2015/04/04/elasticsearch-study-session-at-nagoya/">名古屋で開催</a>したりもしました。</p>
<h2 id="discusselasticcoをオープン5月">discuss.elastic.coをオープン（5月）</h2>
<ul>
<li><a href="https://discuss.elastic.co">https://discuss.elastic.co</a></li>
</ul>
<p>これまでは、Google Groupsを使っていましたが、Elasticが提供しているプロダクトが
別々のグループであったために、プロダクトにまたがった質問がやりにくかったり、検索がしにくかったりという問題点がありました。
今では、過去のGoogle Groupsのデータも移行されているので、是非参加して、質問・回答してみてください。
<a href="https://discuss.elastic.co/c/in-your-native-tongue/18-category">日本語でやりとりできるカテゴリ</a>もあるので、どんどん、やりとりしていただければ。</p>
<h2 id="elasticsearch-16-リリース6月">Elasticsearch 1.6 リリース（6月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/06/10/elasticsearch-1-6-0-released-ja/">Elasticsearch 1.6.0リリース（日本語訳）</a></li>
</ul>
<p>2.0に向けたUpgrade APIが含まれるなど、次期リリースに向けた準備が整いつつあるリリースでした。
他にもsynced flushの取り込みやレスポンスのJSONのフィルタリングなど細かな改善も取り込まれています。</p>
<h2 id="found-premiumとstandardリリース7月">Found PremiumとStandardリリース（7月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/07/07/we-just-made-found-more-awesome-ja/">さらに進化したFound（日本語訳）</a></li>
</ul>
<p>Foundに弊社のサポートチームがサポートできるプレミアムが追加されました。
これにより、商用プラグインとして提供しているShieldが（現在はWatcherも）利用できるなど、
より便利になりました。また、Kibana 4も無料で提供されていたりします。</p>
<p><a href="https://www.elastic.co/found/pricing">小さなサイズのものですと、無料で試していただける</a>ものもあるので、試してみてもらえればと。</p>
<h2 id="elasticsearch-17-リリース7月">Elasticsearch 1.7 リリース（7月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/07/22/elasticsearch-1-7-0-and-1-6-1-released-ja/">Elasticsearch 1.7.0 および 1.6.1リリース（日本語訳）</a></li>
</ul>
<p>1.x系、最後のリリースでした。
小さい改善ですが、セキュリティフィックス、クラスタの安定化に寄与する機能改善が含まれています。</p>
<p>この<a href="http://blog.johtani.info/blog/2015/07/16/kansai-1st-elasticsearch-jp/">リリース直前に大阪、京都で勉強会も開催してみました。</a></p>
<h2 id="elasticsearch-200-beta1-リリース8月">Elasticsearch 2.0.0-beta1 リリース（8月）</h2>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/08/27/elasticsearch-2-0-0-beta1-released-ja/">Elasticsearch 2.0.0-beta1リリース（日本語訳）</a></li>
</ul>
<p>待ちに待った、Lucene 5ベースのElasticsearchの登場でした。
doc_valuesがデフォルトになったり、エラーが構造化されて見やすくなったり、
Pipeline Aggregationが導入されたりしています。
また、問題点の洗い出しも兼ねて、ベータリリースとして、本リリースまでに多くのIssueをあげていただきました。</p>
<h2 id="elasticsearch-200-リリース10月">Elasticsearch 2.0.0 リリース（10月）</h2>
<p>2.0の本リリースです。リリースまでに、beta1、2及び、rc1がリリースされました。</p>
<p>追加された機能や目玉の改善については「<a href="https://speakerdeck.com/johtani/elasticsearch-2-dot-0falseshao-jie">Elasticsearch 2.0の紹介</a>」のスライドを参考にしていただければと。</p>
<p>また、Elasticsearch 2.0のリリースに合わせて、商用プラグインやLogstash、Kibanaの新しいバージョンがリリースされました。
Kibanaなどは、プラットフォームとしての機能を備え、Senseや<a href="http://blog.johtani.info/blog/2015/12/01/introduction-timelion/">Timelion</a>と言ったプラグインアプリもリリースされています。</p>
<ul>
<li><a href="http://blog.johtani.info/blog/2015/10/29/logstash-2-0-0-released-ja/">Logstash 2.0.0リリース（日本語訳）
</a></li>
<li><a href="http://blog.johtani.info/blog/2015/10/29/kibana-4-2-0-ja/">Kibana 4.2.0リリース（日本語訳）
</a></li>
<li><a href="http://blog.johtani.info/blog/2015/10/30/sense-2-0-0-beta1-ja/">Senseの歴史 - Sense 2.0.0-beta1の紹介(日本語訳)</a></li>
<li><a href="http://blog.johtani.info/blog/2015/10/30/shield-watcher-and-marvel-2-0-ga-released-ja/">Shield、Watcher、Marvel 2.0.0 GAリリース（日本語訳）</a></li>
</ul>
<h2 id="elasticsearch-210-リリース11月">Elasticsearch 2.1.0 リリース（11月）</h2>
<ul>
<li><a href="https://www.elastic.co/blog/elasticsearch-2-1-0-and-2-0-1-released">Elasticsearch 2.1.0 and 2.0.1 released</a></li>
</ul>
<h2 id="beats-100のリリース11月">Beats 1.0.0のリリース（11月）</h2>
<ul>
<li><a href="https://www.elastic.co/blog/beats-1-0-0">The Beats 1.0.0</a></li>
</ul>
<p>Go言語で書かれた軽量データシッパーになります。
パケットをキャプチャしてElasticsearchに送る<a href="https://www.elastic.co/downloads/beats/packetbeat">Packetbeat</a>、
topコマンドで取れるデータなどを<a href="https://www.elastic.co/downloads/beats/topbeat">Topbeat</a>、
ログファイルなどを取り込み配送する<a href="https://www.elastic.co/downloads/beats/filebeat">Filebeat</a>がリリースされました。</p>
<p><a href="https://www.elastic.co/guide/en/beats/libbeat/current/index.html">libbeat</a>と呼ばれる、
ベースとなるライブラリを元にしたプロダクトで、Logstashのエージェントのような使い方もできるようになっています。</p>
<p>Go言語に興味のある方などは、調べてみてはいかがでしょう？</p>
<h2 id="来年は">来年は？</h2>
<p>日本では、<a href="https://elasticsearch.doorkeeper.jp/events/36330">1/7に第14回Elasticsearch勉強会</a>を開催します。
すでに、38名のキャンセル待ちとなっていますが、おそらく、年明けにキャンセルがそこそこ出ると思うので、まだ間に合うんじゃないかなぁと。</p>
<p>会社としては、<a href="https://www.elastic.co/elasticon/conf/2016/sf">Elastic{ON}16</a>が控えています。参加される方は、ぜひ現地で声をかけてください！！</p>
<p>その他にもイベント、オープンソースカンファレンス（まずは、<a href="https://www.ospn.jp/osc2016.enterprise-osaka/">大阪</a>、<a href="http://www.ospn.jp/osc2016-spring/">東京</a>）などに出没する予定ですので、こちらも参加していただければと。</p>
<p>では、また来年のAdvent Calendarでお会いしましょう！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Timelionの紹介 - Elasticsearch Advent Calendar 2015 1日目</title>
      <link>https://blog.johtani.info/blog/2015/12/01/introduction-timelion/</link>
      <pubDate>Tue, 01 Dec 2015 11:28:11 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/12/01/introduction-timelion/</guid>
      <description>こんにちは、@johtaniです。 早いもので、師走です。今年もあと少しとなりました（今月が一番忙しかったりしますが。。。）。 ということで、A</description>
      <content:encoded><p>こんにちは、<a href="https://twitter.com/johtani">@johtani</a>です。</p>
<p>早いもので、師走です。今年もあと少しとなりました（今月が一番忙しかったりしますが。。。）。
ということで、Advent Calendarの季節が始まりました。</p>
<p>この記事は<a href="http://qiita.com/advent-calendar/2015/elasticsearch">Elasticsearch Advent Calendar 2015</a>の1日目のエントリです。</p>
<p>今日は、最近公開された<a href="https://github.com/elastic/timelion">Timelion</a>の紹介をしたいと思います。</p>
<!-- more -->
<h2 id="timelion">Timelion?</h2>
<p>11/12に公開されたばかりのアプリになります。（<a href="https://www.elastic.co/blog/timelion-timeline">公式のブログはこちら</a>。ブログでは動画による説明もあり）</p>
<p>Kibanaにプラグインとしてインストールすることで使用することができるようになるアプリです。
Timelionと書いて「Timeline」と読むようです。
Kibanaとは異なるグラフ描画のプラグインになっています。</p>
<h3 id="kibana-42からプラットフォーム化">Kibana 4.2からプラットフォーム化</h3>
<p>Kibana 4.2から、Kibanaにプラグイン機構が導入されました。
Kibanaとしての機能以外にも、プラグインとして、アプリを追加できるようになっています。
Timelionもその一つです。</p>
<h3 id="インストール">インストール</h3>
<p>Timelionを試してみるには、ElasticsearchとKibanaが必要になります。（こちらは、すでにインストールされているとして。。。）</p>
<p>Kibanaのコマンドを利用して、プラグインをインストールします。</p>
<pre><code>bin/kibana plugin -i kibana/timelion
</code></pre><p>インストールしたら、Kibanaにアクセスして、Timelionを呼び出します。</p>
<h3 id="timelionへアクセス">Timelionへアクセス</h3>
<p>ブラウザで<code>localhost:5601</code>にアクセスすると、Kibanaが出てきます。
Kibanaのプラグイン選択のアイコンをクリックし、Timelionのアイコンをクリックします。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20151201/switch_to_timelion.jpg" />
    </div>
    <a href="/images/entries/20151201/switch_to_timelion.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>すると、初期画面はこんな感じです。
直近15分のElasticsearchに入っているデータがが全部出てきます。
チュートリアルも出てきてます（初回起動時に出たはず）</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20151201/tutrial_timelion.jpg" />
    </div>
    <a href="/images/entries/20151201/tutrial_timelion.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>Kibanaでの検索窓の部分に関数を指定していくことで、グラフが描画できるツールになっています。</p>
<h3 id="サンプル気温データを可視化">サンプル：気温データを可視化</h3>
<p>百聞は一見に如かずということで、
<a href="http://www.data.jma.go.jp/gmd/risk/obsdl/index.php">気象庁のデータ</a>を使って、
ちょっとしたグラフを書いてみました。
1年間の気温の推移と日照時間になります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20151201/tenperature_naha_and_sapporo.jpg" />
    </div>
    <a href="/images/entries/20151201/tenperature_naha_and_sapporo.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>上のグラフが那覇、下グラフが札幌の気温のグラフになります。</p>
<ul>
<li>赤いライン：最高気温</li>
<li>青いライン：最低気温</li>
<li>黄色い棒グラフ：日照時間</li>
</ul>
<p>最低気温と日照時間はグラフは次のような式で描画しています。</p>
<h5 id="青いラインの最低気温">青いラインの最低気温</h5>
<p>気温のグラフになります。</p>
<pre><code>.es(index='tenki2', q='city:naha', metric='avg:temperature_min').label('min'),
</code></pre><p><code>.es()</code>がelasticsearchに対するデータ取得の関数です。
引数は次のような意味になります。</p>
<ul>
<li>index：対象とするインデックス名</li>
<li>q：検索クエリ。ここでは、cityというフィールドにnahaで検索。</li>
<li>metric：描画対象となっているデータの入ったフィールド。temperature_minというフィールドの1日毎の平均値を取得</li>
</ul>
<p>最低気温と最高気温は別々のフィールドに格納してあります。最高気温の場合は（temperature_max）を指定します。</p>
<p><code>.label(min)</code>で、グラフの凡例の指定です。
残念ながら、日本語の指定は現時点（2015年12月01日時点）ではうまくいかなかったです。（https://github.com/elastic/timelion/issues/17）</p>
<p>デフォルトでは、線グラフが選択されているので、グラフの種類は特に指定はしていません。
明確に指定する場合は<code>lines()</code>を指定します。</p>
<h5 id="黄色い棒グラフの日照時間">黄色い棒グラフの日照時間</h5>
<pre><code>.es(index='tenki2', q='city:naha', metric='avg:sunlight').label(sunlight).bars()
</code></pre><p><code>.es()</code>に関しては最低気温のグラフとほぼ一緒です。異なるのは、metricの取得対象のフィールド名です。</p>
<p><code>.label()</code>で凡例を指定しています。先程と同様です。</p>
<p>最後に、棒グラフにしたいため、<code>.bars()</code>を指定しています。</p>
<p>その他に用意されている関数について知りたい場合は、Timelionのヘルプを表示すると説明が出てきます。
<code>cusum()</code>のような値を累積して表示するような関数も用意されています。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20151201/about_help.jpg" />
    </div>
    <a href="/images/entries/20151201/about_help.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h3 id="まとめ">まとめ</h3>
<p>Kibanaとは少し違うアプローチで時系列データを描画するためのツールとなっています。
線グラフと棒グラフを一つのグラフに描画したりもできますし、
累積のグラフなんかも描画できるようになっています。</p>
<p>実験的なプロジェクトである、Timelionの紹介でした。
ここでのノウハウがkibanaにフィードバックされると色々と面白いことになるんじゃないかなと。</p>
<h3 id="ということで">ということで、</h3>
<p>明日は、<a href="http://qiita.com/zoetro">zoetro</a>さんの「Kibanaのプラグインの話」になります。
お楽しみに！</p>
</content:encoded>
    </item>
    
    <item>
      <title>第13回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/11/10/13th-elasticsearch-jp/</link>
      <pubDate>Tue, 10 Nov 2015 17:22:58 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/11/10/13th-elasticsearch-jp/</guid>
      <description>第13回Elasticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、会場提供していただいたリクルートテクノロジーズ</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/33631">第13回Elasticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、会場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
来年もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<p>7月同様、<a href="http://samuraism.com">サムライズム</a>の<a href="https://twitter.com/yusuke">@yusuke</a>さんに
テキスト翻訳していただき、大変助かりました。</p>
<!-- more -->
<h3 id="チェックイン数など">チェックイン数など</h3>
<ul>
<li>チェックインした人：100名</li>
<li>キャンセルしなかった人：36名</li>
</ul>
<p>でした。
今回は当日の時点でキャンセル待ちがない状態にしていました。
雨もあって、これなかった人もいるのでしょうか。</p>
<h2 id="beyond-the-basics-with-elasticsearch--honza-král--elastic">&ldquo;Beyond the basics with Elasticsearch&rdquo; / Honza Král / Elastic</h2>
<p>スライド：https://speakerdeck.com/elasticsearch/beyond-the-basics-with-elasticsearch<br>
参考ビデオ（別のカンファレンスで話した時のビデオ）：https://www.youtube.com/watch?v=yIixWzjTNog</p>
<p>Pycon HKでアジアに来ていたHonzaに、ついでに日本で話をしてもらうという企画で、
前回から1カ月足らずでの開催となりました。
Elasticsearchの基本的な検索機能とは別の機能に関して少し話をしてもらった感じです。
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-percolate.html">Percolator</a>と<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html">Aggregation</a>の話でした。</p>
<p>詳しくはビデオやスライドを見てもらうのがいいかなと。</p>
<h2 id="how-did-we-use-foundno-for-our-services--株式会社アイリッジtakuya-noguchi-さん-tn961ir">&ldquo;How did we use Found.no for our services?&rdquo; / 株式会社アイリッジ　Takuya Noguchi さん @tn961ir</h2>
<p>スライド：未定</p>
<ul>
<li>Foundユーザー。1.7までの話。</li>
<li>社内で独自にクラスタを構築していたが、managed serviceを利用したいと思っていた。</li>
<li>Found用のACLがShieldに</li>
<li>マルチバイトのインデックス名とかも使いたいが、Nginxとの連携でちょっと。。。</li>
<li>セキュリティ関連の話も。Securityに関する報告はこういうものも用意されてるので、こちらに相談してもらうのがいいかも。https://www.elastic.co/community/security</li>
<li>要望がいくつか。</li>
</ul>
<h2 id="ログ収集の仕組みを再考しようあとマウンテンビューに行ってきました--acroquest-technology株式会社-谷本-心さん-cero_t">&ldquo;ログ収集の仕組みを再考しよう！　あとマウンテンビューに行ってきました。&rdquo; / Acroquest Technology株式会社 谷本 心さん @cero_t</h2>
<p>スライド：http://www.slideshare.net/shintanimoto/lets-reconsider-about-collecting-logs-plus-visiting-elasticmoutain-view</p>
<p>ログの小話から始まり、ログに関する考え方とかを披露してもらいました。
さらに踏み込んだログの活用の方法の話になるかと思いきや、
思いっきり話が飛んで、マウンテンビューのElasticオフィスに遊びに行った写真が出てきましたw</p>
<p>写真の後は、弊社のTanya（<a href="https://www.elastic.co/elasticon/tour/2015/tokyo">来月のElastic{ON} Tour Tokyo</a>で来日予定）から
聞いた弊社製品に関する話をしていただきました。
きっと、Beatsに関して次は話してくれるんだろうなぁ（棒）。
流れ的には、<a href="https://www.elastic.co/elasticon/conf/2016/sf">来年の2月にサンフランシスコで開催されるElastic{ON}16</a>につながりそうだったので、ここで宣伝しときますね。
今年3月に開催されたイベントには残念ながら日本の方はいなかったので、次回は日本の方がいると嬉しいなぁと。</p>
<h2 id="lt-elasticsearch-を使った単語共起頻度の計算--株式会社はてなidtakuya-a-さん">LT &ldquo;「Elasticsearch を使った単語共起頻度の計算」&rdquo; / 株式会社はてな　id:takuya-a さん</h2>
<p>スライド：未定</p>
<p>一風変わったElasticsearchの使い方的な話でした。
検索用にデータを登録してあるElasticsearchから単語の頻度情報を抜き出して、
別のインデックスに登録するという感じでしょうか。
こういうのが、実は、Elasticsearchに機能としてあると便利だったりするのかもなぁと思ってみたり。</p>
<p>LTよりはちょっと長かったですかねw</p>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://arika.hateblo.jp/entry/2015/11/09/204652">elasticsearch勉強会</a></li>
</ul>
<h2 id="まとめ宣伝">まとめ＋宣伝</h2>
<p>今回も@yusukeさんのテキスト翻訳に助けていただきました。ほんとありがとうございます。
今年の勉強会はこれがラストになります。
来月は、トレーニングと<a href="https://www.elastic.co/elasticon/tour/2015/tokyo">Elastic{ON} Tour Tokyo</a>があるので忙しくなりそうですが、
参加予定の方は楽しみにしていてください！</p>
<ul>
<li>Operations : <a href="http://training.elastic.co/class/Operations/Japan/Dec">http://training.elastic.co/class/Operations/Japan/Dec</a></li>
<li>Developer : <a href="http://training.elastic.co/class/Developer/Japan/Dec">http://training.elastic.co/class/Developer/Japan/Dec</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Senseの歴史 - Sense 2.0.0-beta1の紹介(日本語訳)</title>
      <link>https://blog.johtani.info/blog/2015/10/30/sense-2-0-0-beta1-ja/</link>
      <pubDate>Fri, 30 Oct 2015 17:55:06 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/10/30/sense-2-0-0-beta1-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：The Story of Sense - Announcing Sense 2.0.0-beta1 誕生 よくある良いプロジェクト同様、Senseもビールを飲みながら考</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/sense-2-0-0-beta1">The Story of Sense - Announcing Sense 2.0.0-beta1</a></p>
<!-- more -->
<h2 id="誕生">誕生</h2>
<p>よくある良いプロジェクト同様、Senseもビールを飲みながら考えつきました。
<a href="https://en.wikipedia.org/wiki/Amstel">Amstel</a>での手漕ぎボートのセッションの後で。
友人の<a href="https://twitter.com/jkaizer">Jasper</a>と私はJasperの<a href="http://q42.nl/">会社</a>で毎年行われる
<a href="http://w00t.camp/">ハッカソン</a>について話をしていました。
このハッカソンはどのようなアイデアでどんなチームで行うかを聞き取りされる、厳密なハッカソンです。
その時、私とJasperはChromeブラウザに別のヒストリーを表示するという作業をやると<a href="https://twitter.com/anneveling">Anne Veling</a>に話をしていました。</p>
<p>Jasperと私はElasticsearchのユーザでしたが、リッチなREST APIにリクエストを送信するための
便利なツールがないと知っていました。
恥ずべきことに、cURLコマンドを利用するターミナルがその時の一番良いツールでした。
皆さん、ターミナルでボディつきのリクエストをサブミットするのがどのくらい不便かというのをわかるために、
5秒ほどターミナルで実行してみてください。
タイプミスのような単純なことでさえ、すべてのコマンドを再タイプしなければならなかったり、
複数行サポートのターミナルと戦ったりです。
ウェブベースのJSONエディタを見つけ出して、それをベースにすることが必要でした。</p>
<h2 id="終わりなきウィークエンド">終わりなきウィークエンド</h2>
<p>リサーチをして、Anneに電話しました。
私は彼に、History Pageのプロジェクトにもコミットするが、
Elasticsearchユーザなので、便利なコンソールを開発する時間も欲しいという話をしました。
私たちは、<a href="https://ace.c9.io/">Ace</a>オンラインエディタを利用して、
自動でAPIを認識するナレッジベースを構築し、
コンテキストに沿ったサジェストを大なうようにしました。
Anneはすぐに、それが素晴らしいと同意してくれました。
しかし、彼は、ハッカソンの基本的なルール（週末にそれが終わる必要がある）に違反しているので、
そのアイデアを却下するしかありませんでした。
確かに、私たちが提案していたものは行えませんでした。
最後に、私たちは、ChromeのHistory Pageの素晴らしい置き換えについて実装しました。</p>
<p>それでも、私はチャレンジし、それが終わるであろうことを終わるであろうことを証明しなければなりませんでした。
次の週末（といくつかの終業後 :)）に、私はそれを作りました。
Senseの誕生です。
それは、まだバグだらけでしたが、動きました。
これを見せるとみんな興奮しました。</p>
<h2 id="初期">初期</h2>
<p>Knowledge Baseの拡張とバグのフィックスで数日を過ごしました。
Senseは広まり始め、ずっと古いバグのあるバージョンを利用しないといけないのかと私は恐れました。
SenseをChromeのExtentionとしてリリースすることを決め、リリースすると自動的に更新されるようにしました。
History Panelのような機能を一つづつ追加するようにしました。</p>
<p>Elasticにジョインしてから、会社の人たちがSenseを使用しているということを聞き、とても幸せでした。
特に、<a href="https://twitter.com/clintongormley">Clint</a>と話をしたときのことを覚えています。
彼は、&ldquo;You know what Sense should do? It should use this format and allow you to have multiple requests in the editor&rdquo;
「Senseになにをすべきかわかる？フォーマットを使うべきだし、エディタで複数のリクエストを持つようにするべきだ」
と言いました。
もちろん、その他のチャレンジも行いました。これは、簡単なものではなく、Aceの詳細を知る必要がありました。
それは新しいAceモード（Aceによって利用されているハイライティングロジック）です。
これは、Senseのサジェストエンジンに密に統合されました。</p>
<p>次のものが古いSenseのスクリーンショットです。</p>
<p>画像あり。Figure 1. Sense 0.7
<a href="https://www.elastic.co/blog/sense-2-0-0-beta1">※画像に関しては原文をご覧ください。</a></p>
<p>APIのURLを入力すると、JSONのボディが入力されます。
うまく切り離すことができ、AceのスタンダードJSONモードを使っていました。
しかし、ここで、次のようなフォーマットをどうやってサポートするか考える必要がありました。</p>
<pre><code>GET _cluster/health
POST index/_settings
{
  &quot;index&quot;: { &quot;number_of_replicas&quot;: 3 }
}
</code></pre><p>これは、Aceが3つの異なるものをどうやってパースするかを知る必要があるということです。
HTTPメソッドとURLとJSONボディです。
また、困ったことに、前に説明した前に説明した通り、明らかに別々にはならないものでした。
JSONボディが完全であることを知る唯一の方法はかっこを数えることです。
それは、いくつかの作業とAceのカスタマイズが必要でしたが、それらを切り離すことができました。
そして、Senseのシンタックスが生まれたのです（Thanks Clint!）</p>
<h2 id="marvel時代">Marvel時代</h2>
<p>就業時間中、私の優先すべき仕事は<a href="https://www.elastic.co/products/marvel">Marvel</a>の開発になりました。
これは、Elasticsearchのための管理と監視のためのソリューションです。
（side note: Marvelは生まれ変わっています。（&quot;<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">Shield, Watcher, and Marvel 2.0.0 GA Released</a>&quot;））
Marvelは開発環境ではフリーなので、MarvelにSenseを組み込むことにしました。
これにより、Senseの開発が日中も行えるようになり、多くのユーザに利用され始めました。
また、Senseは実際に真の<a href="https://twitter.com/spenceralger">JavaScript開発者</a>によって開発されました。
彼は、コードをクリーンにし、ブラウザにおける最新の技術を私に教えてくれました。</p>
<p>この期間のSenseは数回書き換えられています。
最も顕著なものは、個別のURLとJSONのサジェストエンジンを書き換えて、
1つのサジェストエンジンにしこれらのコンテキストで動作するようにし、さらに3つ目のコンテキスト（URLパラメータ）を追加したことです。</p>
<p>新しいエンジンはまた、複数のサジェストコンテキストをメンテナンスするのが簡単になりました。
例えば、<code>_search API</code>のソートパラメータを考えます。</p>
<pre><code>GET _search
{
  &quot;sort&quot;: [
     &quot;timestamp&quot;: &quot;desc&quot;,
     &quot;price&quot;: {
        &quot;order&quot;: &quot;desc&quot;.
        &quot;missing&quot;: &quot;last&quot;
     },
     &quot;nested_filter&quot;: { &quot;term&quot;: { ... }},
     &quot;_score&quot;
  ]
}

</code></pre><p>ユーザがどこにいるかによって、Senseは単純な値（<code>_score</code>のような）か、
複雑な構造（<code>order</code>と<code>missing</code>のような）やフィルタ（<code>nested_filter</code>のような）も
サジェストする必要があります。
これらのサジェストのパスが一度に処理され、無関係なものは除外されます。</p>
<h2 id="sense-20の紹介">Sense 2.0の紹介！</h2>
<p>Marvel 1.xはKibana 3.0をベースにしていました。
これは、データの探索やダッシュボードツールとして素晴らしいものでした。
しかし、Kibanaチームはさらに素晴らしいものを出しました。
Kibana 4.xはElasticsearchをバックエンドとするUIアプリを簡単に構築することができる
プラットフォームとして設計されています。
実際に、<a href="https://www.elastic.co/guide/en/marvel/current/index.html">Marvel 2.0</a>はKibanaの
プラットフォームで利用できる最初のアプリです。</p>
<p>Senseの話に戻します。
ElasticsearchのAPIとやりとりする一般的なコンソールです。
これをKibanaのアプリぴったりだと気付きました。
ということで、<a href="https://github.com/elastic/sense/">Sense 2.0</a>をKibanaアプリとしてオープンソースで公開しました。
開発及び本番環境で利用してください。</p>
<p>Figure 2. Screenshot Sense 2.0
<a href="https://www.elastic.co/blog/sense-2-0-0-beta1">※画像に関しては原文をご覧ください。</a></p>
<h2 id="リリースのハイライト">リリースのハイライト</h2>
<p>Sense 2.0の新しい機能をここで簡単に紹介します。
（すべての変更点については<a href="https://www.elastic.co/guide/en/sense/current/release_notes.html#_2_0_0_beta1">こちら</a>をご覧ください。）</p>
<h4 id="elasticsearch-20">Elasticsearch 2.0</h4>
<p>SenseのナレッジベースをElasticsearch 2.0サポートに更新しました。
新しいPipeline aggregationにも対応しています。</p>
<h4 id="複数リクエストの実行">複数リクエストの実行</h4>
<p>テストやいくつかの一連のコマンドを繰り返し実行したい時があるでしょう。
その時に、それら全てをSenseに記述し、
実行したいリクエストを選択状態にしてElasticsearchにリクエストできます。</p>
<p>Figure 3. Submit multiple requests
<a href="https://www.elastic.co/blog/sense-2-0-0-beta1">※画像に関しては原文をご覧ください。</a></p>
<p>Senseは、Elasticsearchにリクエストを一つずつ送信し、それぞれの出力結果を右のパネルに表示します。
これは、問題のデバッグや複数のシナリオでのクエリの組み合わせの実行に非常に便利です。</p>
<h4 id="複数リクエストのコピーペースト">複数リクエストのコピーペースト</h4>
<p>複数リクストを選択し、フォーマットしたり、cURLのコマンドとしてコピーすることも可能です。</p>
<p>Figure 4. Copy multiple requests as cURL
<a href="https://www.elastic.co/blog/sense-2-0-0-beta1">※画像に関しては原文をご覧ください。</a></p>
<pre><code># Delete all data in the `website` index
curl -XDELETE &quot;http://localhost:9200/website&quot;
# Create a document with ID 123
curl -XPUT &quot;http://localhost:9200/website/blog/123&quot; -d'
{
  &quot;title&quot;: &quot;My first blog entry&quot;,
  &quot;text&quot;:  &quot;Just trying this out...&quot;,
  &quot;date&quot;:  &quot;2014/01/01&quot;
}'
</code></pre><p>もちろん、複数のcURLコマンドをコピーしてSenseにペースとすると、Senseはそれらをパースしてくれます。</p>
<h2 id="まとめ">まとめ</h2>
<p>Sense 2.0.0のベータリリースです。
実際に多くの作業が終わった認識です。すぐにGAが出るでしょう。</p>
<p>Sense 2.0を知り、試していただくために、新しい<a href="https://www.elastic.co/guide/en/sense/current/index.html">ドキュメント</a>を参考にしてください。
バグやリクエストがある場合は、<a href="https://discuss.elastic.co/c/ecosystem">フォーラム</a>や<a href="https://github.com/elastic/sense/issues">GitHubのIssue</a>に登録をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Shield、Watcher、Marvel 2.0.0 GAリリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/10/30/shield-watcher-and-marvel-2-0-ga-released-ja/</link>
      <pubDate>Fri, 30 Oct 2015 16:21:31 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/10/30/shield-watcher-and-marvel-2-0-ga-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Shield, Watcher, and Marvel 2.0.0 GA Released 本日（10/28）Shield、WatcherおよびMarv</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">Shield, Watcher, and Marvel 2.0.0 GA Released</a></p>
<!-- more -->
<p>本日（10/28）Shield、WatcherおよびMarvel 2.0をリリースしました。
これが、<a href="https://www.elastic.co/blog/elasticsearch-2-0-0-released">Elasticsearch 2.0</a>に対応したこれらのプラグインの最初のリリースです。</p>
<p>Elasticsearch 2.0対応のほかに、ShieldとWatcher 2.0は、
セキュリティとアラートを拡張するいくつかの新しい素敵な機能も備えています。</p>
<h2 id="shield">Shield</h2>
<ul>
<li>
<p>拡張可能なレルム - Sheild 1.xはユーザ認証のコア的なものを定義するのにフォーカスし
3つの認証メカニズム（<em>esusers</em>、LDAP/AD、PKI）を提供しました。
これらを提供することで、多くのユーザおよびユースケースをカバー出来ましたが、
追加の認証メカニズムを統合する必要があることもわかっていました。
ということで、Shieldのレルムベースの認証システムをユーザが利用、拡張できるようにオープンにし、
ユーザ認証を扱うためのレルム実装をプラグインとして拡張できるようにしました。
特定もしくはプロプライエタリな認証メカニズムが必要なユーザもShieldの強力な
セキュリティ機能（ロールベースの認証、セキュアな通信など）をフルに活用できるようになりました。
カスタムレルムの詳細については、<a href="https://www.elastic.co/guide/en/shield/current/custom-realms.html">こちら</a>をご覧ください。</p>
</li>
<li>
<p>フィールドとドキュメントのACL - Shield 2.0はフィールドとドキュメントレベルのアクセス制御機能を提供します。
これは、ロールごとにアクセス可能なフィールドやドキュメントを定義できます。
この新しい機能は、設定の変更するよりも便利です。
このアクセス制御はElasticsearchのLuceneインデックスという最も低レベルで実装されています。
その結果として、このメンテナンスがより簡単であるだけでなく、より良くなっています。
詳細については<a href="https://www.elastic.co/guide/en/shield/current/setting-up-field-and-document-level-security.html">こちら</a>をご覧ください。</p>
</li>
<li>
<p>ユーザなりすまし - Shield 2.0で、ユーザなりすましの機能が実装されました。
これは、ユーザ（適切なパーミッションを持った）が、他のユーザになることができ、
それらのユーザのためにリクエストを実行できます。
これは、Elasticsearch上に構築されたアプリケーションがすでにユーザ認証を行いますが、
認可はElasticsearchサイドで行う必要があるような場合に有用です。
このシナリオで、アプリケーションの&quot;main&quot;ユーザを設定でき、正しくなりすましを割り当て、
ElasticsearchにアプリケーションユーザとしてリクエストをElasticsearchに実行させることができます。
詳細については、<a href="https://www.elastic.co/guide/en/shield/current/submitting-requests-for-other-users.html">こちら</a>をご覧ください。</p>
</li>
</ul>
<h2 id="watcher">Watcher</h2>
<ul>
<li>
<p>SlackとHipChatインテグレーション - SlackとHipChatはチーム/グループコラボレーションツールです。
これらは、急速に主流になり、組織の主な内部コミュニケーションハブとなっています。
Watcher 2.0はチャンネル/ルームやユーザにこれらのコミュニケーションチャネル経由で、Watchの通知を行うことができるアクションを
実装しました。
<a href="https://www.elastic.co/guide/en/watcher/current/configuring-slack.html">slack</a>や<a href="https://www.elastic.co/guide/en/watcher/current/configuring-hipchat.html">hipchat</a>アクションについてはドキュメントをご覧ください。</p>
</li>
<li>
<p>Array Compare Condition - 新しいconditionはタイムシリーズのデータのスパイクを検知するのを簡単にします。
<a href="https://www.elastic.co/guide/en/watcher/current/condition.html#condition-compare">compare</a> conditionは1.xで導入されましたが、このコンディションはElasticsearchのダイナミックスクリプト機能を有効にする必要がアンク使えます。
詳細については<a href="https://www.elastic.co/guide/en/watcher/current/condition.html#condition-array-compare">array_compare</a> conditionをご覧ください。</p>
</li>
<li>
<p>Watchの有効・無効化 - ユーザからの多かったリクエストとして、Watchの無効化がありました。
1.xには、登録済みのWatchを無効にする機能がありませんでした。
これは、Watchを消すか、Watchのトリガーを変更することで回避していました。
これは、全体としてはWatchを管理するのを難しくする回避方法でしかありません。
2.0では、APIを呼び出すだけで、Watchの変更をすることなく、簡単にWatchの有効化・無効化が可能になりました。
これは1.0からあるべき基本的な機能でしたが、ついにこの問題を解決しました。
詳細は<a href="https://www.elastic.co/guide/en/watcher/current/watch-active-state.html#watch-active-state">こちら</a>をご覧ください。</p>
</li>
</ul>
<h2 id="marvel">Marvel</h2>
<p>Marvel 2.0を紹介するのに興奮しています。
Kibana 4をベースとした、再設計されたUIを持っています。
Marvel 1.xで学んだ多くのことを導入し、より使いやすく監視しやすいUIになっています。
ShieldとWatcherと同様に、最初のMarvelのリリースは将来的な成長の基盤となり
Elasticsearch2.0を効率的に管理するための主要なメトリックにフォーカスしています。</p>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>再設計により、インタフェースを6ページに減らしています。</p>
<h3 id="cluster-list">Cluster list</h3>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>ユーザやカスタマーの多くは複数のクラスタを利用しています。
新しいMarvelはそれらを集中的にモニタリングする一つのクラスタからそれらを簡単に監視できます。
各クラスタのデータ送信先をこのモニタリングクラスタにするだけです。</p>
<h3 id="cluster-overview">Cluster Overview</h3>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>クラスタオーバービューはある一つのクラスタの主要な性能メトリックを見ることができ、
素早くスパイクを発見できます。
このページはまた、アクティブなシャードのリカバリやリロケーションも見ることができます。</p>
<h3 id="indices-list">Indices List</h3>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>インデックスのリストにはクラスタにあるすべてのインデックスとその属性が表示されます。
テーブルはライブでアップデートされ、フィルタリングやソートも可能です。
一番大きなインデックスは？といったことも調べられます。</p>
<h3 id="index-detail">Index Detail</h3>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>インデックス詳細ページはインデックスの主な性能メトリックを見ることができ、シャードの配置についても表示します。</p>
<h3 id="nodes-list">Nodes List</h3>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>ノードリストはクラスタにあるノードとその主な性能メトリックを見ることができます。
テーブルはライブでアップデートされ、フィルタリングも可能です。
高いCPU利用率やディスクの残り容量なども簡単にわかるようになっています。</p>
<h3 id="node-detail">Node Detail</h3>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>ノード詳細ページは個別のノードに関する主な性能メトリックを見ることができ、ノードにあるシャードのリストも見ることができます。</p>
<p>新しいMarvelはKibana 4の上に構築されたので、管理方法が変わっています。
Marvelのインストールは2つのステップがあります。
marvel-agentとmarvel user interfaceです。</p>
<h2 id="marvel-agent">Marvel Agent</h2>
<p>marvel-agentはElasticsearchクラスタにプラグインとしてインストールします。
主なパフォーマンス情報を取得し、ローカルもしくは分離されたモニタリングクラスタにデータを保存・送信します。</p>
<h2 id="marvel-user-interface">Marvel User Interface</h2>
<p>Marvel UIはKibanaのプラグインとしてインストールします。
これは、Kibana 4.2の新しいプラグインインフラを利用し、
Marvel Appとして、Kibanaのインタフェースとは個別に提供されます。
Kibanaのアプリの切り替えは次の画像の通りです。</p>
<p>画像あり。
<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">※画像に関しては原文をご覧ください。</a></p>
<p>2.0リリースは私たちのプロダクトの大きな一歩です。またユーザの意見を常にお待ちしています。
ぜひ、Webフォーラム（https://discuss.elastic.co）やメール（info@elastic.co）でご意見を。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 2.0.0リリース(日本語訳)</title>
      <link>https://blog.johtani.info/blog/2015/10/29/elasticsearch-2-0-0-released-ja/</link>
      <pubDate>Thu, 29 Oct 2015 16:20:43 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/10/29/elasticsearch-2-0-0-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 2.0.0 GA released Elasticsearch 1.0.0のリリース以降、 477のコミッター2,79</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-2-0-0-released">Elasticsearch 2.0.0 GA released</a></p>
<!-- more -->
<p>Elasticsearch 1.0.0のリリース以降、
477のコミッター2,799のpull requestがあった、
**Elasticsearch 2.0.0 GA（Lucene 5.2.1ベース）**をリリースしました。</p>
<p>それだけでなく、<a href="#shield-watcher">Shield（セキュリティプラグイン）とWatcher（アラーティングプラグイン）</a>、
新しくなった<a href="#marvel">Marvel（モニタリングプラグイン）</a>（プロダクション環境でフリー！）、
また、新しくオープンソースとなった<a href="#sense">Sense editor</a>の2.0.0もリリースしました。</p>
<p><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0">Elasticsearch 2.0.0</a>のダウンロードはこちらから。
また、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html">2.0.0での重要な変更点についてはこちら</a>をご覧ください。
全ての変更点については、次をご覧ください。</p>
<ul>
<li><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0">Changes list for Elasticsearch 2.0.0</a></li>
<li><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-rc1">Changes list for Elasticsearch 2.0.0-rc1</a></li>
<li><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-beta2">Changes list for Elasticsearch 2.0.0-beta2</a></li>
<li><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-beta1">Changes list for Elasticsearch 2.0.0-beta1</a></li>
</ul>
<p>商用プラグインについてはこちらです。</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/shield/2.0.0/release-notes.html#_2_0_0">Shield 2.0.0 change logs</a></li>
<li><a href="https://www.elastic.co/guide/en/watcher/2.0.0/release-notes.html#_2_0_0">Watcher 2.0.0 change logs</a></li>
</ul>
<h2 id="elasticsearchの新機能">Elasticsearchの新機能</h2>
<p>Elasticsearch 2.0.0には素晴らしい新機能があります。</p>
<h4 id="pipeline-aggregations">Pipeline Aggregations</h4>
<p>Aggregationsで導関数や移動平均のような他のAggregationの結果に対する計算が可能となります。
この機能はクライアントサイドで実装しなければなりませんでしたが、
Elasticsearchに計算させることで、より強力な解析のクエリを簡単に組み立て、クライアントのコードを簡略化できます。
これは、予測解析や予測解析や例外検知といった可能性をもたらします。
Pipeline Aggregationについては次をご覧ください。</p>
<ul>
<li><a href="https://www.elastic.co/blog/out-of-this-world-aggregations">Out of this world aggregations.</a></li>
<li><a href="https://www.elastic.co/blog/staying-in-control-with-moving-averages-part-1">Staying in Control with Moving Averages - Part 1.</a></li>
<li><a href="https://www.elastic.co/blog/staying-in-control-with-moving-averages-part-2">Staying in Control with Moving Averages - Part 2.</a></li>
</ul>
<h4 id="queryfilter-merging">Query/Filter merging</h4>
<p>フィルタはもうありません。
全てのフィルタ条件はクエリとなりました。
クエリコンテキストで使用した場合、関連度のスコアに影響し、フィルタコンテキストで使用した場合、
これまでのフィルタのように、ヒットしなかったドキュメントを除外するだけとなります。
この変更はクエリの実行時に自動的に最も効率的な順序で実行するように最適化されることを意味します。
例えば、遅いクエリ（フレーズやgeo）の最初の実行は速い近似フェーズで実行され、
それから、遅い正確なフェーズで結果を修正します。
フィルタコンテキストでは、直近でよく使われた条件が自動的にキャッシュされます。
詳細については、&quot;<a href="https://www.elastic.co/blog/better-query-execution-coming-elasticsearch-2-0">Better query execution coming to Elasticsearch 2.0</a>&ldquo;をご覧ください。</p>
<h4 id="設定可能な圧縮率">設定可能な圧縮率</h4>
<p><code>_source</code>のようなStored fieldsは高速なLZ4（デフォルト）で圧縮するか、インデックスサイズを小さくできるDEFLATE
で圧縮できます。
これは、特にロギングのケースで便利です。
古いインデックスをオプティマイズする前に<code>best_compression</code>に変更することができます。
詳細については&rdquo;<a href="https://www.elastic.co/blog/store-compression-in-lucene-and-elasticsearch">Store compression in Lucene and Elasticsearch</a>&ldquo;をご覧ください。</p>
<h4 id="堅牢に">堅牢に</h4>
<p>新しいElasticsearchはJava Security Managerの元で実行されます。
これは、セキュリティの観点で大きな前進です。
Seciruty ManagerはElastcsearchにより制限をかけ、ハッカーによりシステムに対して何でもできるようなものを制限します。
Elasticsearchはまた、インデキシングの観点でも堅牢になっています。</p>
<ul>
<li>ドキュメントはインデキシングリクエストに答える前に、耐久性のためにディスクに<code>fsync</code>されます。</li>
<li>すべてのファイルはチェックサムにより、早期に障害を検知します。</li>
<li>すべてのファイルはどんなファイルへの書き込みもアトミックです</li>
</ul>
<p>最後に、システム管理者から要請の多かった変更として、
設定されて居ないノードがパブリックなネットワークから参加しないようになりました。
Elasticsearchは<a href="https://manage.contentstack.io/blog/elasticsearch-unplugged">デフォルトではローカルホストのみにバインド</a>します。マルチキャストは無くなりました。（プラグインとして残っています。）</p>
<h4 id="パフォーマンスと信頼性">パフォーマンスと信頼性</h4>
<p>上記以外にも細かな修正がElasticsearchとLuceneにはあります。
より安定し、信頼性をあげ、簡単に設定できるようにするものです。例えば、次のようなものです。</p>
<ul>
<li>ヒープの使用率の低減（doc valuesがデフォルト、マージ時のメモリ使用率の削減、
roaring bitsetsによるフィルタキャッシュ）</li>
<li>構造化され読みやすくなった例外</li>
<li>設定の代わりに、フィードバックループを使用した自動調整</li>
<li>安全で明確で信頼性のあるタイプマッピングの大きな修正</li>
<li>クラスタ状態の差分変更による伝搬の高速化および、大きなクラスタでのより安定的に</li>
<li>normsの圧縮の改善。これまではヒープスペースを大きく利用していた。</li>
<li>マージの自動的な調整（不可解な設定の微調整が必要ない）</li>
<li>より詳細なLuceneのメモリリポート</li>
<li>最適化されたクエリ実行を活用するためにParent/childを書き換え</li>
</ul>
<h4 id="コアプラグイン">コアプラグイン</h4>
<p>公式にサポートされたコアプラグインはElasticsearchと同じバージョン番号で同じタイミングでリリースされます。
インストールするプラグインとElasticsearchの複雑なバージョンの対応表に悩まされる必要はもうありません。
コアプラグインのインストールは次のように簡略化されています。</p>
<pre><code>bin/plugin install analysis-icu
</code></pre><h2 id="a-nameshield-watchershieldとwatcherの新機能a"><a name="shield-watcher">ShieldとWatcherの新機能</a></h2>
<p>商用プラグインも新しい機能をリリースしました。</p>
<h4 id="shield">Shield</h4>
<ul>
<li>フィールドおよびドキュメントレベルのアクセス制御</li>
<li>ユーザのなりすまし</li>
<li>カスタム拡張可能な認証レルム</li>
</ul>
<h4 id="watcher">Watcher</h4>
<ul>
<li>個別のWatchを有効/無効に</li>
<li>SlackやHipChatへの通知</li>
</ul>
<p>これらの詳細については“<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">Shield, Watcher, and Marvel 2.0.0 GA Released</a>”をご覧ください。</p>
<p>コアプラグイン同様、商用プラグインもElasticsearchのバージョンと同じものが同時にリリースされます。
インストールは次の通りです。</p>
<pre><code>bin/plugin install license
bin/plugin install shield
bin/plugin install watcher
</code></pre><h2 id="a-namemarvelmarvel-200はプロダクションでの利用もフリーにa"><a name="marvel">Marvel 2.0.0はプロダクションでの利用もフリーに</a></h2>
<p>Marvelモニタリングプラグインはカスタマに非常に価値のあるもので、
ユーザの発展とともに問題を診断したり見つけたりするのに役に立ってきました。
私たちは、何を改善でき、Mαrvelを一から書き直すことで、いろいろとわかったことがあります。</p>
<ul>
<li>Marvel UIを新しい<a href="https://www.elastic.co/blog/kibana-4-2-0">Kibanaプラットフォーム</a>上に構築</li>
<li>ダッシュボードにはより簡単に問題を発見するために、最も重要なメトリックを可視化</li>
<li>1つのインストールで、複数のクラスタのモニタリングをサポート（商用サポート対象）</li>
</ul>
<p>一番良い点はMarvelがすべてのElasticsearchユーザに対してプロダクション環境でフリーになったことです！
ライセンスが必要ですが、課金の必要はありません。
もし、マルチクラスタモニタリングサポートが必要な場合、それは商用サポート対象となります。</p>
<p>詳細に関しては“<a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">Shield, Watcher, and Marvel 2.0.0 GA Released</a>”をご覧ください。</p>
<h2 id="a-namesensesense-editorがオープンソースにa"><a name="sense">Sense editorがオープンソースに</a></h2>
<p>Sense（ブラウザベースのElasticsearchリクエストとDSL向けのエディタ）を
<a href="https://www.elastic.co/blog/kibana-4-2-0">Kibanaプラットフォーム</a>のアプリとして、オープンソースにしました。
また、このリリースで新しい機能が追加されています。</p>
<ul>
<li>複数のcURLリクエストをペースとすると、Sense表記に変更</li>
<li>複数のSenseリクエストをcURL表記にしてコピー</li>
<li>複数のリクエストを一度に実行可能</li>
<li>Elasticsearch 2.0サポートとなった自動補完機能</li>
</ul>
<p>SenseはKibanaのアプリとして次のようにインストールします。</p>
<pre><code>./bin/kibana plugin --install elastic/sense
</code></pre><p>Senseの詳細については、&quot;<a href="https://www.elastic.co/blog/sense-2-0-0-beta1">The Story of Sense - Announcing Sense 2.0.0-beta1</a>&ldquo;をご覧ください。</p>
<h2 id="elasticsearch-migration-plugin">Elasticsearch Migration Plugin</h2>
<p>Elasticsearch Migration PluginはElasticsearch 1.xから2.0にアップグレードする時の良い出発点となります。
1.xのElasticsearchクラスタにサイトプラグインとしてインストールすると、
アップグレードする前に解決すべき問題があるかどうかを検知してくれます。
（例えば、Lucene 3のような古いインデックスや、2.0.0にした場合に動作しない問題のある
マッピング（<a href="https://www.elastic.co/blog/great-mapping-refactoring">The Great Mapping Refactoring</a>）のような問題）</p>
<p>プラグインに関して<a href="http://github.com/elastic/elasticsearch-migration">Elasticsearch Migration repository</a>をご覧ください。</p>
<h2 id="まとめ">まとめ</h2>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0">Elasticsearch 2.0.0</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)や<a href="https://discuss.elastic.co/c/elasticsearch">Webフォーラム</a>などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Kibana 4.2.0リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/10/29/kibana-4-2-0-ja/</link>
      <pubDate>Thu, 29 Oct 2015 16:20:19 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/10/29/kibana-4-2-0-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Kibana 4.2.0 released Elasticsearch 2.0 + Kibana 4.2 = 💚 Elasticsearch 2.0サポートのKibanaの最初のリリースです。 これ</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/kibana-4-2-0">Kibana 4.2.0 released</a></p>
<!-- more -->
<p>Elasticsearch 2.0 + Kibana 4.2 = 💚
Elasticsearch 2.0サポートのKibanaの最初のリリースです。
これが何を意味するでしょう？
速さ、安定さ、新しい機能。
試してみたい方は、<a href="https://www.elastic.co/downloads/kibana">いますぐダウンロード</a>してください。
そうでない方は、Kibana 4.2の楽しい機能について読んでみてください。</p>
<h3 id="暗黒面は怖い">暗黒面は怖い？</h3>
<p>そんなことありません。
私たちは常にチャートチャートとダッシュボードを組み立てている組み立てている間は明るいバックグラウンドを使うことを推奨してきましたが、
時々、巨大なスクリーンで暗い部屋で誰も明るい画面から目を背けないようにしたいでしょう。
その影響を小さくするためにダークモードを導入しました。
あなたは、NOCや天文台、その他の暗い場所でKibanaのダッシュボードを楽しむことができます。</p>
<p>画像あり。
<a href="https://www.elastic.co/blog/kibana-4-2-0">※画像に関しては原文をご覧ください。</a></p>
<h3 id="地図のカスタマイズ">地図のカスタマイズ</h3>
<p>Kibanaの地図は素晴らしいですが、もっと多くのオプションが望まれていると聞きました。
もし地図に関して知識があるなら、Kibana 4.2のWMSバックグラウンド地図サポートを試してみてください。
WMSは非常に強力で、US Geological Surveyを含む多くの無料サービスがあります。
<a href="http://viewer.nationalmap.gov/example/services/serviceList.html">http://viewer.nationalmap.gov/example/services/serviceList.html</a></p>
<p>画像あり。
<a href="https://www.elastic.co/blog/kibana-4-2-0">※画像に関しては原文をご覧ください。</a></p>
<h3 id="シナリオは">シナリオは？</h3>
<p>何かおかしい時、何が起こっているかを正しく知ってもらいたいので、Kibanaがそのタイミングで注目したいコンポーネントがあるなら、
どのように動いているかという概要を知るためのサーバステータスページを作りました。
もちろん、全てがOKであるというのを知りたいだけの場合でも、settingメニューのStatusタブからいつでも呼び出せます。</p>
<p>画像あり。
<a href="https://www.elastic.co/blog/kibana-4-2-0">※画像に関しては原文をご覧ください。</a></p>
<h3 id="全てにおいて速く">全てにおいて速く</h3>
<p>ブラウザリフレッシュはKibana 4.2の新しいコードビルディングシステムのおかげで、さらに早くなりました。
また、メモリを覚えてます？<strike>Pepperidge Farm</strike>Kibanaが覚えています。
Kibana 4.2は小さな小さな小さなメモリフットプリントを管理している間、長い長い長い時間実行されているダッシュボードを見ることができるような
大きなメモリのクリーンアップも含んでいます。</p>
<h3 id="もっとありますが">もっとありますが。。。</h3>
<p>小さな微調整がいくつもあります。また、今後紹介する本当に刺激的なものの基礎を気づき上げてきました。
これからも<a href="http://elastic.co/blog">Elasticのブログ</a>、<a href="https://twitter.com/elastic">Twitter</a>、<a href="https://github.com/elastic/kibana">KibanaのGitHubリポジトリ</a>に注目し、<strike>モンスタートラック</strike><strong>アナリティクス</strong>の瞬間に立ち会ってください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Logstash 2.0.0リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/10/29/logstash-2-0-0-released-ja/</link>
      <pubDate>Thu, 29 Oct 2015 16:19:57 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/10/29/logstash-2-0-0-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Logstash 2.0.0 released Logstash 2.0.0が本日（10/28）リリースされました。 このリリースは</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/logstash-2-0-0-released">Logstash 2.0.0 released</a></p>
<!-- more -->
<p>Logstash 2.0.0が本日（10/28）リリースされました。
この<a href="https://www.elastic.co/downloads/logstash">リリース</a>は
いくつかの設定に関する重要な変更があります。
詳細については、<a href="https://github.com/elastic/logstash/blob/2.0/CHANGELOG.md">changelog</a>または、新しい<a href="https://www.elastic.co/guide/en/logstash/2.0/breaking-changes.html">breaking changes</a>ドキュメントをご覧下さい。</p>
<p>これまでの2.0.0直前のリリースに関する変更点はこちらをご覧ください。</p>
<ul>
<li><a href="https://www.elastic.co/blog/logstash-2-0-0-beta1-released">beta1</a></li>
<li><a href="https://www.elastic.co/blog/logstash-2-0-0-beta2-released">beta2</a></li>
<li><a href="https://www.elastic.co/blog/logstash-2-0-0-beta3-released">beta3</a></li>
<li><a href="https://www.elastic.co/blog/logstash-2-0-0-rc1-released">RC</a></li>
</ul>
<p>ここでは、2.0の主な変更点の概要を説明します。</p>
<h3 id="elasticsearch-20との互換性">Elasticsearch 2.0との互換性</h3>
<p>多くの機能および改善を含んだ<a href="https://www.elastic.co/blog/elasticsearch-2-0-0-released">Elasticsearch 2.0がリリース</a>されました。
Logstash 2.0はこのリリースに対応しています。
Logstashのこれまでのリリースでは、デフォルトで、Javaの <code>node client</code>をElasticsearchとの通信として
使用してきました。
2.0では、HTTPクライアントがデフォルトになります。
これにより、シームレスにユーザのデータを取り込み、付加価値をつけ、Elasticsearchに保存して解析することができます。</p>
<p>HTTPは他のプロトコル（<code>node</code>や<code>transport</code>）同等の機能を持っていますが、
単一のクライアントに接続する時に、少しだけ遅いですが、管理や動作がより簡単です。
HTTPプロトコルを使うことで、Elasticsearchのバージョンのアップグレードが、Logstashのアップグレードすることなく
行うことができます。
デフォルトをHTTPに変更したさらに詳しい情報については<a href="https://www.elastic.co/blog/logstash-2-0-0-beta1-released">beta1のブログ</a>をご覧ください。</p>
<p>他のプロトコル（<code>node</code>と<code>transport</code>）もサポートしますが、これらを利用する場合には、
プラグインを別途インストールする必要があります。</p>
<pre><code>bin/plugin install --version 2.0.0 logstash-output-elasticsearch_java
</code></pre><h4 id="互換性のマトリックス">互換性のマトリックス</h4>
<p>LogstashとElasticsearchのバージョンの互換性は次のようになります。</p>
<p>画像あり。
<a href="https://www.elastic.co/blog/kibana-4-2-0">※画像に関しては原文をご覧ください。</a>
#Image <a href="https://www.elastic.co/assets/bltde5b69e2164aa82f%2Fcompat_matrix.png">https://www.elastic.co/assets/bltde5b69e2164aa82f%2Fcompat_matrix.png</a></p>
<h2 id="shield-20との互換性">Shield 2.0との互換性</h2>
<p>このリリースはShield 2.0リリースにも対応しています。
HTTPプロトコルで、追加のプラグインは必要ありません。
<a href="https://www.elastic.co/guide/en/shield/current/logstash.html">こちらのドキュメント</a>をご覧ください。
<code>transport</code>プロトコルでは、Shield 2.0対応のプラグインを個別にインストールする必要があります。</p>
<pre><code>bin/plugin install --version 2.0.0 logstash-output-elasticsearch_java_shield
</code></pre><h2 id="パフォーマンスの改善">パフォーマンスの改善</h2>
<p>このリリースはまた、多くの部分のパフォーマンスの改善を含んでおり、Logstashを利用してデータをより早く処理することができます。
いくつかをここで説明します。</p>
<ul>
<li>
<p><strong>UserAgentとGeoIPフィルタ</strong>：これらのフィルタで、LRUキャッシュを追加して改善しています。
これにより、IPとユーザエージェントがまとまって現れるというWebリクエストの特性を用いています。
ユーザエージェントフィルタのケースでは、サンプルデータセットにおいて3.7倍ほど早くなりました。
GeoIPでは、1.69倍早くなっています。</p>
</li>
<li>
<p><strong>JSONプロセシング</strong>：LogstashでJSONのsiriaraizu/でシリアライズに利用しているJrJacksonを新しいバージョンにしました。
これにより、JSONの処理が改善されています。</p>
</li>
<li>
<p><strong>フィルタワーカーのより良い値をデフォルトに</strong>：以前のリリースでは、<code>filter_workers</code>の設定は1がデフォルトでした。
これは、フィルタの処理を行うワーカーが1つであるという意味です。
<code>filter_workers</code>の設定のデフォルト値はCPUコア数の半分の値を設定します。フィルタ実行の並列性が上がります。
ですので、複雑なgrokパターンやuseragentフィルタの処理がにとっては重要です。</p>
</li>
</ul>
<h2 id="filebeat-support">Filebeat Support</h2>
<p><a href="https://www.elastic.co/products/beats/filebeat">Filebeat</a>のベータバージョンを<a href="https://www.elastic.co/blog/beats-beta4-filebeat-lightweight-log-forwarding">先日リリース</a>しました。
これは、Logstash Forwarderの次期バージョンです。
Filebeatはファイルベースのログをさらに処理するためにLogstashに送るためのエージェントです。
2.0.0は<a href="https://www.elastic.co/guide/en/logstash/2.0/plugins-inputs-beats.html">logstash-input-beatsプラグイン</a>を使えばFilebeat 1.0.0-beta4とすぐに動作します。</p>
<h2 id="シャットダウン操作">シャットダウン操作</h2>
<p>これまでのLogstashでは、シャットダウンが開始した時に、例外の機構でシャットダウンが開始したことを
プラグインに通知していました。
この処理はサードパーティのコードを使ったプラグインで問題を起こしていました。
Logstashはどの例外を処理するか知らないため、予期しない動作をしていました。
これを修正するためにAPI呼び出し（例えば<code>stop</code>）を各プラグインにシャットダウンのイベントを通知し、
プラグイン自身がきちんと停止するようにしました。
これは、200以上のプラグインに新しいAPIを利用するように修正しないといけないことを意味しました。
しかし、Logstashの停止についてはまだ完全にはフィックスしていません。
とちゅうでおわっているoutputがシャットダウンを遅らせる可能性があるからです。
2.0でAPIの破壊的な変更は適切なリリースでの変更を繰り返すことができる出発点です。</p>
<p>プラグインの開発者へ：もし、Logstash　1.5のプラグインを開発しているなら、
シャットダウンに関する新しいAPIのリストに関するbreaking changesのドキュメントに助言をください。
また、<a href="https://github.com/logstash-plugins/logstash-input-example">example input</a>リポジトリにて、新しいシャットダウンメカニズムの使い方のサンプルコードを提供しています。</p>
<h2 id="ドキュメント">ドキュメント</h2>
<p>2.0に更新された<a href="https://www.elastic.co/guide/en/logstash/2.0/index.html">ドキュメントはこちら</a>です。設定の変更についてもこちらをご覧ください。</p>
<h3 id="20へのアップデート">2.0へのアップデート</h3>
<p>2.0へアップデートする前に、<a href="https://www.elastic.co/guide/en/logstash/2.0/upgrading-logstash.html">アップデートガイド</a>もご覧ください。</p>
<h2 id="フィードバック">フィードバック</h2>
<p>2.0のリリースできたことに、多くのコントリビューター、ユーザに感謝しています。
このリリースに含まれている多くのパッチと全てのプレリリースのテストにも感謝しています。
将来の修正やリリースなどについては<a href="https://www.elastic.co/guide/en/logstash-roadmap/current/index.html">ロードマップ</a>をご覧ください。
2.0は<a href="https://www.elastic.co/downloads/logstash">今日リリース</a>されました。
ご意見ご感想は<a href="https://discuss.elastic.co/c/logstash">Webフォーラム</a>で！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Release, we have（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/10/29/release-we-have-ja/</link>
      <pubDate>Thu, 29 Oct 2015 14:18:59 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/10/29/release-we-have-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Relase, we have ※画像に関しては原文をご覧ください。 Elasticにとって大きな1日</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/release-we-have">Relase, we have</a>
※画像に関しては原文をご覧ください。</p>
<!-- more -->
<p>Elasticにとって大きな1日（社内では「release bonanza」と呼んでいる）です。
多くの主要なプロダクトを新たにリリースしました。
そして、本日、それらを一緒に利用する時にそれらを一緒に利用する時にユーザの体験についてまとめてみました。</p>
<p>次の通りです。</p>
<p><a href="https://www.elastic.co/blog/elasticsearch-2-0-0-released">Elasticsearch 2.0</a>リリース。
大きなマイルストーン、チームによる改善、そして、コミュニティからの素晴らしい貢献。
Pipeline Aggsと呼ばれる新しいタイプのaggregations、
クエリとフィルタのコンセプトを統合することにより簡素化されたクエリDSL、
better compressionオプション、
JavaのSecurity Managerを有効にすることによる強化されたセキュリティ、
FSの挙動に関する強化（fsync、checksum、atmicなリネーム）、
パフォーマンス、マッピングの挙動の一貫性などなどです。
また、我々のチームによる改善も含まれているLucene 5ベースにアップグレードしています。</p>
<p><a href="https://www.elastic.co/blog/kibana-4-2-0">Kibana 4.2</a>リリース。
Elasticsearch 2.0対応、ダークテーマ、カスタマイズ可能な地図、多くの改善。
Kibana 4.2の多くに作業については外部プラグインサポートといった、内蔵に関するものでした。
この後の説明に続きます。</p>
<p><a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">Marvel 2.0</a>リリース。
Elasticsearch 2.0対応、合理化されたメトリックス、簡素化されたUI、
多くはKibanaプラグイン（Kibanaプラットフォーム上に構築）としての書き換えです。
このKibana拡張の最初の努力は、Kibanaのプラグインをどうやって書くか、
Kibanaユーザに公式に何をする必要があるかといったものを特定するのに役立ちました。
おっと、忘れるところでした、Marvelを全てのユーザにフリーで使えるようにしました。
マルチクラスタサポートについては有償となります。</p>
<p><a href="https://www.elastic.co/blog/sense-2-0-0-beta1">Sense 2.0</a>リリース。
2つ目のKibanaプラグインがこれです。
SenseをKibanaプラグインとして書き換えました。
Elasticsearch 2.0サポート、複数リクエストの実行、
curlへのコピーなどです。
おっと、忘れるところでした。オープンソースとすることにしました！</p>
<p><a href="https://www.elastic.co/blog/shield-watcher-and-marvel-2-0-ga-released">Shield + Watcher 2.0</a>リリース。ElasticsearchのためのセキュリティプラグインであるShieldと、アラート管理のためのプラグインであるWatcherにも
多くの結果が入っています。
最も要求のあった機能である、フィールドお呼びドキュメントレベルでのセキュリティについて、Luceneに落とし込んで実装しました。
また、セキュリティの操作についてプラガブルに実装できるように変更しました。
Watcherは監視の無効化、SlackやHipChatへの通知（bot ops向け）が可能です。</p>
<p><a href="https://www.elastic.co/blog/logstash-2-0-0-released">Logstash 2.0</a>リリース。
Elasticsearch 2.0のサポート、クリーンな停止、全面的なパフォーマンス改善、<a href="https://www.elastic.co/products/beats">Beats</a>サポート。</p>
<p>ご覧の通り、すべてのプロダクトに関する大きな結果です。
チーム間およびFoundの開発者との間での密な連携に感謝します。
これらが私たちが公式にElasticsearch / Kibanaをホストしている<a href="https://www.elastic.co/found">Found</a>で
利用可能です。</p>
<p>ひゅう、息切れしました。
チームがしてきたことは、感動的で、謙虚で、刺激的です！
Elasticが会社として、全てのユーザ、コントリビュータがどのように私たちの大きなミッションに対する結果をもたらしたかという素晴らしい良い例です。
ユーザに愛され、楽しまれ、成功に導き、革新させる製品を是非ご利用ください。ありがとうございます。</p>
<p>&ldquo;A Lion, in Africa?&rdquo; - まだまだ終わりではありません。この文言で終わりにしますが、すぐに（本当にすぐに）戻ってきます。;)</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 2.0.0-beta2リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/09/18/elasticsearch-2-0-0-beta2-released-ja/</link>
      <pubDate>Fri, 18 Sep 2015 17:31:41 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/09/18/elasticsearch-2-0-0-beta2-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 2.0.0-beta2 released 本日（9/17）、Lucene 5.2.1ベースのElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-2-0-0-beta2-released">Elasticsearch 2.0.0-beta2 released</a></p>
<p>本日（9/17）、<strong>Lucene 5.2.1</strong>ベースの<strong>Elasticsearch 2.0.0-beta2</strong>をリリースしました。
本リリースが2.0.0のRCの前の最後のベータリリースになります。</p>
<p><strong>注意事項</strong>
本リリースはベータリリースであり、テストを目的としたものとなります。
Elasticsearch 2.0.0-beta2はElasticsearch 2.0.0-beta1と互換がありません。
また、Elasticsearch 2.0.0 GAと互換性があるかどうかの保証はありません。</p>
<p><strong>本番環境には利用しないでください。</strong></p>
<!-- more -->
<p><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-beta2">Elasticsearch 2.0.0-beta2のダウンロードおよび、すべての変更についてはリンク</a>をごらんください。</p>
<p>2.0.0-beta1をテストし、問題点を報告していただいた皆様、ありがとうございます。
2.0.0-beta1のあとのElasticsearchのコアの部分の修正のほとんどはバグフィックスになりますが、
<a href="https://github.com/elastic/elasticsearch/pull/12893">geo_shapeフィールドの<code>points_only</code>最適化</a>のようなちょっとした改善も含んでいます。</p>
<p>また、本リリースでは、商用プラグインの重要な新機能もあります。
こちらについては<a href="https://www.elastic.co/blog/shield-and-watcher-2.0.0-beta2-released">Shield and Watcher 2.0.0-beta2 released</a>をごらんください。
簡単な紹介は次の通りです。</p>
<h3 id="shieldの新機能">Shieldの新機能</h3>
<h5 id="ドキュメントおよびフィールドレベルのセキュリティ">ドキュメントおよびフィールドレベルのセキュリティ</h5>
<p>Shieldは、クエリを利用したインデックスにあるドキュメントへのアクセスを制御するためのロールを定義できるようになりました。
また、ドキュメントにある特定のフィールドに関するアクセス制限も可能です。
フィルタされたエイリアスのような形ではなく、ドキュメントを検索したり、IDで取得したりする場合にこれらの制限が利用できます。
詳細は<a href="https://www.elastic.co/guide/en/shield/2.0.0-beta2/setting-up-field-and-document-level-security.html">Field- and Document-level Security</a>をごらんください</p>
<h5 id="ユーザなりすまし">ユーザなりすまし</h5>
<p>特定のユーザーに他のユーザーに扮して、彼らのためにリクエストを実行する能力を与えることが、現在できます。
これは、認証がアプリケーションによって実行される場合に便利です。
そして、それは、ユーザの許可レベルを考慮するようにElasticsearchにリクエストします。
詳細は<a href="https://www.elastic.co/guide/en/shield/2.0.0-beta2/submitting-requests-for-other-users.html">Submitting Requests for Other Users</a>をごらんください。</p>
<h5 id="プラガブルな認証レルム">プラガブルな認証レルム</h5>
<p>このリリースで、サードパーティの拡張のための認証レルムのインフラを公開しました。
もし、特定の認証要求があり、Shieldがサポートしていない（が、内部の認証管理システムを使いたいような）場合、
これらの要求に見合う新しい認証レルムを利用するプラグインを作成可能です。
詳細は<a href="https://www.elastic.co/guide/en/shield/2.0.0-beta2/custom-realms.html">Custom Realms</a>をごらんください。</p>
<h3 id="watcherの新機能">Watcherの新機能</h3>
<h5 id="監視の一時">監視の一時</h5>
<p>新しく、<code>active</code> / <code>inactive</code> の状態がwatchに追加されました。
これらは、Watchを中断したり、要求に応じて再開させたりできます。
詳しくは、<a href="https://www.elastic.co/guide/en/watcher/2.0.0-beta2/watch-active-state.html">Active State</a>をごらんください。</p>
<h5 id="チャットのための新しいアクション">チャットのための新しいアクション</h5>
<p><code>slack</code>と<code>hipchat</code>アクションが追加されました。
これは、Watcherが通知を、SlackやHipchatのユーザに直接送ったり、
チームのチャットルームに送ったりすることが出来るようにします。
詳細については、<a href="https://www.elastic.co/guide/en/watcher/2.0.0-beta2/actions.html#actions-slack">Slack action</a>および、<a href="https://www.elastic.co/guide/en/watcher/2.0.0-beta2/actions.html#actions-hipchat">Hipchat action</a>をごらんください。</p>
<h2 id="20に関するこれまでのブログ記事">2.0に関するこれまでのブログ記事</h2>
<p>これまでのリリースについての情報はこれらのブログ記事をごらんください。</p>
<p>* <a href="https://www.elastic.co/blog/elasticsearch-2-0-0-beta1-released">Elasticsearch 2.0.0.beta1 released</a></p>
<ul>
<li><a href="https://www.elastic.co/blog/elasticsearch-2.0.0.beta1-coming-soon">Elasticsearch 2.0.0.beta1 coming soon!</a></li>
<li><a href="https://www.elastic.co/blog/great-mapping-refactoring">The Great Mapping Refactoring</a></li>
<li><a href="https://www.elastic.co/blog/store-compression-in-lucene-and-elasticsearch">Store compression in Lucene and Elasticsearch</a></li>
<li><a href="https://www.elastic.co/blog/better-query-execution-coming-elasticsearch-2-0">Better query execution coming to Elasticsearch 2.0</a></li>
<li><a href="https://www.elastic.co/blog/out-of-this-world-aggregations">Out of this world aggregations</a></li>
<li><a href="https://www.elastic.co/blog/staying-in-control-with-moving-averages-part-1">Staying in Control with Moving Averages - Part 1</a></li>
<li><a href="https://www.elastic.co/blog/staying-in-control-with-moving-averages-part-2">Staying in Control with Moving Averages - Part 2</a></li>
<li><a href="https://www.elastic.co/blog/core-delete-by-query-is-a-plugin">The Delete by Query API Is now a plugin</a></li>
<li><a href="https://www.elastic.co/blog/elasticsearch-unplugged">Elasticsearch unplugged - Networking changes in 2.0</a></li>
</ul>
<p>また、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/index.html">Elasticsearch 2.0.0-beta2のドキュメント</a>や<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html">2.0のbreaking changesのリスト</a>もごらんください。</p>
<h2 id="elsticsearch-migration-plugin">Elsticsearch Migration Plugin</h2>
<p>Elasticsearch Migration Pluginは、既存のインデックスをアップグレードする
必要があるか、他に必要な行動がないかについて、Elasticsearch
2.0.0-beta2を試す前に確認する助けとなります。
Lucene 3のような古いインデックスや、2.0.0にした場合に動作しない問題のある
マッピングのような問題を発見できます。</p>
<p>プラグインの動作に関しては[Elasticsearch Migration repository](Elasticsearch Migration repository)をごらんください。</p>
<h2 id="テストしましょう">テストしましょう！</h2>
<p>Elasticsearch 2.0.0 GAをすぐにリリースできるようにより多くのベータテスターをお待ちしています。</p>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-beta2">Elasticsearch 2.0.0-beta2</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)や<a href="https://discuss.elastic.co/c/elasticsearch">Webフォーラム</a>などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch unplugged - 2.0におけるネットワークの変更(日本語訳)</title>
      <link>https://blog.johtani.info/blog/2015/08/28/elasticsearch-unplugged-ja/</link>
      <pubDate>Fri, 28 Aug 2015 12:01:30 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/08/28/elasticsearch-unplugged-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch unplugged - Networking changes in 2.0 Elasticsearchをローカルのマシンで起</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-unplugged">Elasticsearch unplugged - Networking changes in 2.0</a></p>
<p>Elasticsearchをローカルのマシンで起動します。
そして、昨日試したデータを削除するために<code>DELETE *</code>を実行します。
すると、悲しそうな叫びを同僚が発していることに気づき、なぜそんなことになっているのか不思議に思うでしょう。。。</p>
<!-- more -->
<p>Elasticsearchはいつも、親しみやすいものでした。
複数ノードのクラスタがどのように機能するのかをテストするには、
ローカルのマシンでいくつかのElasticsearchのインスタンスを起動するだけでした。
起動したインスタンスはマルチキャストを利用して自動的にお互いを見つけて、1つのクラスタになり、負荷を共有し始めます。
しかし、これは親しみやすすぎました。
カンファレンスなどで、ローカルのマシンでElasticsearchを起動してみてください。
すると100ノードのクラスタに参加しているのがすぐにわかるでしょう。</p>
<p>もうすぐリリースされる、2.0.0-beta1では、Elasticsearchが通信先を選択するネットワークの機能に関する変更があります。
ただし、これまで通り、簡単に開発者が経験できる機能も残っています。</p>
<h2 id="localhostへのバインド">localhostへのバインド</h2>
<p>以前、Elasticsearchはデフォルトで、利用可能なネットワークインタフェース全てにバインドしていました。
そこから、一番適したインタフェースを<code>publish_host</code>として選択しようとします。
このアドレスはElasticsearchがクラスタの他のノードとやりとりするためのアドレスです。</p>
<p>Elasticsearch 2.0では、デフォルトでは、<code>localhost</code>にのみバインドします。
<code>127.0.0.1</code>（IPv4）と<code>[::1]</code>（IPv6）の両方にバインドしようとします。
また、どちらかのみの環境でも動作します。
この変更は、特に指定がない限り、Elasticsearchがネットワーク上の他のノードと接続しません。
本番環境に移行する場合は、<code>network.host</code>パラメータを使って設定しましょう。
設定は、<code>elasticsearch.yml</code>に記述するか、コマンドラインで指定します。</p>
<pre><code>bin/elasticsearch --network.host 192.168.1.5
bin/elasticsearch --network.host _non_loopback_
</code></pre><p><code>network.host</code>の全てのオプションについては、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-network.html">network settingsのドキュメント</a>をごらんください。</p>
<h2 id="マルチキャストは廃止">マルチキャストは廃止</h2>
<p>Elasticsearch 1.xはネットワークの他のノードに接続・探索するためにマルチキャストを使用しました。
マルチキャストは魔法のような挙動です。。。
残念ながら、マルチキャストのサポートは良くも悪くもあります。
Linuxはローカルホストでマルチキャストの待ち受けをしていません。
OS/Xは構成されたアドレスの全てのインタフェースにマルチキャストで配信できます。
また、ネットワークによってはマルチキャストはデフォルトでは使用できなくなっています。</p>
<p>Elasticsearch 2.0は異なるアプローチを採用しました。
マルチキャストを廃止します（ただし、新たにプラグインとして提供します）。
代わりに、ローカルホストでは、Elasticsearchは<code>transport.tcp.port</code>で指定されている範囲（デフォルトは<code>9300-9400</code>）の最初の5ポートに対してユニキャストを使用できるようにします。</p>
<p>これは、開発者のための、設定することなく自動的にクラスタを組むという機能を残しています。
しかし、本番に移行するときは、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-discovery-zen.html#unicast">unicast hosts</a>で次のようにリストを指定する必要があります。</p>
<pre><code>discovery.zen.ping.unicast.hosts: [ 192.168.1.2,  192.168.1.3 ]
</code></pre><p>unicast hostsとしてクラスタにあるノードの全てのリストを指定する必要はありません。
少なくとも、マスタノードとして選出されるべきものを指定します。
巨大なクラスタでは、3つの専用のマスタノードを持っており、この3つをunicast hostsとして設定することを推奨しています。</p>
<p>これにより、開発の知識・経験が、私たちの推奨する本番でのネットワーク設定に、より近いものとなります。</p>
<h2 id="ノード情報の変更">ノード情報の変更</h2>
<p>最後に、<code>inet[/127.0.0.1:9200]</code>といったシンタックスを廃止します。
これは、Elasticsearchが<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-info.html">nodes-info API</a>などで、使用していたIPアドレスのためのシンタックスです。
今は、RFCに準拠した形で表示します。
<code>127.0.0.1:9200</code>（IPv4）や<code>[::1]:9200</code>（IPv6）のようにです。</p>
<p>質問がある場合は、<a href="https://discuss.elastic.co/c/elasticsearch">ElasticsearchのWebフォーラム</a>で質問してください。ベータはもうすぐです！（翻訳した時点で、すでにベータリリースされています。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 2.0.0-beta1リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/08/27/elasticsearch-2-0-0-beta1-released-ja/</link>
      <pubDate>Thu, 27 Aug 2015 10:29:12 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/08/27/elasticsearch-2-0-0-beta1-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 2.0.0-beta1 released 本日（8/26）、Lucene 5.2.1ベースのElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-2-0-0-beta1-released">Elasticsearch 2.0.0-beta1 released</a></p>
<p>本日（8/26）、<strong>Lucene 5.2.1</strong>ベースの<strong>Elasticsearch 2.0.0-beta1</strong>をリリースしました。
本リリースは469名のコミッターからの2,500以上ものpull requestを含んでいます。
pull requestのうち、約850が2.0のための新規のものとなります。</p>
<p><strong>注意事項</strong>
本リリースはベータリリースであり、テストを目的としたものとなります。
Elasticsearch 2.0.0-beta1は Elasticsearch 2.0.0 GAと互換性があるかどうかの保証はありません。</p>
<p><strong>本番環境には利用しないでください。</strong></p>
<!-- more -->
<p><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-beta1">Elasticsearch 2.0.0-beta1のダウンロードおよび、すべての変更についてはリンク</a>をごらんください。</p>
<p>Elasticsearch 2.0.0-beta1には次の新しい変更が含まれています。</p>
<ul>
<li>Pipeline Aggregations：これは、他のaggregationsの結果に対するAggregationを実行できます（導関数、移動平均、Holt Winter予測アルゴリズムなども含む）</li>
<li>ディスクやファイルシステムキャッシュにより適したより良いデータの圧縮</li>
<li>doc-valuesがデフォルトになったこと、マージ実行時のメモリ使用量の低減、フィルターキャッシュのためのroaring bitsetsなどにより、ヒープの使用率がより効率的に。</li>
<li>構造化された例外</li>
<li>最適化されたクエリ実行順序、フィルタの自動キャッシュ、より高速なクエリに書き換えられたparent-child</li>
<li>設定の代わりに、フィードバックループを使用した自動調整</li>
<li>トランザクションログへの書き込みがデフォルトで、アトミックでかつ冗長に</li>
<li>安全で明確で信頼性のあるタイプマッピング</li>
<li>デフォルトでローカルホストでのみクラスタを構成</li>
<li>クラスタ状態の差分によりより高速に変更を伝搬</li>
</ul>
<p>上記の変更以外にも、多くのElasticsearchおよびLuceneに対する継続的な変更が含まれています。
これらは、Elasticsearch 2.0をより安全に、より簡単に、より良いものにしています。
本リリースに関するより詳しい情報が次のブログにあるので、参考にしてください。</p>
<ul>
<li><a href="https://www.elastic.co/blog/elasticsearch-2.0.0.beta1-coming-soon">Elasticsearch 2.0.0.beta1 coming soon!</a></li>
<li><a href="https://www.elastic.co/blog/great-mapping-refactoring">The Great Mapping Refactoring</a></li>
<li><a href="https://www.elastic.co/blog/store-compression-in-lucene-and-elasticsearch">Store compression in Lucene and Elasticsearch</a></li>
<li><a href="https://www.elastic.co/blog/better-query-execution-coming-elasticsearch-2-0">Better query execution coming to Elasticsearch 2.0</a></li>
<li><a href="https://www.elastic.co/blog/out-of-this-world-aggregations">Out of this world aggregations</a></li>
<li><a href="https://www.elastic.co/blog/staying-in-control-with-moving-averages-part-1">Staying in Control with Moving Averages - Part 1</a></li>
<li><a href="https://www.elastic.co/blog/staying-in-control-with-moving-averages-part-2">Staying in Control with Moving Averages - Part 2</a></li>
<li><a href="https://www.elastic.co/blog/core-delete-by-query-is-a-plugin">The Delete by Query API Is now a plugin</a></li>
<li><a href="https://www.elastic.co/blog/elasticsearch-unplugged">Elasticsearch unplugged - Networking changes in 2.0</a></li>
</ul>
<p>また、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/index.html">Elasticsearch 2.0.0-beta1のドキュメント</a>も参考になります。
特に、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html">2.0での重大な変更点について</a>は必ずごらんください。</p>
<h2 id="core-plugins">Core plugins</h2>
<p>コアプラグインの開発の方法を変更しました。
公式にサポートしているプラグインは、現在<a href="https://github.com/elastic/elasticsearch/tree/master/plugins">elasticsearchのリポジトリ</a>に含まれています。
これにより、コアと一緒にテストされ、Elasticsearchと同じタイミングでリリースされます。
コアプラグインはElasticsearchと同じバージョン番号隣ます。
インストールは次のようになります。</p>
<pre><code>sudo bin/plugin install analysis-icu
</code></pre><p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/index.html">プラグインの新しいドキュメント</a>は私たちのWebサイトの<a href="https://www.elastic.co/guide/index.html">Guide</a>にあります。</p>
<h2 id="commercial-plugins">Commercial plugins</h2>
<p>私たちの商用プラグインもElasticsearchと同じバージョン番号となり、Elasticsearchと一緒にリリースされます。
ShieldやWatcherはすでに2.0.0-beta1が利用可能です。
インストールのコマンドはは次のようになります。</p>
<pre><code>sudo bin/plugin install license
sudo bin/plugin install shield
sudo bin/plugin install watcher
</code></pre><p>MarvelおよびSenseに関する新しい情報もありますが、もう少しお待ちください。</p>
<p>2.0.0-beta1の商用プラグインに関するドキュメントは次のリンクからごらんください。</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/shield/2.0.0-beta1/index.html">Shield 2.0.0-beta1</a></li>
<li><a href="https://www.elastic.co/guide/en/watcher/2.0.0-beta1/index.html">Watcher 2.0.0-beta1</a></li>
</ul>
<h2 id="elasticsearch-migration-plugin">Elasticsearch Migration plugin</h2>
<p>Elasticsearch 2.0.0-beta1を試す前に、
既存のインデックスのアップグレードするためになにか行う必要があるかどうかを確認するためのElasticsearch Migration Pluginもリリースしました。
2.0.0では機能しない、問題のあるマッピングなどを見つけるために便利なプラグインです。</p>
<p>このプラグインの利用方法については<a href="http://github.com/elastic/elasticsearch-migration">Elasticsearch Migration repository</a>をごらんください。</p>
<h2 id="既知の問題">既知の問題</h2>
<p>同じインデックスの異なるタイプに、同じ名前の<code>ip</code>タイプのフィールドを追加した時に、問題があることがわかっています。
この問題は次のリリースでフィックスされます。詳細は<a href="https://github.com/elastic/elasticsearch/issues/13112">#13112</a>をごらんください。</p>
<h2 id="テストしましょう">テストしましょう！</h2>
<p>Elasticsearch 2.0.0 GAをすぐにリリースできるようにより多くのベータテスターをお待ちしています。</p>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0-beta1">Elasticsearch 2.0.0-beta1</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)や<a href="https://discuss.elastic.co/c/elasticsearch">Webフォーラム</a>などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Delete by Query APIはプラグインへ（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/08/20/core-delete-by-query-is-a-plugin-ja/</link>
      <pubDate>Thu, 20 Aug 2015 13:24:04 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/08/20/core-delete-by-query-is-a-plugin-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：The Delete by Query API Is now a plugin Elasticsearchの2.0.0-beta1では、これまで</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/core-delete-by-query-is-a-plugin">The Delete by Query API Is now a plugin</a></p>
<p>Elasticsearchの2.0.0-beta1では、これまであった <a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.6/docs-delete-by-query.html">Delete by Query API</a>が<a href="https://github.com/elastic/elasticsearch/pull/10859">削除</a>され、
新しく <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/plugins-delete-by-query.html">Delete by Query plugin</a>に<a href="https://github.com/elastic/elasticsearch/pull/11516">置き換え</a>られています。</p>
<!-- more -->
<p>もし、Delete by Query を利用する場合、2.0にアップグレードしたあとは、プラグインをインストールし、ドキュメントに従ってください。</p>
<pre><code>bin/plugin install delete-by-query
</code></pre><h2 id="なぜプラグインに">なぜプラグインに？</h2>
<p>ElasticsearchのコアなAPIの品質を保つためであり、以前のDelete by Queryの実装は簡単にはフィックスできない大きな問題がありました。</p>
<ul>
<li>各リクエストのあとに、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.6/indices-refresh.html">refresh</a>を実行します。これは、<a href="https://github.com/elastic/elasticsearch/issues/3593">削除されたデータが想定外に検索に出てこないようにするため</a>です。<br>
また、<a href="https://github.com/elastic/elasticsearch/issues/6025">セグメントが大量にでき、マージが大量に発生し、ヒープが大量に消費されてインデキシングが劇的にスローダウンし</a>、クラスタの複数のノードがクラッシュしてしまう状況も引き起こしました。</li>
<li>このクエリは、プライマリ、レプリカの両方で実行されるため、ことなるドキュメントを削除し、矛盾したレプリカ（データの破損）を引き起こしました。</li>
<li>アップグレードが不安定になります。これは、Delete by Queryリクエストがトランザクションログの中にクエリとして残るためです。そのため、アップグレードのあとに正確にパースされなかったり正確に実行されないかもしれません。例えば、インデックスエイリアスに対するリクエストで、それが削除された後の場合に<a href="https://github.com/elastic/elasticsearch/issues/10262">このようなバグ</a>が発生します。</li>
</ul>
<p>対照的に、<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/plugins-delete-by-query.html">新しいプラグイン</a>は、安全な実装です。
<a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/scan-scroll.html">scanとscrollリクエスト</a>でクエリにマッチしたIDを見つけ、そのIDを使って、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html">bulk indexing API</a>で削除します。</p>
<p>この実装は、遅い必要があります。特に、クエリが多くのドキュメントを削除する場合です。
もし、多くのドキュメントをこのAPIを利用して削除する場合、アプリケーションをテストしてください。
そして、<a href="https://www.elastic.co/blog/lucenes-handling-of-deleted-documents">代わりにインデックス全体を消す</a>ようなアプローチに切り替えることができないか検討してください。</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/delete-by-query-plugin-reason.html">Delete by Query pluginのドキュメント</a>に、新しい実装についての違いなどのより詳しい説明があります。</p>
<h2 id="elasticsearch-coreを最小限に">Elasticsearch coreを最小限に</h2>
<p>プラグインに切り替えることは、簡単な決断ではありませんでした。
多くのユーザは問題なく、Delete by Queryを利用していました。
しかし、危険が常にそこにあり、些細とは言い切れない数のユーザが上記のような深刻な問題に遭遇していました。</p>
<p>さらに、Elsticsearchのコアは信頼できるものでなければなりません。
他のコアAPIを利用して実装できる機能は、コアに含みません。特に、それがバグを含んでいる場合。
コアのすべての機能は強固であるべきで、Delete by Queryは人気があり、高性能ですが、そうではありませんでした。</p>
<p>必要に応じて、このような難しいトレードオフの末、信頼性と品質を選びます。</p>
<h2 id="マッピングの削除の廃止">マッピングの削除の廃止</h2>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.6/indices-delete-mapping.html">タイプのマッピングを削除する機能</a>も2.0で<a href="https://github.com/elastic/elasticsearch/issues/8877">廃止</a>されます。
これは、同じフィールド名を、異なるフィールドのタイプで再利用した場合に、インデックスの破損を引き起こす可能性があるためです。</p>
<p>しかし、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-all-query.html">Match All Query</a>で、Delete by Queryプラグインに対してタイプを指定することで、タイプのすべてのドキュメントを削除することはできます。
または、1つのインデックスに異なるタイプを複数含める代わりに、個別のインデックスに分割するようなアプローチに変更することを検討してください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第11回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/07/31/11th-elasticsearch-jp/</link>
      <pubDate>Fri, 31 Jul 2015 14:41:53 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/31/11th-elasticsearch-jp/</guid>
      <description>第11回Elasticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、会場提供していただいたリクルートテクノロジーズ</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/28321">第11回Elasticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、会場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、そして、Shayありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<p>今回は、CTOのShayが来日していたので、英語でいろいろと喋ってもらいました。
4月同様、<a href="http://samuraism.com">サムライズム</a>の<a href="https://twitter.com/yusuke">@yusuke</a>さんに
テキスト翻訳していただき、大変助かりました。
今回はQAベースのトークだったのでちょっときつかったですね、申し訳ない。。。</p>
<!-- more -->
<h3 id="チェックイン数など">チェックイン数など</h3>
<ul>
<li>チェックインした人：141名</li>
<li>キャンセルしなかった人：51名</li>
</ul>
<p>でした。
今回はあらかじめ220名（全員が来たらキャパオーバー）としていたので、キャンセル待ちの人は
当日の午後にはいなくなっていた状態です。まぁ、こんなもんかな。結構入りましたね。ありがたいです。</p>
<h2 id="lt">LT</h2>
<p>今回は、少し趣向を変えて、4社の方達にLTをしていただきました。
Shayが来日しているのもあり、事前に英語でスライドを作っていただけると助かりますとお願いさせていただきました。
英語でスライドを作っていただいていたので、伝わりやすくて助かりました、スピーカーの方々ありがとうございました！<br>
（海外のユーザにもリンクを紹介しやすいので、英語でスライド作ってもらえるといろいろと知ってもらえるのかも。）</p>
<h3 id="elasticsearch-and-recruit-technologies-co-ltd--株式会社リクルートテクノロジーズ守谷-純之介さん">Elasticsearch and Recruit Technologies Co., Ltd. / 株式会社リクルートテクノロジーズ　守谷 純之介さん</h3>
<p>スライド：未定</p>
<p>N-Gramと形態素のハイブリッドの話などをしていただきました。
@ITで連載もされてますね。ありがとうございます。</p>
<ul>
<li><a href="http://www.atmarkit.co.jp/ait/articles/1507/08/news009.html">リクルート全社検索基盤のアーキテクチャ、採用技術、開発体制はどうなっているのか (1/2)</a></li>
<li><a href="http://www.atmarkit.co.jp/ait/articles/1507/29/news010.html">ElasticsearchとKuromojiを使った形態素解析とN-Gramによる検索の適合率と再現率の向上 (1/3)</a></li>
</ul>
<p>Shayからは、<a href="https://www.elastic.co/products/hadoop">elasticsearch-hadoop</a>があるから検討してねと質問（お願い？）がありましたw。</p>
<h3 id="elasticsearch-as-a-dmp--株式会社インティメートマージャー松田和樹さんmats116">Elasticsearch as a DMP / 株式会社インティメート・マージャー　松田和樹さん　@mats116</h3>
<p>スライド：<a href="http://www.slideshare.net/im_docs/elasticsearch-as-a-dmp">Elasticsearch as a DMP</a></p>
<p>いくつかのデータソースからAEROSPIKE経由でelasticsearchにデータを登録しているようです。
Data Management Platformのエンジンの一部として、elasticsearchを利用しているようです。</p>
<p>Shayからの質問：「どの機能を使って関心のある単語を抽出していますか？」<br>
回答：「<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">Significant Term Aggregation</a>」です。<br>
Shay：「おぉ、チェックしてみますw」。</p>
<h3 id="real-time-social-big-data-analytics-using-elasticsearch--株式会社ホットリンク宮田洋毅さんkakka_jp">Real-time social big data analytics using elasticsearch / 株式会社ホットリンク宮田洋毅さん　@kakka_jp</h3>
<p>スライド：未定</p>
<p>ソーシャルメディアのデータを解析するのにelasticsearchにデータを入れて解析。
時間軸での解析やテキストマイニングなんかをしているみたいでした。
いろいろと独自のプラグインを作ってるようです。（興味あるなぁ）</p>
<p>Shayからの質問：「ノード数は？」「30ノードで30シャード」</p>
<h3 id="elasticsearch-in-hatena-bookmark--株式会社はてなidskozawa">Elasticsearch in Hatena Bookmark / 株式会社はてな　id:skozawa</h3>
<p>スライド：<a href="http://www.slideshare.net/shunsukekozawa5/elasticsearch-in-hatena-bookmark">Elasticsearch in Hatena Bookmark</a></p>
<ul>
<li>はてなブックマークの検索の歴史（MySQL -&gt; Sedue -&gt; Solr -&gt; Elasticsearch）</li>
<li>はてなブックマークの検索（ユーザが利用）と社内利用と、ログ解析で利用してる</li>
</ul>
<p>Shayからの質問：「昨年会いましたよね？今はクラスタのサイズはどのくらいのサイズですか？」「メインクラスタは9データノード」</p>
<h2 id="open-qa-with-shay">Open QA with Shay</h2>
<p>思い出せるものだけ。。。（あとで追記します）</p>
<ul>
<li>Elasticsearch 2.0の話
<ul>
<li><a href="https://www.elastic.co/blog/out-of-this-world-aggregations">Pipeline Aggregation</a>とか。</li>
</ul>
</li>
<li>Spark Streaming対応してる？
<ul>
<li>まだ検討中</li>
</ul>
</li>
<li>elasticsearch-hadoopってどんなもの？HDFSにインデックス作ったりするの？
<ul>
<li>いえ、Hadoopの入出力先としてelasticsearchが使える感じ</li>
</ul>
</li>
<li>個人的にAWSのCloudSearchとAWSでElasticsearchはどっちがいい？
<ul>
<li>時系列データはCloudSearchだと難しいだろうし、AWS上なら<a href="http://found.no">found.no</a>があるよ！</li>
</ul>
</li>
<li>PostgreSQLみたいに信頼性の高いデータストアを目指してる（まだ、プライマリデータストアには使わないで）</li>
</ul>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://tech.im-dmp.net/archives/4941">Elasticsearch勉強会でLTしてきました | Intimate Merger Engineer Blog </a></li>
<li><a href="http://togetter.com/li/853331">『第11回elasticsearch勉強会』のまとめ #elasticsearchjp</a>
* <a href="http://suzuki.tdiary.net/20150728.html#p01">[Elasticsearch] 第11回 Elasticsearch 勉強会へ参加してきた - 雑文発散(2015-07-28)</a></li>
<li><a href="http://kakakakakku.hatenablog.com/entry/2015/07/27/224203">第11回 Elasticsearch 勉強会に参加したら英語力に危機感を覚えて最高だった</a></li>
</ul>
<h3 id="まとめ">まとめ</h3>
<p>今回はShayが来日したので特別バージョンでした。
もっと英語を翻訳するサポートしないとですね、反省してます。。。ぜんぜん流暢じゃないしw</p>
<p>次回は9月に開催予定ですが、12月にまたShayが再度来日する予定です。
丸1日のイベントを検討中で、Shay以外にも開発者が来日すると思います。
どんな話が聞きたい、どんな人と話をしたいなどあれば、コメントいただければ（対応できるかは。。。）</p>
<p>勉強会のスピーカーは随時募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。 よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.7.1 および 1.6.2リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/07/29/elasticsearch-1-7-1-and-1-6-2-released-ja/</link>
      <pubDate>Wed, 29 Jul 2015 21:35:53 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/29/elasticsearch-1-7-1-and-1-6-2-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 1.7.1 and 1.6.2 released 本日（7/29）、Lucene 4.10.4ベースのE</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-1-7-1-and-1-6-2-released">Elasticsearch 1.7.1 and 1.6.2 released</a></p>
<p>本日（7/29）、<strong>Lucene 4.10.4</strong>ベースの<strong>Elasticsearch 1.7.1</strong>および<strong>Elasticsearch 1.6.2</strong> のバグフィックス版をリリースしました。
これらのリリースは稀ですが、データの欠損が発生する重要なバグのフィックスを含んでいます。
<strong>すべてのユーザにアップグレードを推奨します。</strong></p>
<p>ダウンロードおよびすべての変更については次のリンクをごらんください。</p>
<ul>
<li>最新安定版：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-7-1">Elasticsearch 1.7.1</a></li>
<li>1.6系バグフィックス：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-6-2">Elasticsearch 1.6.2</a></li>
</ul>
<!-- more -->
<p>問題のバグ(<a href="https://github.com/elastic/elasticsearch/pull/12487">#12487</a>)は、
同時に複数のノードが故障またはリスタートをした場合の非常にまれな状況で、
シャードのすべてのコピーがクラスタから削除されてしまう状況を発生させます。
このバグは1.5.0から含まれています。</p>
<p>このリリースはまた、IPv4アドレスのCIDRマスクのバグのフィックス、
Shieldユーザがmore-like-this APIを利用できないバグのフィックスなど、
いくつかの変更も含んでいます（詳細は<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-7-1">更新リスト</a>をごらんください）。</p>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-7-1">Elasticsearch 1.7.1</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.7.0 および 1.6.1リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/07/22/elasticsearch-1-7-0-and-1-6-1-released-ja/</link>
      <pubDate>Wed, 22 Jul 2015 15:33:37 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/22/elasticsearch-1-7-0-and-1-6-1-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 1.7.0 and 1.6.1 released 本日（7/16）、Lucene 4.10.4ベースのE</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-1-7-0-and-1-6-1-released">Elasticsearch 1.7.0 and 1.6.1 released</a></p>
<p>本日（7/16）、<strong>Lucene 4.10.4</strong>ベースの<strong>Elasticsearch 1.7.0</strong>および<strong>Elasticsearch 1.6.1</strong> のバグフィックス版をリリースしました。
これらのリリースはセキュリティフィックスを含んでおり、<strong>すべてのユーザにアップグレードを推奨します。</strong></p>
<p>ダウンロードおよびすべての変更については次のリンクをごらんください。</p>
<ul>
<li>最新安定版：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-7-0">Elasticsearch 1.7.0</a></li>
<li>1.6系バグフィックス：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-6-1">Elasticsearch 1.6.1</a></li>
</ul>
<!-- more -->
<p>1.7.0が1.x系の最後のリリースとなります。
今後の新機能については、Elasticsearch 2.0以降で取り込まれる予定です。</p>
<p>Elasticsearch 1.7.0は小さなリリースですが、2つの重要なセキュリティフィックスと
クラスタの安定化とリカバリに関する2つの重要な機能を含んでいます。</p>
<ul>
<li><a href="#security">セキュリティフィックス</a></li>
<li><a href="#delayed">シャードアロケーションを遅らせる</a></li>
<li><a href="#prioritization">インデックスリカバリの優先度</a></li>
</ul>
<h2 id="a-namesecurityセキュリティフィックスa"><a name="security">セキュリティフィックス</a></h2>
<p>Elasticsearch 1.6.1 と 1.7.0 は次の2つのセキュリティフィックスを含んでいます。</p>
<h3 id="リモートコード実行の脆弱性">リモートコード実行の脆弱性</h3>
<p>Elasticsearch 1.6.1より前のバージョンには、transport protocol（ノードとJavaクライアント間での通信に利用）により、
リモートでコードが実行される脆弱性があります。
これは、<a href="http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3253">CVE-2015-3253</a>でのGroovyに関係しています。</p>
<p>Groovyのダイナミックスクリプティングがオフでも脆弱性があります。
アップグレードをしないユーザは、transport protocol のポート（デフォルトで9300）信頼したエージェントからのみの
アクセスに限定することで、脆弱性から保護できます。</p>
<p>この問題を<a href="https://www.elastic.co/community/security">CVE-2015-5377</a>としました。</p>
<h3 id="ディレクトリ探索の脆弱性">ディレクトリ探索の脆弱性</h3>
<p>Elasticsearch 1.0.0から1.6.0までのバージョンで、ElasticsearchのJVMプロセスによって読み込みが可能なファイルを
取得することができるディレクトリ探索攻撃の脆弱性があります。
アップグレードをしないユーザは、信頼できない場所からの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html">Snapshot-Restore API</a>の呼び出しを防ぐためにファイアウォール、リバースプロキシやShieldを使用することができます。</p>
<p>この問題を<a href="https://www.elastic.co/community/security">CVE-2015-5531</a>としました。</p>
<h2 id="a-namedelayedシャードアロケーションを遅らせるa"><a name="delayed">シャードアロケーションを遅らせる</a></h2>
<p>Elasticsearch 1.6.0で<a href="https://www.elastic.co/blog/elasticsearch-1-6-0-released#synced-flush">Synced Flushing</a>が導入されました。
これは、ノードのリスタート時に、更新が止まっているシャードのリカバリを劇的にスピードアップします。
しかし、この変更は、シャードの配置を無効にしている環境でのみうまく実行されます。
ノードが一時的にクラスタから外れている場合や予期せぬリブートの場合には役に立ちません。</p>
<p>このシナリオとは次のようなものです。</p>
<ul>
<li>ノードの想定外のシャットダウン</li>
<li>マスタがたのノードにシャードを再配置</li>
<li>各シャードが新しい場所にネットワーク越しにコピー</li>
<li>その間に、外れていたノードが再度クラスタにジョイン</li>
<li>マスタは新しいノードにシャードを再配置。新しいノードに存在する既存のシャードが全く再利用されない可能性がある</li>
</ul>
<p>ノードレベルとクラスタレベルの両方の並列的なリカバリを抑制しても、
この&quot;シャードシャッフル&quot;がクラスタに対して負荷をかける可能性があります。
これは、外れたノードが再度ジョインするのを単に待つことにより防げるかもしれません。</p>
<h3 id="待ちましょう">待ちましょう！</h3>
<p>Elasticsearch 1.7.0は<a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.7/delayed-allocation.html"><code>index.unassigned.node_left.delayed_timeout</code></a>設定を追加しました。デフォルトでは1分です。
これは、ノードがクラスタから外れたとき、ほかのノードにこれらのノードを再配置するまでマスタが1分待つということです。
ノードがこの1分の間に復帰した場合、マスタはローカルにあるシャードを再度配置します。</p>
<h3 id="なぜ1分">なぜ1分？</h3>
<p>ノードがシャットダウンし、リスタートし、復帰するために十分な時間が1分だからです。
しかし、ノードが復帰しない場合にはまだ再配置が発生することを意味します。
デフォルト値を決定するのは難しいです。
この設定をどのくらいに減らすか、増やすかを決める必要があるかもしれません。</p>
<p>このデフォルト値は、<code>config/elasticsearch.yml</code>ファイルに設定できますが、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html">インデックス設定の更新API</a>を使って設定することも可能です。</p>
<p>このデフォルトに関する知見をぜひフィードバックしてください。</p>
<h2 id="a-nameprioritizationインデックスリカバリの優先度a"><a name="prioritization">インデックスリカバリの優先度</a></h2>
<p>1.7.0の2つ目の重要な機構はフルクラスタリスタートのような後に、
どの順番でインデックスをリカバリするかという優先度をつけることができるという機能です。</p>
<p>電源故障による、ロギング用のクラスタのダウンを想像してください。
クラスタが普及した場合、500個のインデックスをリカバリするような場合、499個のインデックスのデータは古く、
500番目のインデックスが重要です。
もっとも最近作成されたインデックスがリカバリされるまで、インデキシングを待つというようなことはできません。</p>
<p>これまでは、インデックスはランダムな順序でリカバリされ、重要なインデックスがリカバリされるまで待つしかありませんでした。
1.7.0では、インデックスは優先度の順番でリカバリされます。
この優先度は次のプロパティで指定できます。</p>
<ul>
<li><code>index.priority</code>設定（大きな値が優先度が高い）</li>
<li>インデックス作成日（新しいものが優先度が高い）</li>
<li>インデックス名</li>
</ul>
<p>既存のクラスタについて特に変更せずとも、最も最近作成されたインデックスが古いものよりも復旧されます。
古いインデックスの優先度を上げるためには、<code>index.priority</code>設定に0よりも大きな値を設定します。</p>
<pre><code>PUT important_index/_settings
{
  &quot;index.priority&quot;: 5
}
</code></pre><p>この設定は、存在するインデックスに対して更新できます。リカバリ中にもです。</p>
<h2 id="まとめ">まとめ</h2>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-7-0">Elasticsearch 1.7.0</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>大阪と京都でElasticsearch勉強会を開催しました。 #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/07/16/kansai-1st-elasticsearch-jp/</link>
      <pubDate>Thu, 16 Jul 2015 10:40:57 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/16/kansai-1st-elasticsearch-jp/</guid>
      <description>東京以外での勉強会の第2弾として、関西で勉強会を開催してきました。 Elasticsearch勉強会 in 大阪 Elasticsearch勉強会 in 京</description>
      <content:encoded><p>東京以外での勉強会の第2弾として、関西で勉強会を開催してきました。</p>
<ul>
<li><a href="https://elasticsearch.doorkeeper.jp/events/27555">Elasticsearch勉強会 in 大阪</a></li>
<li><a href="https://elasticsearch.doorkeeper.jp/events/27553">Elasticsearch勉強会 in 京都</a></li>
</ul>
<p>会場提供をしていただいた、Yahoo!大阪、はてなのみなさん、ご協力ありがとうございました！</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150715/osaka.jpg" />
    </div>
    <a href="/images/entries/20150715/osaka.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<!-- more -->


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150715/kamogawa.jpg" />
    </div>
    <a href="/images/entries/20150715/kamogawa.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>ここからはいつものメモです。
ちなみに、大阪の勉強会に、<a href="https://twitter.com/takuya_a">@takuya_a</a>さんと<a href="https://twitter.com/5kozawa">@5kozawa</a>さんの両名にお越しいただき話をしていただきました。
なので、勉強会の内容はほぼ同一になります。</p>
<h2 id="introduction-elastic-johtani">Introduction Elastic @johtani</h2>
<p>スライド：<a href="https://speakerdeck.com/johtani/introduction-elastic-1">Introduction Elasticsearch</a></p>
<p>初めての関西での勉強会ということで、ElasticsearchのOSSおよび商用プラグインの紹介をしてきました。
もちろん、Kibanaのデモもちょっとだけ。スプラトゥーンに関するデータをKibanaでちょっとだけ。
突貫でデータをかき集めたのでもう少し改良しないとですが。</p>
<h2 id="elasticsearch-での類似文書検索と-more-like-this-api-詳解--株式会社はてなidtakuya-a">Elasticsearch での類似文書検索と More Like This API 詳解 / 株式会社はてな　id:takuya-a</h2>
<p>スライド：<a href="https://speakerdeck.com/takuyaa/elasticsearch-defalselei-si-wen-shu-jian-suo-to-more-like-this-query-api-xiang-jie">Elasticsearch での類似文書検索と More Like This Query API 詳解</a></p>
<p>Elasticsearchの<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-mlt-query.html">More Like This</a>のソースコードリーディングみたいな感じで、
内部でどうやって処理されているかの説明を詳しくしてもらいました。</p>
<p>前のはてなエンジニアセミナーで話をされていた検索精度の件に絡んだ内容になっているかと。
（大阪で発表してもらった時より京都での発表が分かりやすくなってました。1日で改善されたのすごい！）
MoreLikeThisだとチューニングつらいので、自分で作るために<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html">TermVectorAPI</a>でやってみたという流れかと。</p>
<p>以下は発表後に出てきた質問のいくつかです。</p>
<p>Q:MoreLikeThisに対してTermVectorで柔軟にできる？
A:TermVectorのAPIで統計情報が取れるので、それを使うことでさらなるデータの更新ができる。</p>
<p>Q:TFとかの統計情報が必要なら、すべてインデックスをしたあとじゃないとちゃんとした値はとれないのでは？
A:TermVectorで取得したものをどうやって使うか</p>
<p>Q:TermVectorAPi&hellip;聞こえなかった
A:。。。</p>
<h2 id="elasticsearchを用いたはてなブックマークのトピック生成--株式会社はてなidskozawa">Elasticsearchを用いたはてなブックマークのトピック生成 / 株式会社はてな　id:skozawa</h2>
<p>スライド：<a href="http://www.slideshare.net/shunsukekozawa5/elasticsearch-in">Elasticsearchを用いたはてなブックマークのトピック生成</a></p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">Significant Terms Aggregation</a>を活用してる話。
トピックページの生成のために、Significant Terms Aggregationをどうやって利用しているかなどのお話でした。</p>
<p>トピックの集合の重複だったり、精度の判定方法とかいろいろ詳しく説明していただきました。</p>
<p>Q:2011年と12年で11年の方が多いのは？
A:ブックマークの件数に比例</p>
<p>Q:Significant terms aggsのsizeはいくつをつかってますか？
A:20を指定してます。</p>
<p>Q:Yahooとかニュースをストップワードとしてますが、Yahoo自体のニュースに関してはどーしてるんですか？
A:本文とタイトルから別々に作っていて、タイトルからは弾かれますが、本文から作った時に出てきます。</p>
<h2 id="はてなブックマークにおける-elasticsearch-の運用まわりの話--株式会社はてなidhagihala">はてなブックマークにおける Elasticsearch の運用まわりの話 / 株式会社はてな　id:hagihala</h2>
<p>スライド：未定（おそらく公開される）</p>
<p>体調が回復しきっていない中の発表ありがとうございました。
大幅に修正された資料が出てくるかなと。（ツイートできない数値がちらほらあったので）</p>
<p>Elasticsearchのクラスタの構成、どういった点で困ってたのでどういう調べ方をしたのか、どういった対処をしたのか。
どのあたりが次の課題かなどの話もありました。</p>
<h2 id="感想反省点など">感想・反省点など</h2>
<p>大阪、京都ともに30名弱の方の参加をしていただきました。ありがとうございました。
反省点としては、ハッシュタグを告知し忘れてました。。。</p>
<p>勉強会はやはり、東京が異常に活発で、大阪や京都はまだそれほどでもないのかなぁとも。
大阪はエンジニアの人や会社も多い気がするんですが。私の告知の仕方もあるかもなぁと。
次回があれば、大阪での事例も聞きたいので、スピーカーをもっと探さないとなと。</p>
<h2 id="関連ブログなど">関連ブログなど</h2>
<p>見つけたら、リンク追加していきます。</p>
<ul>
<li><a href="http://skozawa.hatenablog.com/entry/2015/07/17/194709">Elasticsearch勉強会 in 大阪/京都で発表しました</a>
* <a href="http://stop-the-world.hatenablog.com/entry/2015/07/22/014047">「Elasticsearch での類似文書検索と More Like This Query API 詳解」というタイトルで発表しました</a></li>
</ul>
<h2 id="その他余談">その他（余談）</h2>
<p>大阪のYahoo!さんは立地条件（梅田のすぐそば）がよく、</p>


<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150715/yahoo_osaka.jpg" />
    </div>
    <a href="/images/entries/20150715/yahoo_osaka.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>夜景も綺麗でした。大阪城とかも見えてました。（夜景じゃないけど。。。）</p>


<div class="box" style="max-width:600">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150715/view_from_yahoo.jpg" />
    </div>
    <a href="/images/entries/20150715/view_from_yahoo.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>京都は祇園祭の真っ最中。</p>


<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150715/naginata2.jpg" />
    </div>
    <a href="/images/entries/20150715/naginata2.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>水曜日はお休みをいただいて、観光してました。ちょっと日焼けが。。。
おかげで、リフレッシュできました。三十三間堂とか良かった:)</p>
<p>あまり、関西に縁がない（大阪15年ぶり、京都10年ぶり）ので、
もっとユーザが増えて勉強会の機運が高まると嬉しいなと。:)</p>
</content:encoded>
    </item>
    
    <item>
      <title>Mappingのすばらしいリファクタリング（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/07/08/great-mapping-refactoring-ja/</link>
      <pubDate>Wed, 08 Jul 2015 18:11:31 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/08/great-mapping-refactoring-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：The Great Mapping Refactoring Elasticsearchのユーザの悩みの最も大きなものの一つは、 タイプと</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/great-mapping-refactoring">The Great Mapping Refactoring</a></p>
<!-- more -->
<p>Elasticsearchのユーザの悩みの最も大きなものの一つは、
タイプとフィールドのマッピングに関する多義性です。
この多義性は、インデックス時の例外やクエリ時の例外、
正しくない結果、リクエストからリクエストへ変化する結果、
また、インデックスの故障やデータのロスを結果として引き起こします。</p>
<p>Elasticsearchをより強固で予測可能な振る舞いをするようにする作業において、
フィールドやタイプのマッピングをより厳格でより信頼性を高くするかといったことに
多くの変更を費やしました。
多くのケースで、Elasticsearch v2.0で新しいインデックスを作るときにのみ、
新しいルールを強制し、これまでのインデックスに関しては後方互換性を保つようにします。</p>
<p>しかし、幾つかのケースでは、先ほど説明したようなフィールドマッピングの
コンフリクトなどが存在するため、それらを利用できないです。</p>
<blockquote>
<p>コンフリクトしたフィールドのマッピングをもつインデックスはElasticsearch v2.0にはアップグレードできません。</p>
</blockquote>
<p>もし、これらのインデックスのデータが必要ない場合は、インデックスを消せばいいです。
そうでない場合はマッピングを正しくして再度インデックスする必要があるでしょう。</p>
<p>マッピングを正しく変更することは、私たちが簡単に決めることではありません。
ここからは、現在ある問題点と、私たちがどのように実装して解決したかについて説明します。</p>
<ul>
<li><a href="#conflicting-field-mappings">フィールドマッピングのコンフリクト</a></li>
<li><a href="#ambiguous-field-lookup">あいまいなフィールドのルックアップ</a></li>
<li><a href="#type-meta-fields">タイプのメタフィールド</a></li>
<li><a href="#analyzer-settings">アナライザ設定</a></li>
<li><a href="#index_name-and-path"><code>index_name</code>と<code>path</code></a></li>
<li><a href="#mapping-update">同期的なマッピングの更新</a></li>
<li><a href="#delete-mapping">マッピングの削除</a></li>
<li><a href="#prepare-2_0">2.0のための準備</a></li>
</ul>
<h2 id="a-nameconflicting-field-mappingsフィールドマッピングのコンフリクトa"><a name="conflicting-field-mappings">フィールドマッピングのコンフリクト</a></h2>
<p>これまで、わたしたちはドキュメントのタイプは「データベースのテーブルのようなもの」と説明していました。
タイプの目的を説明する簡単な方法だったからです。
しかし、残念なことにこれは、真実ではありません。
「同じ」インデックスの「異なるタイプ」にある同じ名前のフィールドは、
内部的に、Luceneのフィールド名が同じものになります。</p>
<p>もし<code>error</code>フィールドとして、ドキュメントタイプが<code>apache</code>のものには数値（integer）を、
ドキュメントタイプが<code>nginx</code>のものには文字列（string）を割り当てた場合、
Elasticsearchは同じLuceneのフィールドに数値と文字列のデータをもつことになります。
このフィールドに対して、検索やaggregationを行う場合、おかしな結果を受け取るか、例外が帰ってくるか、
インデックスが破損することになります。</p>
<p>この問題を解決するために、まず、ドキュメントタイプの名前をフィールドの名前の前に追加することを考えました。
各フィールドは完全に別のものとなります。
このアプローチの利点はドキュメントタイプが実際のテーブルのようになることです。</p>
<p>しかし、この方法には多くの欠点があります。</p>
<ul>
<li>フィールドは常に、他のタイプとは異なるものであると区別するためもしくは、複数のタイプに同じフィールドのクエリのためにワイルドカードをつけた場合、
ドキュメントタイプを前につける必要があります。</li>
<li>複数のドキュメントタイプに対して同じフィールド名で検索する場合、クエリを個別に発行しなければならなく遅くなります。</li>
<li>多くの検索で、既存の多くのクエリを壊してしまうために、単純な<code>match</code>や<code>term</code>クエリの代わりに、multi-fieldクエリを使う必要があります。</li>
<li>圧縮の効率の悪さから、ヒープ利用量、ディスク使用量、I/Oなどが、増加します。</li>
<li>複数のドキュメントタイプに対するaggregationは、global ordinalの利点を利用できなくなるために、遅くなり、メモリの使用量も増えます。</li>
</ul>
<h3 id="解決方法">解決方法</h3>
<p>最終的に、同じインデックスの同じ名前を持つ全てのフィールドは、同じマッピングを持つ必要があるというルールを採用することに決めました。
ただ、<code>copy_to</code>や<code>enabled</code>のようなパラメータはタイプごとに指定することができるようになっています。
これにより、データの破損、クエリ時の例外そして、おかしな結果が発生する問題を防ぎます。
クエリとaggregationは現在でも高速なままで、圧縮率を最大化し、ヒープ使用量やディスク使用率の低減させます。</p>
<p>この解決方法の欠点は、個別のテーブルとしてタイプを扱いたいユーザが彼らの考え方を変える必要があるということです。
これは、思ったよりも問題ではありません。
実際には、多くのフィールド名はデータの明確なタイプを表現しています。
<code>created_date</code>は常に、日付ですし、<code>number_of_hits</code>フィールドはいつも数値です。
フィールドマッピングがコンフリクトしているユーザはデータを失ったり、おかしなデータを受け取ったり、データを欠損させています。
ベストプラクティスにユーザが従っているかどうかによらず、インデックス時に正しい振る舞いを強制することが現在の違いです。</p>
<p>ユーザの多くがコンフリクトしていないフィールドマッピングをもっていれば、
コンフリクトが起きた場合、技術がこれらのシチュエーションを扱うことが可能になると思いませんか？
そこにはいくつかの解決方法があります。</p>
<h4 id="タイプの代わりにインデックスを別々に">タイプの代わりにインデックスを別々に</h4>
<p>最も簡単な解決方法です。インデックスを別々のインデックスとし、実際のデータベーステーブルのようにします。
インデックスをまたいだ検索はタイプをまたいだ検索のように動作しますし、
ソートやaggregationも同じデータタイプへのクエリのように動作します。これまでと同じ制限です。</p>
<h4 id="コンフリクトしたフィールドの名前の変更">コンフリクトしたフィールドの名前の変更</h4>
<p>コンフリクトがごくわずかな場合、（Logstashやアプリケーションで使っているものも一緒に）よりわかりやすいフィールド名に変更することで解決できます。
例えば、2つの<code>error</code>フィールドがあった場合に、<code>error_code</code>と<code>error_message</code>に変更します。</p>
<h4 id="copy_toもしくはmulti-fieldsを利用"><code>copy_to</code>もしくはmulti-fieldsを利用</h4>
<p>異なるドキュメントタイプのフィールドは別々の<code>copy_to</code>を設定できます。
元の<code>error</code>フィールドは<code>index</code>の設定に<code>no</code>が設定してあり、全てのドキュメントタイプで無効化されていますが、
特定のタイプだけ、<code>error</code>フィールドの値を数値の<code>error_code</code>フィールドにコピーすることができます。</p>
<pre><code>PUT my_index/_mapping/mapping_one
{
  &quot;properties&quot;: {
    &quot;error&quot;: {
      &quot;type&quot;: &quot;string&quot;,
      &quot;index&quot;: &quot;no&quot;,
      &quot;copy_to&quot;: &quot;error_code&quot;
    },
    &quot;error_code&quot;: {
      &quot;type&quot;: &quot;integer&quot;
    }
  }
}
</code></pre><p>他のタイプでは文字列の<code>error_message</code>にコピーします。</p>
<pre><code>PUT my_index/_mapping/mapping_two
{
  &quot;properties&quot;: {
    &quot;error&quot;: {
      &quot;type&quot;: &quot;string&quot;,
      &quot;index&quot;: &quot;no&quot;,
      &quot;copy_to&quot;: &quot;error_message&quot;
    },
    &quot;error_message&quot;: {
      &quot;type&quot;: &quot;string&quot;
    }
  }
}
</code></pre><p>同様の解決方法として<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-core-types.html#multi-fields">multi-field</a>も使えます。</p>
<h4 id="各データタイプに対してネストしたフィールドに">各データタイプに対してネストしたフィールドに</h4>
<p>ときどき、Elasticsearchに送ったドキュメントやドキュメントがもっているフィールドを制御できない場合があります。
部分的なコンフリクトに加え、闇雲に、ユーザが送ってきたフィールドを受け入れると、マッピングが肥大化します。
タイムスタンプやIPアドレスをフィールド名に使うようなドキュメントがあると考えてください。</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-nested-type.html"><code>nested</code> フィールド</a>にすることで、<code>str_val</code>、<code>int_val</code>、<code>date_val</code>というような各データタイプを利用できます。</p>
<p>このアプローチによって、次のドキュメントは</p>
<pre><code>{
  &quot;message&quot;: &quot;some string&quot;,
  &quot;count&quot;:   1,
  &quot;date&quot;:    &quot;2015-06-01&quot;
}
</code></pre><p>アプリケーションによって、次のようにフォーマットしなおす必要があります。</p>
<pre><code>{
  &quot;data&quot;: [
    {&quot;key&quot;: &quot;message&quot;, &quot;str_val&quot;:  &quot;some_string&quot; },
    {&quot;key&quot;: &quot;count&quot;,   &quot;int_val&quot;:  1             },
    {&quot;key&quot;: &quot;date&quot;,    &quot;date_val&quot;: &quot;2015-06-01&quot;  }
  ]
}
</code></pre><p>この解決方法は、アプリケーションサイドでより多くの作業が必要ですが、コンフリクトの問題とマッピングの肥大化の問題を同時に解決します。</p>
<h2 id="a-nameambiguous-field-lookupあいまいなフィールドのルックアップa"><a name="#ambiguous-field-lookup">あいまいなフィールドのルックアップ</a></h2>
<p>現在、フィールドの指定には&quot;short name&rdquo;、フルパス、ドキュメントタイプを前につけたフルパスが利用できます。
これらのオプションがあいまいさをもたらしています。
サンプルとして次のマッピングをご覧ください。</p>
<pre><code>{
  &quot;mappings&quot;: {
    &quot;user&quot;: {
      &quot;properties&quot;: {
        &quot;title&quot;: {
          &quot;type&quot;: &quot;string&quot;
        }
      }
    },
    &quot;blog&quot;: {
      &quot;properties&quot;: {
        &quot;title&quot;: {
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;user&quot;: {
          &quot;type&quot;: &quot;object&quot;,
          &quot;fields&quot;: {
            &quot;title&quot;: {
              &quot;type&quot;: &quot;string&quot;
            }
          }
        }
      }
    }
  }
}
</code></pre><ul>
<li><code>title</code>は<code>user.title</code>、<code>blog.title</code>、<code>blog.user.title</code>のどれでしょう？</li>
<li><code>user.title</code>は<code>user.title</code>または<code>blog.user.title</code>のどちらでしょう？</li>
</ul>
<p>答えは「場合によります。」です。Elasticsearchが最初に見つけたものになります。
フィールドはリクエストごとに変わるため、各ノードでマッピングがどのようにシリアライズされたかに依存します。</p>
<p>2.0では、フィールドを指定する時に、ドキュメントタイプを除いたフルパス名を使用するべきでしょう。</p>
<ul>
<li><code>user.title</code>は、<code>blog</code>タイプの<code>user.title</code>を意味します。</li>
<li><code>title</code>は、<code>user</code>と<code>blog</code>タイプの<code>title</code>フィールドを意味します。</li>
<li><code>*title</code>は<code>user.title</code>と<code>title</code>フィールドの両方にマッチします。</li>
</ul>
<p><code>user</code>タイプの<code>title</code>フィールドと<code>blog</code>タイプの<code>title</code>の違いはどのように指定するのでしょう？</p>
<p>指定できません。<a href="#conflicting-field-mappings">フィールドマッピングのコンフリクト</a>で説明した変更により、
<code>title</code>フィールドは両方のタイプで同じフィールドになります。
本質的に<code>title</code>と呼ばれる1つのフィールドになります。</p>
<p><code>user.</code>や<code>blog.</code>のようなタイプのプレフィックスはタイプを指定することによるフィルタリングで効果があります。
クエリの<code>blog.title</code>フィールドは<code>blog</code>タイプのドキュメントだけを検索し、<code>user</code>タイプのドキュメントを検索しません。
このシンタックスは誤解を招きやすいです。なぜなら、いつでも動作するわけではないからです。
aggregationやsuggestionはすべてのタイプに関する結果を含みます。
この利用のため、上記の例のあいまいさがあるので、タイプのプレフィックスはサポートしません。</p>
<p><strong>重要</strong> short nameやタイププレフィックスを利用したpercolatorは更新する必要があります。</p>
<h2 id="a-nametype-meta-fieldsタイプのメタフィールドa"><a name="type-meta-fields">タイプのメタフィールド</a></h2>
<p>すべてのタイプはメタフィールドを持っています。<code>_id</code>、<code>_index</code>、<code>_routing</code>、<code>_parent</code>、<code>_timestamp</code>などです。
これらのほとんどは<code>index</code>、<code>store</code>、<code>path</code>のような幾つかの設定をサポートしています。
これらの設定について次のようにシンプルにしました。</p>
<ul>
<li><code>_id</code>と<code>_type</code>は変更不可</li>
<li><code>_index</code>は、ドキュメントのもつインデックスを保存するために<code>enabled</code></li>
<li><code>_routing</code>は<code>required</code>のみを指定</li>
<li><code>_size</code>は<code>enabled</code>のみ</li>
<li><code>_timestamp</code>はデフォルトで保存される</li>
<li><code>_boost</code>と<code>_analyzer</code>は廃止。古いインデックスのものは無視される</li>
</ul>
<p>ドキュメントのフィールドから<code>_id</code>と<code>_routing</code>と<code>_timestamp</code>の値を抽出することができました。
この機能は廃止されます。これは、ドキュメントのパースとコンフリクトを起こすためです。
代わりに、これらの値はURLもしくはquery stringで指定可能です。</p>
<p><code>_boost</code>と<code>_analyzer</code>フィールドは例外で、すでにあるメタフィールドの設定は古いインデックスのものが採用されます。</p>
<h2 id="a-nameanalyzer-settingsアナライザ設定a"><a name="analyzer-settings">アナライザ設定</a></h2>
<p>これまで、indexとsearchのアナライザがインデックス、タイプ、フィールド、ドキュメント（<code>_analyzer</code>フィールドで）の
それぞれのレベルで指定可能でした。
同じフィールドに対して異なるanalysis chainの組み合わせができることにより、おかしな関連度を引き起こしていました。
フィールドマッピングのコンフリクトを解消することに加え、アナライザの設定も簡略化します。</p>
<ul>
<li>Analyzedな文字列フィールドは、<code>analyzer</code>設定と<code>search_analyzer</code>設定（<code>analyzer</code>設定の値をデフォルトとする）を指定できます。<code>index_analyzer</code>設定は<code>analyzer</code>となります。</li>
<li>複数のタイプで同じ名前のフィールドがある場合、フィールドはすべて、同じアナライザの設定を持たなければなりません。</li>
<li>タイプレベルのデフォルト設定の<code>analyzer</code>、<code>index_analyzer</code>、<code>search_analyzer</code>設定は廃止されます。</li>
<li>デフォルトアナライザはインデックスごとにインデックスの<code>analysis</code>設定で設定します。これらは<code>default</code>もしくは<code>default_search</code>という名前で設定します。</li>
<li>ドキュメントごとの<code>_analyzer</code>フィールドはサポートしません。既存のインデックスのものは無視されます。</li>
</ul>
<h2 id="a-nameindex_name-and-pathindex_nameとpatha"><a name="index_name-and-path"><code>index_name</code>と<code>path</code></a></h2>
<p><code>index_name</code>と<code>path</code>設定は（Elasticsearch v1.0.0から利用できる）<code>copy_to</code>によって置き換わりました。
既存のインデックスについてはこれらは機能しますが、新しいインデックスでは指定できません。</p>
<h2 id="a-namemapping-update同期的なマッピングの更新a"><a name="mapping-update">同期的なマッピングの更新</a></h2>
<p>現在、これまで存在していないフィールドを含むドキュメントをインデキシングするとき、
フィールドはローカルのマッピングに追加され、それから、マスターに変更（新しいマッピングをすべてのシャードに適用する更新）が送信されていました。
同時に2つのシャードに同じフィールドを追加することができます。
また、そのとき、異なる2つのマッピングがある可能性があります。
1つは<code>double</code>でもう1つは<code>long</code>だったり、<code>string</code>と<code>date</code>だったりと。</p>
<p>このような場合、マスターに最初に届いたマッピングが採用されます。
しかし、「負けた」マッピングをもつシャードでは、すでに異なるデータのタイプを利用しているため、
これを利用し続けます。
そのご、ノードをリスタートしたときに、シャードが別のノードに移動し、マスターにあるマッピングを適用します。
このとき、インデックスが破損したりデータを失ったりします。</p>
<p>これを防ぐために、シャードはインデキシングを続ける前に、新しいマッピングがマスターによって採用されるかどうかを待つようになりました。
これはすべてのマッピングが安全に更新されます。
新しいフィールドをもっているドキュメントをインデキシングすると、前よりも処理が遅くなるでしょう。
受け入れられることを待つ必要があるためです。
しかし、クラスタの状態の更新処理のスピードが次の2つの新しい機能によって大きく改善されています。</p>
<ul>
<li><strong>クラスタ状態の差分</strong>：可能であれば、クラスタの状態の変更はクラスタ状態全体の変更ではなく、部分的なものとする。</li>
<li><strong>シャードへのリクエストの非同期化</strong>：シャードアロケーション処理中に、マスタノードは、
割り当てられていないシャードのコピーの日付が最新のものを持っているかを見つけるために、リクエストをデータノードに対して送信します。
ここで、クラスタ状態を変更する呼び出しがブロッキングで行われていました。v1.6.0から、このリクエストはバックグラウンドで非同期で実行されます。
これにより、マッピング更新のようなペンディングタスクをより早く処理できるようになります。</li>
</ul>
<h2 id="a-namedelete-mappingマッピングの削除a"><a name="delete-mapping">マッピングの削除</a></h2>
<p>（そのタイプのドキュメントがある場合）タイプマッピングを削除できないようにします。
マッピングを削除した後に、削除されたフィールドの情報は、Luceneレベルでは存在し続け、
もし、後から同じ名前のフィールドが追加されたときにインデックスの破損を引き起こします。
そのようなマッピングは残しておくか、新しいインデックスに再インデックスすることができます。</p>
<h2 id="a-nameprepare-2_020のための準備a"><a name="prepare-2_0">2.0のための準備</a></h2>
<p>マッピングがコンフリクトしているかどうかを決めることは、手動で行うには慎重に行う必要があります。
私たちは、<a href="https://github.com/elastic/elasticsearch-migration">Elasticsearch Migration Plugin</a>を提供します。
これは、2.0で非推奨になったり廃止された機能を利用しているかどうかを見つけるために役に立つでしょう。</p>
<p>もし、コンフリクトしたマッピングを持っている場合、
正しいマッピングを持つ新しいインデックスにデータを再インデックスするか、
必要ないなら削除します。
これらのコンフリクトを解決しない限り2.0にはアップグレードできないでしょう。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 2.0.0.beta1リリース間近（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/07/07/elasticsearch-2-dot-0-0-dot-beta1-coming-soon-ja/</link>
      <pubDate>Tue, 07 Jul 2015 15:25:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/07/elasticsearch-2-dot-0-0-dot-beta1-coming-soon-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 2.0.0.beta1 coming soon! Elasticsearch 2.0.0.beta1のリリースの準備をしています。</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-2.0.0.beta1-coming-soon">Elasticsearch 2.0.0.beta1 coming soon!</a></p>
<p>Elasticsearch 2.0.0.beta1のリリースの準備をしています。
これは、Lucene 5.2.1に含まれる多くの改善が利用できるようになります。
このリリースに関するいくつかの機能は次のようなものです。</p>
<!-- more -->
<h4 id="pipeline-aggregations">Pipeline Aggregations</h4>
<p>差分や移動平均、他のAggregationsの結果に対する
series arithmeticのようなaggregationが利用可能になります。
この機能は、これまでは、クライアントサイドで実行する必要がありました。
しかし、この計算をより強力な解析クエリを構築してElasticsearchで
実行することができるようになります。
クライアントのコードをより簡潔にすることができます。
これにより、予測解析や異常検知のようなことができるようになります。</p>
<h4 id="queryfilter-merging">Query/Filter merging</h4>
<p>Filterはなくなります。全てのフィルタは、クエリになります。
クエリコンテキストで利用されると、効率的に関連度スコアを計算し、
フィルタコンテキストで利用されると、単に、
マッチしていないドキュメントを除外する（今のフィルタのようなもの）だけです
この変更は、クエリ実行が自動的に、より効率的な順番で実行されるように
最適化されることを意味します。
例えば、フレーズやgeoクエリのような遅いクエリは
まず、近似フェーズを実行し、それから、より遅い実際のフェーズが
結果に対して行われます。
フィルタコンテキストにおいて、頻繁に利用される条件は自動的にキャッシュされます。</p>
<h4 id="configurable-store-compression">Configurable store compression</h4>
<p><code>index.codec</code>設定により、高速化のためのLZ4圧縮（<code>default</code>）か
インデックスサイズを小さくするためのDEFLATE（<code>best_compression</code>）を
選択できます。これは、ロギングでとくに役に立ちます。
これにより、古いインデックスオプティマイズする前に<code>best_compression</code>に
変更できます。</p>
<p>これらに関するブログ記事がすぐに公開されるでしょう。</p>
<h3 id="performance-and-resilience">Performance and resilience</h3>
<p>以降では、新しいメジャーリリースに関して簡単に紹介します。
2.0の変更の多くは内部の機能に関するものであり、
直接ユーザに関連するわけではないからです。</p>
<p>新しいメジャーバージョンのテーマは、パフォーマンス、安定性、
堅牢性、予測可能性、そして使い勝手の良さです。</p>
<ul>
<li>物事が予測した通りに動作する</li>
<li>何か問題があった場合に、Elasticsearchから役立つフィードバックがある</li>
<li>ローレベルの設定を扱う必要はなく、Elasticsearchが良い設定を決定する</li>
<li>これらに加え、データがより安全に</li>
</ul>
<p>これらの目標は完全ではありません。
まだ、多くの改善があります。しかし、2.xブランチで、
すでに500コミットを超える大きな改善が実施されています。</p>
<ul>
<li>on-diskの doc valuesをデフォルトで利用（これまではfielddata）。
ヒープ使用量を減らして、スケーラビリティを向上</li>
<li>セグメントマージ処理中のメモリ使用量の削減</li>
<li>normsの圧縮率の改善。ヒープスペースを利用している大きな処理のひとつだったため。</li>
<li>全てのリクエストの後に、transaction logをfsyncすることで、デフォルトで耐久性を向上</li>
<li>全てのファイル変更をアトミックに（部分的なファイルの書き出しはなし）</li>
<li>マージを自動で制限</li>
<li>フレーズクエリやスパンクエリを高速化</li>
<li>フィルタキャッシュをより効率化するための圧縮されたビットセット</li>
<li>クラスタ状態の差分更新</li>
<li>構造化されたJSON形式の例外</li>
<li>よりきめ細かいLuceneのメモリレポート</li>
<li>デフォルトではlocalhostにのみバインド。開発のノードが他のクラスタにジョインするのを防ぐ</li>
<li>parent/childのクエリ実行最適化のためにリライト</li>
<li>Java Security Managerで必要最小限なパーミッションで実行</li>
<li>全てのコアなプラグインをelasticsearchリポジトリに移行し、Elasticsearchのバージョンに同期してリリースされる予定</li>
</ul>
<h2 id="アップグレード前に">アップグレード前に</h2>
<p>メジャーバージョンのアップグレードは問題のあるものを一掃する機会を与えてくれます。
できる限り、これらの変更をアップグレードするために、簡単な方法を提供しようとしています。
しかし、Elasticsearch 2.0にアップグレードする前に、必要な処理が2つあります。</p>
<p>1つ目は、フィールドとタイプマッピングに関することです。
mapping APIは、現在、それほど厳密ではありません。
内蔵された保護機構を提供する代わりに、ユーザがベストプラクティスを知っていると信頼していました。
2.0では、mappingはより厳密で安全ですが、いくつかの変更では、後方互換性を保っていません。
詳細については<a href="https://www.elastic.co/blog/great-mapping-refactoring">The Great Mapping Recatoring</a>をごらんください。</p>
<p>2つ目はElasticsearch 0.20以前のユーザに関する変更です。
これは、Lucene 3.xを使っています。
Elasticsearch 2.xはLucene 5をベースにしています。
Lucene 5はLucene 4.xによって作成されたインデックスの読み込みはサポートしていますが、
Lucene 3.xに関してはサポートしていません。</p>
<p>Elasticsearch 0.20以前のバージョンによって生成されたインデックスを持っている場合、
Elasticsearch 2.xのクラスタをスタートすることはできません。
これらの古いインデックスを削除するか、Elaticsearch 1.6.0以上に含まれている
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-upgrade.html">upgrade API</a>を使用してアップグレードする必要があります。</p>
<p>upgrade APIの実行は2つのジョブを実行します。</p>
<ul>
<li>古いLuceneフォーマットのセグメントを最新のフォーマットで書き換えます</li>
<li>Elasticsearch 2.xによって読み込めるようという印をインデックスに追加します</li>
</ul>
<p>全てのセグメントを最新バージョンにアップグレードするのも良い案ですが、
アップグレード前に必要な処理を最小限に抑えることも可能です。
（Lucene 3.xのセグメントだけをアップグレード）
その場合は、<code>only_ancient_segments</code>パラメータを指定します。</p>
<h2 id="elasticsearch-migration-plugin">Elasticsearch Migration Plugin</h2>
<p>Elasticsearch 2.0 に移行する前に、インデックスがアップグレードが必要なのか、
ほかになにかするべきことがあるのかをチェックする助けになる
Elasticsearch Migration Pluginをリリースしました。</p>
<p>まず、プラグインをインストールします</p>
<pre><code>./bin/plugin -i elastic/elasticsearch-migration
</code></pre><p>プラグインのインストール後はノードのリスタートは必要ありません。</p>
<p>以下のリンクをブラウザで開きます。</p>
<p>http://localhost:9200/_plugin/migration</p>
<p>（<code>localhost:9200</code>はインストールしたホスト名に変更してください。）</p>
<p>Migration pluginに関してバグやご意見がある場合は、<a href="http://github.com/elastic/elasticsearch-migration/issues">GitHubのIssue</a>にお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>さらに進化したFound（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/07/07/we-just-made-found-more-awesome-ja/</link>
      <pubDate>Tue, 07 Jul 2015 15:20:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/07/we-just-made-found-more-awesome-ja/</guid>
      <description>4ヶ月前に、Found joined our team at Elasticをアナウンスしました。 Foundの素晴らしいチームと一緒に仕事をしていますが、彼らによって、より</description>
      <content:encoded><p>4ヶ月前に、<a href="https://www.elastic.co/blog/welcome-found">Found joined our team at Elastic</a>をアナウンスしました。
Foundの素晴らしいチームと一緒に仕事をしていますが、彼らによって、より素晴らしい
hosted Elasticsearchを提供することになりました。</p>
<!-- more -->
<p>私たちがともに密接に働くことにより、本日（2015/7/1）、
<a href="https://www.elastic.co/products/found">新しい2つのFound</a>を提供することになりました。
Found StandardはこれまでのFoundの機能に加え、さらに低価格を提供します。
Found Premiumは、SLAサポートと、ShieldやWatcherを将来Found上で提供します。</p>
<h2 id="found-standard">Found Standard</h2>
<p>Foundは素晴らしいです。専用のElasticsearchクラスタ、簡単なスケール、
ビルトインのセキュリティそして、時間単位での課金などを持っています。
私たちは、hosted Elasticsearchを探している方に、
Foundが適したソリューションであると思っていますし、
すべての方に利用できて手頃な価格であるということを確信したいと思っています。</p>
<p>本日（2015/07/01）からFoundの価格をかなり低価格にし、
<strong>月額50ドル以下</strong>でhosted Elasticsearchを簡単に試してもらえるようにしました。</p>
<p>価格を下げることは正しい重要なステップですが、
Foundを利用している全ての人に、より良い経験を持っていただきたいと考えています。
低価格化と一緒に、<strong>free backups</strong>と<strong>built in SSD</strong>もFoundで提供を始めることになります。</p>
<p>Foundの重要な特徴の一つが、高可用性のために、クラスタをいくつのデータセンターに持つかを
選択できることです。
データは重要です。これが正しい選択でユーザの助けになると考えています。
これにより、私たちの価格は、<strong>複数のデータセンターにより安価に</strong>配置することができます。</p>
<p>また、KibanaもElasticsearchのデータを可視化する素晴らしい方法だと考えています。
Kibana 4が最新バージョンですが、
これは、サーバサイドコンポーネントを持っています。
これは、サービスとしてこれを提供するために、追加の料金がかかることを意味します。
Foundチームが築いた素晴らしい基盤とKibanaチームの努力により、
hosted Elasticsearchクラスタで<strong>無料のKibana 4</strong>を7月15日より提供することになりました。</p>
<h2 id="found-premium">Found Premium</h2>
<p>また、私たちは、オープンソースプロダクトに関してサブスクリプションを提供していますが、
Found Standardに対しても提供することになりました。
これが、Found Premiumです。</p>
<p>フォーラムベースのサポートよりもSLAベースのサポートを望んでいる場合、
プロダクトを開発しているチームからのサポートを受けることができるオプションを
提供し始めました。
クリティカルなイベントを持っていたり、私たちのプロダクトに関する
問題を予測するためのベストなヘルプやガイダンス、アドバイスを探しているような場合にサポートします。</p>
<p>さらに近い将来、サブスクリプションの一部として、<a href="https://www.elastic.co/products/shield">Shield</a>（Elasticasearchのセキュリティプラグイン）や<a href="https://www.elastic.co/products/watcher">Watcher</a>（アラーティングプラグイン）が利用できるようになります。</p>
<p>私たちのチームがともに働き、多くのことを可能にし、すばらしい仕事をユーザに提供したかを
将来も楽しみです。
私は非常に誇りに思っていますし、気に入っていただけたらと思っています。
ぜひ、<a href="https://www.elastic.co/webinars/getting-started-with-found">7/15のWebnarに参加して</a>くわしい話を聞いていただき、疑問を解消してください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>JustTechTalk#02 形態素解析のあれやこれや@ジャストシステムに参加しました。</title>
      <link>https://blog.johtani.info/blog/2015/07/06/attend-justsystem-techtalk-no2/</link>
      <pubDate>Mon, 06 Jul 2015 10:49:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/07/06/attend-justsystem-techtalk-no2/</guid>
      <description>JustTechTalk#02 形態素解析のあれやこれや@ジャストシステム に参加してきました。 ジャストシステムさんの形態素解析器JMATの話とKagome、Janome、</description>
      <content:encoded><p><a href="https://justsystems.doorkeeper.jp/events/27174">JustTechTalk#02 形態素解析のあれやこれや@ジャストシステム</a></p>
<p>に参加してきました。
ジャストシステムさんの形態素解析器JMATの話とKagome、Janome、Kuromoji.js、ssslaの開発者の
パネルディスカッションでした。</p>
<p>ということで、いつものメモです。</p>
<!-- more -->
<h2 id="ジャストシステムの形態素解析その２機械学習編">ジャストシステムの形態素解析その２（機械学習編）</h2>
<ul>
<li>
<p>JMATの話</p>
<ul>
<li>前回は辞書の話</li>
<li>今回は学習の話</li>
</ul>
</li>
<li>
<p>教師あり/教師なし</p>
<ul>
<li>JMATは教師あり</li>
<li>教師なしは研究段階</li>
</ul>
</li>
<li>
<p>ラティス構造を辞書ベースで構築して、コストの総和が最小の経路を求める</p>
<ul>
<li>連接、単語生成とか。</li>
</ul>
</li>
<li>
<p>学習は3フェーズ</p>
<ul>
<li>ベース、能動、部分アノテーション</li>
<li>ベース
<ul>
<li>300万文のコーパスから1万文のみを利用（なぜ？今から説明）
<ul>
<li>64GBマシン買ってみたけど、複数実験するには追いつかない</li>
<li>オンライン学習がメジャーでない時代に作り始めたので、つかってない</li>
<li>CRF学習器を改善
<ul>
<li>結果として50万文くらいで精度が良くなる</li>
</ul>
</li>
</ul>
</li>
<li>辞書チームからNGがでて、方向転換</li>
</ul>
</li>
<li>方向転換した結果が3つのフェーズらしい</li>
<li>ピタジョブに採用？</li>
</ul>
</li>
</ul>
<h3 id="疑問">疑問</h3>
<p>* JMATって、Webの検索の前処理とか分類とかに主に利用するのかな？</p>
<ul>
<li>ATOKでもこのノウハウって利用してるんかな？</li>
<li>辞書もあるらしいけど、辞書更新されると学習器のデータとかどーなるんだろ？</li>
</ul>
<h2 id="形態素解析器の実装言語talkについて">形態素解析器の実装言語Talkについて</h2>
<ul>
<li>
<p>kuromoji.jsの@takuya_aさん</p>
<ul>
<li>Typed Arrayサポートが高速にできてる理由でもあるらしい</li>
</ul>
</li>
<li>
<p>Kagomeの@ikawahaさん</p>
<ul>
<li>Goはいろいろないらしい</li>
</ul>
</li>
<li>
<p>Janomeの@moco_betaさん</p>
</li>
<li>
<p>sssla（茶筌のRuby clone）</p>
</li>
<li>
<p>なんで作ったの？</p>
<ul>
<li>形態素解析の<strike>ライブラリ</strike>「解析部分」はNLPのHelloWorldだから</li>
</ul>
</li>
<li>
<p>なんで、その言語？</p>
<ul>
<li>Python 3系は文字列とバイト配列の扱いがすごく楽！</li>
</ul>
</li>
<li>
<p>その言語で困った点は？</p>
<ul>
<li>Goだと、辞書を内包するのが大変</li>
<li>JSは苦労したところしかない（1hくらいしゃべれるぞ！）。基本的なデータ構造とかもない</li>
<li>Pythonはパフォーマンスを考えないと</li>
<li>Ruby（1.6だったので）もパフォーマンスが</li>
</ul>
</li>
<li>
<p>その言語を開発するときに必須のものは？</p>
<ul>
<li>Goはとくにない。エディタはどれでもOK</li>
<li><a href="http://browserify.org/">browserify</a>が便利</li>
</ul>
</li>
</ul>
<p>* ほかの人たちの言語をdisってください
* JSは論外。Pythonのコードフォーマッターが揺れるのが。。。Rubyはバージョンが。。。
* Goはブラウザで動かない。Pythonもブラウザで動かない。Rubyも(ry
* ほかのは触ったことないので。。。
* Pythonは2.xか3.xか決めてくれ！</p>
<ul>
<li>なんで、Kuromojiベースなの？
<ul>
<li>Java読みやすいから。</li>
</ul>
</li>
<li>MeCabとKuromojiの違いは？
<ul>
<li>未知語の処理が結構違う</li>
</ul>
</li>
</ul>
<h2 id="感想">感想</h2>
<p>きれいなロビーで良かったのですが、マイクがあると嬉しかったかもしれません。
前回の辞書の話も聞いてみたかったかも。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Hatena Engineer Seminar #5 @ Tokyoに参加しました。 #hatenatech</title>
      <link>https://blog.johtani.info/blog/2015/06/17/attend-hatena-engineer-seminar-5/</link>
      <pubDate>Wed, 17 Jun 2015 00:21:33 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/06/17/attend-hatena-engineer-seminar-5/</guid>
      <description>ひさびさに、勉強会メモ。 Hatena Engineer Seminar #5 @ Tokyoに当選したので行ってきました。 いつもは近寄らないオシャレな街をドキドキしながら行ってきました。 と</description>
      <content:encoded><p>ひさびさに、勉強会メモ。
<a href="http://hatena.connpass.com/event/15973/">Hatena Engineer Seminar #5 @ Tokyo</a>に当選したので行ってきました。</p>
<p>いつもは近寄らないオシャレな街をドキドキしながら行ってきました。</p>
<!-- more -->
<p>ということで、簡単なメモです。</p>
<h2 id="はてなブックマーク全文検索の精度改善-idtakuya-a">はてなブックマーク全文検索の精度改善 id:takuya-a</h2>
<h3 id="問題検索精度がよくない">問題：検索精度がよくない</h3>
<ul>
<li>京都で検索 →　「ポーランドの京都」「京都大学のまるまる教授」のようなもんがヒット</li>
<li>京都っぽいエントリが出て欲しい。
<ul>
<li>京都っぽい？？？</li>
</ul>
</li>
<li>問題点をブレイクダウン</li>
</ul>
<h4 id="課題">課題</h4>
<ol>
<li>クエリ考えるの大変だよね</li>
<li>順序が新着順なのが辛い</li>
<li>適合率と再現率の両立</li>
</ol>
<p>そして（ドラムロール）、できました！（さすが）</p>
<p>アイデア：はてブのタグを利用する。
関連キーワードを抽出して、クエリ拡張する。</p>
<h3 id="関連キーワードとは">関連キーワードとは？</h3>
<ol>
<li>タグ検索する</li>
<li>検索にヒットしたTerm Vectorsを取得</li>
<li>特徴語をTop25件取得</li>
<li>もっともスコアが高いタームを特徴語とする
<ul>
<li>英語のストップワードとかが問題点となってたり。</li>
<li>→Dynamic stop word listというのを利用して排除（IDF、RIDF、Gain）</li>
</ul>
</li>
</ol>
<h3 id="今後の課題">今後の課題</h3>
<ul>
<li>再現率の向上</li>
<li>解析用のフィールド・辞書を追加（精度向上や解析ミスなど）</li>
</ul>
<h3 id="トークに出てきた機能など">トークに出てきた機能など</h3>
<p>トークに出てきたElasticsearchの機能については、こんなツイートをしてたので、参考にしてもらえれば。</p>
<blockquote class="twitter-tweet" lang="ja"><p lang="ja" dir="ltr">これのkuromoji_stemmerを使ってるっぽい？ <a href="https://twitter.com/hashtag/hatenatech?src=hash">#hatenatech</a> / elastic/elasticsearch-analysis-kuromoji - <a href="https://t.co/3F2sBYXLPH">https://t.co/3F2sBYXLPH</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/610759870564859904">2015, 6月 16</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" lang="ja"><p lang="ja" dir="ltr"><a href="https://twitter.com/hashtag/hatenatech?src=hash">#hatenatech</a> Term Vectors APIのドキュメントはこちら - <a href="https://t.co/HhBmTDr46i">https://t.co/HhBmTDr46i</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/610760840170450944">2015, 6月 16</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" lang="ja"><p lang="it" dir="ltr"><a href="https://twitter.com/hashtag/hatenatech?src=hash">#hatenatech</a> min_score - <a href="https://t.co/Sc0exzJRC1">https://t.co/Sc0exzJRC1</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/status/610763959944097793">2015, 6月 16</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<h4 id="個人的な疑問">個人的な疑問</h4>
<p>Q：クエリにヒットするタグがそもそもなかったら？</p>
<h2 id="はてなブックマークに基づく関連記事レコメンドエンジンの開発-idskozawa">はてなブックマークに基づく関連記事レコメンドエンジンの開発 id:skozawa</h2>
<h3 id="課題一部のエントリに対して関連記事が出ない">課題：一部のエントリに対して関連記事が出ない</h3>
<p>タグがない記事について関連エントリが出ない＝既存はタグを利用している
例：レシピで考える</p>
<h4 id="現行システム">現行システム</h4>
<ul>
<li>ユーザがつけたタグ情報を利用してMoreLikeThisで計算</li>
</ul>
<h4 id="新規システム">新規システム</h4>
<ol>
<li>類似記事検索</li>
<li>特徴語の抽出</li>
<li>特徴語を分類</li>
<li>関連記事検索</li>
<li>関連記事をスコアリング</li>
</ol>
<h4 id="個人的な疑問-1">個人的な疑問</h4>
<p>Q：毎回計算してるのかな？記事登録とかされたタイミングでやってるのかな？
Q：Termの精度などどうなんだろ？</p>
<h2 id="brandsafe-はてなのアドベリフィケーションのしくみ-idtarao">『BrandSafe はてな』のアドベリフィケーションのしくみ id:tarao</h2>
<p>BrandSafeはてな：とか。
広告の配信先をフィルタリング</p>
<p>複数の素朴なフィルタの組み合わせ→AdaBoost</p>
<h4 id="個人的な疑問-2">個人的な疑問</h4>
<p>Q：海外とかもいけるのかな？</p>
<h2 id="まとめと感想">まとめと感想</h2>
<p>ということで、簡単なメモでした。ピザごちそうさまでした！
聞いてて少し思ったのは、データ量があるサイトだからうまくいく手法だというのもあるんだろうなというところでした。
あとは、クエリを暗に改善するのとは別に、サジェスト的に表示するのにも使えたりするかも？と思ってみたり。
できるかどうかはわからないですが。。。</p>
<p>Elasticsearchをいろいろと活用してもらってるのがわかって、楽しい勉強会でした。
もっともっといろんなところで宣伝してくださいw</p>
<p>今日の勉強会を聞いて、俄然、京都・大阪でElasticsearch勉強会を開催したい気になってきました。
特に大阪に知り合いがいないので、だれか紹介してもらえると嬉しいです。
お待ちしてます。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.6.0リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/06/10/elasticsearch-1-6-0-released-ja/</link>
      <pubDate>Wed, 10 Jun 2015 13:31:02 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/06/10/elasticsearch-1-6-0-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 1.6.0 released 本日（6/9）、Lucene 4.10.4ベースのElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-1-6-0-released">Elasticsearch 1.6.0 released</a></p>
<p>本日（6/9）、<strong>Lucene 4.10.4</strong>ベースの<strong>Elasticsearch 1.6.0</strong>をリリースしました。
このリリースはElasticsearchの最新の安定バージョンとなります。
また、素晴らしい新機能がいくつか追加されています。</p>
<!-- more -->
<ul>
<li>synced flushによるリスタートの高速化
* シャード配置は保留中のタスクをブロックしない</li>
<li>レスポンスボディのJSONのフィルタリング</li>
<li>共有ファイルシステムリポジトリに対するセキュリティフィックス</li>
<li>古いインデックスのためのUpgrade API</li>
<li>Kibanaユーザのためのハイライトの強化</li>
<li>Windowsユーザのための<code>mlockall</code></li>
<li>より詳細なスクリプト設定</li>
</ul>
<p><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-6-0">すべての変更リストとダウンロード</a>はこちらをごらんください。</p>
<h2 id="synced-flushによるリスタートの高速化">synced flushによるリスタートの高速化</h2>
<p>1.6.0より前のバージョンでは、メンテナンスやローリングアップグレード時の
ノードの再起動で、必要であるかどうかに関わらず、多くの場合、
ノードのすべてのシャードのすべてのデータを再度コピーする必要がありました。
この新しいsynced flush機能により、
sync-flushされたインデックスに対して、既存のデータを再利用し、
より早くクラスタを正常な状態にすることができるようにします。</p>
<p>ここで、この変更以前にどのように動いていたかを説明します。
すでにあるレプリカシャードは、ノードがリスタートした後に、
プライマリから復元するときに、
最初のステップはプライマリにあるセグメントとレプリカにあるセグメントを
比較することです。そして、セグメントに違いがあった場合にコピーされます。
問題は、セグメントプライマリのセグメントのマージと
レプリカのセグメントのマージが別々に起こっており、
各シャードのセグメントが完全に異なるが、
それらが同じデータを持っているという点です。</p>
<p>新しいsynced-flush機能では、<code>sync_id</code>がプライマリと
レプリカシャードに、シャードのコンテンツが同一であるという判別するために、
書き込まれます。これは、リカバリがセグメントの比較のステップを
スキップできることを意味します。
リカバリのスピードを高速にします。</p>
<p>synced flushはアイドル状態のインデックスで自動的に実行されます。
直前の5分間でデータが登録、更新削除されていないインデックスに対してです。
これは、ロギングのユースケースで特に役に立ちます。
機能のインデックスはインデキシングがストップしたあとの5分で自動的に
syncされるでしょう。</p>
<p>ノードのリスタートやクラスタのリスタートが必要で、
自動的に発生するsyncを待てない場合は次のようなことが可能です。</p>
<ul>
<li>インデキシングを停止（実行中のリクエストが停止するのも待つ）</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html">シャードのアロケーションを停止</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-synced-flush.html">synced-flush</a>リクエストの発行</li>
<li>ノードのリスタート</li>
<li>シャードのアロケーションの再開</li>
<li>クラスタの状態がグリーンになるまで待つ</li>
<li>インデキシングの再開</li>
</ul>
<p><strong>NOTE:</strong> &ldquo;シャードのアロケーションを停止&quot;のステップが必要です。
これがない場合、Elasticsearchはノードの再起動が始まると、
異なるノードにシャードの再配置を始めます。
これは、新しいノードにシャードデータの全てをコピーする必要があります。</p>
<p>ドキュメントのインデキシング、更新、削除のあとに最初のフラッシュが
発生したときに、
シャードの<code>sync_id</code>が自動的に無効化されます。
詳細については<a href="https://github.com/elastic/elasticsearch/issues/11336">#11336</a>と<a href="https://github.com/elastic/elasticsearch/issues/11179">#11179</a>をごらんください。</p>
<h2 id="シャード配置は保留中のタスクをブロックしない">シャード配置は保留中のタスクをブロックしない</h2>
<p>多数のノードやインデックスを持っているユーザは
クラスタ全体のリスタートのあとのシャードのリカバリで、
長い間、リカバリが止まって見えることに気づいたかもしれません。
これらのリカバリが止まって見える間は、クラスタ設定の更新のような軽微なアクションでさえ、
例外が発生したり、その設定が反映されるまでに長時間かかるといったことが起きていました。
この問題の兆候は保留中のタスクのキューが大きくなることです。</p>
<p>これらの遅延の原因はシャードの配置のプロセスにあります。
配置されるべきシャードのコピーを
持っているのがどのノードかを全てのデータノードに聞きます。
多くのシャードや遅いディスクを持ったデータノードは
反応するのに時間がかかります。
特に、シャードのリカバリがすでにI/Oを利用しているような時です。
このバージョン以前のものは、シャード情報のためのリクエストを
同期的に処理していました。
クラスタ状態の更新はアロケーションプロセスを続けるために
必要な情報を待っている間、ブロックされます。</p>
<p><a href="https://github.com/elastic/elasticsearch/issues/11262">#11262</a>での変更は
この情報のためのリクエストを非同期にします。
クラスタ状態の更新はこのタスクによってブロックされません。
これは、保留中のタスクがより早く処理でき、
クラスタが変更に対してより早く反応できます。
処理中のshard infoリクエストの数は
<code>number_of_in_flight_fetch</code>キーとしてcluster-health APIで取得できます。</p>
<p>さらに、シャードがある理由で復旧に失敗すると、
クラスタは、シャードのリカバリが成功するまで、同じノードに対して
シャードをアロケーションしないようにします。</p>
<h2 id="レスポンスボディのjsonのフィルタリング">レスポンスボディのJSONのフィルタリング</h2>
<p>Elasticsearchは全ての情報を返します。
例えば、検索リクエストは<code>_index</code>、<code>_type</code>、<code>_id</code>、
<code>_score</code>、<code>_source</code>を返します。
しかし、全ての情報が必要でない場合があります。
また、これらのデータを遅いネットワークで転送することは
遅延の原因となります。</p>
<p>ユーザはこの検索メタデータを無効にするための特殊な設定を
行ったり、他のAPIのレスポンスのフォーマットを
コントロールするための設定があります。
<a href="https://github.com/elastic/elasticsearch/issues/10980">#10980</a>の変更で、任意のレスポンスボディのJSONに対して、
必要な要素だけを取得する機能が追加されました。
<code>filter_path</code>パラメータを使用します。</p>
<p>例えば、検索リクエストからは<code>total</code>数と、各要素の<code>hits</code>の配列を欲しい場合、
次のように指定します。</p>
<pre><code>GET _search?filter_path=hits.total,hits.hits
</code></pre><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-info.html">nodes-info API</a>から各ノードの<code>http_address</code>だけを取得したい場合は、
ノード名の部分にワイルドカード(<code>*</code>)を使用します。</p>
<pre><code>GET _nodes?filter_path=nodes.*.http_address
</code></pre><p>単一の<code>*</code>がJSON階層の1つの階層に対しての
ワイルドカードとして機能します。
2つの<code>**</code>は複数階層に対してとなります。
複数のフィルタはカンマ区切りで指定可能です。
詳細について<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering">Response filtering</a>をごらんください。</p>
<h2 id="共有ファイルシステムリポジトリに対するセキュリティフィックス">共有ファイルシステムリポジトリに対するセキュリティフィックス</h2>
<p>本リリースはsnapshot-restoreで使われる
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository">共有ファイルシステムリポジトリ</a>に関するセキュリティ強化の変更が含まれます。
現在、Elasticsearchのユーザは、Elasticsearchプロセスによって書き込み可能
任意のディレクトリに<code>.snapshot</code>ファイルを書き込むことができます。
<a href="https://github.com/elastic/elasticsearch/issues/11284">#11284</a>の変更で、リポジトリのために使用できるディレクトリを
強制的に指定できるようになりました。
適切なディレクトリが<code>config/elasticsearch.yml</code>設定ファイルの
<code>path.repo</code>に指定される必要があります。</p>
<p>次のように設定されたElasticsearchインスタンスはこのセキュリティ問題に対して影響を受けにくいです。</p>
<ul>
<li><code>root</code>ではなく<code>elasticsearch</code>ユーザとしてElasticsearchを実行</li>
<li><code>elasticsearch</code>ユーザが<code>data</code>ディレクトリに対してのみ
書き込み権限を持っていて、共有ファイルシステムリポジトリに対しても利用できる</li>
<li>ファイアウォールやプロキシ、Shieldを使って、snapshot APIの実行を任意のユーザから実行されるのを防いでいる</li>
</ul>
<p>この問題を<a href="https://www.elastic.co/community/security">CVE-2015-4165</a>としています。</p>
<h2 id="古いインデックスのためのupgrade-api">古いインデックスのためのUpgrade API</h2>
<p>Elasticsearch 2.0以降では、
Lucene 5ベースとなり、Lucene 3
（Elasticsearchのバージョンでは0.90以前）
によって書き出されたセグメントを含んだインデックスを読み込むことが
できなくなります。
これらの「古いインデックス」はLucene 4にアップグレードする必要があり、
2.0-compatibleとして印をつける必要があります。
そうしなければ、Elasticsearch 2.0に以降できないでしょう。</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-upgrade.html">upgrade API</a>は
、最新のLuceneフォーマットにインデックスにある全てのセグメントを
アップグレードするためにすでに利用できます。
また、最新のフォーマットは性能向上やバグフィックスといった利点もあります。
さらに、2.0-compatibleとして古いインデックスをマークする設定も
書き込むことができます。
さらに、<code>upgrade_only_ancient_segments</code>オプションが
Lucene 3のセグメントだけをアップグレードするために利用でき、
移行前の必要な処理を減らすことができます。</p>
<h2 id="kibanaユーザのためのハイライトの強化">Kibanaユーザのためのハイライトの強化</h2>
<p>KibanaユーザはElasticsearchのハイライトについて2つの点で問題を見つけていました。</p>
<ul>
<li>ワイルドカードでフィールド名を指定した場合に、ハイライトに適さないフィールドも帰ってくる（日付や数値のフィールドなど）</li>
<li>古いインデックスが非常に大きなターム（&gt; 32kB）を含んでいて、ハイライトが失敗する。
最近のバージョンでは、これらの大きなタームはインデックス時に除去される</li>
</ul>
<p><a href="https://github.com/elastic/elasticsearch/issues/11364">#11364</a>の変更で
これらの問題が修正されました。
ワイルドカードを利用したフィールド名では、stringフィールドのみを返し、非常に長いタームによる例外は無視するようになります。</p>
<h2 id="windowsユーザのためのmlockall">Windowsユーザのための<code>mlockall</code></h2>
<p>速いGCはノードの安定性と性能について重要です。
小さなバイトのヒープでさえ、ディスクにスワップすることを許可してしまうと、GCに対して大きな影響が出てしまいます。
ですので、これらのコストは排除されるべきです。</p>
<p>Linuxユーザは<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory"><code>bootstrap.mloclall</code>設定</a>による恩恵を受けています。
これは、RAMにJVMのヒープを起動時にロックします。
<a href="https://github.com/elastic/elasticsearch/issues/10887">#10887</a>では、同様の機能をWindowsユーザにも提供します。</p>
<h2 id="より詳細なスクリプト設定">より詳細なスクリプト設定</h2>
<p>Scriptsはリクエストにインラインで指定できます。
<code>.scripts</code>インデックスにインデックスもでき、<code>config/</code>ディレクトリ配下にファイルとして保存もできます。
これまでは、インラインかインデックスされたスクリプトの両方を同時に有効無効にすることが選択できましたが、
<code>.scripts</code>インデックスをプロキシやShieldで保護することもできました。</p>
<p><a href="https://github.com/elastic/elasticsearch/issues/10116">#10116</a>で追加されたより詳細なスクリプトの設定で、インラインか、インデックスされたものか、ファイル化を個別に言語ごとに設定できるようになりました。
また、例えば、search APIではスクリプトを許可するが、update APIでは許可しないといったような設定も可能です。</p>
<h2 id="最後に">最後に</h2>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-6-0">Elasticsearch 1.6.0</a>を試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)や<a href="https://discuss.elastic.co/c/elasticsearch">Webフォーラム</a>などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第10回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/06/02/10th-elasticsearch-jp/</link>
      <pubDate>Tue, 02 Jun 2015 15:06:36 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/06/02/10th-elasticsearch-jp/</guid>
      <description>第10回Elsticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、開場提供していただいたリクルートテクノロジーズさ</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/25297">第10回Elsticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<!-- more -->
<p>今回も新規の方が結構いたような気がしました。
最終的に、124人がアプリでチェックインした形になりました。
直前にキャンセル待ちから繰り上がると来れない人がいますよねぇ。
多少キャパシティオーバーするくらいの人数で募集するのがいいのでしょうか。
あと、カードが2枚不明で。。。心あたりある人いないでしょうか？</p>
<p>さて、いつもの通り簡単なメモです。</p>
<h2 id="elasticon報告有償プラグインの紹介elastic-jun-ohtani-johtani">Elastic{ON}報告＋有償プラグインの紹介　Elastic Jun Ohtani @johtani</h2>
<p>スライド：<a href="https://speakerdeck.com/johtani/elastic-on-bao-gao-toshang-yong-puraguinfalseshao-jie">elastic{ON}報告と商用プラグインの紹介</a></p>
<p>少し時間が経ってしまいましたが、弊社初のカンファレンス<a href="http://www.elasticon.com">elastic{ON}</a>の紹介をしました。
約1300名の方に参加していただいたカンファレンスで、非常に盛り上がりました。
Microsoft、GitHubなど、いろいろな会社の方が話をしたり、弊社のエンジニアが濃い話をしたりと。
今回は、日本の方はいなかったですが、次回は日本からも参加してもらえると嬉しいです！</p>
<p>あとは、5月に弊社にも日本の営業の人が入社したので、有償プラグインについて簡単ですが説明をしました。
プラグインなどに興味があるかたがいらっしゃいましたら、Twitterなどで連絡いただければと。
もちろん、弊社サイトからの問い合わせでも大丈夫です。</p>
<p><a href="https://www.elastic.co/elasticon">カンファレンスの資料やビデオが弊社サイトで公開</a>されています。
ぜひ一度見ていただければと。</p>
<h2 id="awsで実現するelasticsearchの大規模運用-株式会社インティメートマージャー松田和樹さんmats116httpstwittercommats116">AWSで実現するelasticsearchの大規模運用 株式会社インティメート・マージャー　松田和樹さん　<a href="https://twitter.com/mats116">@mats116</a></h2>
<p>スライド：<a href="http://www.slideshare.net/im_docs/elasticsearch-48873206">第10回elasticsearch勉強会 公開用資料</a></p>
<p>パブリックDMPのサービスの裏側でElasticsearchを利用しているというお話でした。
AWS Auto Scalingに詳しくないので、勉強しないといけないんですが、
リバランスがどのくらいの頻度で発生するのかはちょっと気になります。</p>
<p>SSDを利用したり、doc valuesを利用したりと、性能を気にしながら利用されている点、負荷試験を行って検証されていたりと、
参考になる話でした。
今回はインフラ側の話に寄っていたので、今度はアプリ側でどんな使い方をしているかといった話を聞いてみたいですね！</p>
<h2 id="spark-in-small-or-middle-scale-data-processing-with-elasticsearch株式会社ビズリーチ-島本多可子さん-chibochibo03httpstwittercomchibochibo03">Spark in small or middle scale data processing with Elasticsearch　株式会社ビズリーチ 島本　多可子さん <a href="https://twitter.com/chibochibo03">@chibochibo03</a></h2>
<p>スライド：<a href="http://www.slideshare.net/chibochibo/spark-with-elasticsearch">Spark in small or middle scale data processing with Elasticsearch</a></p>
<p>ScalaとSparkとElasticsearchで検索サービスを作っている話でした。
サービスのアーキテクチャの選別についての説明を順を追って説明していただきました。
失敗と言われていたアーキテクチャを見た時に、「あー、それは。。。」と思っていたら、
思った通りの改善案のアーキテクチャが出てきたので少しホッとしましたw</p>
<p>JSONのクエリが辛いという話がありましたが、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-validate.html">validate API</a>などを利用してもらって、事前にチェックをしてもらうと
少しは改善できるかもなぁと。</p>
<p>Sparkをぼんやりとしかわかってないので、もう一度話を聴きたいなぁと思ったので、
押しかけて話を聴きたいと思います。</p>
<p>話の中で出てきた自作のScalaのElasticsearchクライアントがHTTPのクライアントになった理由が知りたかったです。</p>
<h2 id="lt">LT</h2>
<h3 id="elasticsearchのサジェスト機能を使った話株式会社アイスタイル渡邊-紘太朗さん-ktaro_whttpstwittercomktaro_w">Elasticsearchのサジェスト機能を使った話　株式会社アイスタイル　渡邊 紘太朗さん <a href="https://twitter.com/ktaro_w">@ktaro_w</a></h3>
<p>スライド：<a href="http://www.slideshare.net/ktaro_w/elasticsearch-48826694">Elasticsearchのサジェスト機能を使った話</a></p>
<p>ぴったり5分でしたwまだ2年目なのにこんなにうまく話をしていただけるとは。。。</p>
<p>Gatling便利そうですね。サーバが1台しかないので、単一インデックスの方が性能が出るだろうなと。
Elasticsearchは1インデックスに対してデフォルトだと5シャードで、シャード単位でLuceneのインデックスが作成されます。
この話で行くと、18インデックスを作ると、かなりの数のファイルI/Oが発生するので、いろいろなインデックスに検索をすると
キツいだろうなと。</p>
<p>サジェストについての日本語の資料が少ないという事だったので、ブログを書いてもらえると嬉しいですw</p>
<h3 id="elasticsearchで作る形態素解析サーバ株式会社エヌツーエスエム菅谷信介さん">Elasticsearchで作る形態素解析サーバ　株式会社エヌツーエスエム　菅谷信介さん</h3>
<p>スライド：<a href="http://www.slideshare.net/shinsuke/es-analyzeapi201506">Elasticsearchで作る形態素解析サーバ</a></p>
<p>いつも発表ありがとうございます。私以外の最多発表者じゃないかという話でした。
今回はElasticsearchを形態素解析サーバにしてしまおうという話で、ちょっと面白い話でした。
Elasticsearch以外の場所で形態素解析したい場合には手軽に使えるかもしれないですし、Elasticsearchと同じ解析結果を別の場所で欲しい場合にも便利かも。</p>
<p><a href="https://github.com/johtani/elasticsearch-extended-analyze">extended analyze API</a>の紹介までしていただいて。。。</p>
<p>ちなみに、今は、extended analyze プラグインも指定したAttributeの情報だけ返せるようになってたり、
マルチバリューへの対応もしていたりします。
そのうち本家のanalyze APIに機能を取り込む予定です。（早くしないと）</p>
<h3 id="開発効率up-elasticsearch-client-tool-作ってみたナレッジワークス株式会社木戸国彦さん-9215httpstwittercom9215">開発効率UP! Elasticsearch Client Tool 作ってみた　ナレッジワークス株式会社　木戸国彦さん <a href="https://twitter.com/9215">@9215</a></h3>
<p>スライド：<a href="https://speakerdeck.com/kunihikokido/kai-fa-xiao-lu-atupu-elasticsearch-client-tool-zuo-tutemita">開発効率アップ!Elasticsearch Client Tool 作ってみた</a></p>
<p>Hello Elasticsearch!にはお世話になっている人が多いんじゃないかなと。
今回はSublime Textのプラグインのお話でした。（すみません、Sublime Text使ってなくて。。。）
AtomとかIntellijのプラグインもあるとうれしいなー</p>
<h3 id="変わり種プラグインの作り方日本ibm黒澤亮二さん">変わり種プラグインの作り方　日本IBM　黒澤亮二さん</h3>
<p>スライド：<a href="http://www.slideshare.net/kuron99/elasticsearch-plugin-48848087">変わり種プラグインの作り方</a></p>
<p>Elasticsearchの拡張ポイントの話と、簡単なプラグインの作り方と少しElasticsearch内部の話をしていただきました。
Foundの資料が上がってました。さすが。あそこのブログは面白い話が多いんですよね。
社内で実際に使われてる話とかも聞いてみたい！</p>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://suzuki.tdiary.net/20150602.html">第10回 Elasticsearch 勉強会へ参加してきた昨日の話</a></li>
<li><a href="http://qiita.com/t-sato/items/45ec24b8df9155d6488f">第10回elasticsearch勉強会 #elasticsearch #elasticsearchjp</a></li>
<li><a href="http://blog.shibayu36.org/entry/2015/06/02/162724">第10回elasticsearch勉強会に行ってきました</a></li>
<li><a href="http://tech.im-dmp.net/archives/3271">elasticsearch勉強会に登壇してきました</a></li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>懇親会で24Fに移動していただくということで、少し手間をかけてしまいました、すみませんでした。
今回も初参加の方がそこそこいたんじゃないかなと。
あとは、AWSサミットがあるために上京してて参加しましたという方もいらっしゃいました。
大きなカンファレンスの期間の前後に行うとこんなメリットもあるんですね、今後の参考にしたいと思います。
次回は7/27を予定しています。CTOのShayが来日予定です！</p>
<p>あと、東京以外の勉強会も検討しつつあります。興味のある方はコメントやTwitterで反応をいただけると嬉しいです。</p>
<p>スピーカーは随時募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。
よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Logstashを使ったElasticsearchの再インデックス（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/05/26/reindex-elasticsearch-with-logstash-ja/</link>
      <pubDate>Tue, 26 May 2015 16:08:10 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/05/26/reindex-elasticsearch-with-logstash-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Reindex Elasticsearch With Logstash Thanks David! マッピングを変更したり、インデックスの設定を変更したり、あるサ</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://david.pilato.fr/blog/2015/05/20/reindex-elasticsearch-with-logstash/">Reindex Elasticsearch With Logstash</a></p>
<p>Thanks David!</p>
<!-- more -->
<p>マッピングを変更したり、インデックスの設定を変更したり、あるサーバから他のサーバや、
あるクラスタから他のクラスタ（例えば複数のデータセンターのような場合）にデータを再インデックスしたくなることがあるでしょう。</p>
<p>後者のような場合は<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html">SnapshotやRestoreの機能</a>を利用することもできますが、インデックスの設定を変更をしたい場合は
その他の方法が必要になります。</p>
<p><a href="https://www.elastic.co/blog/logstash-1-5-0-ga-released">Logstash 1.5.0</a>で、
<a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-elasticsearch.html">elasticsearch input</a>と<a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html">elasticsearch output</a>を使うことで、とても簡単に再インデックスができます。</p>
<p>ではやってみましょう。</p>
<h2 id="古いクラスタ">古いクラスタ</h2>
<p>elasticsearch 1.5.2 はすでにダウンロード済みとして、<code>localhost:9200</code>で<code>old</code>という名前のクラスタを起動します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">bin/elasticsearch --cluster.name<span style="color:#f92672">=</span>old
</code></pre></div><p>クラスタに<code>person</code>という名前のインデックスが存在します。
これは、5シャードで、100万件のドキュメントを持っています。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="http://david.pilato.fr/blog/images/reindex-es01/sense01.png" />
    </div>
    <a href="http://david.pilato.fr/blog/images/reindex-es01/sense01.png" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="新しいクラスタ">新しいクラスタ</h2>
<p>次に新しいクラスタを起動します。
<code>localhost:9201</code>で<code>new</code>という名前のクラスタを起動します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">bin/elasticsearch --cluster.name<span style="color:#f92672">=</span>new
</code></pre></div><p>こちらは、空です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -XGET <span style="color:#e6db74">&#34;http://localhost:9201/person&#34;</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;error&#34;</span>: <span style="color:#e6db74">&#34;IndexMissingException[[person] missing]&#34;</span>,
   <span style="color:#f92672">&#34;status&#34;</span>: <span style="color:#ae81ff">404</span>
}
</code></pre></div><h2 id="logstashのインストール">Logstashのインストール</h2>
<p>次に、Logstash 1.5.0をダウンロードして、インストールします。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget http://download.elastic.co/logstash/logstash/logstash-1.5.0.tar.gz
tar xzf logstash-1.5.0.tar.gz
cd logstash-1.5.0
</code></pre></div><p>logstashの設定ファイル<code>logstash.conf</code>を次のように設定します。</p>
<pre><code>input {
  # We read from the &quot;old&quot; cluster
  elasticsearch {
    hosts =&gt; [ &quot;localhost&quot; ]
    port =&gt; &quot;9200&quot;
    index =&gt; &quot;person&quot;
    size =&gt; 500
    scroll =&gt; &quot;5m&quot;
    docinfo =&gt; true
  }
}

output {
  # We write to the &quot;new&quot; cluster
  elasticsearch {
    host =&gt; &quot;localhost&quot;
    port =&gt; &quot;9201&quot;
    protocol =&gt; &quot;http&quot;
    index =&gt; &quot;%{[@metadata][_index]}&quot;
    index_type =&gt; &quot;%{[@metadata][_type]}&quot;
    document_id =&gt; &quot;%{[@metadata][_id]}&quot;
  }
  # We print dots to see it in action
  stdout {
    codec =&gt; &quot;dots&quot;
  }
}
</code></pre><h2 id="実行と修正">実行と修正</h2>
<p>実行します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">bin/logstash -f logstash.conf
</code></pre></div><h3 id="ドキュメントのチェックと修正">ドキュメントのチェックと修正</h3>
<p>何が起きたでしょう？</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -XGET <span style="color:#e6db74">&#34;http://localhost:9200/person/person/AU1wqyQWZJKU8OibfxgH&#34;</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;person&#34;</span>,
   <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;person&#34;</span>,
   <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;AU1wqyQWZJKU8OibfxgH&#34;</span>,
   <span style="color:#f92672">&#34;_version&#34;</span>: <span style="color:#ae81ff">1</span>,
   <span style="color:#f92672">&#34;found&#34;</span>: <span style="color:#66d9ef">true</span>,
   <span style="color:#f92672">&#34;_source&#34;</span>: {
      <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;Tali Elyne&#34;</span>,
      <span style="color:#f92672">&#34;dateOfBirth&#34;</span>: <span style="color:#e6db74">&#34;1955-05-03&#34;</span>,
      <span style="color:#f92672">&#34;gender&#34;</span>: <span style="color:#e6db74">&#34;female&#34;</span>,
      <span style="color:#f92672">&#34;children&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;marketing&#34;</span>: {
         <span style="color:#f92672">&#34;cars&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;shoes&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;toys&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;fashion&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;music&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;garden&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;electronic&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;hifi&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;food&#34;</span>: <span style="color:#ae81ff">846</span>
      },
      <span style="color:#f92672">&#34;address&#34;</span>: {
         <span style="color:#f92672">&#34;country&#34;</span>: <span style="color:#e6db74">&#34;Germany&#34;</span>,
         <span style="color:#f92672">&#34;zipcode&#34;</span>: <span style="color:#e6db74">&#34;0099&#34;</span>,
         <span style="color:#f92672">&#34;city&#34;</span>: <span style="color:#e6db74">&#34;Bonn&#34;</span>,
         <span style="color:#f92672">&#34;countrycode&#34;</span>: <span style="color:#e6db74">&#34;DE&#34;</span>,
         <span style="color:#f92672">&#34;location&#34;</span>: [
            <span style="color:#ae81ff">7.075943707068682</span>,
            <span style="color:#ae81ff">50.72883500730124</span>
         ]
      }
   }
}
</code></pre></div><p>もう一方のクラスタと比較してみましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -XGET <span style="color:#e6db74">&#34;http://localhost:9201/person/person/AU1wqyQWZJKU8OibfxgH&#34;</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;person&#34;</span>,
   <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;person&#34;</span>,
   <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;AU1wqyQWZJKU8OibfxgH&#34;</span>,
   <span style="color:#f92672">&#34;_version&#34;</span>: <span style="color:#ae81ff">1</span>,
   <span style="color:#f92672">&#34;found&#34;</span>: <span style="color:#66d9ef">true</span>,
   <span style="color:#f92672">&#34;_source&#34;</span>: {
      <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;Tali Elyne&#34;</span>,
      <span style="color:#f92672">&#34;dateOfBirth&#34;</span>: <span style="color:#e6db74">&#34;1955-05-03&#34;</span>,
      <span style="color:#f92672">&#34;gender&#34;</span>: <span style="color:#e6db74">&#34;female&#34;</span>,
      <span style="color:#f92672">&#34;children&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;marketing&#34;</span>: {
         <span style="color:#f92672">&#34;cars&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;shoes&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;toys&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;fashion&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;music&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;garden&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;electronic&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;hifi&#34;</span>: <span style="color:#66d9ef">null</span>,
         <span style="color:#f92672">&#34;food&#34;</span>: <span style="color:#ae81ff">846</span>
      },
      <span style="color:#f92672">&#34;address&#34;</span>: {
         <span style="color:#f92672">&#34;country&#34;</span>: <span style="color:#e6db74">&#34;Germany&#34;</span>,
         <span style="color:#f92672">&#34;zipcode&#34;</span>: <span style="color:#e6db74">&#34;0099&#34;</span>,
         <span style="color:#f92672">&#34;city&#34;</span>: <span style="color:#e6db74">&#34;Bonn&#34;</span>,
         <span style="color:#f92672">&#34;countrycode&#34;</span>: <span style="color:#e6db74">&#34;DE&#34;</span>,
         <span style="color:#f92672">&#34;location&#34;</span>: [
            <span style="color:#ae81ff">7.075943707068682</span>,
            <span style="color:#ae81ff">50.72883500730124</span>
         ]
      },
      <span style="color:#f92672">&#34;@version&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
      <span style="color:#f92672">&#34;@timestamp&#34;</span>: <span style="color:#e6db74">&#34;2015-05-20T09:53:44.089Z&#34;</span>
   }
}
</code></pre></div><p>Logstashは<code>@version</code>と<code>@timestamp</code>フィールドを追加してしました。
これらを除去したいので、<a href="http://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html">Mutate filter plugin</a>の<a href="http://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-remove_field"><code>remove_field</code></a>を使います。</p>
<pre><code>filter {
  mutate {
    remove_field =&gt; [ &quot;@timestamp&quot;, &quot;@version&quot; ]
  }
}
</code></pre><h3 id="マッピングのチェックと修正">マッピングのチェックと修正</h3>
<p>実際に、logstashは<code>_source</code>フィールドを既存のドキュメントから読み込み、
それらを新しいクラスタに直接投入しています。
しかし、logstashはマッピングについてはケアしていません。</p>
<p>古いマッピングと新しいマッピングを比較するために、マッピングを取得してみましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -XGET <span style="color:#e6db74">&#34;http://localhost:9200/person/person/_mapping&#34;</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;person&#34;</span>: {
      <span style="color:#f92672">&#34;mappings&#34;</span>: {
         <span style="color:#f92672">&#34;person&#34;</span>: {
            <span style="color:#f92672">&#34;properties&#34;</span>: {
               <span style="color:#f92672">&#34;address&#34;</span>: {
                  <span style="color:#f92672">&#34;properties&#34;</span>: {
                     <span style="color:#f92672">&#34;city&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
                        <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>
                     },
                     <span style="color:#f92672">&#34;country&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
                        <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>
                     },
                     <span style="color:#f92672">&#34;countrycode&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
                        <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>
                     },
                     <span style="color:#f92672">&#34;location&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;geo_point&#34;</span>
                     },
                     <span style="color:#f92672">&#34;zipcode&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
                     }
                  }
               },
               <span style="color:#f92672">&#34;children&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
               },
               <span style="color:#f92672">&#34;dateOfBirth&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;date&#34;</span>,
                  <span style="color:#f92672">&#34;format&#34;</span>: <span style="color:#e6db74">&#34;dateOptionalTime&#34;</span>
               },
               <span style="color:#f92672">&#34;gender&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
                  <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>
               },
               <span style="color:#f92672">&#34;marketing&#34;</span>: {
                  <span style="color:#f92672">&#34;properties&#34;</span>: {
                     <span style="color:#f92672">&#34;cars&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;electronic&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;fashion&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;food&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;garden&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;hifi&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;music&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;shoes&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;toys&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     }
                  }
               },
               <span style="color:#f92672">&#34;name&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
               }
            }
         }
      }
   }
}
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -XGET <span style="color:#e6db74">&#34;http://localhost:9201/person/person/_mapping&#34;</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;person&#34;</span>: {
      <span style="color:#f92672">&#34;mappings&#34;</span>: {
         <span style="color:#f92672">&#34;person&#34;</span>: {
            <span style="color:#f92672">&#34;properties&#34;</span>: {
               <span style="color:#f92672">&#34;address&#34;</span>: {
                  <span style="color:#f92672">&#34;properties&#34;</span>: {
                     <span style="color:#f92672">&#34;city&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
                     },
                     <span style="color:#f92672">&#34;country&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
                     },
                     <span style="color:#f92672">&#34;countrycode&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
                     },
                     <span style="color:#f92672">&#34;location&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;double&#34;</span>
                     },
                     <span style="color:#f92672">&#34;zipcode&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
                     }
                  }
               },
               <span style="color:#f92672">&#34;children&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
               },
               <span style="color:#f92672">&#34;dateOfBirth&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;date&#34;</span>,
                  <span style="color:#f92672">&#34;format&#34;</span>: <span style="color:#e6db74">&#34;dateOptionalTime&#34;</span>
               },
               <span style="color:#f92672">&#34;gender&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
               },
               <span style="color:#f92672">&#34;marketing&#34;</span>: {
                  <span style="color:#f92672">&#34;properties&#34;</span>: {
                     <span style="color:#f92672">&#34;cars&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;electronic&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;fashion&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;food&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;garden&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;hifi&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;music&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;shoes&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     },
                     <span style="color:#f92672">&#34;toys&#34;</span>: {
                        <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;long&#34;</span>
                     }
                  }
               },
               <span style="color:#f92672">&#34;name&#34;</span>: {
                  <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
               }
            }
         }
      }
   }
}
</code></pre></div><p>これにより、いくつかの相違を発見できます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"> <span style="color:#e6db74">&#34;location&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {
    <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;geo_point&#34;</span>
 }
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"> <span style="color:#e6db74">&#34;location&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {
    <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;double&#34;</span>
 }
</code></pre></div><p>データをインデックスする「前」に、実際に利用したいマッピングでインデックスを作成しておくことで、
この問題に対処できます。
この時点で、オリジナルのマッピングを望んだ形に変更することができます。例えば、アナライザを変更したりです。
また、インデックスの設定を新しく定義することもできます。
デフォルトでは、Elasticsearchは5つのシャードと各シャードに対して1つのレプリカを作成します。
しかし、この時点でもう一度変更することが可能です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -XDELETE <span style="color:#e6db74">&#34;http://localhost:9201/person&#34;</span>
curl -XPUT <span style="color:#e6db74">&#34;http://localhost:9201/person&#34;</span> -d<span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">  &#34;settings&#34;: {
</span><span style="color:#e6db74">    &#34;number_of_shards&#34;: 1,
</span><span style="color:#e6db74">    &#34;number_of_replicas&#34;: 0
</span><span style="color:#e6db74">  }
</span><span style="color:#e6db74">}&#39;</span>
curl -XPUT <span style="color:#e6db74">&#34;http://localhost:9201/person/person/_mapping&#34;</span> -d<span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">  &#34;person&#34;: {
</span><span style="color:#e6db74">    &#34;properties&#34;: {
</span><span style="color:#e6db74">      &#34;address&#34;: {
</span><span style="color:#e6db74">        &#34;properties&#34;: {
</span><span style="color:#e6db74">          &#34;city&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;string&#34;,
</span><span style="color:#e6db74">            &#34;index&#34;: &#34;not_analyzed&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;country&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;string&#34;,
</span><span style="color:#e6db74">            &#34;index&#34;: &#34;not_analyzed&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;countrycode&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;string&#34;,
</span><span style="color:#e6db74">            &#34;index&#34;: &#34;not_analyzed&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;location&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;geo_point&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;zipcode&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;children&#34;: {
</span><span style="color:#e6db74">        &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;dateOfBirth&#34;: {
</span><span style="color:#e6db74">        &#34;type&#34;: &#34;date&#34;,
</span><span style="color:#e6db74">        &#34;format&#34;: &#34;dateOptionalTime&#34;
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;gender&#34;: {
</span><span style="color:#e6db74">        &#34;type&#34;: &#34;string&#34;,
</span><span style="color:#e6db74">        &#34;index&#34;: &#34;not_analyzed&#34;
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;marketing&#34;: {
</span><span style="color:#e6db74">        &#34;properties&#34;: {
</span><span style="color:#e6db74">          &#34;cars&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;electronic&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;fashion&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;food&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;garden&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;hifi&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;music&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;shoes&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;toys&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;name&#34;: {
</span><span style="color:#e6db74">        &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  }
</span><span style="color:#e6db74">}&#39;</span>
</code></pre></div><p>さて、もう一度再インデックスしましょう！</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">bin/logstash -f logstash.conf
</code></pre></div>

<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="http://david.pilato.fr/blog/images/reindex-es01/sense02.png" />
    </div>
    <a href="http://david.pilato.fr/blog/images/reindex-es01/sense02.png" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="インデックスやタイプ名の変更">インデックスやタイプ名の変更</h2>
<p>もちろん、インデックス名やタイプ名、IDを変更したい場合も変更が可能です！:)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">  <span style="color:#960050;background-color:#1e0010">elasticsearch</span> {
    <span style="color:#960050;background-color:#1e0010">host</span> <span style="color:#960050;background-color:#1e0010">=&gt;</span> <span style="color:#f92672">&#34;localhost&#34;</span>
    <span style="color:#960050;background-color:#1e0010">port</span> <span style="color:#960050;background-color:#1e0010">=&gt;</span> <span style="color:#e6db74">&#34;9201&#34;</span>
    <span style="color:#960050;background-color:#1e0010">protocol</span> <span style="color:#960050;background-color:#1e0010">=&gt;</span> <span style="color:#e6db74">&#34;http&#34;</span>
    <span style="color:#960050;background-color:#1e0010">index</span> <span style="color:#960050;background-color:#1e0010">=&gt;</span> <span style="color:#e6db74">&#34;europe_people&#34;</span>
    <span style="color:#960050;background-color:#1e0010">index_type</span> <span style="color:#960050;background-color:#1e0010">=&gt;</span> <span style="color:#e6db74">&#34;someone&#34;</span>
    <span style="color:#960050;background-color:#1e0010">document_id</span> <span style="color:#960050;background-color:#1e0010">=&gt;</span> <span style="color:#e6db74">&#34;%{[@metadata][_id]}&#34;</span>
  }
</code></pre></div></content:encoded>
    </item>
    
    <item>
      <title>discuss.elastic.co にぜひ参加を</title>
      <link>https://blog.johtani.info/blog/2015/05/21/join-the-conversation-ja/</link>
      <pubDate>Thu, 21 May 2015 17:45:03 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/05/21/join-the-conversation-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Join the Conversation: Discuss.Elastic.Co 3つのOSSプロジェクトの開発をスケールアップし始め、 コミュニティサポー</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/join-the-conversation">Join the Conversation: Discuss.Elastic.Co</a></p>
<!-- more -->
<p>3つのOSSプロジェクトの開発をスケールアップし始め、
コミュニティサポートのために必要なやりとりに対してメーリングリストでは難しいということがわかってきました。
私たちは複数のメーリングリストを持っています。Elasticsearch、Logstash、そして英語以外の様々な言語のメーリングリストです。
このような状況では、あたらしい人たちはどこで質問をするのが良いのか混乱します。</p>
<p>また、メーリングリストの流量が増え、「参考になる話題」を見つけるのが難しくなってきました。
様々なユーザに採用してもらい、様々なユースケースが出てくることで、様々な質問が出てきています。
汎用的なメーリングリストではノイズの中から望んだ情報を見つけるのは難しいです。
また、ユーザ全てがメーリングリスト満足しているわけではありません。</p>
<p>Elasticは、ユーザの問題を解くことが大好きです。
コミュニティのメンバー皆さんに気に入っていただけるであろうソリューションを見つけ、フォーラムを
<a href="https://discuss.elastic.co">https://discuss.elastic.co</a> に移すことにしました。
ぜひ参加して、この新しいツールについてのご意見を聞かせてください。</p>
<p>メーリングリストは好きだけど、ウェブフォーラムは苦手？問題ありません。
フォーラムにユーザプロファイル（GitHub、Facebook、Twitter、Google Appsのアカウントと連携するか、emailアドレスを利用すれば簡単に作れます）を作り、
email通知の設定をすればOKです。
これで、<a href="https://discuss.elastic.co/t/email-only-interaction-with-the-forums/106">emailでのやりとり</a>ができるようになります。</p>
<p>利用して、議論を楽しんでください。
もちろん、改善案などにかんする<a href="https://discuss.elastic.co/c/meta">ご意見もお待ちしています</a>！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.5.2 および 1.4.5リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/04/28/elasticsearch-1-5-2-and-1-4-5-released-ja/</link>
      <pubDate>Tue, 28 Apr 2015 15:14:08 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/04/28/elasticsearch-1-5-2-and-1-4-5-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 1.5.2 Released 本日（4/27）、Lucene 4.10.4ベースのEla</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-1-5-2-and-1-4-5-released">Elasticsearch 1.5.2 Released</a></p>
<p>本日（4/27）、<strong>Lucene 4.10.4</strong>ベースの<strong>Elasticsearch 1.5.1</strong>および<strong>Elasticsearch 1.4.5</strong> をセキュリティバグフィックス版をリリースしました。
ダウンロードおよびすべての変更については次のリンクをごらんください。</p>
<ul>
<li>最新安定版：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-2">Elasticsearch 1.5.2</a></li>
<li>1.4系バグフィックス：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-4-5">Elasticsearch 1.4.5</a></li>
</ul>
<p><em>本リリースはディレクトリトラバーサルの脆弱性のフィックスです。すべてのユーザにアップグレードを勧めます。</em></p>
<!-- more -->
<p>過去のリリースに関するブログは以下のリンクを参照してください。</p>
<ul>
<li><em>1.5</em>:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-5-1-released/">1.4.1</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-5-0-released/">1.5.0</a></li>
<li><em>1.4</em>:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-4-released/">1.4.4</a>,<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-3-released/">1.4.3</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-2-released/">1.4.2</a>,<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/">1.4.1</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.4.0</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">1.4.0.Beta1</a></li>
</ul>
<p>すべての<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-2">1.5.2</a>および<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-4-5">1.4.5</a>の変更についてはリンクをごらんください。以下では、セキュリティの問題について紹介します。</p>
<h2 id="ディレクトリトラバーサル脆弱性の発見">ディレクトリトラバーサル脆弱性の発見</h2>
<p>1.5.2および1.4.5以前の全バージョンのElasticsearchで、ディレクトリトラバーサル攻撃に対する脆弱性がみつかりました。
攻撃者はElasticsearchを実行しているサーバからファイルを取得することができます。
この脆弱性はインストールしたばかりのElasticsearchには存在しません。
この脆弱性は&quot;site plugin&quot;がインストールされると露呈します。
ElasticのMarvelプラグインおよびコミュニティサポートの多くのプラグイン（例：Kopf、BigDesk、Head）がsite pluginです。
Elastic Shield、Licensing、Cloud-AWS、Cloud-GCE、Cloud-Azure、analysis pluginおよびriverプラグインはsite pluginでは<em>ありません</em>。</p>
<p>この問題を<a href="https://www.elastic.co/community/security">CVE-2015-3337</a>としました。</p>
<p>バージョン1.5.2と1.4.5はこの脆弱性に対して対策済みで、私たちはすべてのユーザにアップグレードを勧めています。</p>
<p>アップグレードを望まないユーザはいくつかの方法でこの脆弱性に対して対応可能ですが、これらの方法はsite pluginを動作させなくします。</p>
<ul>
<li>site pluginをインストールしているノードの<code>elasticsearch.yml</code>の<code>http.disable_sites</code>を<code>true</code>に設定し、Elasticsearchのノードを再起動</li>
<li>ファイアウォールもしくはプロキシを利用して、<code>/_plugin</code>へのHTTPリクエストをブロック</li>
<li>すべてのsite pluginをすべてのElasticsearchノードからアンインストール</li>
</ul>
<p>この問題を報告していただいた、DocuSignのJohn Heasmanに感謝いたします。</p>
<h2 id="他の変更について">他の変更について</h2>
<ul>
<li><a href="https://github.com/elastic/elasticsearch/pull/10526">インデックスされたスクリプトおよびテンプレート</a>を上書きもしくは削除時に、キャッシュからも完全に削除する。</li>
<li>geo-shapeの多数のフィックス（<code>distance_error_pct</code>を利用した場合の、重要な<a href="https://github.com/elastic/elasticsearch/pull/10679">precisionに関するフィックス</a>を含む）</li>
<li>インデックステンプレートのデフォルトマッピングがバルクインデキシング中にも考慮するように修正</li>
<li>Shadowレプリカが<a href="https://github.com/elastic/elasticsearch/pull/10688">ファイルシステムの遅延</a>に対する対障害性を向上し、プライマリシャードの<a href="https://github.com/elastic/elasticsearch/pull/10585">よりスムーズなリロケーション</a>をサポート</li>
<li><a href="http://github.com/elastic/elasticsearch/issues/10602">geo-contexts</a>をcompletion suggesterで使用した場合のマッピングのリフレッシュループを改善</li>
</ul>
<p>いくつかの重要な変更がv1.4.5にバックポートされています。</p>
<ul>
<li><a href="http://github.com/elastic/elasticsearch/issues/10463">大きなシャードのリカバリを早くする</a>ためのシャードリカバリ中のマージを可能に
* <a href="http://github.com/elastic/elasticsearch/issues/9797">truncated translogs</a>の操作をグレースフルに</li>
<li>マージが遅くなる場合に、<a href="http://github.com/elastic/elasticsearch/issues/9986">delete-by-queryを減速</a></li>
</ul>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-2">Elasticsearch 1.5.2</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第9回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/04/17/9th-elasticsearch-jp/</link>
      <pubDate>Fri, 17 Apr 2015 14:41:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/04/17/9th-elasticsearch-jp/</guid>
      <description>第9回Elsticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、開場提供していただいたリクルートテクノロジーズさん</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/23012">第9回Elsticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<!-- more -->
<p>今回はトレーニングで来日していたIgorとNathanによる特別公演でした。
昨年同様、<a href="http://samuraism.com">サムライズム</a>の<a href="https://twitter.com/yusuke">@yusuke</a>さんに
テキスト翻訳していただき、大変助かりました。ほんとうにすごかった。。。</p>
<h3 id="チェックイン数など">チェックイン数など</h3>
<ul>
<li>今回はチェックインした人：119名</li>
<li>キャンセルしなかった人：45名</li>
</ul>
<p>でした。今回はキャンセル待ちのまま当日を迎えた人もいなかったので良かったかなと。
今回から懇親会ページを別にしてみました。本編の勉強会に参加登録していた方には何度かメールを出していたので、
見つけていなかった人は以内とは思うのですが、勉強会のページと間違える人がいたらしいという話を聞きました。
Doorkeeperで1イベントで複数のチケットにそれぞれの参加者数を設定できるようになると嬉しいかもなぁ。</p>
<p>さて、いつもの通り簡単なメモです。
本当に簡単にですが。</p>
<h2 id="resiliency-in-elasticsearch-and-lucene--igor-motov">Resiliency in Elasticsearch and Lucene / Igor Motov</h2>
<p>スライド：https://speakerdeck.com/elastic/resiliency-in-elasticsearch-and-lucene</p>
<p>※上記スライドは少し古いバージョンです。公開されたら差し替える予定です。</p>
<p>サンフランシスコで行われた<a href="https://www.elastic.co/elasticon">Elastic{ON}</a>（弊社初のカンファレンス）で行われたセッションの
改良版といったところでしょうか。
話の中で登場した機能などのリンクをざっとアップしておきます。</p>
<ul>
<li><a href="http://www.elastic.co/guide/en/elasticsearch/guide/current/fielddata.html">Fielddata</a></li>
<li><a href="http://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html">Doc Values</a></li>
<li><a href="http://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html">Resiliency Status</a></li>
</ul>
<h2 id="kibana4-whats-new---nathan-zamecnik">Kibana4: What&rsquo;s New ? / Nathan Zamecnik</h2>
<p>スライド：未定</p>
<p>Kibana4の紹介をデモを交えてという感じでした。
こちらは、スライドよりもデモを見てもらうのが一番いいのですが。。。</p>
<p>いくつかQAがあったので補足を。ちなみに、Issueのラベルに実装される予定のバージョンが付与されてたりします。</p>
<ul>
<li>Q：グラフをPDFでエクスポートとかできますか？
<ul>
<li>A：4.3.0で実装される予定です。関連Issueはこちら。https://github.com/elastic/kibana/issues/509</li>
</ul>
</li>
<li>Q：巨大な数値の場合にKB、MBなどといった表示は可能ですか？
<ul>
<li>A：4.1.0で実装される予定です。関連Issueはこちら。https://github.com/elastic/kibana/issues/1543</li>
</ul>
</li>
<li>Q：地図のズームを固定することはできますか？
<ul>
<li>A：4.1.0で実装される予定です。関連Issueはこちら。https://github.com/elastic/kibana/issues/1442</li>
</ul>
</li>
</ul>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://suzuki.tdiary.net/20150415.html#p01">[Elasticsearch] 第9回 Elasticsearch 勉強会へ参加してきた</a></li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>今回は特別バージョンでした。かなり詳しい話だったので面白かったと思います。
Kibanaはデモを見ていただけましたし。また、海外から人を呼べるといいなぁ。</p>
<p>次回は6月ごろをめどに計画しようかと。
スピーカーは随時募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。 よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.5.1リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/04/13/elasticsearch-1-5-1-released-ja/</link>
      <pubDate>Mon, 13 Apr 2015 11:34:31 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/04/13/elasticsearch-1-5-1-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 1.5.1 Released 本日（4/9）、Lucene 4.10.4ベースのElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-1-5-1-released">Elasticsearch 1.5.1 Released</a></p>
<p>本日（4/9）、<strong>Lucene 4.10.4</strong>ベースの<strong>Elasticsearch 1.5.1</strong> をリリースしました。
このリリースはElasticsearchの最新の安定バージョンとなります。</p>
<p>すべての変更については<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-1">download Elasticsearch 1.5.1 here</a>をごらんください。</p>
<!-- more -->
<p>本リリースはシャードを新しいノードに配置するスピードを改善するためのバグフィックスを含んでいます。
シャードのリカバリーの最初のフェーズで、コピー元のノードからコピー先のノードへすべてのセグメントをコピーします。
このフェーズ中には登録、更新削除のリクエストはトランザクションログに記録され、リカバリが終了したあとに
コピー先のノードでトランザクションログが再生されます。
シャードが大きい場合、トランザクションログに多数のイベントがたまってしまいます。</p>
<p>以前では、新しいセグメントのマージはリカバリ中のコピー先のノードでは、実行できませんでした。
大きなトランザクションログは結果として、小さな新しいセグメントを多く生成し、リカバリのスピードに非常に影響を与えます。
Issue <a href="https://github.com/elastic/elasticsearch/pull/10463">#10463</a>は
リカバリ中のコピー先のシャードのマージを可能にする変更です。</p>
<p>その他の注目すべきバグフィックスは次のものになります。</p>
<ul>
<li>多くの削除によりバージョンマップがいっぱいになった場合にrefreshを実行するように変更(<a href="https://github.com/elastic/elasticsearch/pull/10312">#10312</a>)</li>
<li>多数のスナップショットを含んだリポジトリの管理の改善(<a href="https://github.com/elastic/elasticsearch/pull/10366">#10366</a>)</li>
<li>実験的な機能である<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html">inner hits</a>のバグフィックス(<a href="https://github.com/elastic/elasticsearch/pull/10388">#10388</a>, <a href="https://github.com/elastic/elasticsearch/pull/10353">#10353</a>, <a href="https://github.com/elastic/elasticsearch/pull/10309">#10309</a>, <a href="https://github.com/elastic/elasticsearch/pull/10235">#10235</a>)</li>
</ul>
<p>最後に、<a href="https://www.elastic.co/blog/deprecating_rivers">Riverが非推奨となりました</a>、まだ見ていない場合は記事をご覧ください。</p>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-1">Elasticsearch 1.5.1</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch勉強会 in 名古屋を開催しました。#elasticsearch #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/04/04/elasticsearch-study-session-at-nagoya/</link>
      <pubDate>Sat, 04 Apr 2015 09:47:19 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/04/04/elasticsearch-study-session-at-nagoya/</guid>
      <description>Elasticsearch勉強会 in 名古屋を開催しました。 初の東京以外での勉強会です。 企画、セッションなどお手伝いいただいた@smogamiさ</description>
      <content:encoded><p><a href="https://elasticsearch.doorkeeper.jp/events/21984">Elasticsearch勉強会 in 名古屋</a>を開催しました。
初の東京以外での勉強会です。
企画、セッションなどお手伝いいただいた<a href="https://twitter.com/smogami">@smogami</a>さん、<a href="https://twitter.com/mzp">@mzp</a>さんありがとうございました！</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:500">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150406/nagoya.jpg" />
    </div>
    <a href="/images/entries/20150406/nagoya.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<!-- more -->
<h2 id="elasticsearchelk-stack紹介-johtani">Elasticsearch/ELK stack紹介 @johtani</h2>
<p>スライド：<a href="https://speakerdeck.com/johtani/introduction-elasticsearch-and-elk-elasticsearchmian-qiang-hui-in-nagoya">Introduction Elasticsearch</a></p>
<p>初回（次回があるかはわかりませんが。。。）ということもあり、Elasticsearchの説明を行いました。
あと、LogstashとKibanaも。
Kibanaについては、手元の環境でいつものアクセスログのデモやなどを行いました。
また、LTの後に時間があったので、前回の勉強会で利用したチェックリストの説明なども。</p>
<h2 id="スタンドファームにおけるelasticsearch導入事例-mzphttpstwittercommzp-さん">スタンドファームにおけるElasticsearch導入事例 <a href="https://twitter.com/mzp">@mzp</a> さん</h2>
<p>スライド：後日アップ？</p>
<p>* 使ってるのはKibana3</p>
<ul>
<li>アクセスログが保存されてたけど、活用できてなかった。</li>
<li>Fluentd、Elasticsearch、Kibanaをいれて、可視化してみた。</li>
<li>普通にログ検索が簡単にできて嬉しい</li>
<li>システムのレスポンスの性能値などを可視化できるようにして性能改善中</li>
</ul>
<h2 id="kibanaでログ分析を1年続けてみたら業務システムの保守と運用が捗った仮-smogamihttpstwittercomsmogami-さん">Kibanaでログ分析を1年続けてみたら業務システムの保守と運用が捗った(仮) <a href="https://twitter.com/smogami">@smogami</a> さん</h2>
<p>スライド：「<a href="https://speakerdeck.com/exoego/how-and-why-i-have-been-leveraging-kibana-for-devops">Kibanaでログ分析を1年続けてみたら業務システムの保守と運用が捗った</a>」</p>
<ul>
<li>名古屋でJavaの勉強会を主催してみたり（最近できてないけど）</li>
<li>導入するのになかなか大変だった（ファイアウォールだったりが）。。。</li>
</ul>
<p>Kibanaを使ってどんなことをしてるのか？</p>
<ul>
<li>既存システムなどの機能の実行回数やレスポンス時間の推移</li>
<li>曜日ごとにもチェック</li>
<li>どの機能がよく使われるのか？</li>
<li>対象となっているシステムはJavaのシステム。</li>
</ul>
<p>QA</p>
<ul>
<li>Q：ログの出力は新規に追加したのか？
<ul>
<li>A : ログの出力自体はLog4Jの設定を変更しただけ。もともと、各メソッドの開始と終了にそれぞれ時間が出力される仕組みがある。<br>
ログの読み込み自体は自作ツールを利用。</li>
</ul>
</li>
</ul>
<h2 id="飛び込みlt-dabitshttpstwittercomdabits-さん">飛び込みLT <a href="https://twitter.com/dabits">@dabits</a> さん</h2>
<p>スライド：未定</p>
<p>Kibanaの使い道</p>
<ul>
<li>KPIツール</li>
<li>エゴサーチツール - Twitterや2chなどのデータを解析ソーシャル分析みたいな感じ？
* ダッシュボードを用意してあげる場合もあるが、触っていろんな機能を試す人も。</li>
</ul>
<h2 id="感想反省点など">感想・反省点など</h2>
<p>30名弱の方に参加していただきました。ありがとうございました。
東京の勉強会でもそうですが、半分くらいが検索、半分くらいがログ解析関連に興味がある感じでした。
飛び込みLTもしていただけましたし。会場内限定の話もいくつか。</p>
<h4 id="場所">場所</h4>
<p>場所が少しわかりにくかったかなと。。。建物の入り口に看板がないので、1名に看板役として立っていただきました。
ただ、設備は充実していましたし、室内も綺麗でよかったです。</p>


<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150406/seminar_room.jpg" />
    </div>
    <a href="/images/entries/20150406/seminar_room.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="懇親会">懇親会</h4>
<p>11名（+私）でした。美味しい手羽先などをいただきながら、Elasticsearch以外のことでも盛り上がりましたw。
また、名古屋の観光名所なども教えてもらったりと有意義な時間でしたw。</p>
<p>ということで、少しでもElasticsearch、Kibana、Logstashなどのユーザが増えてくれればうれしいかなと。
私抜きでも勉強会はできると思うので、今後も開いてもらえるとうれしいかぎりです。
初めての東京以外での勉強会でどんな感じの方が利用しているのか、興味があるのかといったことも知ることができました。</p>
<h2 id="関連ブログなど">関連ブログなど</h2>
<ul>
<li><a href="http://blog.exoego.net/2015/04/kibana4-use-case.html">Kibana4活用事例を話しました</a></li>
</ul>
<h2 id="その他余談">その他（余談）</h2>
<p>コンパルという喫茶店のアイスコーヒー。ちょっと新鮮な体験でした。


<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150406/ice_coffee.jpg" />
    </div>
    <a href="/images/entries/20150406/ice_coffee.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>あとは、日曜日に観光場所として教えてもらった、<a href="http://www.tcmit.org/">トヨタ産業技術記念館</a>にも行ってみました。
一人だったけど、非常に楽しめました。実演とかあって、わかりやすいし。
トヨタが自動織機の会社が始まりだってのは知らなかった。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.5.0リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/04/01/elasticsearch-1-5-0-released-ja/</link>
      <pubDate>Wed, 01 Apr 2015 12:38:16 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/04/01/elasticsearch-1-5-0-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：Elasticsearch 1.5.0 Released 本日（3/23）、Lucene 4.10.4ベースのEla</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="https://www.elastic.co/blog/elasticsearch-1-5-0-released">Elasticsearch 1.5.0 Released</a></p>
<p>本日（3/23）、<strong>Lucene 4.10.4</strong>ベースの<strong>Elasticsearch 1.5.0</strong> をリリースしました。
このリリースはElasticsearchの最新の安定バージョンとなります。
多くの<em>resiliency(復元性、弾力性) enhancement</em>とバグフィックスを含んでおり、
すべてのユーザにアップグレードを推奨しています。</p>
<p>すべての変更については<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-0">download Elasticsearch 1.5.0 here</a>をごらんください。</p>
<p>460PRという大量の変更を含む本リリースは、Elasticsearchをよりresilient(弾力のあるもの)にするために
費やされています。</p>
<!-- more -->
<h3 id="inner-hits">Inner hits</h3>
<p>本リリースで追加された、Elasticsearchに最もリクエストされたものの一つがinner hitsです。
これは、<code>has_child</code>もしくは<code>nested</code>クエリにマッチした子のドキュメントを、各親ドキュメントと一緒に返すことができます。</p>
<p>例えば、<code>blog</code>という親ドキュメントと<code>comment</code>という子ドキュメントを持っているとします。
この時、&ldquo;full text search&quot;というコメントを持ったブログ記事を検索したいとします。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/my_index/blog/_search</span>
{
  <span style="color:#f92672">&#34;query&#34;</span>: {
    <span style="color:#f92672">&#34;has_child&#34;</span>: {
      <span style="color:#f92672">&#34;type&#34;</span>:       <span style="color:#e6db74">&#34;comment&#34;</span>,
      <span style="color:#f92672">&#34;score_mode&#34;</span>: <span style="color:#e6db74">&#34;sum&#34;</span>,
      <span style="color:#f92672">&#34;query&#34;</span>: {
        <span style="color:#f92672">&#34;match&#34;</span>: {
          <span style="color:#f92672">&#34;body&#34;</span>:   <span style="color:#e6db74">&#34;full text search&#34;</span>
        }
      }
    }
  }
}
</code></pre></div><p>上記のリクエストは、親の<code>blog</code>ドキュメントを返します。
しかし、どのコメントが関係しているのかはわかりません。
関連しているコメントを検索して親ごとにグルーピングするために、
少し手間のかかる2回目のクエリを実行する必要があります。</p>
<p>Inner hitsがこれを変えてくれます。
<code>inner_hits</code>パラメータを次のように、上記のクエリに追加するだけです！</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/my_index/blog/_search</span>
{
  <span style="color:#f92672">&#34;query&#34;</span>: {
    <span style="color:#f92672">&#34;has_child&#34;</span>: {
      <span style="color:#f92672">&#34;type&#34;</span>:       <span style="color:#e6db74">&#34;comment&#34;</span>,
      <span style="color:#f92672">&#34;score_mode&#34;</span>: <span style="color:#e6db74">&#34;sum&#34;</span>,
      <span style="color:#f92672">&#34;query&#34;</span>: {
        <span style="color:#f92672">&#34;match&#34;</span>: {
          <span style="color:#f92672">&#34;body&#34;</span>:   <span style="color:#e6db74">&#34;full text search&#34;</span>
        }
      },
      <span style="color:#f92672">&#34;inner_hits&#34;</span>: {}
    }
  }
}

</code></pre></div><p>検索結果の各<code>blog</code>記事に、<code>inner_hits</code>という項目があり、そこに検索にヒットしたコメントの
上位3件（デフォルト値）が返ってきます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">...</span>
<span style="color:#e6db74">&#34;hits&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> [
  {
    <span style="color:#f92672">&#34;_index&#34;</span>:   <span style="color:#e6db74">&#34;my_index&#34;</span>,
    <span style="color:#f92672">&#34;_type&#34;</span>:    <span style="color:#e6db74">&#34;blog&#34;</span>,
    <span style="color:#f92672">&#34;_id&#34;</span>:      <span style="color:#ae81ff">1</span>,
    <span style="color:#f92672">&#34;_score&#34;</span>:   <span style="color:#ae81ff">3.68</span>,
    <span style="color:#f92672">&#34;_source&#34;</span>:  { <span style="color:#960050;background-color:#1e0010">...</span> },
    <span style="color:#f92672">&#34;inner_hits&#34;</span>: {
      <span style="color:#f92672">&#34;comment&#34;</span>: {
        <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">16</span>,
        <span style="color:#f92672">&#34;hits&#34;</span>: [
          {
            <span style="color:#f92672">&#34;_type&#34;</span>:    <span style="color:#e6db74">&#34;comment&#34;</span>,
            <span style="color:#f92672">&#34;_id&#34;</span>:      <span style="color:#ae81ff">5</span>,
            <span style="color:#f92672">&#34;_score&#34;</span>:   <span style="color:#ae81ff">2.79</span>,
            <span style="color:#f92672">&#34;_source&#34;</span>: {
              <span style="color:#f92672">&#34;body&#34;</span>:   <span style="color:#e6db74">&#34;Full text search is the bomb&#34;</span>
            }
          },
          { <span style="color:#960050;background-color:#1e0010">...</span> },
          { <span style="color:#960050;background-color:#1e0010">...</span> }
        ]
      }
    }
  }
]
<span style="color:#960050;background-color:#1e0010">...</span>
</code></pre></div><p><code>inner_hits</code>部分は、第2の検索リクエストに似ています。
<code>size</code>や<code>from</code>パラメータを含めるくことで、挙動をカスタマイズできます。
また、検索から想像するであろう、ページネーション、ソート、ハイライト、<code>_source</code>フィルタリングなどといった機能もサポートします。</p>
<p>Inner hitsはparent-childおよび、<code>nested</code>ドキュメントをサポートします。
この機能は、現時点では<code>experimental</code>ラベルが付与されています。
このラベルは、この機能が将来変更されたり、削除されたりする可能性があるかもしれないことを意味します。
詳細については、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.5/search-request-inner-hits.html">Inner Hits documentation</a>をごらんください。</p>
<h3 id="shadow-replicas">Shadow replicas</h3>
<p>Elasticsearchはそれ自身の冗長性に常に気をつけています。
それは、レプリカシャード（各プライマリシャードの冗長なコピー）を持っています。
これは、プライマリシャードを失った時に、データをロスしないようにするためのものです。
レプリカシャードはまた、検索のスループットをスケールアウトするためにも利用できます。
多くのレプリカ（ノードを伴うことで。）はスループットを増加させます。</p>
<p>しかし、ユーザによってはElsticsearchを分散ファイルシステム上でホスティングしており、すでに、
ファイルシステムがレプリケーションと冗長性を担当しています。
ファイルシステムが同じことしているので、各シャードのコピーを複数持つことはあまり意味がありません。</p>
<p>Shadowレプリカはノードを追加することによる検索スループットをスケールアウトすることが、
余分なストレージやインデキシングのコストを払うことなく、可能になります。
代わりに、各シャドーレプリカはプライマリシャードを持っている共有ファイルシステムにread-onlyでアクセスします。
Shadowレプリカは定期的にファイルシステムのビューをリフレッシュし、プライマリシャードのどんな変更も検知するでしょう。</p>
<p>プライマリシャードが失敗したら、Shadowレプリカがプライマリに昇格し、
失敗したプライマリによって書き込まれたトランザクションログを読み込みリプレイできます。</p>
<p>この機能は<em>experimental</em>マークが付いています。詳細については<a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.5/indices-shadow-replicas.html">Shadow Replicas documentation</a>をごらんください。</p>
<h3 id="resiliency-improvements">Resiliency improvements</h3>
<p>Elasticsearch 1.1 から 1.3では、インデックスのすべてのファイルのチェックサムを追加し、
それらのファイルが壊れているかどうかをチェックするために利用することにフォーカスしました。
1.4では、Zen discoveryと分散モデルについて大きな改良を加えました。</p>
<p>これらの変更にともなう、より詳細な統計情報やより詳細なロギングがElasticsearchやLuceneの以前のバージョンに存在した
未知の問題を明るみに出しました。
Elasticsearch 1.5.0では、これらの問題の多くに対処しています。</p>
<ul>
<li>
<p>ElasticsearchとLuceneの以前のバージョンにあるバグがインデックスの故障を引き起こしていました。<br>
チェックサムコードのおかげで、これらを発見できました。現在は、Elasticsearchの起動時に自動的にLucene3.x<br>
(Elasticsearch 0.20.x以前)が作成したセグメントを検知して、シャードをオープンする前に、新しいフォーマットを使って、
新しいコミットポイントを書き出します。(<a href="https://github.com/elastic/elasticsearch/pull/9899">#9899</a>)</p>
</li>
<li>
<p>1.3.xもしくは以前のバージョンからローリングアップグレードは、ローカルのシャードデータを再利用しようとせずに、<br>
シャード全体をコピーしようとします。1.3.2と以前のバージョンが実行されているノードからローリングアップグレードすることは
圧縮をオフにしない限りできなくなりました。(<a href="https://github.com/elastic/elasticsearch/pull/9925">#9925</a>)<br>
1.3.xやそれ以前のバージョンからアップグレードする場合、ローリングアップデートする代わりにクラスタの再起動を考えたほうがいいかもしれません。</p>
</li>
<li>
<p>非同期環境は予測することが難しいです。時に、最も予測していないことが起きるからです。<br>
シャード配置、リカバリ、削除のコードの多くが単純化され、状態変更をよりアトミックで決定的にするための変更によりリファクタリングされました。<br>
(<a href="https://github.com/elastic/elasticsearch/pull/8720">#8720</a>, <a href="https://github.com/elastic/elasticsearch/pull/9799">#9799</a>, <a href="https://github.com/elastic/elasticsearch/pull/9784">#9784</a>, <a href="https://github.com/elastic/elasticsearch/pull/9801">#9801</a>, <a href="https://github.com/elastic/elasticsearch/pull/9083">#9083</a>, <a href="https://github.com/elastic/elasticsearch/pull/8579">#8579</a>, <a href="https://github.com/elastic/elasticsearch/pull/8436">#8436</a>, <a href="https://github.com/elastic/elasticsearch/pull/8092">#8092</a>, <a href="https://github.com/elastic/elasticsearch/pull/9902">#9902</a>, <a href="https://github.com/elastic/elasticsearch/pull/6644">#6644</a>, <a href="https://github.com/elastic/elasticsearch/pull/8350">#8350</a>, <a href="https://github.com/elastic/elasticsearch/pull/9770">#9770</a>, <a href="https://github.com/elastic/elasticsearch/pull/9616">#9616</a>, <a href="https://github.com/elastic/elasticsearch/pull/9439">#9439</a>, <a href="https://github.com/elastic/elasticsearch/pull/8350">#8350</a>, <a href="https://github.com/elastic/elasticsearch/pull/8494">#8494</a>)</p>
</li>
<li>
<p>同様に、変更はクラスタ状態の更新が常に前進するということを確実にしました。更新の受け取り順序が順不同であったり、<br>
マスターだったノードからの更新を受け取った場合に混乱させていました。
(<a href="https://github.com/elastic/elasticsearch/pull/9632">#9632</a>, <a href="https://github.com/elastic/elasticsearch/pull/9541">#9541</a>, <a href="https://github.com/elastic/elasticsearch/pull/9503">#9503</a>)</p>
</li>
<li>
<p>チェックサムとチェックサムのバリデーションの強化(<a href="https://github.com/elastic/elasticsearch/pull/8723">#8723</a>,<br>
<a href="https://github.com/elastic/elasticsearch/pull/8599">#8599</a>, <a href="https://github.com/elastic/elasticsearch/pull/8587">#8587</a>, <a href="https://github.com/elastic/elasticsearch/pull/8407">#8407</a>, <a href="https://github.com/elastic/elasticsearch/pull/8010">#8010</a>, <a href="https://github.com/elastic/elasticsearch/pull/8018">#8018</a>)</p>
</li>
<li>
<p>disk threshold allocation deciderを速く(<a href="https://github.com/elastic/elasticsearch/pull/8803">#8803</a>)、賢く(<a href="https://github.com/elastic/elasticsearch/pull/7785">#7785</a>)、自動化(<a href="https://github.com/elastic/elasticsearch/pull/8270">#8270</a>)</p>
</li>
<li>
<p>auto-generated IDの利用時のインデキシングのスピードアップのためのに追加された最適化を除去。<br>
たまにドキュメントを重複して登録するため(<a href="https://github.com/elastic/elasticsearch/pull/7729">#7729</a>)</p>
</li>
</ul>
<h3 id="download-now">Download now</h3>
<p>ぜひ、<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-0">Elasticsearch 1.5.0</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elastic">@elastic</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elastic/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>#elasticon に参加中</title>
      <link>https://blog.johtani.info/blog/2015/03/11/attend-elasticon/</link>
      <pubDate>Wed, 11 Mar 2015 16:46:31 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/03/11/attend-elasticon/</guid>
      <description>サンフランシスコからこんにちは。 今、私は、弊社初のユーザカンファレンスelastic{ON}に 参加するために、初のアメリカ出張中です。 初のユ</description>
      <content:encoded><p>サンフランシスコからこんにちは。</p>
<p>今、私は、弊社初のユーザカンファレンス<a href="http://www.elasticon.com">elastic{ON}</a>に
参加するために、初のアメリカ出張中です。
初のユーザカンファレンスですが、世界各国から約1300人！の登録がありました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:500">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150311/elasticon.jpg" />
    </div>
    <a href="/images/entries/20150311/elasticon.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<!-- more -->
<p>本日が初日（Welcome Receptionが昨晩開催されましたが）です。
初日のキーノートで重要な発表が2つありました。</p>
<ul>
<li>ブランド名の変更（<a href="https://www.elastic.co/blog/elastic-you-know-for-more-than-search">Elastic: For - You Know, More Than Search</a>）</li>
<li>Found.noの加入（<a href="https://www.elastic.co/blog/welcome-found">Welcome Found</a>）</li>
</ul>
<p>です。</p>


<div class="box" style="max-width:500">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150311/elastic_sticker.jpg" />
    </div>
    <a href="/images/entries/20150311/elastic_sticker.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>昨日までは、elasticsearchでした。
本日からは、<a href="elastic.co">elastic</a>になります。
ブランド名の変更と同時にサイト、ロゴなども変更されました。</p>
<p>もう一つのビッグニュースがFoundの加入です。
found.noはElasticsearchをクラウドサービスとして提供している会社です。
彼らがジョインすることで、elasticsearchをより気軽に利用できるような環境ができてきます。</p>
<p>Keynoteでの驚きのニュースこれらでした。
もちろん、このカンファレンスはそれだけには止まりません。</p>
<p>NetflixやGitHub、Verison Mobile、WikimediaといったElasticsearch,ELKスタックのユーザの話や、
弊社の人たちによる今後のロードマップや私たちの考え方など多岐にわたる話を聞くことができます。</p>
<p>このような機会を今後も提供できるような会社になれるよう、頑張っていきたいなと思っています。</p>


<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150311/heineken.jpg" />
    </div>
    <a href="/images/entries/20150311/heineken.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>ちなみに、弊社のカンファレンスは私がこれまで経験したことのないカンファレンスになっています。
DJや各種ゲーム（ビリヤードとかパックマンとか）が楽しめるようなパーティが
懇親会として開催されたり、スポンサーブースでウェルカムレセプションが行われたりと、
面白い取り組みにあふれています。
次回開催されることがあれば、ぜひ日本の方達にも参加してもらえたらうれしいなと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>セキュリティ向けプラグインShieldのリリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/02/27/you-know-for-security-shield-goes-ga-ja/</link>
      <pubDate>Fri, 27 Feb 2015 18:49:56 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/27/you-know-for-security-shield-goes-ga-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：you know, for security: shield goes ga 1/27にShield 1.0 をリリースしました。 Elasticsearc</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/you-know-for-security-shield-goes-ga/">you know, for security: shield goes ga</a></p>
<p>1/27にShield 1.0 をリリースしました。
Elasticsearch向けの私たちのセキュリティプラグインの最初のリリースです。
11月にShieldについてアナウンスしてから、Elsaticsearchのためのセキュリティの機能は、
一般的に望まれているものから始まり、具体的な考えと実行できる計画へと変遷し、それが、いま現実となりました。</p>
<p>十分にセキュアな環境に、Elasticsearchクラスタをセキュアな状態でデプロイできるようにするため、
私たちは継続的にカスタマーやユーザーからのリクエストを受け取り、統合されたソリューションになるようにしてきました。</p>
<p>私たちは、そのようなプロダクトがどうあるべきか調査することから始め、
カスタマーとユーザが必要とするセキュリティとはどんなものかを理解するために多くの時間を費やしました。
その結果がShieldです。
Shieldは、ElasticsearchクラスタをセキュアにするElasticsearchの有償プラグインです。
私たちは、ShieldをDev、Gold、Platinumサブスクリプションの一部として、追加料金なしで提供します。</p>
<p>最初のリリースでは、基本的な機能と基盤にフォーカスしています。
Elasticsearch自身に対しても、セキュリティに対して準備してきました。
拡張性の側面だけでなく、Elasticsearchにあるデータフローについても再考してきました。
Elasticsearchクラスタをセキュアにする場合に、具体的な価値を即座に届けるだけでなく、素早く拡張できるようにも開発しました。</p>
<!-- more -->
<h3 id="機能">機能</h3>
<p>Shield 1.0は次の5つにフォーカスしています。</p>
<ul>
<li>認証(Authentication)</li>
<li>認可(Authorization)</li>
<li>暗号化通信とノードの認可(Encrypted Communication &amp; Node Authentication)</li>
<li>IPフィルタリング</li>
<li>監査証跡(Audit Trail)</li>
</ul>
<h3 id="認証authentication">認証(Authentication)</h3>
<p>セキュリティの大部分はアイデンティティについてです（例えば、だれがこのAPIを呼び出したのか？システムに何のサービスが接続するか？など）
サービスのライフタイムのある時点で、サブジェクト（例えばユーザー）を現在実行中のサブプロセスなどに結びつけることです。
この関係性を持つためには、サブプロセスを実行する前にユーザの身元を確認するように命じます。
ユーザの身元の確認のプロセスをAuthenticationと呼び、ElasticsearchのすべてのAPIコールでそれが実行されます。</p>
<p>認証の手法は多くの異なるものがあります。
それぞれの手法は、ユーザが認証されたという資格（Authentication Token）を、それぞれのタイプで提供するようにユーザに要求します。
Shield 1.0ではシンプルに、必要なauthentication tokenをユーザ/パスワードペアとしています。
（これは、Shieldの認証基盤が簡単に拡張でき、将来は異なるauthentication tokenもサポートできることを意味します。）</p>
<p>ユーザの資格を受け取ることだけでは不十分で、次に、それらをチェックする必要があります。
Shieldでは、これはレルムの責務です。
レルムは認証プロバイダ/サービスとしてみることができます。
妥当なユーザであると判断/解決されたか、
authentication tokenが適切な資格を持っていない/単に知らないユーザであるということで、拒否されたかです。
Shieldの認証メカニズムでは、複数のレルムを設定でき、さらに、あるレルムの戻り値を扱う他のレルム、というようなchainとすることもできます。
Shield 1.0は3つのレルムをサポートします。</p>
<ul>
<li>esusers - Elasticsearchによって管理されるファイルベースのレルムです。
これは、ファイルにユーザを定義することができます。（Apacheサーバのhtpasswdファイルのようなもの）
このレルムは外部への依存はなく、Shieldをインストールすれば、デフォルトで使用できます。
このレルムは配置が簡単で、マルチテナントなElasticsearchクラスタに対して使用できます。
マルチテナントなElasticsearchクラスタとは、クラスタを複数のアプリでシェアすることをテナントと言います。
また、すべてのユーザがパスワードを忘れてしまうような&quot;emergency&quot;な代替レルムも対応可能です。
(誰もシステムに入れないような状況のことです)</li>
<li>LDAP - 外部のLDAPサーバでユーザを認証するレルムです。
このレルムは組織のLDAPサーバで管理/保存されているユーザをすでに持っている組織を対象としています。</li>
<li>Active Directory - LDAPのタイプの1つで、Active Directoryに対する設定になります。</li>
</ul>
<p>レルムはelasticsearch.yml設定ファイルで、次のように設定可能です。</p>
<blockquote>
<p>shield.authc
realms:</p>
<pre><code>    esuser:
        type: esusers
        order: 0

    ldap:
        type: ldap
        order: 1
        url: ldaps://url/to/ldap1/server

    ldap_fallback:
        type: ldap
        order: 2
        url: ldaps://url/to/ldap2/server
</code></pre>
</blockquote>
<p>上記のようにrealmsが一つのチェインとして参照されます。
レルムごとに、設定された順序で、それらは参照されます。</p>
<p>NOTE : Shieldには、esusersファイルに保存されたユーザを管理するためのコマンドラインツールもあります。</p>
<h2 id="認可authorization">認可(authorization)</h2>
<p>認可(Authorization)は保護されたリソースにアクセスするユーザを許可するか拒否するかということです。
モダンなシステムは、ユーザのパーミッションのために、ロールベースのアクセスコントロール（RBAC）モデルを利用します。
このモデルでは、各ユーザはロールの集合に関連していて、それぞれのロールには、パーミッションの集合が定義されています。
これは、洗練された設定で、パーミッションを機能的なグループで共有させることができます。
例えば、次のようなロールを定義したとします。</p>
<ul>
<li><em>employee</em> - すべての従業員は部門をまたいだ会社のデータへアクセスできます（例えば、コンタクトやディレクトリ情報など）</li>
<li><em>sales</em> - すべての営業職は営業データにアクセスできる（例えば、流通ルート、ルート、顧客）</li>
<li><em>finace</em> - すべての財務の従業員は財務データにアクセスできる（例えば、予算、経費、伝票）</li>
</ul>
<p>財務部門の<code>Ann</code>は従業員と財務のロールを持っており、会社のディレクトリと財務データにアクセスでできます。</p>
<p>認可プロセスはユーザがリクエストに関連したユーザが必要で、このプロセスのために、認証フェーズの後に直接実行されます。</p>
<p>Shieldは2つのタイプのリソースを定義します。クラスタとインデックスです。
これらは、すべてのAPIコールで保護されます。
さらに、それらに関連したパーミッションとロールも定義できます。
一度定義をすると、ロールはユーザもしくはLDAP/ADのグループに関係します。
ロールは<strong>roles.yml</strong>設定ファイルで定義されます。
設定のサンプルは次のようになります。</p>
<pre><code>admin:
    cluster: all
    indices:
        '*' : all

monitor:
    cluster: monitor
    indices:
        '*': monitor

employee:
    indices:
        'company_directory' : read

sales:
    indices:
        'opportunities' : read, write
        'accounts' : read, write

finance:
    indices:
        'expenses' : read, write
        'purchases' : read, write
</code></pre><p>上記のサンプルで、次の5つのロールを定義しています。</p>
<ul>
<li><em>admin</em> - 管理者のロールで、すべてのクラスターレベルの操作とすべてのインデックスに対してすべてのインデックスレベルの操作を実行可能です。
(¥*インデックスはすべてのインデックスにマッチするワイルドカード)</li>
<li><em>monitor</em> - システム/クラスタのモニタリングのためのロール。このロールのユーザはすべてのクラスタとインデックスレベルの情報の読み取りの
APIにアクセス可能だが、インデックスのデータへの読み書きや設定の更新は不能</li>
<li><em>employee</em> - compnay_directoryにあるすべてのデータへの読み取りアクセスを与えられたロール。このロールはクラスタレベルへのアクセスやデータの書き込みアクセスは持っていない
(特にcompany。洗濯されたグループの人々はcompanyディレクトリの更新は可能だが、employeeは読み取りのみが可能)</li>
<li><em>sales</em> - opportunitiesとaccountsインデックスの読み書きができるロール</li>
<li><em>finance</em> - expensesとpurchasesの両方に読み書きができるロール</li>
</ul>
<p>上記のサンプルで定義されている<code>all</code>と<code>read</code>と<code>write</code>として名前がつけられた権限です。
これらは、予約語で、Elasticsearchのローレベルのアクションを複数含んだ権限です。
（<code>write</code>は<code>index, delete, delete_by_query, bulk, update</code>の操作を含んでいます。）
多くのケースで、これらのハイレベルの名前が付けられた権限で十分ですが、特定のロールに特定のアクションを明示的に指定することもできます。
次のようになります。</p>
<pre><code>hr:
    indices:
        'company_directory' : indices:data/write/index, indices:data/write/update
</code></pre><p>ここまで説明した認可のレルムは、各ユーザに関連するロールを識別するためのものです。
内部のesuserレルムでは、提供されるesuserコマンドラインツールを使ってロールはユーザに割り当てたり変更したりもできます。
LDAPやActive Directoryでは、LDAP/ADグループにShieldのロールを割り当てることができます。</p>
<p>認証と認可の両方を用いることで、ユーザリクエストに対して、ユーザごとに許可/不許可をすることができます。</p>
<h3 id="暗号化通信">暗号化通信</h3>
<p>認可はElasticsearchのデータを機能的な観点（許可されたユーザだけが操作を可能にする）で保護しますが、
クライアントからElasticsearchクラスタへ、もしくはクラスタのノード間では暗号化されていないデータを送るためまだ危険があります。
第三者が登頂したり、オンザフライでデータを書き換えたりといった可能性やクラスタを壊すことができます。</p>
<p>Shield 1.0はElasticsearchのすべての通信チャネルをセキュアにすることができます。
クラスタ内のノード間のチャネルやクライアントに公開されているチャネルです。
これは、SSL/TLS通信を導入して実現します。</p>
<p>Shieldで使えるSSLはElasticsearchのtransportサービスをSSL/TLSで通信できるものに置き換えます。
これは、ノード間通信チャネルと、HTTP transport（REST APIを提供するもの）のそれぞれに設定可能です。</p>
<p>ShieldのSSL/TLSは、スタンダードなJavaのものとkeystoreとtruststoreを基本にしたものが利用可能です。
SSL/TLSを設定すると、各ノードのキーストアに証明書をインポートする必要があります。
CAがサインした証明書を使うことも、CAが信頼したものとして許可許諾されたものを使うことが可能です。
これは、信頼されたすべてのCAとして知られているtrust storeが必要です。
新しいノードをクラスタに追加するときに、すべての必要な少なくとも一つの信頼されたCAから発行されてサインされたものが必要になります。
クラスタで個別のノードがすべてのkeystore/truststoreを更新する必要性なしに。？？</p>
<p>通信チャネルを安全にする方法やSSL/TLS設定をどのように行うかは<a href="http://www.elasticsearch.org/guide/en/shield/current/securing-nodes.html">Shieldのドキュメント</a>をご覧ください。</p>
<h3 id="ノード間認証">ノード間認証</h3>
<p>強く推奨しますが、許可されたノードだけがクラスタに接続できるようにするために、ノードの認証をSSL transportに設定することができます。
これは、<code>shield.transport.client.auth</code>に<code>true</code>を設定することで可能です。
設定した場合、ノード間でSSLハンドシェイクが行われ、接続されたノードが接続に来たノードのクライアント認証を要求しチェックします。
もし、チェックに失敗した場合は、SSLシェイクハンドが失敗し接続が拒否されます。</p>
<h3 id="sslクライアント認証">SSLクライアント認証</h3>
<p>transportレベルでノード認証が必要なようなら、次のような疑問がわくでしょう。
Elasticsearchはクラスタに接続するTransportクライアントを使うときはどのように振る舞うのか？
Transportクライアントはクラスタの他のノードと同じチャネルを使うため、コネクションを確立するときに、ノードが他のノードと異なるかどうかを見極めることはできません。</p>
<p>この時、もっとも単純な解決は、Transportクライアントも同様に許可を与えることです。
それは、認証を解決するときに、他の問題（潜在的な悪意）を引き起こします。
Transportクライアントが他のクラスタのノードを偽装しようとすることです。これは望んでいません。</p>
<p>幸いなことに、良い解決方法があります。
トランスポートプロファイルです。
Elasticsearch 1.4で導入されたトランスポートプロファイルは、トランスポートレイヤー（異なるホスト/ポートにバインドされる）のために複数のネットワークチャネルを設定することができます。
Shieldはこのサポートを、プロファイルごとに異なるSSL設定をできるように拡張します。
また、ノードのタイプとクライアントプロファイルタイプの間に明確な違いを設定することも可能です。
これを用いると、2つのプロファイルを設定できるようになります。
ひとつは、クライアントのためのもので、もうひとつはクラスタのノードのためのものです。
これにより、クライアントのための認証の問題が必要なくなり、Shieldはクライアントプロファイルをもった限定されたクライアントからのリクエストを保証します。</p>
<h3 id="ipフィルタリング">IPフィルタリング</h3>
<p>これは、厳密には、認証カテゴリではありませんが関係しています。
Shieldはそれ自身がIPフィルタリングのメカニズムを持っています。
これは、許可/不許可のIPのリストを設定することができます。
これらのフィルタリングのルールは複数のレベルで設定可能です。
transportチャネル、transportプロファイルレベル、そして、HTTPチャネルです。
次の設定は、それらの設定のサンプルです。（設定ファイルはelasticsearch.ymlになります）</p>
<pre><code>shield:

    transport.filter:
        allow:
            - '127.0.0.1'
            - '2001:0db8:1234:0000:0000:8a2e:0370:73
        deny:
            - '10.0.0.0/8'
            - '2001:0db8:1234::/48'
            - '*.google.com'

    http.filter:
        allow: [ '10.0.0.0/8' ]
        deny: [ '127.0.0.1' ]

transport.profiles:
    client:
        shield.filter.deny: [ '_all' ]
</code></pre><p>このように、IPv4とIPv6、CIDR、ホスト名、ワイルドカードが利用できます。
また、この機能はホストOSのIPテーブルに設定することで追加できるが、Shieldにそれを保持し、それらの設定を単純化し、
デプロイの全体から除去できることにも注意してください（詳細は<a href="http://www.elasticsearch.org/guide/en/shield/current/ip-filtering.html">ドキュメントのIPフィルタリングをご覧ください</a>）。</p>
<h3 id="監査証跡audit-trail">監査証跡（Audit Trail）</h3>
<p>セキュアなシステムの必須機能の一つで、監査硝石により、Elasticsearchに発生した重要なイベントをトラッキングすることが可能です。
これらのイベントを保存することは、Elasticsearchクラスタの重要なアクティビティの証拠を提供でき、
不審な/悪意のある可能性のあるイベントを追跡するときの診断ツールにもなります。</p>
<p>Shield 1.0.0で、監査証跡は監査/アクセスlogを一般的なElasticsearchのログとは個別に保存します。
それらは、構造化されているため、読んだりパースするのが容易で、イベントのタイプも分類されています。
また、情報のレベルを設定することができ、各イベントをlogレベルの設定で書き出すことができます。
以下が、イベントのリストです。</p>
<ul>
<li><em>anonymous_access_denied</em> - 認証トークンがないユーザからのリクエストがあった時のログ</li>
<li><em>authentication_failed</em> - リクエストされたユーザの認証に失敗した時のログ</li>
<li><em>access_denied</em> - 認証されたユーザが許可されていない操作を実行した時のログ</li>
<li><em>access_granted</em> - 認証されたユーザが許可された操作を実行した時のログ</li>
<li><em>tampered_request</em> - 不正に書き換えられたリクエストが到着したのを検知した時のログ</li>
<li><em>connection_granted</em> - ノードもしくはtransportクライアントがIPフィルタのルールにパスした時のログ</li>
<li><em>connection_denied</em> - ノードもしくはtransportクライアントがIPフィルタリングルールの制限により却下された時のログ</li>
</ul>
<p>Shieldの監査証跡についてより詳しく知りたい方は、<a href="http://www.elasticsearch.org/guide/en/shield/current/auditing.html">ドキュメント</a>をごらんください。</p>
<h3 id="次のバージョンでは">次のバージョンでは？</h3>
<p>ここまで紹介したように、これはまだ始まりにすぎません。
Shieldに追加される多くの機能があり、しっかりとした基盤を構築したところです。
Shieldの次のバージョンでは、以下の機能の追加にフォーカスするでしょう。（これらだけに限ったわけではありません。）</p>
<ul>
<li>APIによる設定、管理</li>
<li>より拡張され、柔軟なLDAP/Active Directoryサポート</li>
<li>レルムタイプの追加（kerberos、anonymous、certificatesなどなど）</li>
<li>セッションベースの認証</li>
</ul>
<p>ShieldはElasticsearch社の2番目の（<a href="http://www.elasticsearch.com/products/marvel?_ga=1.40546982.567962035.1389706748">Marvel</a>に続く）商用プロダクトです。
ダウンロードして開発環境で評価してください。
インストールは他のプラグインと同様の方法です（インストール方法についての詳細は<a href="http://www.elasticsearch.org/guide/en/shield/current/index.html">こちら</a>）。
一度インストールすると、30日の試用ライセンスが始まります。
もし、さらに時間が必要な場合は、sales@elasticsearch.comまで連絡してください。</p>
<p>私たちのすべてのプロダクトについてフィードバックをお待ちしています。
Shieldの商用利用、機能、ロードマップ、その他のセキュリティに関するトピックなど、質問がありましたら、
<a href="http://www.elasticsearch.com/contact/?_ga=1.32250170.567962035.1389706748">サイトからご連絡ください</a>。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch Coreトレーニング開催</title>
      <link>https://blog.johtani.info/blog/2015/02/27/2nd-tokyo-training/</link>
      <pubDate>Fri, 27 Feb 2015 17:10:29 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/27/2nd-tokyo-training/</guid>
      <description>4月13日から3日間、ElasticsearchのCoreトレーニングが東京で開催されます。 Early Birdということで、3/14までに申し込みす</description>
      <content:encoded><p><a href="https://purchases.elasticsearch.com/class/Tokyo/2015-04-13">4月13日から3日間、ElasticsearchのCoreトレーニング</a>が東京で開催されます。
Early Birdということで、3/14までに申し込みすると割引があります。
興味のある方は、見ていただければと。</p>
<!-- more -->
<p>また、4/15にElasticsearch勉強会を開催します。
トレーニングに弊社のエンジニアが来日しますので、なにか話をしてもらう予定です。</p>
<p>募集は後日、<a href="https://elasticsearch.doorkeeper.jp">Elasticsearch勉強会</a>のDoorkeeperで行います。
興味のある方は、登録しておいていただければと。</p>
<p>トレーニングや勉強会でお待ちしております。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Kibana 4（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/02/20/kibana-4-literally-ja/</link>
      <pubDate>Fri, 20 Feb 2015 14:05:52 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/20/kibana-4-literally-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：kibana 4. literally. Kibana 4は現在、文字通り、抽象的に、概念的に、精神的に、そしてとても楽しく</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/kibana-4-literally/">kibana 4. literally.</a></p>
<p>Kibana 4は現在、文字通り、抽象的に、概念的に、精神的に、そしてとても楽しく、プロダクションレディになりました。
1週間前に準備はできていましたが、満足できるものであるという確信を得たいと思っていました。
そして、Kibana 4.0.0 GAをリリースしました。
次のものはサンプルのスクリーンショットと前日譚です。
これらに興奮してしまった方のために、2ステップのプランを用意しました。</p>
<ol>
<li>ダウンロードする：<a href="http://www.elasticsearch.org/overview/kibana/installation/">Kibana 4 downloads</a>ページからダウンロードします。</li>
<li>理解する：<a href="http://www.elasticsearch.org/guide/en/kibana/current/index.html">Kibana 4 docs</a>ページを読んで理解します。</li>
</ol>
<p>Tip : もし、まだ、あなたのクラスタが<a href="http://www.elasticsearch.org/downloads/1-4-4/">Elasticsearch 1.4.4</a>でない場合は、アップグレードする必要があります。<br>
Tip2 : Kibana 4 RC1からアップグレードする場合は、configを移行する必要があります。<a href="https://gist.github.com/spalger/8daf6c2b7f2954639e38">こちらのgistを参照</a></p>
<!-- more -->
<h2 id="前日譚---the-back-story">前日譚 - the back story</h2>
<p>Kibanaはすでに問題解決のためのツールになっています。
なぜ、毎晩2時に呼び出されるんでしょう？
そのコードがプロダクションに入ったのはいつですか？
その結果、何を壊しました？
私たちはそれらを解決しました。
世界的に、長い間、だれも夜中の2時に呼び出されませんでした。知ってます？。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="https://www.elastic.co/assets/blt5dddb0351d09a397/Screen-Shot-2015-02-17-at-1.25.15-PM-1024x692.png" />
    </div>
    <a href="https://www.elastic.co/assets/blt5dddb0351d09a397/Screen-Shot-2015-02-17-at-1.25.15-PM-1024x692.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>*しかし、ここには落とし穴があります。*答えが簡単になれば、問題が難しくなります。
楽な勝利は簡単でした。では、難しい問題（深さが3層の問題）を解きましょう。
複数の要素、複数のフィールドそして、複数のデータソースを分析する必要がある問題を解きましょう。
Kibana 4は少ない時間と労力で最も難しい問題を解決してくれます。</p>
<p>Kibana 3で学んだことをKibana 4に取り込みました。
なぜ10億のデータを持っているのに、地図には1000個しかプロットできないのでしょう？
1つのチャートに1つのフィールドなんでしょう？
なぜ、1つのパネルに1つのチャートなんでしょう？
なぜ、1つのダッシュボードに1つのインデックスなんでしょう？
5つのシナリオを用意し、2つのフィールドにまたがったデータを比較し、
1つのダッシュボードに3つのインデックスのデータを表示してみましょう。
さぁ、やりましょう。終わったらアイスクリーム（トッピング付きの）を取りに行きましょう。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="https://www.elastic.co/assets/blte1c56cdf8c51674d/Screen-Shot-2015-02-17-at-1.24.14-PM-1024x624.png" />
    </div>
    <a href="https://www.elastic.co/assets/blte1c56cdf8c51674d/Screen-Shot-2015-02-17-at-1.24.14-PM-1024x624.png" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="the-plot">the plot</h2>
<p>アイスクリームのように、問題には多くの種類があります。
そのために、Kibanaをナポリ風アイスクリーム（3色アイス）のように分割しました。
嫌いな種類は除いて。
もし、あなたがKibanaのユーザ歴が長い場合、最初のタブの<em>Discover</em>がホームであることが正しく感じるでしょう。
これにより、短時間で、検索し、レコードを見つけ、簡単な問題を解決できます。
簡単な問題とは、すべてを物語る1行のデータを見つけることによって解決する問題です。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="https://www.elastic.co/assets/blta5b9c4d326d1bab7/Screen-Shot-2015-02-17-at-1.55.18-PM1-1024x573.png" />
    </div>
    <a href="https://www.elastic.co/assets/blta5b9c4d326d1bab7/Screen-Shot-2015-02-17-at-1.55.18-PM1-1024x573.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>物事が簡単な検索で説明できるものよりも複雑になった時、チャートとグラフで魔法を作る時間です。
<em>Visualize</em>タブを開き、Elasticsearchのaggregationの力を利用してデータを解析しましょう。
<em>Visualize</em>は複数の次元の性質のデータを見せ、今まで尋ねたことがないような質問に対して素早く回答するチャートやテーブル、
地図を作成できます。
あなたが最初に尋ねる質問は「先週サイトが遅かったのはなぜ？」でした。
しかし、データによって明らかにされた質問は「なぜ、クリスマスに東京からの平均ファイルサイズリクエストがスパイクしたのか？」です。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="https://www.elastic.co/assets/blt8458a51cb72ffdc9/Screen-Shot-2015-02-18-at-11.13.37-AM-1024x617.png" />
    </div>
    <a href="https://www.elastic.co/assets/blt8458a51cb72ffdc9/Screen-Shot-2015-02-18-at-11.13.37-AM-1024x617.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>最後に、<em>Dashboard</em>でこれらを1つにします。</p>
<p>大きなスクリーンに配置して、こう言います。
「あなたの答えはこのリンクにあります。また、Wikiに埋め込んで、データをCSVにエクスポートしてメールしました。
アイスクリームを食べた後に、自叙伝の第1章を書きました。もっとアイスを持ってきてください。かき混ぜますから。」</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="https://www.elastic.co/assets/blte214158911741112/Screen-Shot-2015-02-17-at-3.30.30-PM-1024x715.png" />
    </div>
    <a href="https://www.elastic.co/assets/blte214158911741112/Screen-Shot-2015-02-17-at-3.30.30-PM-1024x715.png" itemprop="contentUrl"></a>
  </figure>
</div>

<p>それぞれのタブで見てきた詳細については、<a href="http://www.elasticsearch.org/blog/kibana-4-beta-1-released/">Kibana 4 Beta 1 : Released</a>をごらんください。</p>
<h2 id="to-be-continued">to be continued&hellip;</h2>
<p>居眠りをする時間はあります？いいえ、Kibana 4.1についてすでに作業中で、将来の大きなプランを持っています。
多くの労力はKibana 4の土台の安定と実用性を構築することに使われました。
また、Elasticsearchアプリケーションの将来を構築するプラットフォームを作りました。
すべてのものは拡張できるように設計されています。
例えば、可視化はより良くなるように構築されています。
オープンソースは私たちのGitHubアカウント以上のものです。
それは、新しく素晴らしいものを誰もが作ることができる構造を作ることが私たちの約束です。</p>
<p>Kibanaでグラフなどを構築したり、Elasticsearchを利用したアプリケーションを作成するために、
私たち開発者のブログを参考にしてください。
ちょっと見てみたいですか？
Elastic{ON}15のSpencer Algerのトークをチェックしてください。</p>
<p>あなた方なしでは、私たちはここにはいないですし、あなた方の助けがなければ何もできません。
ぜひ、GitHubでのissueや提案、貢献をお待ちしています。
もしくは、IRCでFreenodeの#kibanaに参加してください。</p>
<h2 id="extra-credit">extra credit</h2>
<p>Kibana 4のすべての話に興味がありますか？私たちのKibana 4ベータに関する過去のブログをチェックしてください。</p>
<ul>
<li><a href="http://www.elasticsearch.org/blog/kibana-4-beta-1-released/">Kibana 4 Beta 1: Released</a></li>
<li><a href="http://www.elasticsearch.org/blog/kibana-4-beta-2-get-now/">Kibana 4 Beta 2: Get it now</a></li>
<li><a href="http://www.elasticsearch.org/blog/kibana-4-beta-3-now-more-filtery/">Kibana 4 Beta 3: Now more filtery</a></li>
<li><a href="http://www.elasticsearch.org/blog/kibana-4-rc1-is-now-available/">Kibana 4 RC1: Freshly baked</a></li>
</ul>
<p>最後に、Kibanaの利用に関する話をお持ちなら、ぜひ聞かせてください。
stories at elasticsearch dot comもしくは<a href="http://www.twitter.com/elasticsearch">@elasticsearch</a>に連絡をください。
あなたの話を世界にどのようにシェアしているかごらんください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.4.4および1.3.9リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/02/20/elasticsearch-1-4-4-and-1-3-9-released-ja/</link>
      <pubDate>Fri, 20 Feb 2015 14:05:34 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/20/elasticsearch-1-4-4-and-1-3-9-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：elasticsearch 1.4.4 and 1.3.9 released 本日（2/20）、Elasticsearch 1.4.</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-4-and-1-3-9-released/">elasticsearch 1.4.4 and 1.3.9 released</a></p>
<p>本日（2/20）、<strong>Elasticsearch 1.4.4</strong>と<strong>Elasticsearch 1.3.9</strong>をリリースしました。
これはバグフィックスリリースとなります。
主に、Lucene expression scriptsを使う場合のRPMとDEBパッケージの
パッケージング問題のフィックスをしたものです。
<a href="http://www.elasticsearch.org/downloads/1-4-4">1.4.4のダウンロードこちらのリンク</a>からアクセスできます。</p>
<!-- more -->
<h2 id="fixes">fixes</h2>
<p>1.4.3のRPMおよびDEBパッケージにはAntlrとASMの<a href="https://github.com/elasticsearch/elasticsearch/pull/9696">依存関係の不足</a>がありました。
この依存はElasticsearchでLucene expression scriptsを利用する場合に必要になります。
Groovyに関する<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-3-and-1-3-8-released/">1.4.3の変更</a>により、多くのユーザがLucene explression scriptsを利用することが予想されるため、すぐに、1.4.4をリリースしました。</p>
<p>また、このリリースには、クラスタの保留タスクに関するいくつかのバグフィックスも含まれています。
さらに、date histogramで負のインターバルの場合に<code>OutOfMemoryError</code>を引き起こすバグも
修正されています。</p>
<p>すべての変更については<a href="http://www.elasticsearch.org/downloads/1-4-4">1.4.4のリリースノート</a>および<a href="http://www.elasticsearch.org/downloads/1-3-9">1.3.9のリリースノート</a>をごらんください。</p>
<h2 id="フィードバック">フィードバック</h2>
<p>私たちはフィードバックをお待ちしています。
Twitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)もしくは<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で教えてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>名古屋でElasticsearch勉強会を開催します</title>
      <link>https://blog.johtani.info/blog/2015/02/18/preparing-elasticsearch-meetup-in-nagoya/</link>
      <pubDate>Wed, 18 Feb 2015 15:22:53 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/18/preparing-elasticsearch-meetup-in-nagoya/</guid>
      <description>来る、4月4日の土曜日の午後に名古屋でElasticsearch勉強会を開催予定です。 「初」の東京以外の勉強会です。 Twitterでこのよう</description>
      <content:encoded><p>来る、4月4日の土曜日の午後に名古屋でElasticsearch勉強会を開催予定です。
「初」の東京以外の勉強会です。</p>
<!-- more -->
<p>Twitterでこのようなツイートを見かけまして。</p>
<blockquote class="twitter-tweet" lang="ja"><p>名古屋でElasticsearchの勉強会やりたい機運（今のところ2人）。</p>&mdash; mogami (@smogami) <a href="https://twitter.com/smogami/status/562864387632136192">2015, 2月 4</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>これは！ということで、名古屋で勉強会をやろうかと思います。30名程度の場所を借りて実施予定です。
募集はいつもの、<a href="http://elasticsearch.doorkeeper.jp">elasticsearch勉強会のDoorkeeper</a>で行う予定です。
ページの準備まで少々待ちください。（おそらく、3月中旬くらい）
私自身はElasticsearchやELKについて話をしようと思っています。そのほかに、2,3名のスピーカーの方を予定しています。
LTなど興味がある人がいたら、連絡ください。</p>
<p>これを機に（？）他の場所でも勉強会を開催したいと考えています。
ニーズがどのくらいありそうなのかが、まだよくわかっていませんが、関西などでニーズがあるんじゃないかと期待していたり。</p>
<p>興味のある方は、コメント欄、Twitterなどでコンタクトしてもらえればと。
（連絡来るとうれしいなぁ。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>第8回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2015/02/16/8th-elasticsearch-jp/</link>
      <pubDate>Mon, 16 Feb 2015 15:02:23 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/16/8th-elasticsearch-jp/</guid>
      <description>第8回Elsticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、開場提供していただいたリクルートテクノロジーズさん</description>
      <content:encoded><p><a href="http://elasticsearch.doorkeeper.jp/events/19923">第8回Elsticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<!-- more -->
<p>今回は出足が好調で、早々に180人の枠を超えるという嬉しい事態でしたが、
キャンセル待ちが残っているにもかからわらず、来られていない方が67名もいるということで、キャンセル待ちの方には申し訳なかったです。
もうすこし、キャンセルをしてもらえると嬉しいんですが。。。
今回はメールを当日に1度しか打ってないからかなぁ。</p>
<p>さて、いつもの通り簡単なメモです。
本当に簡単にですが。</p>
<h2 id="elasticsearch導入チェックリスト-elasticsearch株式会社-jun-ohtani-johtani">「Elasticsearch導入チェックリスト？」 Elasticsearch株式会社 Jun Ohtani @johtani</h2>
<p>スライド：<a href="https://speakerdeck.com/johtani/elasticsearchdao-ru-tietukurisuto">Elasticsearch導入チェックリスト？</a></p>
<p>Elasticsearchを開発環境や本番に導入する前に気にかけて欲しいことについて発表しました。
元ネタは<a href="http://www.elasticsearch.org/webinars/elasticsearch-pre-flight-checklist/">elasticsearch pre-flight checklist</a>です。
少々古いのですが、私が今回話した内容以外にもモニタリングなどについての話も盛り込まれています。
時間がある方は、見ていただければと。</p>
<h2 id="elasticsearch-クエリとスキーマ定義のすごい細かい話株式会社ドワンゴ-藤堂淳也-さん">「Elasticsearch クエリとスキーマ定義のすごい細かい話」株式会社ドワンゴ 藤堂淳也 さん</h2>
<p>スライド：<a href="https://speakerdeck.com/jtodo/elasticsearch-kueritosukimading-yi-falsexi-kaihua">Elasticsearch クエリとスキーマ定義のすごい細かい話</a></p>
<ul>
<li>フィールドのチェックを別途インデキシングするアプリで行っている。利用できるものだけElasticsearchに投げる</li>
<li>実際に本番環境で利用しているマッピングに対してフィールドを追加する手順について</li>
<li>「これもドキュメントに書いてあるんですが」という感じでドキュメントに色々書いてあるので読みましょうというありがたい発表でした。</li>
</ul>
<p>実際に試行錯誤したり検証するときに行ったことを喋ってもらえたので、どういった点を気にしながら運用、設計するかというのがわかりやすかったです。</p>
<h2 id="elasticsearchとkibanaで実現する30億reqdayのリアルタイム分析株式会社サイバーエージェント山田直行さんsatully">「ElasticsearchとKibanaで実現する、30億req/dayのリアルタイム分析」株式会社サイバーエージェント　山田直行さん　@satully</h2>
<p>スライド：<a href="http://www.slideshare.net/Satully/elasticsearch8-elasticsearchkibana-30reqday">ElasticsearchとKibanaで実現する、30億req/dayのリアルタイム分析</a></p>
<p>会場が21時までしか抑えられていないという失態で、ドタバタしてて前半は聞けてないです。。。</p>
<ul>
<li>前回の発表では30日分Elasticsearchに入れていたが、今は3日分のみ保存</li>
<li>レポートなどにはRedshift＋Tableauを利用</li>
<li>Kibana3をメインに使っているが、Kibana4も検討予定？</li>
</ul>
<p>QA</p>
<ul>
<li>Q：なぜ、ELBを挟んでいるのか？
<ul>
<li>A：特に考えておいているわけではない。</li>
</ul>
</li>
<li>Q：インデックスの構成は？
<ul>
<li>A：1日に2つのインデックス。Bitされたもの、入札されないもの</li>
</ul>
</li>
<li>Q：searchのnodeをやめたのは？
<ul>
<li>A：前回発表した勉強会での懇親会で話を聞いたり、他の方と話を聞いて、不要と判断したため</li>
</ul>
</li>
</ul>
<h2 id="はてなのメディア面を支えるelasticsearch株式会社はてな山家雄介さんyanbe">「はてなのメディア面を支えるElasticsearch」株式会社はてな　山家雄介さん　@yanbe</h2>
<p>スライド：未定。おそらく、開発者ブログに公開されるかと。</p>
<p>* アドテク系にもやってるらしい。BrandSafeはてな</p>
<ul>
<li>はてなブックマークのデータを魅せ方を変える機能などで大活躍。<a href="http://bkuma.hatena.ne.jp">B!KUMA</a>とか</li>
<li>その日の話題の見出し自動生成機能。Significant Terms Aggregationsを利用。</li>
<li>こちらの<a href="http://bookmark.hatenastaff.com/entry/2015/02/05/190331">「自然言語処理技術を用いたはてなブックマークの新機能「トピック」をベータリリースしました」</a>エントリに関係あるのかな？</li>
</ul>
<p>記事の魅せ方を検索できる管理画面ではElasticsearchのクエリDSLを活用されているとのことでした。
検索専門の人でなくても検索式を簡単にくみたてられる画面を用意して、ElasticsearchのクエリDSLに変換するようにしていると。
確かに、クエリをそのまま組み立ててもらうよりも利用しやすい画面がある方がいいですよね。バックエンドはJSとPerlのライブラリとのことでした。</p>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<ul>
<li><a href="http://takudo.hatenablog.jp/entry/2015/02/14/101749">2015-02-13 第8回 elasticsearch 勉強会 @ 丸の内 リクルート 41Fアカデミーホール</a></li>
<li><a href="http://suzuki.tdiary.net/20150213.html"> [Elasticsearch] 第8回 elasticsearch 勉強会へ参加してきた</a></li>
<li><a href="http://qiita.com/t-sato/items/8e353ccf7d7bba46f635">第8回elasticsearch勉強会 #elasticsearch #elasticsearchjp</a></li>
<li><a href="http://keyamb.hatenablog.com/entry/2015/02/16/015916">第8回 Elasticsearch 勉強会に行ってきた #elasticsearch #elasticsearchjp</a></li>
<li><a href="http://blog.yoslab.com/entry/2015/02/13/203251">勉強会メモ - 第8回elasticsearch勉強会</a></li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>今回も検索からログまでいろんな話になったので、面白かったかと。
参加された方は新しい方が多かったんじゃないかなぁと。（集計結果で見れないのかな、Doorkeeper）。</p>
<p>今回は、みなさんに21時に41Fから33Fへ移動していただくという大失態があったので、大変申し訳なかったです。
次回（4月中旬）は、このようなことがないように気をつけますので、今後もよろしくお願いいたします。</p>
<p>あと、東京以外の勉強会も検討しつつあります。興味のある方はコメントやTwitterで反応をいただけると嬉しいです。</p>
<p>スピーカーは随時募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。 よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Groovyスクリプトをダイナミックスクリプトなしで実行(日本語訳)</title>
      <link>https://blog.johtani.info/blog/2015/02/12/running-groovy-scripts-without-dynamic-scripting-ja/</link>
      <pubDate>Thu, 12 Feb 2015 15:13:09 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/12/running-groovy-scripts-without-dynamic-scripting-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：running groovy scripts without dynamic scripting Elasticsearch1.3.8と1.4.3のリリースによ</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/running-groovy-scripts-without-dynamic-scripting/">running groovy scripts without dynamic scripting</a></p>
<p>Elasticsearch1.3.8と1.4.3のリリースにより、デフォルトで、リクエストに含まれるGroovyスクリプトや
インデックスに保存されたスクリプトを動的に実行する機能をオフにしました。
しかし、Groovyはまだデフォルトのスクリプト言語です。
本ブログ記事では、少しだけダイナミックだが、サンドボックスではない言語のためのスクリプトを
どのように使い続けるかを説明します。</p>
<p>本ブログ記事は、それが何を意味し、さらに重要なのは、安全に重要なタスクを実行させるためにスクリプトを
どのように使用し続けるかを理解する助けとなるはずです。</p>
<!-- more -->
<h2 id="ダイナミックスクリプトとは">ダイナミックスクリプトとは？</h2>
<p>Elasticsearchに詳しくない方のために、Elasticsearchでは、
さまざまなリクエストの一部としてスクリプトを送信することができます。
search、aggregation、update、upsert、delete by queryなどです。
あなたのユースケースのために、通常の動作よりも拡張した動作をさせるためにスクリプトを追加できます。</p>
<p>例えば、以下のリクエストは、ダイナミックスクリプトを含んでいます。
<code>field1</code>と<code>field2 + shift</code>が同じ値を持っている時だけドキュメントを返します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/_search</span>
{
  <span style="color:#f92672">&#34;query&#34;</span>:{
    <span style="color:#f92672">&#34;filtered&#34;</span>:{
      <span style="color:#f92672">&#34;filter&#34;</span>:{
        <span style="color:#f92672">&#34;script&#34;</span>:{
          <span style="color:#f92672">&#34;script&#34;</span>:<span style="color:#e6db74">&#34;doc[&#39;field1&#39;].value == (doc[&#39;field2&#39;].value + shift)&#34;</span>,
          <span style="color:#f92672">&#34;lang&#34;</span>:<span style="color:#e6db74">&#34;groovy&#34;</span>,
          <span style="color:#f92672">&#34;params&#34;</span>:{
            <span style="color:#f92672">&#34;shift&#34;</span>:<span style="color:#ae81ff">3</span>
          }
        }
      }
    }
  }
}
</code></pre></div><p>言語を変えることもできます。
それは、当然、シンタックスが変わったり、制限が追加（例えば、Groovyスクリプトの代わりに<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_lucene_expressions_scripts">Lucene　Expressions</a>に変更）されることもあります。
<code>lang</code>パラメータによって言語を指定できます。</p>
<h3 id="なぜそれはダイナミック">なぜそれはダイナミック？</h3>
<p>上記の例はダイナミックスクリプトです。
それは、実際のスクリプトの部分はサーバサイドで<em>動的に</em>解釈されコンパイルされる必要があるからです。
ダイナミックスクリプトはElasticsearchのAPIによってデータノードに送信されます。
これは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_indexed_scripts">インデックスされたスクリプト(indexed script)</a>も含みます。</p>
<p>言い換えると、もし、スクリプトがデータノード全てに保存されていなければ、
それは、ダイナミックスクリプトとして扱われます。</p>
<h2 id="dynamic-scriptingをオフにするとどうなるか">dynamic scriptingをオフにするとどうなるか？</h2>
<p>最新のリリースでの変更により、Groovyのdynaic scriptingはデフォルトでオフになりました。
先ほどのスクリプトについても同様で、もし、先ほどのリクエストを実行すると、次のようなエラーが発生します。
(一部省略してあります。)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;error&#34;</span>:<span style="color:#e6db74">&#34;SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[8FJ02MofSnqVvOQ10BXxhQ][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{...}]]]; nested: ScriptException[dynamic scripting for [groovy] disabled]...&#34;</span>,
   <span style="color:#f92672">&#34;status&#34;</span>:<span style="color:#ae81ff">400</span>
}
</code></pre></div><p>エラーメッセージの重要な箇所は「ScriptException[dynamic scripting for [groovy] disabled]」です。</p>
<h2 id="スクリプティングを使い続けるには">スクリプティングを使い続けるには？</h2>
<p>Elasticsearchでスクリプトを実行するには3つの方法があります。
2つのダイナミックな方法は、リクエストごとのスクリプト（上述）かインデックスされた
スクリプト(indexed script)を使う方法です。
インデックスされたスクリプトを使うことは、Elasticsearch自身にGroovyスクリプトを保管することで
利用で、それらを要求に応じて利用することです。
（これは、実際には十分機能しますが、これではまだ、信頼できないユーザに対して彼らのスクリプトを実行できます）
RDBのように保存されたプロシージャとして同じ方法で実行させるものと同様です。
前もって、スクリプトを記述しておき、リクエストの一部として後から、名前で呼び出して実行可能です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/_search</span>
{
  <span style="color:#f92672">&#34;query&#34;</span>:{
    <span style="color:#f92672">&#34;filtered&#34;</span>:{
      <span style="color:#f92672">&#34;filter&#34;</span>:{
        <span style="color:#f92672">&#34;script&#34;</span>:{
          <span style="color:#f92672">&#34;script_id&#34;</span>:<span style="color:#e6db74">&#34;your_custom_script&#34;</span>,
          <span style="color:#f92672">&#34;lang&#34;</span>:<span style="color:#e6db74">&#34;groovy&#34;</span>,
          <span style="color:#f92672">&#34;params&#34;</span>:{
            <span style="color:#f92672">&#34;shift&#34;</span>:<span style="color:#ae81ff">3</span>
          }
        }
      }
    }
  }
}
</code></pre></div><p>あまり変わっていないことに気づくでしょう。
<code>script</code>の部分が、前もって記述されたスクリプトの名前<code>script_id</code>に変更されただけです。</p>
<p>Elasticsearchにスクリプトを提供するダイナミックではない方法はインデックスに保存する代わりに、
ディスクにファイルとしてスクリプトを保存することです。
そうすることで、各スクリプトを設定として保存します。
これは、どのようなスクリプト言語に対してもダイナミックスクリプティングをオフにしたまま、
サンドボックス化されないスクリプトを使い続けることができる方法です。</p>
<p>最初のサンプルで、Groovyスクリプトは<code>doc['field1'].value == doc['field2'].value + shift</code>でした。
これを、<code>.groovy</code>拡張子を持ったファイルとして書き出すことができます。</p>
<pre><code>doc['field1'].value == (doc['field2'].value + shift)
</code></pre><p>もし、このファイルに<code>your_custom_script.groovy</code>ちう名前をつけて、
Elasticsearchのすべてのデータノードの<code>config/scripts</code>ディレクトリに保存すると、
Elasticsearchは60秒（elasticsearch.ymlの<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_automatic_script_reloading"><code>watcher.interval</code>で変更可能</a>）でこのスクリプトを認識し、今後のリクエストに利用できるようにプリコンパイルするでしょう。
そのファイルはElasticsearch実行ユーザによって読み込みができる必要があります。
これをディスクに書き込んだ後、あなたの設定ディレクトリは次のようになっています。</p>
<pre><code>config/
  elasticsearch.yml
  logging.yml
  scripts/
    your_custom_script.groovy
</code></pre><p>これは、各リクエストやインデックスされたスクリプトをスクリプトとして動的に送信しませんが、
信頼された環境にスクリプトを追加することでダイナミックスクリプトとなることを許します。</p>
<h2 id="ディスクに書かれたスクリプトを使用する">ディスクに書かれたスクリプトを使用する</h2>
<p>スクリプトは、ロードされたスクリプトになるまでは、利用できません。
ログファイルに次のようなログが表示されるまではです。</p>
<pre><code>[2015-02-11 11:14:47,066][INFO ][script                   ] [Sergei Kravinoff] compiling script file [/path/to/elasticsearch-1.4.3/config/scripts/your_custom_script.groovy]
</code></pre><p>すべてのElasticsearchのデータノードでスクリプトが読み込まれたら、
それを利用することができます。
利用するために、<code>file</code>（<code>script_id</code>ではありません！）としてスクリプト名を指定します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/_search</span>
{
  <span style="color:#f92672">&#34;query&#34;</span>:{
    <span style="color:#f92672">&#34;filtered&#34;</span>:{
      <span style="color:#f92672">&#34;filter&#34;</span>:{
        <span style="color:#f92672">&#34;script&#34;</span>:{
          <span style="color:#f92672">&#34;file&#34;</span>:<span style="color:#e6db74">&#34;your_custom_script&#34;</span>,
          <span style="color:#f92672">&#34;lang&#34;</span>:<span style="color:#e6db74">&#34;groovy&#34;</span>,
          <span style="color:#f92672">&#34;params&#34;</span>:{
            <span style="color:#f92672">&#34;shift&#34;</span>:<span style="color:#ae81ff">3</span>
          }
        }
      }
    }
  }
}
</code></pre></div><p>Note:<code>lang</code>は必須ではありません。Groovyがデフォルトの言語のためです。
もし、違うスクリプト言語を使いたい、もしくは、デフォルトの言語を（例えば、Lucene Expressionsへ）
変更したい場合、言語が正しいスクリプトを見つけるために提供されている必要があります。
一番良い方法は、アプリケーションが<code>lang</code>パラメータを含んでいることを勧めます。
これは、将来、デフォルトのスクリプト言語が変更されても、問題ないからです。</p>
<h2 id="質問">質問？</h2>
<p>もし、質問があれば、遠慮なくTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)で教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch　1.4.3および1.3.8リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2015/02/12/elasticsearch-1-4-3-and-1-3-8-released-ja/</link>
      <pubDate>Thu, 12 Feb 2015 12:39:53 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/02/12/elasticsearch-1-4-3-and-1-3-8-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：elasticsearch 1.4.3 and 1.3.8 released 本日、Lucene 4.10.3をベースにしたElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-3-and-1-3-8-released/">elasticsearch 1.4.3 and 1.3.8 released</a></p>
<p>本日、<strong>Lucene 4.10.3</strong>をベースにした<strong>Elasticsearch 1.4.3</strong>と、<strong>セキュリティ</strong>とバグフィックスリリースである、<strong>Elasticsearch 1.3.8</strong>をリリースしました。
ダウンロードおよび変更リストはそれぞれ次のリンクからアクセスできます。</p>
<ul>
<li>最新ステーブルリリース：<a href="http://www.elasticsearch.org/downloads/1-4-3">Elasticsearch 1.4.3</a></li>
<li>1.3.x系バグフィックス：<a href="http://www.elasticsearch.org/downloads/1-3-8">Elasticsearch 1.3.8</a></li>
</ul>
<p>過去のリリースに関するブログ（公式）はこちら。</p>
<ul>
<li>1.4:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-2-released/">1.4.2</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/">1.4.1</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.4.0</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">1.4.0.Beta1</a></li>
<li>1.3:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-2-released/">1.3.7</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/">1.3.6</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.3.5</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-4-released/">1.3.4</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-3-released/">1.3.3</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-2-released/">1.3.2</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-1-released/">1.3.1</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/">1.3.0</a>.</li>
</ul>
<p>すべての変更については<a href="http://www.elasticsearch.org/downloads/1-4-3">1.4.3のリリースノート</a>および<a href="http://www.elasticsearch.org/downloads/1-3-8">1.3.8のリリースノート</a>をごらんください。
以下では、セキュリティの問題について紹介します。</p>
<!-- more -->
<h2 id="groovy-scripting-の脆弱性">groovy scripting の脆弱性</h2>
<p>Elasticsearchのバージョン1.3.0から1.3.7および1.4.0から1.4.2で、Groovyスクリプトエンジンに脆弱性が発見されました。
脆弱性は、攻撃者がGroovyスクリプトをサンドボックスを避けて構築でき、
ElasticsearchのJava VMを実行しているユーザとしてシェルコマンドを実行できます。</p>
<p>この問題をCVE-2015-1427として報告済みです。</p>
<p>バージョン1.3.8と1.4.3では、デフォルトで、Groovyに対してのサンドボックスをオフにしました。
結果として、<strong>ダイナミックスクリプトの実行はGroovyに対してもオフとなります。</strong></p>
<p>もし、脆弱性のあるバージョンで実行している場合、v1.3.8かv1.4.3にアップグレードするか、ダイナミックなGroovyスクリプトをクラスタの
すべてのノードに対して次の設定を追加することで、オフにします。</p>
<pre><code>script.groovy.sandbox.enabled: false
</code></pre><p>これは、Groovyのサンドボックスをオフにし、リクエストの一部としてインラインで受け付けるダイナミックなGroovyスクリプトや
特殊な<code>.scripts</code>インデックスに保存されているスクリプトを実行しません。</p>
<p>それまでは、各データノードの<code>config/scripts</code>ディレクトリにファイルとして保存されたGroovyスクリプトは
まだ、利用可能です。詳細の情報については<a href="http://www.elasticsearch.org/blog/running-groovy-scripts-without-dynamic-scripting/">Running scripts without dynamic scripting</a>をごらんください。</p>
<h2 id="future-scripting-plans">future scripting plans</h2>
<p>安全なダイナミックスクリプティング言語としてGroovyを失うことは、Elasticsearchにとって痛手です。
update APIやsearch APIやaggregationsフレームワークの一部としてScriptを使います。
それらは、静的なAPIでは簡単に表現できない、カスタムなトリックをユーザに実行できるようにします。</p>
<p>残念ながら、Groovyチームとこの問題を議論した後、Groovy言語もサンドボックスによってきちんと保護されている
というにはあまりにもダイナミックであるという結論に達しました。
Groovyは、デフォルトでは利用できなくなります。
利用可能なダイナミックスクリプト言語としては<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_lucene_expressions_scripts">Lucene Expressions言語</a>のみとなります。
Expressionsははやいですが、それらは非常に限定されています。数値のフィールドでのみ実行可能で、ループをサポートしていません。</p>
<p>より強力で（しかし安全な）ミニ言語になるようにExpressionsを拡張することを調査しています。
これは、Scriptユーザが現在持っている最も一般的なユースケースを少なくとも助けるでしょう。
この拡張は長期間のプロジェクトであり、進化には時間がかかるでしょう。</p>
<p>ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-3">Elasticsearch 1.4.3</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>CROSS 2015で話をしてきました #cross2015</title>
      <link>https://blog.johtani.info/blog/2015/01/29/talk-at-cross2015/</link>
      <pubDate>Thu, 29 Jan 2015 11:59:28 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2015/01/29/talk-at-cross2015/</guid>
      <description>今年もCROSS参加しました。そして、話もしてきました。 今年は横浜の大さん橋でした。横浜はあんまりこないので、乗り換えでおたおたしてしまいま</description>
      <content:encoded><p>今年もCROSS参加しました。そして、話もしてきました。
今年は横浜の大さん橋でした。横浜はあんまりこないので、乗り換えでおたおたしてしまいましたが。。。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150129/oosanbashi.jpg" />
    </div>
    <a href="/images/entries/20150129/oosanbashi.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<!-- more -->
<p>なかなかいい景色でした。（寒いけど）
「おおさんばし」って読むんですね。「だいさんばし」だと思ってた。。。</p>
<p>以下はいつもの、自分用メモです。</p>
<h2 id="俺はどうしてそのデータストアを選択したのか-銀河と小宇宙を語る会">俺はどうしてそのデータストアを選択したのか 〜銀河と小宇宙を語る会〜</h2>
<p><a href="http://2015.cross-party.com/program/c1">http://2015.cross-party.com/program/c1</a></p>
<p>遅れて入ったので、ちゃんと聴けてないです。</p>
<h4 id="最近注目しているデータストアは">最近注目しているデータストアは？</h4>
<ul>
<li>Postgresql。JSON型が気になってる。</li>
<li>AiroSpike。データ型のあるデータストアが気になってる。</li>
<li>MongoDB。<strike>JSON使いたいなら、これじゃないの？</strike></li>
<li>AWSのAurora。<strike>インスタンスタイプを選ばなくていい</strike>（選ばないといけないらしい）とか、勝手にスケールしてくれるし、MySQL互換。</li>
</ul>
<h2 id="今こそ語るエンジニアの幸せな未来">今こそ語るエンジニアの幸せな未来</h2>
<p><a href="http://2015.cross-party.com/program/x3">http://2015.cross-party.com/program/x3</a></p>
<ul>
<li>「無職初日です。」</li>
<li>Web系の人？とか質問されて、自分が何系かいつもわからなくなるなぁ。</li>
<li>「働きがいは会社が提供するのか、個人が見つけるのか？」
<ul>
<li>個人かなぁ。会社がなにをやってるかにもよる気がするかなぁ。</li>
</ul>
</li>
<li>「辞めると伝えると、やりたいようにやれって言われるw」</li>
<li>リモートできるかできないか。</li>
<li>「働きがい」というキーワードが出てると普通は怪しい会社w</li>
<li>今は、働きやすさを高くしないと人が雇えなくなってきている。</li>
<li>欧米のミドルウェアだと、35歳定年説はない。→日本でもそうじゃないですか？</li>
<li>漫然と進んでるとダメ。→そりゃそうだ。</li>
</ul>
<h2 id="全文検索エンジン群雄割拠あなたが使うべきはどれだ">全文検索エンジン群雄割拠〜あなたが使うべきはどれだ！〜</h2>
<p><a href="http://2015.cross-party.com/program/c4">http://2015.cross-party.com/program/c4</a></p>
<p>スライド：https://speakerdeck.com/johtani/elasticsearchfalseshao-jie-tote-zheng-cross-2015</p>
<p>楽しんでいただけましたでしょうか？
ちょっと話が長くなってしまい、あとの方の時間が少なかった気がしますが。。。</p>
<p>Kibanaのバックエンドとして認識されている人もいたので、検索エンジンですよというのをアピールするいい機会になったので良かったです。
もちろん、Kibanaとの組み合わせも面白いので、少しでも興味をもっていただき、触っていただけたらなぁと。</p>
<p>話をする機会を用意していただいた、<a href="https://twitter.com/yamakatu">やまかつさん</a>、その他のスピーカーのみなさん、ありがとうございました！。</p>
<p>Elasticsearchに関して何か興味質問などありましたら、気軽にコンタクトしてください。Twitterとかブログコメントなどで。</p>
<h2 id="プレモルタイム以降">プレモルタイム以降</h2>
<p>プレモルの写真撮るの忘れてました。。。重要なのに。。。</p>
<p>美味しくプレモルをいただきながら、何人かの方に声をかけていただき、話をすることができました。
こういう時間がとってあるのがいいですよね。
色々なところでElasticsearchを使っていただいているようで、うれしい限りです。
DMMの方とも話ができたし。</p>
<h2 id="まとめ">まとめ</h2>
<p>今年はプレモルを飲みに行くだけかなぁと思っていたのですが、話をする人になってました。（おかしいなぁ）
来年もあれば、きっと参加するかなぁと。ではまた来年！</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20150129/night.jpg" />
    </div>
    <a href="/images/entries/20150129/night.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>夜景きれいですね。（端っこに写ってる船は<a href="http://www.asukacruise.co.jp/facility/view/">飛鳥II</a>でした。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2014）</title>
      <link>https://blog.johtani.info/blog/2014/12/31/looking-back-2014/</link>
      <pubDate>Wed, 31 Dec 2014 20:41:26 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/31/looking-back-2014/</guid>
      <description>今年は大晦日になってしまいました。しかも、ちょっと飲んでたり。。。 第9流しながら書いてます。 振り返り(2013年に書いた抱負から) まずは昨年</description>
      <content:encoded><p>今年は大晦日になってしまいました。しかも、ちょっと飲んでたり。。。
第9流しながら書いてます。</p>
<!-- more -->
<h2 id="振り返り2013年に書いた抱負から">振り返り(2013年に書いた抱負から)</h2>
<p>まずは昨年書いた抱負からの振り返りです。</p>
<ul>
<li>Elasticsearch勉強会の継続、Solr勉強会の<strike>継続＋ミックスした検索勉強会の開催</strike></li>
<li>IDEAのさらなる活用</li>
<li>もっと開発(プラグインとか)</li>
<li><strike>AWSをもう少し活用</strike></li>
<li>海外のイベントに行ってみたい</li>
<li>読書と英語を継続</li>
</ul>
<p>AWS活用できてないです、、、
ミックスした勉強会は、ドタバタしててできませんでした。</p>
<p>IDEAはさらなる活用までいってるかは不明ですが、最近は<a href="http://samuraism.com">隣にとても詳しい人</a>が座っているので色々と教えてもらってます。
開発については、ちょっとずつですが、PR書いたりしてます。</p>
<h2 id="振り返り今年あったできごと">振り返り(今年あったできごと)</h2>
<ul>
<li>初海外（<a href="http://blog.johtani.info/blog/2014/03/16/post-from-berlin/">ベルリン</a>と<a href="http://blog.johtani.info/blog/2014/07/01/join-elasticsearch/">アムステルダム</a>）</li>
<li>Elasticsearchに転職</li>
<li><a href="http://blog.johtani.info/blog/2014/05/09/reading-agile-data-science/">初献本?</a></li>
<li>Elasticsearch勉強会の定期開催</li>
<li><a href="http://www.amazon.co.jp/dp/4048662023/ref=as_sl_pc_tf_lc?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=4048662023&amp;adid=0TJJ6P11DE20CX6F42NJ&amp;&amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2Fblog%2Farchives%2F">ElasticSearch Serverの翻訳</a></li>
<li><a href="http://www.amazon.co.jp/dp/B00MPDUQQI/ref=as_sl_pc_tf_lc?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=B00MPDUQQI&amp;adid=0HM28YENR7JAXB3ESSX0&amp;&amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2Fblog%2Farchives%2F">サーバ/インフラエンジニア養成読本の執筆</a></li>
<li>勉強会以外でもスピーカー</li>
<li>英語の継続（切実）</li>
</ul>
<p>初ものが多かったです。
初の海外の会社、初の海外旅行（仕事）、初の献本（著者の人から、多分、初）、初の翻訳本、初のムック本執筆などなど。</p>
<p>初の海外でした。なのに、夏には海外の会社に転職してみてるとか、ちょっと自分でも信じられません。
ベルリンでは、海外の勉強会にも参加できて非常に有意義でした。
ヨーロッパの街並みは、日本では経験できない雰囲気で、向こうの方には普通の街並みも自分にはとても新鮮でした。</p>
<p>転職自体は2回目ですが、海外の会社というのは初めてです。日々、英語の勉強になりますし、いろんな刺激を受けています。
英語の勉強自体も継続中です。英語力が不足しているのは自覚してるので、少しでも英語に触れるようにDLifeで海外ドラマを録画して、通勤時間帯にみるなどを試みてます。</p>
<p>書籍には、昨年につづき関わることができました。来年も何かしらの書籍に関われることができればなと。
（気になる人は以下のリンクから。。。）</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=B00MPDUQQI&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=4048662023&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<p>分かりきったことですが、Elasticsearchな1年でした。
転職してあっという間の半年です。まだまだやりたいことがいっぱいあるので、来年もバランスよく頑張らないと。</p>
<h2 id="来年の抱負">来年の抱負</h2>
<ul>
<li>英語の継続</li>
<li>海外のイベントへの参加</li>
<li>多岐にわたるイベントでのスピーカー</li>
<li>日本の人員の倍増！？</li>
<li>Elasticsearchに関する日本語の情報発信</li>
<li>Elasticsearch座談会みたいなものの開催</li>
</ul>
<p>英語の継続については書くまでもないですね。
海外のイベントへの参加もかなぁ。<a href="http://www.elasticsearch.org/blog/its-time-elasticon15-registration-is-finally-open/">3月にサンフランシスコに行く</a>とは思います。
そのほかには、今度こそ<a href="http://berlinbuzzwords.de">Berlin Buzzwords</a>に行きたいです。</p>
<p>Elasticsearch勉強会以外のイベントでもElasticsearchを広めるべく色々なところでスピーカーをしたいなと思っています。
検索、ログ解析で少しは知名度が上がってきていますが、別の言語のコミュニティやイベントでは、まだまだ、elasticsearch自体を知らない人もいると思います。まずは知ってもらうところからかなと。</p>
<p>日本の人員の倍増については、今年も達成してます（一人が二人になりましたw）。
来年も倍増できればうれしいなと。同じ母国語の同僚がいるのはやっぱりうれしいし、心強いですから。</p>
<p>来年は（も）、日本語での情報をどんどん発信していきます。サイトやガイドなども
日本語にしたいなと。
あとは、これまで、勉強会でスピーカーをしていただいた人だけを集めた座談会のようなものも開催したいなと考えています。
私自身も色々と聞きたいことがありますし。
そのほかにも色々と要望をお待ちしています。Twitterやブログのコメント欄などで気軽にコンタクトしていただければと。</p>
<p>最後は、いつものようにですが、来年も勉強会やいろんなイベントに参加する予定です。 ブログも週1程度で書く努力しないとなぁ。 こんな話を書いてくださいとかのリクエストもお待ちしています。</p>
<p>今年はあと、数時間ですが、色々とお世話になりました。 この場を借りてお礼申し上げます。</p>
<p>来年も、いろんな方に絡んでいくとは思いますが、よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 2.0系のIssueの紹介</title>
      <link>https://blog.johtani.info/blog/2014/12/25/pickup-elasticsearch-2-0-0-labels/</link>
      <pubDate>Thu, 25 Dec 2014 15:53:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/25/pickup-elasticsearch-2-0-0-labels/</guid>
      <description>この記事はElasticsearch Advent Calndar 2014の25日目のエントリです。 あっという間に最終日です。来年につなげるという意味で、Elasti</description>
      <content:encoded><p>この記事は<a href="http://qiita.com/advent-calendar/2014/elasticsearch">Elasticsearch Advent Calndar 2014</a>の25日目のエントリです。</p>
<p>あっという間に最終日です。来年につなげるという意味で、Elasticsearchの2系のIssueをいくつかピックアップして紹介してみます。</p>
<!-- more -->
<p>現在、ElasticsearchのGitHubリポジトリは、大きく3つのブランチで作業しています。
<code>master</code>、<code>1.x</code>、<code>1.4</code>です。<code>master</code>と<code>1.x</code>の大きな違いとしては、<code>master</code>はLuceneの5.x系を採用している点です。</p>
<p>なお、これから紹介するIssueは現在、確定していない項目も含んでいます。実際に2.0がリリースされるタイミングでは
採用されない場合もあります。</p>
<h2 id="upgrade-master-to-lucene-50-snapshot-8347-closed">Upgrade master to lucene 5.0 snapshot #8347 (closed)</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/pull/8347">https://github.com/elasticsearch/elasticsearch/pull/8347</a></p>
<p>先ほど書きましたが、Luceneの5に対応するためのPRです。
Lucene 5に関してはLuceneのコミッターのMikeさんの<a href="http://blog.mikemccandless.com/2014/11/apache-lucene-500-is-coming.html">ブログ記事</a>も参考になります。</p>
<p>Lucene 5に変更することで、BitSetに関する改善が多く含まれることになります。
メモリの利用量、圧縮などの改善が多く含まれています。
もう1点大事な点としては、Lucene 5系ではLucene 3系のインデックスを読み込むことができなくなる点です。
Luceneの下位互換の範囲は1つ前のメジャーバージョン（5.x系の場合は4.xまでが対象）となっています。</p>
<h2 id="filter-cache-add-a-_cache-auto-option-and-make-it-the-defaultclosed">Filter cache: add a <code>_cache: auto</code> option and make it the default.（closed）</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/pull/8573">https://github.com/elasticsearch/elasticsearch/pull/8573</a></p>
<p>Filter cacheは、<code>true</code>もしくは<code>false</code>の設定が利用できますが、filterの種類にも依存します。
その辺りの条件を加味しつつ、よしなにCacheをコントロールしてくれます。</p>
<h2 id="remove-andornot-in-favour-of-bool-filter-8960open--discuss">Remove and/or/not in favour of <code>bool</code> filter #8960（open / discuss）</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/8960">https://github.com/elasticsearch/elasticsearch/issues/8960</a></p>
<p>似ているが少し異なる<code>and</code>、<code>or</code>、<code>not</code>フィルタと<code>bool</code>フィルタが存在しています。
これらをわかりやすくするために、<code>bool</code>フィルタに統一しましょうという話し合いをしています。</p>
<h2 id="input-validation-9059open--discuss">Input validation #9059（open / discuss）</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/9059">https://github.com/elasticsearch/elasticsearch/issues/9059</a></p>
<p>色々な入力に関するチェックを追加しようというIssueです。
たとえば、ディレクトリ名やファイル名、URLのパスやクエリストリング、フィールドのパスやスクリプトなどです。
Validationがあると、変な設定をして頭をかかえることもなくなるかなぁと。</p>
<h2 id="refactor-analysis-framework-8961open">Refactor analysis framework #8961（open）</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/8961">https://github.com/elasticsearch/elasticsearch/issues/8961</a></p>
<p>新しくAnalyzerを作った場合に、色々な場所に登録必要があったりします。インデックスレベルとノードレベルです。（Kuromojiプラグインなどが参考になります。）
また、インデックスごとにカスタムのAnalyzerを設定するので、1つのノードに同じAnalyzerを何度も設定しないといけません。
よりシンプルにするために、Analyzerをノード単位で設定しようという提案です。</p>
<h2 id="remove-possibility-for-conflicting-field-definitions-and-ambiguous-field-resolution-8870open">Remove possibility for conflicting field definitions and ambiguous field resolution #8870（open）</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/8870">https://github.com/elasticsearch/elasticsearch/issues/8870</a></p>
<p>同じインデックスに、異なるtypeで、同じフィールド名があった場合、いろいろと良くないことがあったりします。
たとえば、フィールドのタイプがintegerとstringと異なる場合に、インデックスレベルで検索を行うとうまく検索できなかったりと。
この問題を解消するために、より明確にしようというIssueです。
たとえば、フィールド名を指定するためには、フルパスで記述をするだとか、フィールドマッピングに関してはインデックスレベルで内部で保持をするなど。</p>
<h2 id="validation-of-mappings-request-to-reject-unsupported-fields-7205closed">Validation of mappings request to reject unsupported fields #7205（closed）</h2>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/7205">https://github.com/elasticsearch/elasticsearch/issues/7205</a></p>
<p>1.xでも取り込まれますが、嬉しい機能なので紹介します。
これまでは、mappingsでスペルミスをした場合（たとえば、field設定で&quot;indexx&quot;といったミス）には、その項目は単に無視されるだけでした。
これが、v1.xでは、エラーに</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、簡単ですが、v2.0.0に向けたIssueをピックアップして紹介してみました。
上記以外にも多くの改善、提案が2.0に向けて行われています。
興味のある方は、<a href="https://github.com/elasticsearch/elasticsearch/issues?q=is%3Aopen+is%3Aissue+label%3Av2.0.0">v2.0.0</a>ラベルでIssueを検索してみてはいかがでしょうか？</p>
<p>今年もあとわずかとなりました。
今年の2月にElasticsearchの1.0がリリースされ、あっという間に1.4なりました。まだまだ改善しています。</p>
<p>来年もElasticsearchに興味をもっていただければ嬉しいです。
<a href="http://www.elasticon.com/">Elasticsearch初のユーザカンファレンスのサイトもオープン</a>しました。
Elasticsearchに関するいろいろな話が聞ける機会だと思います。登録をお待ちしています。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Marvel 1.3.0リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2014/12/18/marvel-1-3-0-released-ja/</link>
      <pubDate>Thu, 18 Dec 2014 17:06:48 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/18/marvel-1-3-0-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：marvel 1.3.0 released 12/17に、Elasticsearch Marvel 1.3.0をリリースしました</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/marvel-1-3-0-released/">marvel 1.3.0 released</a></p>
<p>12/17に、<strong>Elasticsearch Marvel 1.3.0</strong>をリリースしました。
Marvelの以前のリリースから、Elasticsearchでは様々なことがありました（Elasticsearch 1.4のリリースなど）。
このバージョンでは、モニタリングにクエリキャッシュや新しいcircuit breakerなどのような機能を追加してあります。
Senseのナレッジベースは最新のAPIを含むものに拡張されています。
また、<a href="http://www.elasticsearch.org/overview/shield/">Shield</a>のリリースに向けた準備として、HTTPsのサポートも追加しました。</p>
<p>アップグレードのために、Elasticsearchの全てのノードに最新版のMarvelプラグインをインストールする必要があります。
また、他のJavaプラグインと同様に、Marvelの新バージョンを有効にするために、各ノードを（1台ずつ）リスタートする必要があるでしょう。
アップグレードプロセスについての詳細は、<a href="http://www.elasticsearch.org/guide/en/marvel/current/upgrade.html#upgrade">Marvelドキュメント</a>をごらんください。</p>
<!-- more -->
<p>まとめとして、ここに本リリースに関する改善点をいかにリストアップしておきます。</p>
<h3 id="agent">agent</h3>
<ul>
<li>追加：
<ul>
<li>httpsのサポート</li>
<li>デフォルトのMarvelの設定（以前は常に9200）ではなく、ローカルノードのポートを自動的に検出</li>
</ul>
</li>
<li>改善：
<ul>
<li>marvelインデックステンプレートに関するエラーチェックと耐障害性(それに対するチェックと追加時のチェック)</li>
<li>エラーログに関するくり返しの抑制</li>
<li>URLパラメータによるインデックス名を指定する_bulk exportコマンド。これは、<code>rest.action.multi.allow_explicit_index</code>がfalseに設定されているときに有用</li>
</ul>
</li>
<li>修正：
<ul>
<li>ES 1.4.0のtribe nodeがMarvelのインストール時に初期化されない問題</li>
</ul>
</li>
<li>削除：
<ul>
<li>UIで表示されないoptional shard level statsを除去</li>
</ul>
</li>
</ul>
<h3 id="monitoring-ui">monitoring ui</h3>
<ul>
<li>追加：
<ul>
<li>ES 1.4.0で導入された新しいcircuit breakerを追加</li>
<li>circuit breakerのlimitをグラフにプロット</li>
<li>QueryCacheのグラフを追加</li>
<li>index throttlingのグラフの追加</li>
<li>Index writerとバージョンのmapのメモリ使用量のグラフの追加</li>
</ul>
</li>
<li>修正：
<ul>
<li>Network Transport Bytes Receivedグラフに実際の送信量を表示</li>
<li>Node Statsダッシュボードでいくつかのスレッドプールの不足</li>
</ul>
</li>
</ul>
<h3 id="sense">sense</h3>
<ul>
<li>
<p>追加：</p>
<ul>
<li>mappingsをインデックスでオートコンプリートするしないの設定を可能に</li>
<li>Cluster Reroute API</li>
<li>Search APIのQuery Cacheパラメータ</li>
<li>Analyze API</li>
<li>Validate Query API</li>
<li>Put Percolator API</li>
<li>cluster.routing.allocation.*設定</li>
<li>Function Scoreクエリのweightパラメータ</li>
<li>Flush API</li>
<li>Terms Aggregationのshow_term_doc_count_errorパラメータ</li>
<li>Update API</li>
<li>_geo_distanceソートオプション</li>
<li>Significant Terms aggregationを1.4.0にアップデート</li>
<li>Mapping APIにメタデータフィールドを追加</li>
<li>Get Index API</li>
<li>Scripted Metric Aggregation</li>
<li>simple_query_stringクエリ</li>
<li>More Like Thisクエリを1.4.0にアップデート</li>
<li>has_childクエリ/フィルタのmin_childrenとmax_childrenオプション</li>
<li>terms aggs/significant terms aggsのヒントオプション</li>
<li>Mappings APIのtransform</li>
<li>インデックスされたscriptとtemplate
* Geo Bounds aggregation</li>
<li>Top Hits aggregation</li>
<li>Terms aggregationのcollect_modeオプション</li>
<li>Percentiles Rank aggregation</li>
<li>Disk Threshold Allocator設定</li>
</ul>
</li>
<li>
<p>修正：</p>
<ul>
<li>URLオートコンプリートの挙動（プロトコルとホストのような組み合わせ）</li>
<li>nested typeマッピングのinclude_in_parentとinclude_in_rootの不足</li>
<li>Rangeフィルタでのgt、gte、lt、lte</li>
<li>Existsフィルタのオートコンプリート</li>
<li>Snapshot、Restore APIのリポジトリ設定の時オートコンプリートの失敗</li>
</ul>
<p>いつものように、Elasticsearch Marvelを改善するために、フィードバックをお待ちしています。
<a href="https://groups.google.com/forum/?fromgroups#!forum/elasticsearch">ElasticsearchユーザML</a>や<a href="http://www.twitter.com/elasticsearch">Twitter</a>に質問や意見お送りください。</p>
</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.4.2および1.3.7リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2014/12/18/elasticsearch-1-4-2-released-ja/</link>
      <pubDate>Thu, 18 Dec 2014 14:26:14 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/18/elasticsearch-1-4-2-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：elasticsearch 1.4.3 and 1.3.8 released 本日、Lucene 4.10.3をベースにしたElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-3-and-1-3-8-released/">elasticsearch 1.4.3 and 1.3.8 released</a></p>
<p>本日、<strong>Lucene 4.10.3</strong>をベースにした<strong>Elasticsearch 1.4.3</strong>と、<strong>セキュリティ</strong>フィックスとバグフィックスリリースである、<strong>Elasticsearch 1.3.8</strong>をリリースしました。
ダウンロードおよび変更リストはそれぞれ次のリンクからアクセスできます。</p>
<ul>
<li>最新ステーブルリリース：<a href="http://www.elasticsearch.org/downloads/1-4-3">Elasticsearch 1.4.3</a></li>
<li>1.3.x系バグフィックス：<a href="http://www.elasticsearch.org/downloads/1-3-8">Elasticsearch 1.3.8</a></li>
</ul>
<p>過去のリリースに関するブログ（公式）はこちら。</p>
<ul>
<li>1.4:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/">1.4.1</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.4.0</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">1.4.0.Beta1</a></li>
<li>1.3:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/">1.3.6</a>,
<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.3.5</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-4-released/">1.3.4</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-3-released/">1.3.3</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-2-released/">1.3.2</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-1-released/">1.3.1</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/">1.3.0</a>.</li>
</ul>
<p>すべての変更については<a href="http://www.elasticsearch.org/downloads/1-4-2">1.4.2のリリースノート</a>および<a href="http://www.elasticsearch.org/downloads/1-3-7">1.3.7のリリースノート</a>をごらんください。
以下では、重要な変更について紹介します。</p>
<!-- more -->
<h2 id="bug-fixes">bug fixes</h2>
<p>Elasticsearchに対して広範囲にわたってランダムなテストを行っています。以下の問題を見つけ、修正するのに役立っています。</p>
<ul>
<li>プライマリシャードを持つnodeがレプリカシャードをプライマリから復旧している間に、リスタートした場合に、プライマリ上のトランザクションログが削除されデータをロスする(<a href="https://github.com/elasticsearch/elasticsearch/pull/8917">#8917</a>)</li>
<li>scriptインデックスが普及した場合に、ScriptService全体がデッドロック(<a href="https://github.com/elasticsearch/elasticsearch/pull/8901">#8901</a>)</li>
<li>Index Writerのロックを強制的に解放することによるシャードの破損(<a href="https://github.com/elasticsearch/elasticsearch/pull/8892">#8892</a>)</li>
</ul>
<h2 id="パフォーマンス改善">パフォーマンス改善</h2>
<p>複雑な設定をもつ大きめのクラスタをもつユーザは、小さなスケールではわからない性能ボトルネックに直面します。
彼らの報告が次の改善をもたらす助けとなりました。</p>
<ul>
<li>使用可能なディスク空間に基づいてシャードの配置を決定する、disk allocation deciderの速度改善とクラスタリスタート後のリカバリ速度の改善(<a href="https://github.com/elasticsearch/elasticsearch/pull/8803">#8803</a>)</li>
<li>以前よりも高速な共有ファイルシステムでのSnapshot生成(<a href="https://github.com/elasticsearch/elasticsearch/pull/8749">#8749</a>)</li>
<li>不要なクラスタ状態変更の削減とそれによるネットワークトラフィックの削減およびリカバリの速度向上(<a href="https://github.com/elasticsearch/elasticsearch/pull/8933">#8933</a>, <a href="https://github.com/elasticsearch/elasticsearch/pull/8413">#8413</a>)</li>
<li>index stats APIはシャードリカバリによるブロックしない(<a href="https://github.com/elasticsearch/elasticsearch/pull/8910">#8910</a>)</li>
</ul>
<h2 id="試してみてください">試してみてください。</h2>
<p>ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-2">Elasticsearch 1.4.2</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>JJUG ナイトセミナーでLuceneの簡単な紹介をしてきました。#JJUG</title>
      <link>https://blog.johtani.info/blog/2014/12/17/jjug-night-seminar-dec-2014/</link>
      <pubDate>Wed, 17 Dec 2014 18:41:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/17/jjug-night-seminar-dec-2014/</guid>
      <description>「【東京】JJUG ナイト・セミナー「機械学習・自然言語処理特集！」12/17（水）開催」でLuceneの話をしてきました。 本当にごく簡単な入</description>
      <content:encoded><p><a href="http://jjug.doorkeeper.jp/events/18378">「【東京】JJUG ナイト・セミナー「機械学習・自然言語処理特集！」12/17（水）開催」</a>でLuceneの話をしてきました。
本当にごく簡単な入門です。
Luceneをさわるきっかけにしてもらえたら嬉しいです。</p>
<p>そのほかにも面白い話が聞けましたので、簡単ですがメモを。</p>
<!-- more -->
<h2 id="jjugの2014年振り返り">JJUGの2014年振り返り</h2>
<ul>
<li>だいたい、毎月ナイトセミナーかCCCを開催</li>
<li>イベント系に、のべ3100名が参加</li>
</ul>
<h2 id="java-でカジュアルにはじめる機械学習">Java でカジュアルにはじめる機械学習</h2>
<h3 id="小宮-篤史さんスマートニュース株式会社">小宮 篤史さん(スマートニュース株式会社)</h3>
<p>スライド：<a href="https://speakerdeck.com/komiya_atsushi/number-jjug-java-dekaziyuarunihazimeruji-jie-xue-xi">#JJUG - Java でカジュアルにはじめる機械学習</a><br>
ブログ：<a href="http://blog.k11i.biz/2014/12/jjug-java.html">#JJUG ナイトセミナー「機械学習・自然言語処理特集！」で Java でカジュアルに機械学習する話をしてきました</a></p>
<ul>
<li>ガチの人は寝ててください。</li>
<li>機械学習でできること
<ul>
<li>分類・識別</li>
<li>予測・回帰</li>
<li>パターンマイニング・アソシエーションルール</li>
<li>クラスタリング</li>
</ul>
</li>
<li>上2つは教師あり学習/下2つは教師なし学習</li>
<li>データとしては、日構造では扱えないので、「特徴量」を抽出して「特徴ベクトル」を作って、処理をするのが機械学習</li>
</ul>
<p>得られた結果の正しさの測定などなど</p>
<ul>
<li>機械学習の実装は辛いので、車輪の再発明をやめましょう！</li>
</ul>
<p>Javaで使える機械学習</p>
<ul>
<li>Weka：とりあえず使ってみるならこれ？</li>
<li>MLlib：Sparkで使われてる</li>
<li>Mahout：オワコン？</li>
<li>SAMOA：Stormの上で利用できる</li>
<li>Jubatus：Javaクライアントあり。</li>
<li>h2o：Deep learningをJavaでやるなら、これ。</li>
<li>ほかにもあったけど、スライド見ていただければ。</li>
</ul>
<p>機械学習をはじめるのに使えるデータセット</p>
<ul>
<li>
<p>UCI Machine learning repository</p>
<ul>
<li>Iris（アヤメデータ）は機械学習界のHello world</li>
</ul>
</li>
<li>
<p>Wekaを使ったサンプルコード</p>
</li>
</ul>
<h2 id="sparkmllibではじめるスケーラブルな機械学習">Spark/MLlibではじめるスケーラブルな機械学習</h2>
<h3 id="猿田-浩輔さん株式会社エヌティティデータ">猿田 浩輔さん(株式会社エヌ・ティ・ティ・データ)</h3>
<p>スライド：（後日、リンクがあれば更新予定）</p>
<p>* Spark+MLlibを語る上で外せない話題</p>
<ul>
<li>
<p>Hadoopとの違い？</p>
</li>
<li>
<p>まずはHadoopの話</p>
</li>
<li>
<p>HadoopによるK-meansのデモ</p>
</li>
<li>
<p>Hadoopの問題点に対するSparkの解決策</p>
</li>
<li>
<p>Spark 1.0系からJava8で書ける</p>
</li>
</ul>
<p>QA：</p>
<p>Q： データをキャッシュできるという話でしたが、キャッシュするということは、ジョブが途中で失敗した場合は最初からやり直しになるのでしょうか？
A： キャッシュしたデータが残っている場合は、途中から再開出来ます。キャッシュしたデータを持ったマシンがこけたら、最初からやり直しです。</p>
<h2 id="luceneと日本語の検索">Luceneと日本語の検索</h2>
<h3 id="自分">自分</h3>
<p>スライド：<a href="https://speakerdeck.com/johtani/lucenetori-ben-yu-falsejian-suo">Luceneと日本語の検索</a>
サンプルのリポジトリ：<a href="https://github.com/johtani/jjug-example">jjug-example</a></p>
<p>自然言語処理にからめて何か話をしてくださいと話を受けていたのですが、自然言語処理については「形態素解析」くらいしか出てこなかったですけど。。。
Luceneがどんなものかを超概要で話をしてみました。少しでもLuceneがどんなものかをわかってもらえたら嬉しいです。</p>
<p>もっと詳しく知りたい方は、スライドにある参考資料などを見ていただければと。</p>
<p>Javaで書くのもいいんですが、もっと簡単に検索したい場合はElasticsearchを使うのが便利ですよ！で締めくくりたかったのですが、発表では失敗してしまいました。。。
Elasticsearchの起動からデータ登録、検索までは<a href="https://speakerdeck.com/johtani/elasticsearch-and-kibana">こちらのスライド</a>を見ていただければ簡単さがわかると思います。</p>
<p>また、Kuromojiを利用した時に、Tokenizerなどが出力するTokenの品詞情報を見たい場合に便利な<a href="https://github.com/johtani/elasticsearch-extended-analyze">Elasticsearch用プラグイン</a>も作っています。
こちらも、Elasticsearchと一緒に使ってみてください。</p>
<h2 id="まとめ">まとめ</h2>
<p>機械学習に関していろんなツールがあるのだなぁと。
懇親会でもちょっと話しましたが、アルゴリズムの選定とか、アルゴリズムに適したデータの作成など、前処理のノウハウとかが大変そうだなぁといつも思います。
機械学習はいつもぼやーっとしか理解してないので。。。</p>
<p><a href="https://www.youtube.com/channel/UCZ5gDYmqI0tOeg-fEkCD2CQ">JJUGさんはYouTubeの動画</a>もあるようなので、過去の面白そうなセミナーも合わせてみてみると面白いと思います。</p>
<p>毎度のことですが、なんでも良いので、発表した後のフィードバックをいただけるとうれしいです。
今後の励みや改善につながるので。</p>
</content:encoded>
    </item>
    
    <item>
      <title>コーヒーブレイク</title>
      <link>https://blog.johtani.info/blog/2014/12/16/a-coffee-break/</link>
      <pubDate>Tue, 16 Dec 2014 12:01:32 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/16/a-coffee-break/</guid>
      <description>たまには、他愛のないブログを。 仕事しながら、コーヒーを飲んでます。 最近は、サムライズムさんのオフィスに入り浸って仕事してます。 オフィスには、</description>
      <content:encoded><p>たまには、他愛のないブログを。</p>
<p>仕事しながら、コーヒーを飲んでます。</p>
<!-- more -->
<p>最近は、サムライズムさんのオフィスに入り浸って仕事してます。
オフィスには、コーヒーミル付きのコーヒーメーカーがあるんです。</p>
<p>↓こんな感じ（現物は違うけど）</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=B00O7HUQWE&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<p>で、近所のコーヒー問屋で豆を買ってきてます。
そこのマスターに、いつもサジェストされるんです。</p>
<p>「この豆は、日が経つにつれて、細かく挽いて飲んでください。」</p>
<p>「この豆は、細かめに挽いて飲むのがオススメです。」</p>
<p>などなど。
オススメされるんですが、全自動のコーヒーメーカーは残念ながらここまでやってくれません。</p>
<p>ということで、手動のミルを買ってみました。家でも使えるし。</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=B0044ZA066&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<p>ちょっと考え事しながらとか、頭の切り替えに、ミルを回すのもいいかなと。
気持ち、美味しいコーヒーな気がします（気のせいかも）</p>
</content:encoded>
    </item>
    
    <item>
      <title>Logstashプラグインのエコシステムの変更（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2014/12/14/plugin-ecosystem-changes/</link>
      <pubDate>Sun, 14 Dec 2014 01:00:40 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/14/plugin-ecosystem-changes/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：exciting logstash plugin ecosystem changes Logstash 1.5.0 Beta 1(お試しはこちら)のリリースで、 プラグインのインストー</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/plugin-ecosystem-changes/">exciting logstash plugin ecosystem changes</a></p>
<p>Logstash 1.5.0 Beta 1(<a href="http://www.elasticsearch.org/overview/logstash/download/">お試しはこちら</a>)のリリースで、
プラグインのインストール、管理、公開の方法を変更しています。
ユーザやコミュニティからフィードバックをもらいました。
その目的は、プラグインの利用や開発をより簡単にすることです。
このプロジェクトは始まったばかりです。プラグインのコミュニティを探し、
共有するためのワンストップソリューションを提供するこのアイデアを改善していく予定です。
このブログで、この決定を行った理由を説明し、新しいワークフローをと今後のロードマップを説明します。</p>
<!-- more -->
<h2 id="プラグインがあります">プラグインがあります！</h2>
<p>Logstashは、プラグイン（input、filter、output、codec）が豊富にあります。
これらは、Elasticsearchにより開発されたものと、コミュニティからコントリビュートされたものです。
Logstashの主な特長の1つは、これらのプラグインの有効性と動作を拡張するプラグインを追加するのが簡単なことです。
現在、165以上のプラグインがエコシステムにあり、これらは、2つのプロジェクトに分かれています。</p>
<ul>
<li><code>logstash-core</code>は最もよく使われるプラグインで、Logstashにデフォルトで含まれます</li>
<li><code>logstash-contrib</code>はコミュニティにより開発されたプラグインを含み、別途ダウンロードできます</li>
</ul>
<h2 id="新プラグインエコシステムの変更">新プラグインエコシステムの変更</h2>
<p>1.5.0では、全てのプラグインは、Logstashコアから分離され、rubygemsを使って個別にパッケージングされます。
rubygemsを選択したのは、依存関係のあるライブラリの配布とパッケージングがパワフルで一般的なものだからです。
さらに、<a href="http://rubygems.org/">rubygems.org</a>プラットフォームは配布や探索に影響があります。
また、Logstashにプラグインをインストール、アップデート、削除するのが簡単な基盤も追加しました。
<code>contrib</code>プロジェクトは徐々に終了します。全てのプラグインは個別のプロジェクトになります。</p>
<h2 id="プラグインエコシステム変更の理由">プラグインエコシステム変更の理由</h2>
<p>多数のプラグインをもっていると、配布と公開に関して難題が出てきます。
私たちが変更するに至った理由は次のようなものです。</p>
<ul>
<li>現在は、プラグインの更新に伴い、Logstashの新バージョンのリリースが必要</li>
<li>開発者は、Logstashのリリース間隔とは別に、新バージョンをリリースをしたい</li>
<li>プラグイン開発者は、外部依存を記述できるようにしたい</li>
<li>Logstashコアの配布パッケージのダウンロードサイズを小さくし、ユーザは必要なプラグインのみインストール</li>
<li><code>logstash-contrib</code>を1つのリポジトリとして管理するのは難しい</li>
</ul>
<h2 id="詳細">詳細：</h2>
<h3 id="ソースコードの場所">ソースコードの場所</h3>
<p>Logstashのソースコードは、今後も<a href="https://github.com/elasticsearch/logstash">現在のGitHubのリポジトリ</a>のままです。
しかし、プラグインに関するコードやテストコードは含まなくなります。
この分離により、個別のプラグインの改善と同様にコアの改善に集中できます。
これにより、Logstashプロジェクトの全体の品質も向上します。</p>
<p>全プラグインのソースコードは、新しいGitHub organization、<a href="https://github.com/logstash-plugins">logstash-plugins</a>にて管理します。
各プラグインは個別のリポジトリとして、ここに配置されます。
一見すると、これはメンテナンスが難しくなるように思えます。しかし、テスト、Issue、依存関係を明確にすることができます。
私たちの目的は、テスト、ドキュメント、gemの公開の自動化であり、これを簡単にするためのツールを追加します。</p>
<p>しかし、プラグインの開発者はプラグインのソースコードソースコードをlogstash-pluginsに置く必要はありません。
ー コミュニティで利用可能にするために、<a href="http://rubygems.org/">rubygems.org</a>でそれを公開するだけで良いです。</p>
<h2 id="ワークフロー">ワークフロー</h2>
<p>ここで、新プラグインエコシステムのやりとり/ワークフローについて、いくつかの観点から説明します。</p>
<h3 id="logstashユーザ">logstashユーザ:</h3>
<p>ユーザは、これまでのリリース同様にLogstashのバイナリをダウンロードします。
Logstash 1.5.0は、1.4.2でパッケージされていたプラグインと同等のものが含まれています。
新しいシステムに簡単に移行できるようにです。
そして、ユーザは、最初のデプロイの後に、Logstashプラグインのをインストール、アップグレードできるようになります。</p>
<p><code>$LS_HOME/bin/plugin</code>スクリプトがプラグイン操作に関連するコマンドになります。</p>
<h5 id="プラグインのインストール">プラグインのインストール</h5>
<p>プラグインのほとんどはgemとして<a href="http://rubygems.org/">rubygems.org</a>にアップロードされます。
例えば、もしユーザが<a href="https://github.com/logstash-plugins/logstash-output-kafka">Apache Kafka outputプラグイン</a>をインストールする場合、次のコマンドを実行します。</p>
<pre><code>bin/plugin install logstash-output-kafka
</code></pre><p>または、ファイルをダウンロード済みの場合は次のコマンドとなります。</p>
<pre><code>bin/plugin install /path/to/logstash-output-kafka-1.0.0.gem
</code></pre><h5 id="プラグインの削除">プラグインの削除</h5>
<pre><code>bin/plugin uninstall logstash-output-kafka
</code></pre><h5 id="1つまた全プラグインのアップデート">1つまた全プラグインのアップデート</h5>
<pre><code>bin/plugin update
</code></pre><pre><code>bin/plugin update logstash-output-kafka
</code></pre><h5 id="プラグインのリストアップ">プラグインのリストアップ</h5>
<pre><code>bin/plugin list
</code></pre><pre><code>bin/plugin list elasticsearch ( List all plugins containing a name )
</code></pre><pre><code>bin/plugin list --group output ( list all outputs )
</code></pre><h4 id="ドキュメント">ドキュメント</h4>
<p>プラグインが個別に管理されても、<a href="http://www.elasticsearch.org/guide/en/logstash/current/index.html">全プラグインのドキュメントは1カ所</a>です。</p>
<h3 id="logstash-plugin開発者">logstash plugin開発者:</h3>
<p>プラグイン開発者と作者は、Logstashエコシステムのためにプラグインを公開することができます。
プラグインは、gemやJavaライブラリの依存関係を宣言できます。
より重要なのは、Logstashのリリース間隔に関係なく、プラグインの改善版をリリースできます。</p>
<p>Rubygemsテクノロジはパッケージングシステム、依存関係管理、ホスティングのために選択されてきました。
Rubyのgemを公開することに慣れている開発者は、Logstashプラグインを簡単に公開することができます。
Elasticsearchはこれらの機能に関して開発者を支援するために、ツールを提供、メンテナンスします。</p>
<h4 id="開発およびローカルでのテスト">開発およびローカルでのテスト</h4>
<p>JRuby <code>1.7.16</code>がプラグインを開発するための唯一の前提条件です。
プラグインにパッチを提供するのは以前と同様です。
例えば、<code>logstash-output-kafka</code>にパッチを送るのは次のようになります。</p>
<ol>
<li><code>git clone https://github.com/logstash-plugins/logstash-output-kafka.git</code></li>
<li>変更</li>
<li>プラグインをローカルでテスト
<ul>
<li><code>bundle install</code></li>
<li><code>bundle exec rspec</code></li>
<li>Logstashの他のバージョンもしくはローカルでテストする場合、Gemfileを編集し、    次のように別のロケーションを加えます。<code>gem &quot;logstash&quot;, :github =&gt; &quot;elasticsearch/logstash&quot;, :ref =&gt; &quot;master&quot;</code></li>
</ul>
</li>
<li>新しいPull Requestを<code>logstash-output-kafka</code>に対して作成</li>
<li>コミュニティでコードレビューを受け、Elasticsearchがパッチを受け入れ</li>
</ol>
<h4 id="バージョン">バージョン</h4>
<p>バージョン情報は、それぞれのプラグインの<code>.gemspec</code>で管理します。
例えば、Apache Kafka outputのgemspecは<a href="https://github.com/logstash-plugins/logstash-output-kafka/blob/master/logstash-output-kafka.gemspec">こちら</a>です。
バージョニングは<a href="http://semver.org/">semantic versioning</a>のルールに従い、
Logstashのバージョニングとは別に、プラグインの開発者によって管理されます。
Logstash 1.5.0がリリースされると、マイルストーン1のプラグインはバージョン1.0.0となり、マイルストーン2のプラグインはバージョン2.0.0となるでしょう。</p>
<h4 id="公開">公開</h4>
<p>開発者が変更を加えプラグインを公開したいと思った時、<code>.gemspec</code>のバージョン番号を変更します。
全テストが成功した時、Elasticsearchはrubygems.orgにプラグインを手動で公開します。
もし、テストが失敗した場合、プラグインは公開されません。
長期的には、プラグインの公開の自動化を行いたいと思っています。
この変更は新しいため、公開の自動化を提供する前に、自動化についてより理解し、プラグインのテスト基盤を改良したいと思っています。</p>
<h4 id="issue">Issue</h4>
<p>Issueは、各プラグインのGitHubリポジトリに対してオープンなければなりません。
Logstashコアのリポジトリは、コアのパイプラインや共通的な機能に関連するIssueについて扱います。</p>
<h4 id="ドキュメント-1">ドキュメント</h4>
<p>プラグインのドキュメントはソースコード自体から生成されます。
それぞれのプラグインのドキュメントは、そのプラグインのリポジトリに含まれます。
Elasticsearchは
<a href="http://www.elasticsearch.org/guide">elasticsearch.org/guide</a>に全てのプラグインのドキュメントを集め生成できる基盤を提供します。</p>
<h4 id="移行">移行</h4>
<p>全ての新しいpull requestとissueは<a href="https://github.com/logstash-plugins">logstash-plugin</a> organisation配下にある各プラグインのリポジトリに対してオープンする必要があります。</p>
<h5 id="すでにあるprはどうすれば良いですか">すでにあるPRはどうすれば良いですか？</h5>
<p>気にしないでください。すでにあるpull requestは開発者によって移行する必要はありません。
LogstashチームがLogstashコアリポジトリに対してのPRを、個別の関連するプラグインのリポジトリに対してマージします。</p>
<pre><code>git clone … # clone the specific plugin repo
# now apply the patch
curl -s https://github.com/elasticsearch/logstash/pull/XXXX | git am --3way
git push
</code></pre><p><strong>Note:このプロセスはすでにあるPRに対してgit historyを管理します</strong></p>
<h5 id="github-issue">GitHub Issue</h5>
<p>現在、LogstashリポジトリにオープンされているIssueは、それぞれのプラグインのリポジトリに移行します。
Logstashチームがgithub.com APIを利用してこの処理を自動的に行います。
安心してください。私たちが個別のプラグインに対する既存のIssueを移行します。</p>
<h2 id="今後のロードマップ">今後のロードマップ</h2>
<p>これは、最初のステップであり、これらの変更は、ユーザや開発者に対してエコシステムをよりよくするために、
しっかりとした基盤を提供します。</p>
<p>短期的には、開発者のためにpull requestのフィードバックでテスト自動化を提供する基盤を追加していきます。
プラグインリポジトリのブートストラップや管理のためのツールも提供していきます。</p>
<p>長期的には、すべてのLogstashプラグインを探し、公開するためのコミュニティポータルを提供したいと思っています。
このアイデアは、Puppet ForgeやAWS marketplaceのようなものです。</p>
<p><a href="http://www.elasticsearch.org/blog/logstash-1-5-0-beta1-released/">Logstash 1.5.0 Beta 1</a>をリリースし、これは新しいエコシステムを提供します。
ぜひ、試していただき、これらの変更に関して感じたことを教えてください。
あなたのフィードバック(<a href="http://twitter.com/elasticsearch">Twitter</a>もしくは<a href="https://github.com/elasticsearch/logstash/issues/new">GitHub</a>)はとても貴重です！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Logstash 1.5.0 Beta1リリース(日本語訳)</title>
      <link>https://blog.johtani.info/blog/2014/12/12/logstash-1-5-0-beta1-released-ja/</link>
      <pubDate>Fri, 12 Dec 2014 17:17:26 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/12/logstash-1-5-0-beta1-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：logstash 1.5.0.beta1 released Logstash 1.5.0 Beta1をリリースしました。こちらのページからダウンロードで</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/logstash-1-5-0-beta1-released/">logstash 1.5.0.beta1 released</a></p>
<p>Logstash 1.5.0 Beta1をリリースしました。<a href="http://www.elasticsearch.org/overview/logstash/download/">こちらのページ</a>からダウンロードできます。</p>
<p><strong>Note: ベータリリースです。本番環境では使用しないでください。</strong></p>
<!-- more -->
<h2 id="150の主な変更点は">1.5.0の主な変更点は？</h2>
<p>1.5.0の主なテーマはプラグイン管理、パフォーマンス改善、<a href="http://kafka.apache.org/">Apache Kafka</a>インテグレーションです。Logstashの主な特徴の1つは
プラグインを利用できることであり、パイプラインの動作を拡張するためにプラグインを追加するのが簡単なことです。
このリリースで、プラグインの開発、管理、公開がより簡単になります。
また、Logstashの速度をより良くしたため、より多くのデータを短時間に処理することができます。
興味ありませんか？では、詳細を見ていきましょう。</p>
<h2 id="plugin-ecosystemの変更">plugin ecosystemの変更</h2>
<p>Logstashは165ものプラグイン(inputs、filters、outputs、codecs)を持っており、
これらはElasticsearchとコミュニティからのコントリビュートで開発されています。
多くのプラグインを管理することは、使いやすさと素早さの間のトレードオフがあります。
Logstashの全てのプラグインをまとめることは使いやすさがある一方、プラグインの更新を取り込むために
Logstashの新しいリリースを待ってもらうことになります。
Logstashからプラグインを分離して個別に配布する場合、更新は簡単になりますが、使いやすさ（特に新しいユーザに）に影響が出ます。</p>
<p>私たちは、プロジェクトを前進させるために、これらのバランスをとることを考えました。
これまで、全ての利用可能なプラグインは’core’と&rsquo;contrib'の2つに分割していました。
&lsquo;core'にあるよく使われるプラグインは、Logstashに含めていました。
コミュニティによりコントリビュートされたプラグインは&rsquo;contrib'パッケージとして分離して配布していました。
1.5.0のリリースで、ユーザに対してより良いプラグイン管理をできるように変更しました。
全てのプラグインは、それ自身によるパッケージに移行しました。
パッケージングフレームワークとしてrubygemsを使い、<a href="http://rubygems.org/">rubygem.org</a>経由でこれらのプラグインを配布、公開します。
また、Logstashにプラグインのインストール、更新、削除を簡単にするための構造も追加しました。</p>
<p>例えば、S3 output pluginをインストールするには、以下のコマンドを実行します。</p>
<pre><code>$LS_HOME/bin/plugin install logstash-output-s3
</code></pre><p>それだけです！Logstashがgemと依存するgemをrubygems.orgからダウンロードし、インストールします。
あなたは、S3にデータを送ることができるようになります。</p>
<p>ダウンロード可能なLogstashリリースはプラグインをまだ多く含んでいますが、
いつでも、個別にプラグインをアップグレードし、インストールすることができます。
プラグインエコシステムの変更に関する詳細のブログ記事をお待ち下さい。</p>
<h2 id="パフォーマンス改善">パフォーマンス改善</h2>
<p>Logstash 1.5.0はより高速になっています。パフォーマンスが改善された2カ所について説明します。</p>
<h3 id="grok-filter">grok filter</h3>
<p>Grok filterはLogstashで、構造化データを抽出するためにパターンを記述するのに使われます。
本リリースで、人気のある幾つかのパターンのgrok filterのスループットを100%に改善しました。
言い換えると、grok filterを使うときに、Logstashを通してより多くのデータを処理することができます。</p>
<p>私たちのベンチマークテストで、1.5.0と1.4.2のスループットの比較をしました。
利用したデータは690万件のApache Webアクセスlogで、<code>COMBINEDAPACHELOG</code>のgrok patternです。
1.5.0で、スループットは34,000 event per sec(eps)から50,000 epsに増加しました。
両方のテストを8コアのマシンでLogstashで8つのワーカーを実行しました。
これらのテストで、一つのgrok filterを実行し、
<code>stdin</code>と<code>stdout</code>を使ったパイプラインでイベントのスループットを計測しました。
全体的なパフォーマンスは、様々なハードウェアやLogstashのコンフィグによって変化することに注意してください。</p>
<h3 id="json-serialization--deserialization">json serialization / deserialization</h3>
<p>JSONのシリアライズ/でシリアライズを<a href="https://github.com/guyboertje/jrjackson">JrJackson</a>ライブラリを利用して実装しました。
これにより、100%以上のスループットの改善がありました。
先ほど説明したパフォーマンステストにおいて、1.3KBのサイズの500,00 JSONイベントを送信し、
16,000 epsから30,000 epsにスループットが改善しました。
45,000サイズのイベントで、850 epsから3500 epsにスループットが増加しました。
すばらしいです。</p>
<h2 id="apache-kafka-integration">apache kafka integration</h2>
<p>いまでは、Apache Kafkaが大規模スケールデータ処理システムでよく利用されます。
Logstashの配備のスケーリングにおいて、Kafkaもまた、shippingインスタンスとindexingインスタンス間の
データを保存するための中間メッセージバッファとして使うことができます。</p>
<p>1.5.0で、Logstash Kafkaのinputとoutputのプラグインのビルトインサポートを追加しました。
これは、<a href="https://github.com/joekiller/logstash-kafka">Joseph Lawson</a>によって最初に開発されました。
私たちは、これらのプラグインにインテグレーションテストとドキュメントを追加することにより改良し、
新しいKafkaの機能を開発し続けます。
また、<a href="http://avro.apache.org/">Apache Avro</a> codecを追加することで、Kafkaに保存されたイベントを
簡単に取得でき、ELKスタックを使ってそれらを解析できるようにしました。</p>
<p>Kafka inputを追加するのは次のコマンドです。</p>
<pre><code>$LS_HOME/bin/plugin install logstash-input-kafka
</code></pre><p>Kafka outputは次のコマンドです。</p>
<pre><code>$LS_HOME/bin/plugin install logstash-output-kafka
</code></pre><h2 id="セキュリティに関する改善">セキュリティに関する改善</h2>
<p>認証と経路暗号化のサポートを追加し、Elasticsearchのoutput、input、filterのセキュリティを改良しました。
例えば、HTTPプロトコルでSSL/TLSにより暗号化を有効にでき、
HTTPベーシック認証をユーザ名とパスワードをリクエストに与えることで設定できます。
これらの機能は、時期にリリースされる<a href="http://www.elasticsearch.org/overview/shield/">Elasticsearch Shield</a>セキュリティプロダクトとLogstashを統合できます。</p>
<h2 id="ドキュメント">ドキュメント</h2>
<p>これまで、Logstashのドキュメントは[logstash.net])(<a href="http://logstash.net/">http://logstash.net/</a>)に置いてあり、
他のELKスタックと一緒に動かす時に、情報を探すのが厄介でした。
1.5.0および、今後のバージョンのドキュメントはelasticsearch.orgの<a href="http://www.elasticsearch.org/guide/en/logstash/current/index.html">Logstash Guide</a>に移行します。
この移行で<a href="http://elasticsearch.org/guide">elasticsearch.org/guide</a>にELKスタックを利用、
学習するためにドキュメントが1つになりました。
このベータリリースのイテレーションで、私たちはプレゼンテーションとドキュメントの品質を改善することに活発に取り組んでいきます。
(過去のLogstashのドキュメントの全てはいままでの<a href="http://logstash.net/docs/1.4.2/">logstash.net</a>で引き続き公開していく予定です。)</p>
<h2 id="バグフィックスと改善">バグフィックスと改善</h2>
<p>ここまでの新しい機能に加えて、Logstash 1.5.0では、多くのバグフィックスと多くの機能改善があります。
ここで、これらのいくつかを紹介します。</p>
<ul>
<li>出力しない&rsquo;metadata'をイベントに格納可能に。これは、例えば、date filterに使う中間フィールドのために必要。(<a href="https://github.com/elasticsearch/logstash/issues/1834">#1834</a>,<a href="https://logstash.jira.com/browse/LOGSTASH-1798"> #LOGSTASH-1798</a>)</li>
<li>HTTPを利用しているときのファイルデスクリプタリークの修正。Logstashがストールするのを防ぎ、OOMエラーからクラッシュするケースも防ぎます。(<a href="https://github.com/elasticsearch/logstash/issues/1604">#1604</a>)</li>
<li>Twitter input:<code>full_tweet</code>オプションの追加、Twitter rate limitingエラーのハンドリング(<a href="https://github.com/elasticsearch/logstash/issues/1471">#1471</a>)</li>
<li>イベントを生成するfilter(multiline、clone、split、metrics)により、
後続の条件文にこれらのイベントを正しく伝搬(<a href="https://github.com/elasticsearch/logstash/issues/1431">#1431</a>)</li>
<li>Elasticsearch output:Logstashはデフォルトで<code>message.raw</code>フィールドを作成しない。messageフィールドはElasticsearch
により<code>not_analyzed</code>でマルチフィールドとして追加される。マルチフィールドはディスクスペースが2倍必要だが、利点がない。</li>
<li>bin/logstashの複数のサブコマンドを除去(<a href="https://github.com/elasticsearch/logstash/issues/1797">#1797</a>)</li>
</ul>
<p>これらの機能、改善、バグフィックスについては、Logstash 1.5.0.Beta1 の<a href="https://github.com/elasticsearch/logstash/blob/master/CHANGELOG">changelog</a>をごらんください。</p>
<h2 id="試してみてください">試してみてください！</h2>
<p>ぜひ、Logstash 1.5.0 Beta 1をダウンロードして試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/logstash/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>2014年のElasticsearch</title>
      <link>https://blog.johtani.info/blog/2014/12/01/about-elasticsearch-in-2014/</link>
      <pubDate>Mon, 01 Dec 2014 18:05:48 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/12/01/about-elasticsearch-in-2014/</guid>
      <description>早いもので、師走です。今年もあと少しとなりました。ということで、Advent Calendarの季節が始まりました。 この記事はElastics</description>
      <content:encoded><p>早いもので、師走です。今年もあと少しとなりました。ということで、Advent Calendarの季節が始まりました。</p>
<p>この記事は<a href="http://qiita.com/advent-calendar/2014/elasticsearch">Elasticsearch Advent Calndar 2014</a>の1日目のエントリです。</p>
<p>1日目ということで、簡単に今年の変遷を振り返りつつ、今年導入された新機能についてピックアップしてみようかと思います。</p>
<!-- more -->
<h2 id="10リリースlucene-460">1.0リリース(Lucene 4.6.0)</h2>
<p>今年一番の目玉と思いますが、1月にRCが公開されて、1.0.0が2月にリリースされました。
（ElasticSearch Serverの翻訳が昨年末に終わってレビューをしていた段階での発表だったので個人的にはきついタイミングでした）
1.0の主な変更点はこちら。</p>
<h3 id="elasticsearchsが小文字に">Elasticsearch（Sが小文字に）</h3>
<p>1.0からSが小文字になりました。(<a href="https://github.com/elasticsearch/elasticsearch/issues/4634">#4634</a>)
0.90以前のバージョンについては、Sが大文字になっています。
ややこしいですが、今年の3月に出版された黒い<a href="http://www.amazon.co.jp/dp/4048662023/ref=as_sl_pc_tf_lc?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=4048662023&amp;adid=0YAE2J8RE36SX7N3R06Z&amp;&amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2F">ElasticSearch Server日本語版</a>は原著が0.20で日本語版にするタイミングで0.90に対応しました。
このため、こちらの書籍のタイトルはSが大文字となっています。
（なお、<a href="https://www.packtpub.com/big-data-and-business-intelligence/elasticsearch-server-second-edition">原著の2nd Edition</a>は小文字になっています）</p>
<h3 id="snapshotrestoreの導入とgatewayの廃止">Snapshot/Restoreの導入とGatewayの廃止</h3>
<p>0.90以前のバージョンでは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/modules-gateway.html#modules-gateway">gatewayというモジュール</a>で、S3などにインデックスのメタデータなどを保存する機能がありました。
この機能は、0.20からlocal以外はdeprecatedとなりました。</p>
<p>インデックスのバックアップ、リストアのために、1.0で実装されたのが<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html#modules-snapshots">Snapshot/Restore</a>です。
Snapshot/Restoreでは、インデックスごと、もしくはクラスタ全体をリモートにあるリポジトリにスナップショットを取ることが可能となりました。
初期リリースの段階では、共有ファイルシステムのみでしたが、現在は、S3やHDFSなどに保存が可能となっています。</p>
<h3 id="aggregation">Aggregation</h3>
<p>Facetをより強力にしたものです。Facetでは、指定したフィールドの集計のみでした。
データの解析などを行うには、独自で集計する必要がありました。
この機能をより柔軟に行えるように実装したのが<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html">Aggregation</a>です。</p>
<p>たとえば、アクセスログを日毎に集計し、さらに日毎の集計に対して国別の集計やユーザエージェントごとの集計をさらに行うといった感じです。
Facetの場合は、日毎の検索結果に対して個別に集計するのみでしたが、Aggregationを使うことで、1週間の検索結果に対して、
日毎に国別の集計を行うといったことが可能になっっています。</p>
<h3 id="cat-api">cat API</h3>
<p>&ldquo;=^.^=&quot;猫が出てくるAPIです。(違う)</p>
<p>Elasticsearchでは、クラスタの状態などが全てREST APIで取得でき、JSONで結果が帰ってきていました。
JSONはプログラムなどで処理を行う場合は便利ですが、コンソールで確認したり、管理系のツールでメールで通知する場合などは見にくいことがあります。
これを解消したのが<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat.html"><code>_cat</code> API</a>です。（<a href="http://www.elasticsearch.org/blog/introducing-cat-api/">公式の紹介ブログはこちら</a>）</p>
<h3 id="circuit-breaker">Circuit Breaker</h3>
<p>OOMが発生しそうなfielddataの読み込みを検知して、事前に防ぐ機構になります。
初期段階ではFielddataに対してのものから実装されました。</p>
<h2 id="11リリースlucene-461">1.1リリース(Lucene 4.6.1)</h2>
<p>3月にリリースされました。Elasticsearchはまだまだ発展しているため、リリースのサイクルが短いのが特徴です。</p>
<p>1.x系では、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#setup-upgrade">Rolling Upgrade</a>が導入されました。このため、クラスタ全体を停止することなく、クラスタのアップグレードが可能になりました。</p>
<h3 id="search-templates">search templates</h3>
<p>検索クエリをテンプレートとして登録することができる<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html#search-template">search templates</a>です。
JSONでクエリを記述できるのは便利ですが、毎回組み立てるのは大変かもしれません。
特に、固定のクエリをプログラムから利用するような場合などです。
テンプレートとして登録しておくことで、検索時に値を埋め込むだけで検索ができるようになりました。</p>
<h3 id="aggregationの強化">Aggregationの強化</h3>
<p>Aggregationの種類が増えました。</p>
<ul>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html#search-aggregations-metrics-cardinality-aggregation">cardinality</a>：ユニークユーザ数の集計などが行えるaggregationです。HyperLogLog++アルゴリズムを利用した実装になっています。</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">significant_terms</a>：単語の数による集計ではなく、コレクション全体に対する単語の頻度と、検索結果に対する単語の頻度を計算することで、重要度を計ることができます。</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html#search-aggregations-metrics-percentile-aggregation">percentiles</a>：パーセンタイル値を計算できます。</li>
</ul>
<h2 id="12リリースlucene-48系">1.2リリース(Lucene 4.8系)</h2>
<h3 id="java-7必須">Java 7必須</h3>
<p>利用しているLuceneがJava 7必須となったためです。また、Java 6のEOLも切れてますし。</p>
<h3 id="dynamic-scriptingがデフォルトオフ">dynamic scriptingがデフォルトオフ</h3>
<p>採用していたMVELがサンドボックス化に対応していないため、危険を回避するためにオフとなりました。</p>
<h3 id="インデキシングとマージング">インデキシングとマージング</h3>
<p>インデキシングとマージ処理に関するさまざまな改善。</p>
<ul>
<li>flushのthreasholdを操作回数ではなく、サイズや時間によるものに変更</li>
<li>デフォルトをConcurrentMergeSchedulerに変更</li>
</ul>
<h2 id="13リリースlucene-490系">1.3リリース(Lucene 4.9.0系)</h2>
<h3 id="セキュリティ関連">セキュリティ関連</h3>
<ul>
<li>JSONPのデフォルトオフ</li>
<li>MVELの非推奨化（1.4で削除）＋<code>script.disable_dynamic</code>のデフォルト値が<code>sandbox</code></li>
</ul>
<h3 id="aggregationの強化-1">aggregationの強化</h3>
<ul>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-top-hits-aggregation.html#search-aggregations-metrics-top-hits-aggregation">top hits</a>：Field Collapsing/combiningと呼ばれる機能です。たとえば、いくつかのサイトのHTMLを収集して検索機能を提供する場合に、ドメインごとに1件ずつ検索結果に出したい場合などに利用できる機能です。</li>
</ul>
<p>その他にも以下のaggregationが追加されています。</p>
<ul>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-rank-aggregation.html">percentile ranks</a></li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-geobounds-aggregation.html">geo bounds</a></li>
</ul>
<h3 id="mappingのtransform">mappingのtransform</h3>
<p>Mappingに<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-transform.html#mapping-transform">transform</a>機能が追加されました。
mappingにドキュメントの値を元に、インデキシング時に変換処理を記述できます。
たとえば、特定のフィールドにある値がある場合にだけ、あるフィールドに値を入れるなどといったことが可能になります。</p>
<h3 id="ディスク関連">ディスク関連</h3>
<ul>
<li>disk based shard allocation deciderが導入されました。ノードのディスクの使用率を元に、シャードを配置しても良いかといった決定を行う機構です。</li>
<li>チェックサムによるファイルのチェック（Lucene4.9で導入されたコードへの切り替え）</li>
</ul>
<h2 id="14リリースlucene-410系">1.4リリース(Lucene 4.10系)</h2>
<p>ベータ版が出されるほど、多くの改善が入っています。</p>
<h3 id="resiliency">resiliency</h3>
<ul>
<li><a href="#memory-mgmt">メモリ使用量の低下</a>によるノードの安定性向上
<ul>
<li>DocValues、リクエストごとのcircuit breakerなど</li>
</ul>
</li>
<li>discoveryアルゴリズムの改善による<a href="#cluster-stability">クラスタの安定性</a>向上</li>
<li><a href="#checksums">チェックサム</a>の導入による破損したデータの検知</li>
</ul>
<h3 id="セキュリティ関連-1">セキュリティ関連</h3>
<ul>
<li>CORSをデフォルト無効</li>
<li>Groovyがデフォルトのスクリプト言語に。</li>
</ul>
<h3 id="aggregationの強化-2">Aggregationの強化</h3>
<p>以下のaggregationが追加されています。</p>
<ul>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-filters-aggregation.html#search-aggregations-bucket-filters-aggregation">filter</a>、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-children-aggregation.html#search-aggregations-bucket-children-aggregation">children</a>、<a href="%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88">scripted_metric</a></li>
</ul>
<h3 id="upgrade-api">Upgrade API</h3>
<p>インデックスを最新のバージョンのものにアップグレードするためのAPIです。
Luceneは下位互換を保ってくれているため、古いバージョンのインデックスも読み込むことが可能です。
ただ、最新バージョンで使える機能が制限されていたりということもあります。
クラスタにあるインデックスをアップグレードするのにかかる時間や必要かどうかといったことを取得できる仕組みも提供します。</p>
<p>また、Lucene自体は、1つ前のメジャーバージョン（4.x系だと3.x系まで）までの互換性は提供していますが、
2つ前のメジャーバージョンの互換性がなくなります。
Luceneも5.x系のブランチが作成されており、5系のリリースにより、3系との互換性がなくなります。
5系のリリースに対応する場合にも、こちらのAPIが助けになるかと。</p>
<h3 id="141">1.4.1</h3>
<p>11/27に<a href="http://blog.johtani.info/blog/2014/11/27/elasticsearch-1-4-1-released-ja/">1.4.1がリリース</a>されました。
シャードの配置やparent/child、nestedドキュメントの改善などが行われています。</p>
<h2 id="まとめ">まとめ</h2>
<p>ということで、駆け足で、1月から11月までのElasticsearchの流れを追ってみました。
1.0で大きな機能追加、改善が行われ、その後も活発に開発が行われています。
要望などがあれば、MLで聞いてみたりやGitHubに登録するなどを行っていただければと。</p>
<p>あと、今年から来年にかけての大きなイベントとして、
<a href="http://www.elasticon.com/">Elasticsearch初のユーザカンファレンスのサイトがオープン</a>しました。
Elasticsearchに関するいろいろな話が聞ける機会だと思うので、興味のある方は見ていただければと。</p>
<p>では、また次のAdvent Calendarで！（最終日の予定ですが、空きがあるのでなにか書くかも）</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.4.1および1.3.6リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2014/11/27/elasticsearch-1-4-1-released-ja/</link>
      <pubDate>Thu, 27 Nov 2014 11:43:32 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/11/27/elasticsearch-1-4-1-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：elasticsearch 1.4.1 and 1.3.6 released 本日、Lucene 4.10.2をベースにしたElas</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/">elasticsearch 1.4.1 and 1.3.6 released</a></p>
<p>本日、<strong>Lucene 4.10.2</strong>をベースにした<strong>Elasticsearch 1.4.1</strong>と、バグフィックスリリースである、<strong>Elasticsearch 1.3.6</strong>をリリースしました。
ダウンロードおよび変更リストはそれぞれ次のリンクからアクセスできます。</p>
<ul>
<li>最新ステーブルリリース：<a href="http://www.elasticsearch.org/downloads/1-4-1">Elasticsearch 1.4.1</a></li>
<li>1.3.x系バグフィックス：<a href="http://www.elasticsearch.org/downloads/1-3-6">Elasticsearch 1.3.6</a></li>
</ul>
<p>過去のリリースに関するブログ（公式）はこちら。</p>
<ul>
<li>1.4:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.4.0</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">1.4.0.Beta1</a></li>
<li>1.3:<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">1.3.5</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-4-released/">1.3.4</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-3-released/">1.3.3</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-2-released/">1.3.2</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-1-released/">1.3.1</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/">1.3.0</a>.</li>
</ul>
<p>すべての変更については<a href="http://www.elasticsearch.org/downloads/1-4-1">1.4.1のリリースノート</a>および<a href="http://www.elasticsearch.org/downloads/1-3-6">1.3.6のリリースノート</a>をごらんください。
以下では、重要な変更について紹介します。</p>
<!-- more -->
<h2 id="shard-allocation">shard allocation</h2>
<p>Elasticsearch 1.3.0で、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#disk">disk based shard allocation</a>が
デフォルトで有効になっています。
もし、ノードのディスクの使用量が<code>law</code>で指定された値（85%）を超えた場合、ノードにはシャードが配置されません。
また、<code>high</code>で指定された値（90%）を超えた場合、シャードを他のノードへ移動します。</p>
<p>Elasticsearch 1.4.1では、disk based shard allocationに3つの改良が追加されました。</p>
<ul>
<li>ディスク使用量のチェックはシャードがクラスタに配置されるタイミングでのみ実施していた。現在は60秒ごとに使用量をチェック。(<a href="https://github.com/elasticsearch/elasticsearch/pull/8270">#8270</a>)</li>
<li>ディスクフルメッセージは<code>DEBUG</code>レベルでログに出力されていました。なぜ、新しいシャードが配置されないのかを説明するのが困難でした。現在は<code>WARN</code>レベルで30秒ごとにログに出力されます。(<a href="https://github.com/elasticsearch/elasticsearch/pull/8382">#8382</a>)</li>
<li>以前は、シャードをもう一つのノードへ動かすべきかどうか決めるとき、allocation deciderはノードにあるシャードのサイズを考慮するだけでした。現在は、動かされるシャードのサイズも考慮します。これにより、必要最小限のシャードの移動量となります。(<a href="https://github.com/elasticsearch/elasticsearch/pull/8569">#8569</a>)</li>
</ul>
<h2 id="parentchild-and-nested-documents">parent/child and nested documents</h2>
<p>Elasticsearch 1.4.0で、parent/childとnestedドキュメントに対して（新しいセグメントを開くときに）固定長ビットセットフィルタを構築しキャッシュしました。クエリ、フィルタおよびAggregationを常に速くするためにです。
多くの<code>nested</code>フィールドを持つユーザにとっては、以前のバージョンよりもヒープの使用量が大きくなってしまいました。</p>
<p><code>nested</code> aggregationによって処理されるドキュメントの順序を変更すること(<a href="https://github.com/elasticsearch/elasticsearch/pull/8454">#8454</a>)によって、固定長ビットセットフィルタが子のドキュメントに対して必要でなくなりました。
現在は、親のドキュメント（つまり、nested<strong>ではない</strong>ドキュメント）を表すフィルタのみをキャッシュしています。これにより必要なキャッシュ空間のサイズを減少しました。(<a href="https://github.com/elasticsearch/elasticsearch/pull/8414">#8414</a>、<a href="https://github.com/elasticsearch/elasticsearch/pull/8440">#8440</a>)</p>
<h2 id="date-ranges">date ranges</h2>
<p>2つの日付範囲に関する問題がこのリリースで修正されました。
1つ目は、日付を丸めるかというものです。例えば、<code>timestamp</code>フィールドに1秒の解像度の値があるとします。
<code>{&quot;lt&quot;: &quot;2014/11/26||/d&quot;}</code>という<code>range</code>フィルタは<code>2014/11/26 00:00:00</code>未満のタイムスタンプのデータを結果として返しました。
しかし、<code>lt</code>を<code>lte</code>に変更した場合、<code>2014/11/27 00:00:00</code>以外の値も含めたいです。</p>
<p>以前は、<code>lte</code>は<code>2014/11/27 00:00:00</code>のタイムスタンプも含めてしまっていました。現在は、想定通りの動作をします。(<a href="https://github.com/elasticsearch/elasticsearch/pull/8556">#8556</a>)</p>
<p>2つ目のバグは日付の範囲条件に<code>now()</code>を利用したaliasとpercolatorフィルタです。
<code>now()</code>の値を、フィルタが作成したタイミングで決定していました。フィルタが実行されるたびに更新せずにです。
<a href="https://github.com/elasticsearch/elasticsearch/pull/8534">#8534</a>で、<code>now()</code>はaliasとpercolatorで想定通りの動作をします。</p>
<h2 id="試してみてください">試してみてください。</h2>
<p>ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-1">Elasticsearch 1.4.1</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>インデックステンプレートとLogstash</title>
      <link>https://blog.johtani.info/blog/2014/11/25/import-apache-accesslog-using-logstash-2/</link>
      <pubDate>Tue, 25 Nov 2014 16:25:46 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/11/25/import-apache-accesslog-using-logstash-2/</guid>
      <description>前回の「Logstashを利用したApacheアクセスログのインポート」の続きです。 前回の記事では、Logstashの設定ファイルについて説</description>
      <content:encoded><p>前回の「<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/">Logstashを利用したApacheアクセスログのインポート</a>」の続きです。
前回の記事では、Logstashの設定ファイルについて説明しました。
今回は「Elasticsearchに設定するインデックステンプレート」について説明します。</p>
<!-- more -->
<h2 id="テンプレートの設定">テンプレートの設定</h2>
<p>Elasticsearchでは、登録するデータの特性に合わせてMappingを定義する方がデータを効率良く扱うことができる場合があります。
この場合、通常ですと、インデックス作成時にMappingを指定します。</p>
<p>ただ、今回は、<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash#output-elasticsearch">インデックス名に「年」を含める形</a>で指定してあります。
「年」はLogstashで処理したデータによって決まります。このため、あらかじめMappingを指定してインデックスを作成するのは難しいです。</p>
<p>このような場合に便利な機能として、「<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#indices-templates">インデックステンプレート</a>」があります。</p>
<h3 id="インデックステンプレートとは">インデックステンプレートとは</h3>
<p>実際のテンプレートの説明に入る前に、少しだけ説明を。
インデックステンプレートとは、インデックスが作成されるタイミングで自動的に適用される設定をテンプレートとして登録できる機能のことです。
実際にテンプレートが適用されるかどうかは、インデックス名で判断されます。</p>
<p>例えば、大して重要でもなく、データ量も少ないインデックス用のテンプレートとして、シャード数が1、レプリカ数が0、&quot;_source&quot;を保存しない設定のテンプレートを登録する場合、
次のようになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">curl -XPUT localhost:<span style="color:#ae81ff">9200</span>/_template/template_1 -d <span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">  &#34;template&#34; : &#34;te*&#34;,
</span><span style="color:#e6db74">  &#34;settings&#34; : {
</span><span style="color:#e6db74">    &#34;number_of_shards&#34; : 1,
</span><span style="color:#e6db74">    &#34;number_of_replicas&#34; : 0
</span><span style="color:#e6db74">  },
</span><span style="color:#e6db74">  &#34;mappings&#34; : {
</span><span style="color:#e6db74">    &#34;type1&#34; : {
</span><span style="color:#e6db74">      &#34;_source&#34; : { &#34;enabled&#34; : false }
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  }
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">&#39;</span>
</code></pre></div><p><code>_template</code>がインデックステンプレートを登録するためのエンドポイントです。
<code>template_1</code>がこのテンプレートのIDです。削除などについては、このIDを利用します。</p>
<p>そして、重要なのは、&ldquo;<code>template</code>&quot;の設定です。
&ldquo;<code>template</code>&quot;には、このテンプレートが適用されるべきインデックス名を記載します。
上記サンプルでは<code>te*</code>となっているため、<code>te</code>で始まる名前のインデックスを作成した場合にテンプレートにある設定が適用されます。</p>
<h3 id="今回利用するテンプレート">今回利用するテンプレート</h3>
<p>私がJJUG CCCや第7回Elasticsearch勉強会のKibana4のデモで利用したインデックスのテンプレートは次のものになります。
&ldquo;<code>template</code>&quot;には、<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/#output-elasticsearch">前回の記事で紹介したoutput/elasticsearchの設定</a> に合致する<code>new_demo_access_log-*</code>を指定しています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">curl -XPUT localhost:<span style="color:#ae81ff">9200</span>/_template/new_access_log_for_demo -d <span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">  &#34;template&#34;: &#34;new_demo_access_log-*&#34;,
</span><span style="color:#e6db74">  &#34;settings&#34;: {
</span><span style="color:#e6db74">    &#34;number_of_shards&#34;: &#34;2&#34;,
</span><span style="color:#e6db74">    &#34;number_of_replicas&#34;: &#34;0&#34;
</span><span style="color:#e6db74">  },
</span><span style="color:#e6db74">  &#34;mappings&#34;: {
</span><span style="color:#e6db74">    &#34;_default_&#34;: {
</span><span style="color:#e6db74">      &#34;dynamic_templates&#34;: [
</span><span style="color:#e6db74">        {
</span><span style="color:#e6db74">          &#34;string_template&#34;: {
</span><span style="color:#e6db74">            &#34;mapping&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;not_analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            },
</span><span style="color:#e6db74">            &#34;match_mapping_type&#34;: &#34;string&#34;,
</span><span style="color:#e6db74">            &#34;match&#34;: &#34;*&#34;
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      ],
</span><span style="color:#e6db74">      &#34;properties&#34;: {
</span><span style="color:#e6db74">        &#34;path&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;multi_field&#34;,
</span><span style="color:#e6db74">          &#34;fields&#34;: {
</span><span style="color:#e6db74">            &#34;no_analyzed&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;not_analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            },
</span><span style="color:#e6db74">            &#34;analyzed&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        },
</span><span style="color:#e6db74">        &#34;referer&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;multi_field&#34;,
</span><span style="color:#e6db74">          &#34;fields&#34;: {
</span><span style="color:#e6db74">            &#34;no_analyzed&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;not_analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            },
</span><span style="color:#e6db74">            &#34;analyzed&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        },
</span><span style="color:#e6db74">        &#34;agent&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;multi_field&#34;,
</span><span style="color:#e6db74">          &#34;fields&#34;: {
</span><span style="color:#e6db74">            &#34;no_analyzed&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;not_analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            },
</span><span style="color:#e6db74">            &#34;analyzed&#34;: {
</span><span style="color:#e6db74">              &#34;index&#34;: &#34;analyzed&#34;,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        },
</span><span style="color:#e6db74">        &#34;geoip&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;object&#34;,
</span><span style="color:#e6db74">          &#34;properties&#34;: {
</span><span style="color:#e6db74">            &#34;location&#34;: {
</span><span style="color:#e6db74">              &#34;geohash&#34;: true,
</span><span style="color:#e6db74">              &#34;geohash_precision&#34;: 10,
</span><span style="color:#e6db74">              &#34;type&#34;: &#34;geo_point&#34;,
</span><span style="color:#e6db74">              &#34;lat_lon&#34;: true,
</span><span style="color:#e6db74">              &#34;geohash_prefix&#34;: true
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        },
</span><span style="color:#e6db74">        &#34;response&#34;: {
</span><span style="color:#e6db74">          &#34;copy_to&#34;: &#34;response_int&#34;,
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;string&#34;
</span><span style="color:#e6db74">        },
</span><span style="color:#e6db74">        &#34;bytes&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;long&#34;
</span><span style="color:#e6db74">        },
</span><span style="color:#e6db74">        &#34;response_int&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;integer&#34;
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  }
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">&#39;</span>
</code></pre></div><h4 id="settings設定">settings設定</h4>
<p>デモ用であり、手元で2台のノードを起動するということもあり、<code>number_of_shards</code>に<code>2</code>を、<code>number_of_replicas</code>に<code>0</code>を指定してあります。</p>
<h4 id="mappings設定">mappings設定</h4>
<h5 id="インデックスのタイプ">インデックスのタイプ</h5>
<p>Mappingsの指定は通常、特定の<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type">タイプ</a>を指定します。
今回のデモでは、1種類しかないのですが、タイプ名を特に意識しないために、<code>_default_</code>を使用しました。
この場合、任意のタイプに適用されることとなります。
タイプを指定してMappingの設定を行う場合は<code>_default_</code>の部分に特定のタイプ名を記入します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">&#34;mappings&#34;: </span>{
  <span style="color:#66d9ef">&#34;_default_&#34;: </span>{
    ...
</code></pre></div><h5 id="ダイナミックテンプレート">ダイナミックテンプレート</h5>
<p>次は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates">ダイナミックテンプレート</a>です。
インデックステンプレートはインデックスの設定をテンプレート化しました。ダイナミックテンプレートはフィールドに対してテンプレートを設定できます。</p>
<p>以下のダイナミックテンプレートでは、<code>string</code>タイプのフィールドのデフォルト設定を変更しています。
通常、<code>string</code>タイプのフィールドは<code>analyzed</code>となりますが、<code>not_analyzed</code>に変更してあります。
詳しく検索したいフィールドの方が少ないためです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">...
<span style="color:#66d9ef">&#34;dynamic_templates&#34;: </span>[
  {
    <span style="color:#66d9ef">&#34;string_template&#34;: </span>{
      <span style="color:#66d9ef">&#34;mapping&#34;: </span>{
        <span style="color:#66d9ef">&#34;index&#34;: </span><span style="color:#e6db74">&#34;not_analyzed&#34;</span>,
        <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;string&#34;</span>
      },
      <span style="color:#66d9ef">&#34;match_mapping_type&#34;: </span><span style="color:#e6db74">&#34;string&#34;</span>,
      <span style="color:#66d9ef">&#34;match&#34;: </span><span style="color:#e6db74">&#34;*&#34;</span>
    }
  }
],
...  
</code></pre></div><h5 id="multi_field指定">multi_field指定</h5>
<p>検索もしたいし、Terms Aggregationでも利用したいフィールドについては、<code>multi_field</code>を利用して、
<code>analyzed</code>と<code>not_analyzed</code>の2種類のフィールドを用意しています。
<code>multi_field</code>設定を用いることで、1つのJSONのデータから、異なる形のフィールドを用意することが可能です。</p>
<p>今回のテンプレートでは、<code>path</code>、<code>referer</code>、<code>agent</code>に<code>multi_field</code>を指定しました。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">...
<span style="color:#66d9ef">&#34;path&#34;: </span>{
  <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;multi_field&#34;</span>,
  <span style="color:#66d9ef">&#34;fields&#34;: </span>{
    <span style="color:#66d9ef">&#34;no_analyzed&#34;: </span>{
      <span style="color:#66d9ef">&#34;index&#34;: </span><span style="color:#e6db74">&#34;not_analyzed&#34;</span>,
      <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;string&#34;</span>
    },
    <span style="color:#66d9ef">&#34;analyzed&#34;: </span>{
      <span style="color:#66d9ef">&#34;index&#34;: </span><span style="color:#e6db74">&#34;analyzed&#34;</span>,
      <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;string&#34;</span>
    }
  }
},
...
</code></pre></div><p>例えば、上記の設定の場合、入力のJSONは<code>path</code>というデータのみですが、インデックス上には<code>path.no_analyzed</code>と
<code>path.analyzed</code>というフィールドができあがります。
実際に検索する場合は、<code>path.analyzed:検索したい文字列</code>という形で検索をすることで、いわゆる部分一致のような検索が可能です。
また、完全一致をしたい場合は<code>path.no_analyzed:検索したい文字列</code>という指定になります。
用途を考えると、<code>request</code>も指定したほうが良いかもしれません。</p>
<h5 id="geoip">geoip</h5>
<p><a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash#filter-geoip">Logstashでgeoipデータ</a>を付与していました。
このgeoipのデータをKibana4で利用するために、geoデータとして登録する必要があります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">&#34;geoip&#34;: </span>{
  <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;object&#34;</span>,
  <span style="color:#66d9ef">&#34;properties&#34;: </span>{
    <span style="color:#66d9ef">&#34;location&#34;: </span>{
      <span style="color:#66d9ef">&#34;geohash&#34;: </span><span style="color:#66d9ef">true</span>,
      <span style="color:#66d9ef">&#34;geohash_precision&#34;: </span><span style="color:#ae81ff">10</span>,
      <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;geo_point&#34;</span>,
      <span style="color:#66d9ef">&#34;lat_lon&#34;: </span><span style="color:#66d9ef">true</span>,
      <span style="color:#66d9ef">&#34;geohash_prefix&#34;: </span><span style="color:#66d9ef">true</span>
    }
  }
},
</code></pre></div><p>上記の設定がgeoデータの指定です。
<code>type</code>に<code>object</code>が指定してありますが、これは、geoipのデータがネストしているためです。
geoipオブジェクトのうち、緯度経度のデータは<code>location</code>に入っているため、こちらに緯度経度関係の設定を指定します。</p>
<ul>
<li><code>&quot;type&quot;: &quot;geo_point&quot;</code>：<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html#mapping-geo-point-type"><code>geo_point</code></a>タイプであることを指定</li>
<li><code>&quot;geohash&quot;: true</code>：緯度経度のデータをもとに、geohashの値もインデックス</li>
<li><code>&quot;geohash_precision&quot;: 10</code>：geohashの精度の指定</li>
<li><code>&quot;lat_lon&quot;: true</code>：緯度経度を個別の<code>.lat</code>、<code>.lon</code>というフィールドにもインデックス</li>
<li><code>&quot;geohash_prefix&quot;: true</code>：該当するgeohashのみでなく、その親にあたるgeohashについてもインデックスする</li>
</ul>
<h5 id="responseresponse_intbytes">response、response_int、bytes</h5>
<p>最後は、response、response_int、bytesです。</p>
<p>responseには、HTTPステータスコードが入ります。
文字列としても扱いたいですが、integerとして、Renge Aggregationなどを行いたいので、
response_intというフィールドにも値を入れています。
<code>multi_field</code>でも可能ですが、ここでは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#copy-to"><code>copy_to</code></a>を利用しました。
<code>copy_to</code>を用いることで、異なるフィールドに値をコピーすることができます。</p>
<p>bytesについては、longで扱いたいとういう理由だけです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">
<span style="color:#66d9ef">&#34;response&#34;: </span>{
  <span style="color:#66d9ef">&#34;copy_to&#34;: </span><span style="color:#e6db74">&#34;response_int&#34;</span>,
  <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;string&#34;</span>
},
<span style="color:#66d9ef">&#34;bytes&#34;: </span>{
  <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;long&#34;</span>
},
<span style="color:#66d9ef">&#34;response_int&#34;: </span>{
  <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;integer&#34;</span>
}
</code></pre></div><h2 id="まとめ">まとめ</h2>
<p>今回はデモに利用したインデックスてプレートについて説明しました。
前回の、Logstashの設定とこのインデックステンプレートを用いることで、Kibanaで解析するデータの準備ができます。
実際の操作などについては、また次回の記事で説明しようかと思います。</p>
<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Logstashを利用したApacheアクセスログのインポート</title>
      <link>https://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/</link>
      <pubDate>Fri, 21 Nov 2014 17:30:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/</guid>
      <description>JJUG CCCや第7回Elasticsearch勉強会のKibana4のデモにアクセスログを利用しました。 ただ、セッションでは、どうやってElas</description>
      <content:encoded><p>JJUG CCCや第7回Elasticsearch勉強会のKibana4のデモにアクセスログを利用しました。</p>
<p>ただ、セッションでは、どうやってElasticsearchに投入したのかという詳しい話をしていませんでした。
本記事では、データ取り込み時に利用したLogstashの設定ファイルについて説明します。</p>
<!-- more -->
<p>Logstashの設定の説明に入る前に、全体の流れを。
「ApacheアクセスログをKibana4により可視化」です。</p>
<h2 id="材料の準備">材料の準備</h2>
<p>「ApacheアクセスログをKibana4により可視化」に必要な材料は次の通りです。
（今回は起動するところまでいかないので、実際に必要なのは次回以降になります。）</p>
<ul>
<li>Java 7（u55以上を1つ）</li>
<li>Logstash 1.4.2（1つ）</li>
<li>Elasticsearch 1.4.0（1つ）</li>
<li>Kibana4 Beta2（1つ）</li>
<li>Apacheのアクセスログ（適量）</li>
</ul>
<p>Apacheのアクセスログ以外は、公式サイトからダウンロードできます。
それぞれをダウンロードして、起動できるようにしておきましょう。</p>
<p>※1台のマシン上で行う場合は、アクセスログの量を少なめにするなどの対策をとりましょう。
※今回は、1台のマシン（Mac）上で、VMなどを利用せず、それぞれ直接起動するものとします。</p>
<h2 id="可視化の手順と流れ">可視化の手順と流れ</h2>
<p>可視化の流れとしては、</p>
<ol>
<li>Logstashでファイルを読み込み、各種処理（パースしたり、情報を追加したり、切り出したり）</li>
<li>Elasticsearchに保存</li>
<li>Kibanaでグラフを作ったり、検索してみたり</li>
</ol>
<p>です。</p>
<p>今回は、1のLogstashでファイルを読み込んだりする設定ファイルの説明です。</p>
<h3 id="logstashの設定">Logstashの設定</h3>
<h4 id="logstashの基本">Logstashの基本</h4>
<p>まずは、Logstashの設定ですが、簡単にLogstashの説明を。
Logstashは大きく3つのパーツに分かれています。</p>
<ol>
<li>input：データの入力処理</li>
<li>filter：inputで読み込んだデータに対する操作など</li>
<li>output：データの出力処理</li>
</ol>
<p>inputでデータを読み込み（複数可）、filterでデータに対して各種処理を行い、outputでデータを指定されたところに出力（複数可）します。</p>
<h4 id="アクセスログの読み込み設定">アクセスログの読み込み設定</h4>
<p>アクセスログの読み込み処理は大まかに次のようなものとなります。</p>
<ol>
<li>アクセスログを読み込む（input/file）</li>
<li>読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</li>
<li>日付のパース（filter/date）</li>
<li>クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</li>
<li>リクエストのパスの第1階層の抽出（filter/grok）</li>
<li>ユーザエージェントのパース（filter/useragent）</li>
<li>Elasticsearchへの出力（output/elasticsearch）</li>
</ol>
<p>設定ファイルは次のようなものになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">input {
  file {
    path <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;/Users/johtani/demo_access_log/*/*.log&#34;</span>
    start_position <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;beginning&#34;</span>
  }
}

filter {
  grok {
    match <span style="color:#f92672">=&gt;</span> { <span style="color:#e6db74">&#34;message&#34;</span> <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;%{COMBINEDAPACHELOG}&#34;</span> }
    break_on_match <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">false</span>
    tag_on_failure <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;_message_parse_failure&#34;</span><span style="color:#f92672">]</span>
  }
  date {
    match <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;timestamp&#34;</span>, <span style="color:#e6db74">&#34;dd/MMM/YYYY:HH:mm:ss Z&#34;</span><span style="color:#f92672">]</span>
    locale <span style="color:#f92672">=&gt;</span> en
  }
  geoip {
    source <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;clientip&#34;</span><span style="color:#f92672">]</span>
  }
  grok {
    match <span style="color:#f92672">=&gt;</span> { <span style="color:#e6db74">&#34;request&#34;</span> <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;^/%{WORD:first_path}/%{GREEDYDATA}$&#34;</span> }
    tag_on_failure <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;_request_parse_failure&#34;</span><span style="color:#f92672">]</span>
  }
  useragent {
    source <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;agent&#34;</span>
    target <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;useragent&#34;</span>
  }
}

output {
  elasticsearch {
    host <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;localhost&#34;</span>
    index <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;new_demo_access_log-%{year}&#34;</span>
    cluster <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;demo_cluster&#34;</span>
    protocol <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;http&#34;</span>
  }
}
</code></pre></div><h5 id="1-アクセスログを読み込むinputfile">1. アクセスログを読み込む（input/file）</h5>
<p>inputの<a href="http://logstash.net/docs/1.4.2/inputs/file">fileモジュール(a)</a>を使用してアクセスログのファイルを読み込みます。
<code>path</code>でアクセスログのファイルのパスを指定します。
今回利用したアクセスログは<code>demo_access_log/2010/access20100201.log</code>といった日毎のファイルに分割されていたため、
<code>*</code>を利用してファイルのパスを指定しました。
また、今回は既存のファイルの読み込みだけのため、<code>start_position</code>に<code>beginning</code>を指定してあります。
デフォルトでは<code>end</code>が指定されるため、Logstashを起動後に追記されたログから対象になってしまうためです。
その他の設定については、公式ガイドをご覧ください。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">input {
  file { <span style="color:#75715e"># a</span>
    path <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;/Users/johtani/demo_access_log/*/*.log&#34;</span> <span style="color:#75715e"># b</span>
    start_position <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;beginning&#34;</span> <span style="color:#75715e"># c</span>
  }
}
</code></pre></div><blockquote>
<p>Logstashでは、ファイルをどこまで読み込んだかという情報を保持するために、<a href="http://logstash.net/docs/1.4.2/inputs/file#sincedb_path">sincedb</a>を利用しています。
設定変更後に同じファイルを最初から読み込みたい場合などは、こちらのファイルを一旦削除するなどの対応が必要です。</p>
</blockquote>
<p>ちなみに、読み込んだデータは次のようなJSONになっています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;message&#34;</span>: <span style="color:#e6db74">&#34;読み込んだアクセスログ&#34;</span>,
  <span style="color:#f92672">&#34;@version&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
  <span style="color:#f92672">&#34;@timestamp&#34;</span>:<span style="color:#e6db74">&#34;2014-11-21T06:16:21.644Z&#34;</span>,
  <span style="color:#f92672">&#34;host&#34;</span>:<span style="color:#e6db74">&#34;jupiter.local&#34;</span>,
  <span style="color:#f92672">&#34;path&#34;</span>:<span style="color:#e6db74">&#34;/Users/johtani/demo_access_log/2010/access20100201.log&#34;</span>}
<span style="color:#960050;background-color:#1e0010">}</span>
</code></pre></div><p>特に指定がない場合は、<code>message</code>に読み込んだデータが入ってきます。
<code>@timestamp</code>がLogstashが読み込んだ時刻、<code>host</code>はLogstashが動作しているホスト名です。
<code>path</code>はfileモジュールが読み込んだファイルのパスを設定しています。
この後の処理で、どこの項目に対して処理を行うかといったことが重要になるので、</p>
<h5 id="2-読み取ったアクセスログを各フィールドipアドレスユーザエージェントなどに分割filtergrok">2. 読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</h5>
<p>2.〜6.の処理は、inputで読み込んだ1アクセスログに対する処理となります。</p>
<p>ここでは、<a href="http://logstash.net/docs/1.4.2/filters/grok">grokフィルタ</a>を使用して
Apacheのアクセスログを各フィールドに分割します。
Logastashでは、簡単に使えるようにいくつかの<a href="https://github.com/elasticsearch/logstash/tree/v1.4.2/patterns">パターン</a>が用意されています。
Apacheのログのために、<a href="https://github.com/elasticsearch/logstash/blob/v1.4.2/patterns/grok-patterns#L91"><code>COMBINEDAPACHELOG</code></a>というのが用意されています。
今回はこちらを使用しています。その他にも日付などパターンが用意されているので、試してみてください。</p>
<p><code>message</code>にアクセスログが入っているので、こちらの項目に対して<code>COMBINEDAPACHELOG</code>のパターンを
<code>match</code>で適用してフィールドに抜き出します。
<code>tag_on_failure</code>は、<code>match</code>でパースに失敗した場合に、<code>tag</code>というフィールドに指定した文字列を出力する機能になります。
デフォルトだと<code>_grokparsefailure</code>が付与されますが、ここでは、どの処理で失敗したがを判別するために文字列を変更しています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">filter {
  grok {
    match <span style="color:#f92672">=&gt;</span> { <span style="color:#e6db74">&#34;message&#34;</span> <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;%{COMBINEDAPACHELOG}&#34;</span> }
    break_on_match <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">false</span>
    tag_on_failure <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;_message_parse_failure&#34;</span><span style="color:#f92672">]</span>
  }
  <span style="color:#f92672">...</span>
</code></pre></div><p><code>clientip</code>、<code>ident</code>、<code>auth</code>、<code>timestamp</code>、<code>verb</code>、<code>request</code>、<code>httpversion</code>、<code>response</code>、<code>bytes</code>、<code>referrer</code>、<code>agent</code>がgrokフィルタにより抜き出された項目です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;message&#34;</span>:<span style="color:#e6db74">&#34;アクセスログ&#34;</span>,
  <span style="color:#f92672">&#34;@version&#34;</span>:<span style="color:#e6db74">&#34;1&#34;</span>,
  <span style="color:#f92672">&#34;@timestamp&#34;</span>:<span style="color:#e6db74">&#34;2014-11-21T07:20:54.387Z&#34;</span>,
  <span style="color:#f92672">&#34;host&#34;</span>:<span style="color:#e6db74">&#34;jupiter.local&#34;</span>,
  <span style="color:#f92672">&#34;path&#34;</span>:<span style="color:#e6db74">&#34;/Users/johtani/demo_access_log/2010/access20100201.log&#34;</span>,
  <span style="color:#f92672">&#34;clientip&#34;</span>:<span style="color:#e6db74">&#34;クライアントのIPアドレス&#34;</span>,
  <span style="color:#f92672">&#34;ident&#34;</span>:<span style="color:#e6db74">&#34;-&#34;</span>,
  <span style="color:#f92672">&#34;auth&#34;</span>:<span style="color:#e6db74">&#34;-&#34;</span>,
  <span style="color:#f92672">&#34;timestamp&#34;</span>:<span style="color:#e6db74">&#34;01/Feb/2010:00:00:26 +0900&#34;</span>,
  <span style="color:#f92672">&#34;verb&#34;</span>:<span style="color:#e6db74">&#34;GET&#34;</span>,
  <span style="color:#f92672">&#34;request&#34;</span>:<span style="color:#e6db74">&#34;/images/favicon.ico&#34;</span>,
  <span style="color:#f92672">&#34;httpversion&#34;</span>:<span style="color:#e6db74">&#34;1.1&#34;</span>,
  <span style="color:#f92672">&#34;response&#34;</span>:<span style="color:#e6db74">&#34;200&#34;</span>,
  <span style="color:#f92672">&#34;bytes&#34;</span>:<span style="color:#e6db74">&#34;318&#34;</span>,
  <span style="color:#f92672">&#34;referrer&#34;</span>:<span style="color:#e6db74">&#34;\&#34;-\&#34;&#34;</span>,
  <span style="color:#f92672">&#34;agent&#34;</span>:<span style="color:#e6db74">&#34;\&#34;Mozilla/5.0 (Windows; U; Windows NT 5.1; ja; rv:1.9.1.7) Gecko/20091221 Firefox/3.5.7 (.NET CLR 3.5.30729)\&#34;&#34;</span>
}
</code></pre></div><h5 id="3-日付のパースfilterdate">3. 日付のパース（filter/date）</h5>
<p>Logstashは特に指定がない場合、inputでデータを取り出した日付が<code>@timestamp</code>となります。
そして、このフィールドが特に指定がない場合は、Elasticsearchのデータの日付となり、Kibanaで利用する日付となります。</p>
<p>リアルタイムにアクセスログを読み込む場合は、読み込んだ日時でもほぼ問題はありませんが、過去データの場合はそうもいきません。
そこで、<a href="http://logstash.net/docs/1.4.2/filters/date"><code>dateフィルタ</code></a>を使用して、<code>@timestamp</code>の値を書き換えます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">date {
  match <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;timestamp&#34;</span>, <span style="color:#e6db74">&#34;dd/MMM/YYYY:HH:mm:ss Z&#34;</span><span style="color:#f92672">]</span>
  locale <span style="color:#f92672">=&gt;</span> en
}
</code></pre></div><p>上記では、<code>timestamp</code>という項目に対して<code>dd/MMM/YYYY:HH:mm:ss Z</code>という日付パターンの場合に値を書き換える設定となります。
なお、日付の月の部分が<code>Feb</code>となっているため、<code>locale</code>に<code>en</code>を指定しています。Logstashが動作するマシンの<code>locale</code>が<code>ja</code>などの場合にパースに失敗するためです。</p>
<h5 id="a-namefilter-geoip4-クライアントipアドレスにgeoipの情報を付加filtergeoipa"><a name="filter-geoip">4. クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</a></h5>
<p>どの国からのアクセスかなどを判別したいので、IPアドレスを元にgeoipを利用してより詳細な情報を付与します。
Logstashでもこの機能が用意されており、簡単に利用ができます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">geoip {
  source <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;clientip&#34;</span><span style="color:#f92672">]</span>
}
</code></pre></div><p>これだけです。対象とするIPアドレスのフィールドを指定しているだけです。
<code>geoip</code>というフィールドが追加され、次のような情報が付与されます。
国名、緯度経度、タイムゾーンなどです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#960050;background-color:#1e0010">...</span>  
  <span style="color:#f92672">&#34;geoip&#34;</span>: {
    <span style="color:#f92672">&#34;ip&#34;</span>: <span style="color:#e6db74">&#34;IPアドレス&#34;</span>,
    <span style="color:#f92672">&#34;country_code2&#34;</span>: <span style="color:#e6db74">&#34;JP&#34;</span>,
    <span style="color:#f92672">&#34;country_code3&#34;</span>: <span style="color:#e6db74">&#34;JPN&#34;</span>,
    <span style="color:#f92672">&#34;country_name&#34;</span>: <span style="color:#e6db74">&#34;Japan&#34;</span>,
    <span style="color:#f92672">&#34;continent_code&#34;</span>: <span style="color:#e6db74">&#34;AS&#34;</span>,
    <span style="color:#f92672">&#34;latitude&#34;</span>: <span style="color:#ae81ff">36</span>,
    <span style="color:#f92672">&#34;longitude&#34;</span>: <span style="color:#ae81ff">138</span>,
    <span style="color:#f92672">&#34;timezone&#34;</span>: <span style="color:#e6db74">&#34;Asia/Tokyo&#34;</span>,
    <span style="color:#f92672">&#34;location&#34;</span>: [
      <span style="color:#ae81ff">138</span>,
      <span style="color:#ae81ff">36</span>
    ]
  }
  <span style="color:#960050;background-color:#1e0010">...</span>
}
</code></pre></div><h5 id="5-リクエストのパスの第1階層の抽出filtergrok">5. リクエストのパスの第1階層の抽出（filter/grok）</h5>
<p>リクエストされたURLは<code>request</code>フィールドにありますが、個別のURLだと、大まかな集計が大変です。
もちろん、クエリで処理することもできますが、Logstashで処理するついでに、第1階層のディレクトリ名を抽出しておくことで、
検索や集計を行いやすくしておきます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">grok {
  match <span style="color:#f92672">=&gt;</span> { <span style="color:#e6db74">&#34;request&#34;</span> <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;^/%{WORD:first_path}/%{GREEDYDATA}$&#34;</span> }
  tag_on_failure <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;_request_parse_failure&#34;</span><span style="color:#f92672">]</span>
}
</code></pre></div><p>また、grokフィルタの登場です。
今回は、<code>WORD:first_path</code>という記述方法で、<code>WORD</code>パターンにマッチした文字列を<code>first_path</code>というフィールドに展開する指定をしています。</p>
<p>例えば、サイトのスクリプトなどが<code>scripts</code>というディレクトリにある場合は、<code>first_path</code>の値を利用して、
後続のフィルタでログデータを出力しないといった処理にも使えます。</p>
<h5 id="6-ユーザエージェントのパースfilteruseragent">6. ユーザエージェントのパース（filter/useragent）</h5>
<p>Logstashではユーザエージェントの文字列から、いくつかの情報を付与するフィルタも用意されています。
<a href="http://logstash.net/docs/1.4.2/filters/useragent"><code>useragent</code>フィルタです。</a></p>
<pre><code>useragent {
  source =&gt; &quot;agent&quot;
  target =&gt; &quot;useragent&quot;
}
</code></pre><p><code>agent</code>というフィールドにユーザエージェントの文字列があるので、このフィールドに対してフィルタを適用します。
元の文字列も取っておきたいので、<code>useragent</code>という別のフィールドに出力するように指定してあります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#e6db74">&#34;useragent&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {
  <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;Firefox&#34;</span>,
  <span style="color:#f92672">&#34;os&#34;</span>: <span style="color:#e6db74">&#34;Windows XP&#34;</span>,
  <span style="color:#f92672">&#34;os_name&#34;</span>: <span style="color:#e6db74">&#34;Windows XP&#34;</span>,
  <span style="color:#f92672">&#34;device&#34;</span>: <span style="color:#e6db74">&#34;Other&#34;</span>,
  <span style="color:#f92672">&#34;major&#34;</span>: <span style="color:#e6db74">&#34;17&#34;</span>,
  <span style="color:#f92672">&#34;minor&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>
}<span style="color:#960050;background-color:#1e0010">,</span>
</code></pre></div><p>このように、OS名やバージョン名などが抽出できます。</p>
<h5 id="a-nameoutput-elasticsearch7-elasticsearchへの出力outputelasticsearcha"><a name="output-elasticsearch">7. Elasticsearchへの出力（output/elasticsearch）</a></h5>
<p>最後は、<a href="http://logstash.net/docs/1.4.2/outputs/elasticsearch">Elasticsearchへのデータの出力設定</a>です。</p>
<p><code>index</code>にて、出力するindex名を指定してあります。
また、年毎のインデックス名にするために<code>%{year}</code>を利用しています。
<a href="http://logstash.net/docs/1.4.2/configuration#sprintf">sprintf format</a>です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby">elasticsearch {
  host <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;localhost&#34;</span>
  index <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;new_demo_access_log-%{year}&#34;</span>
  cluster <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;demo_cluster&#34;</span>
  protocol <span style="color:#f92672">=&gt;</span> <span style="color:#e6db74">&#34;http&#34;</span>
}
</code></pre></div><h2 id="まとめ">まとめ</h2>
<p>ということで、今回はアクセスログをLogstashにて読み込む時の設定について説明してきました。
次回は、実際にLogstashを起動してElasticsearchにデータを登録するところまでを説明します。</p>
<p>JJUG CCCや勉強会のデモに用いたデータは、
Elasticsearchにデータを登録する前にテンプレートも設定してありました。こちらについても、次回説明しようと思います。</p>
<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第7回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2014/11/19/hold-on-7th-elasticsearch-jp/</link>
      <pubDate>Wed, 19 Nov 2014 11:19:07 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/11/19/hold-on-7th-elasticsearch-jp/</guid>
      <description>第7回Elsticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、開場提供していただいたリクルートテクノロジーズさん</description>
      <content:encoded><p><a href="http://elasticsearch.doorkeeper.jp/events/16837">第7回Elsticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<p>昨日も紹介しましたが、<a href="http://qiita.com/advent-calendar/2014/elasticsearch">Elasticsearch Advent Calendar 2014</a>を用意してみました。まだ、空きがありますので、登録お待ちしております！</p>
<!-- more -->
<p>今回は出足が好調で、早々に180人の枠を超えるという嬉しい事態でした。
最終的な参加人数は130名程度で、懇親会参加者が50名弱といったところです。</p>
<h2 id="kibana4">「Kibana4」</h2>
<h3 id="elasticsearch-inc-jun-ohtani-johtani">Elasticsearch Inc. Jun Ohtani @johtani</h3>
<p>スライド：<a href="https://speakerdeck.com/johtani/kibana4">Kibana4</a></p>
<p>ということで、Kibana4の紹介と、Kibana4のBeta2を利用したデモを行いました。
デモの開始のところで少し環境がうまく動いてなくて手間取ってしまいましたが。。。</p>
<p>発表で1点だけ修正があります。JRubyを選択しているのがElasticsearchのライブラリを使用するためという説明をしましたが、
こちらは、Logstashに関する話でした。Kibana4は現時点では、ElasticsearchへのProxyとしての動作が主なものとなります。Rubyでも動作可能です。
bin/kibanaについてはJavaを使った起動になります。
参考：https://github.com/elasticsearch/kibana/tree/master/src/server</p>
<p>発表でも主張しましたが、ダウンロードして、Elasticsearchを用意すれば簡単に動作させることが可能です。
ぜひ、ローカルで試して見てもらえればと思います。
今回のデモのデータを入れるのに利用したLogstashの設定などについては、ブログで記事を書こうと思います。</p>
<h2 id="niconicoの検索を支えるelasticsearch">niconicoの検索を支えるElasticsearch</h2>
<h3 id="株式会社ドワンゴ-伊藤-祥-さん">株式会社ドワンゴ 伊藤 祥 さん</h3>
<p>スライド：<a href="https://speakerdeck.com/shoito/niconico-elasticsearch">niconicoの検索を支えるElasticsearch</a></p>
<ul>
<li>リアルタイム検索の実現、新しい検索への対応</li>
<li>検索のアーキテクチャとか。</li>
<li>Capistranoでデプロイとかを管理</li>
<li>1.4.1が出たら、クラスタを更新予定</li>
</ul>
<p>ということで、実際に導入した話から、現在の運用の仕方、クラスタのアップグレードなど多岐にわたる内容でおもしろかったです。
遭遇した問題点とかもあったので。
Marvel便利なのでぜひ導入を検討してもらえればw</p>
<h2 id="elasticsearch-at-crowdworks">Elasticsearch at CrowdWorks </h2>
<h3 id="株式会社クラウドワークス-九岡-佑介-さん-mumoshu">株式会社クラウドワークス 九岡 佑介 さん @mumoshu</h3>
<p>スライド：<a href="http://www.slideshare.net/mumoshu/20141118-es">Elasticsearch at CrowdWorks</a></p>
<ul>
<li>会社の紹介</li>
<li>仕事が検索対象</li>
<li>検索時間が1桁減少！</li>
<li>Graceful Degradationで失敗したら、InnoDB FTSで代替：<a href="https://github.com/crowdworks/gracefully">Gracefully</a></li>
<li><a href="http://www.found.no">found.no</a>のサービスを利用</li>
<li>elasticsearch-modelの拡張を作成してOSSとして公開：<a href="https://github.com/crowdworks/elasticsearch-model-extensions">elasticsearch-model-extensions</a></li>
</ul>
<p>Gracefullyで切り替えとかは面白いなと思いました。
検索での利用の話でしたが、他のシーンでも使えそうですよね。
日本にFoundユーザがいるのも初めて知りました。
彼らの開発者ブログも質の良い情報が載っているので、参考になりますよね。</p>
<p>次は、どんなMappingで運用しているのかとか、どういった工夫をしているかといった点を詳しく聞きたいなと思いました。
またお待ちしております。</p>
<h2 id="1分で作るelasticsearchプラグイン">1分で作るElasticsearchプラグイン</h2>
<h3 id="株式会社エヌツーエスエム-菅谷-信介-さん">株式会社エヌツーエスエム 菅谷 信介 さん</h3>
<p>スライド：<a href="http://www.slideshare.net/shinsuke/plugins-ates7">Elasticsearchプラグインの作り方</a></p>
<p>* プラグインの作り方とか。</p>
<ul>
<li>十数個のプラグインの紹介。プラグインはこちらで公開中。<a href="https://github.com/codelibs/">https://github.com/codelibs/</a></li>
<li>実際に、業務で必要なものから作成</li>
<li>まだまだ作りたいものがある</li>
</ul>
<p>コミュニティ還元できるものはPR送ってもらえるとうれしいです。
前よりは体制も増えてるので、PRも目にとまるようになってるはずです。</p>
<p>あとは、使ってみたいと思う方も多数いると思うので、ぜひ、OSSなので、貢献しましょう！
フィードバックがあるだけで、OSS活動やってるものにとってはやる気につながると思いますし。</p>
<h2 id="ltgisとして活用するelasticsearch">LT：GISとして活用するElasticsearch </h2>
<h3 id="船戸-隆さん">船戸 隆さん</h3>
<p>スライド：<a href="https://speakerdeck.com/tfunato/gistositehuo-yong-suruelasticsearch">GISとして活用するElasticsearch </a></p>
<ul>
<li>java-jaからIngressの青（Registance）の勧誘に来られた方w</li>
<li>APIをハックして、情報を取得し、Kibanaで可視化</li>
<li>残念ながら、APIが変更されて見れなくなったらしい。</li>
</ul>
<p>Ingress実際にやったことはないのですが、おもしろそうでした。
発表される方の会社の採用紹介ではなく、Ingressの勧誘をされるとは想定外でしたw</p>
<p>興味のあるデータをKibanaで可視化するのも面白い例だと思うので、活用してもらえればと思います。</p>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<p>適当に見つけたブログを列挙してあります。これもあるよ！などあれば、教えてください。</p>
<ul>
<li><a href="http://blog.yoslab.com/entry/2014/11/18/203159">勉強会メモ - 第7回elasticsearch勉強会</a></li>
<li><a href="http://qiita.com/t-sato/items/940ccfa9e4a668b91967">第7回elasticsearch勉強会 #elasticsearch #elasticsearchjp</a></li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>JJUGの時とは違い、Elasticsearch勉強会ではさすがに、企業としてのElasticsearchの知名度が高かったのはありがたいことでした。
自分の発表のために始めた勉強会でもありますが、まだまだ、発表するときは緊張しますし、分かりにくいんじゃないかなぁと思うことも多々あります。
この辺がわかりにくかった、この辺をもっと知りたいなど、フィードバックをお待ちしております。</p>
<p>冒頭にも書きましたが、<a href="http://qiita.com/advent-calendar/2014/elasticsearch">Elasticsearch Advent Calendar 2014</a>の登録をお待ちしております。どんなことでも歓迎なので、Elasticsearch、Kibana、Logstashなどについて書いてもらえるとうれしいです。</p>
<p>次回ももちろん2ヶ月後くらいに行います。 スピーカー募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。 よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.4.0および1.3.5リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2014/11/06/elasticsearch-1-4-0-ja/</link>
      <pubDate>Thu, 06 Nov 2014 01:30:33 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/11/06/elasticsearch-1-4-0-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：elasticsearch-1.4.0 and 1.3.5 released 本日、Lucene 4.10.2をベースにし</description>
      <content:encoded><p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>
<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">elasticsearch-1.4.0 and 1.3.5 released</a></p>
<p>本日、<strong>Lucene 4.10.2</strong>をベースにした<strong>Elasticsearch 1.4.0</strong>と、バグフィックスリリースである、<strong>Elasticsearch 1.3.5</strong>をリリースしました。
ダウンロードおよび変更リストはそれぞれ次のリンクからアクセスできます。</p>
<ul>
<li>最新ステーブルリリース：<a href="http://www.elasticsearch.org/downloads/1-4-0">Elasticsearch 1.4.0</a></li>
<li>1.3.x系バグフィックス：<a href="http://www.elasticsearch.org/downloads/1-3-5">Elasticsearch 1.3.5</a></li>
</ul>
<p>1.3ブランチに関する過去のリリースについてのブログは次のとおりです：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-4-released/">1.3.4</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-3-released/">1.3.3</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-2-released/">1.3.2</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-1-released/">1.3.1</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/">1.3.0</a>.</p>
<!-- more -->
<p>Beta1リリースでも言及しましたが、1.4.0の主なテーマは*resiliency(復元性、弾力性)*です。
Elasticsearchをより安定し信頼性のあるものにし、メモリ管理を改善し、ディスカバリアルゴリズムを改善し、破損したデータの検知を改善しました。
Beta1リリースからのハイライトも含んでいます。</p>
<ul>
<li>Doc values (インデックス時にディスクに保存される<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/fielddata-formats.html#fielddata-formats">fielddata</a>)がヒープ利用率を激減</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-fielddata.html#request-circuit-breaker">Request circuit breaker</a>:
メモリを消費しすぎる検索リクエストの中断</li>
<li>Bloom filterの<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-update-settings.html#codec-bloom-load">デフォルト無効</a>、高速なインデキシングのためにもはや必要とされないため。</li>
<li>ノードディスカバリ、シャードリカバリの数多くのバグフィックス及び改善</li>
<li>データ破損の早期検知のためのチェックサムのさらなる利用</li>
<li>GroovyをMVELの代わりに<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-scripting.html#modules-scripting">デフォルトスクリプト言語に</a></li>
<li>CORSを<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-http.html#_settings_2">デフォルト無効</a>に。XSS攻撃防止の為。</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-shard-query-cache.html#index-modules-shard-query-cache">クエリキャッシュ</a>、変更されていないシャードからすぐにaggregation結果を返す</li>
<li>新しいAggregation：<code>filter</code>(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-filters-aggregation.html#search-aggregations-bucket-filters-aggregation">ドキュメント</a>)、<code>children</code>(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-children-aggregation.html#search-aggregations-bucket-children-aggregation">ドキュメント</a>)、<code>scripted_metric</code>(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-metrics-scripted-metric-aggregation.html#search-aggregations-metrics-scripted-metric-aggregation">ドキュメント</a>)</li>
<li>新しい<code>GET /index</code>API。インデックスのsettings、mappings、warmers、aliasesを1回のリクエストで返却(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-get-index.html#indices-get-index">ドキュメント</a>)</li>
<li>自動付与ドキュメントIDのためのFlake ID。プライマリキーの探索パフォーマンスの改善。</li>
<li>ドキュメントに変更のない更新によるドキュメントの再インデックスの防止</li>
<li><code>function_score</code>クエリの関数で<code>weight</code>パラメータによる個別の改善を可能に。(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/query-dsl-function-score-query.html#_weight">ドキュメント</a>)</li>
</ul>
<p>詳細については<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">1.4.0.Beta1のブログ(英語)</a>(<a href="http://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/">日本語訳</a>)をご覧ください。</p>
<p>Beta1以降の1.4.0の変更の全てについては、<a href="http://www.elasticsearch.org/downloads/1-4-0">1.4.0 release notes</a>でご覧いただけます。
以下では、2つの主な変更について紹介します。</p>
<h2 id="http-pipelining">HTTP Pipelining</h2>
<p>HTTP pipeliningは複数のリクエストを1回のコネクションで、関連するレスポンスを待つことなく送信することができます。
そして、レスポンスは、受け取ったリクエストと同じ順序で返却されます。
HTTP/1.1の仕様で、pipeliningのサポートが必要です。ElasticsearchはHTTP/1.1であるとしてきましたが、pipeliningはサポートしていませんでした。この問題は.NETユーザで問題を引き起こしました。</p>
<p>現在、HTTP pipeliningは公式にサポート済みで、デフォルトで利用できます。<a href="https://github.com/elasticsearch/elasticsearch/pull/8299">#8299</a>をご覧ください。</p>
<h2 id="upgrade-api">Upgrade API</h2>
<p>Luceneのすべてのリリースではバグフィックスや最適化が提供されます。しかし、多くのユーザは古いバージョンのLuceneで作成されたインデックスを持っており、より最新の改善による利点を利用できないことがあります。
新しい<code>upgrade</code>APIは、あなたのインデックスすべてもしくは一部を最新のLuceneフォーマットに透過的にアップグレードできます。</p>
<p><code>GET _upgrade</code>リクエストは、インデックスのアップグレードが必要かどうかを提示し、アップグレードに必要なセグメントのサイズをリポートすることによって、どのくらいの時間が必要かの目安を提供します。
<code>POST _upgrade</code>コマンドはバックグラウンドでインデックスを最新のLuceneフォーマットに書き換えます。</p>
<p>より詳しい情報は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-upgrade.html"><code>upgrade</code>APIドキュメント</a>をご覧ください。</p>
<h2 id="試してみてください">試してみてください。</h2>
<p>Beta1リリースを利用し、経験・体験を報告していただいたベータテスターの方々に感謝します。
1.4.0がこれまでの最高のリリースになると確信しています。
ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-0">Elasticsearch 1.4.0</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>validate APIの利用</title>
      <link>https://blog.johtani.info/blog/2014/10/27/how-to-use-validate-api/</link>
      <pubDate>Mon, 27 Oct 2014 18:42:31 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/10/27/how-to-use-validate-api/</guid>
      <description>久しぶりに翻訳ではないブログを。書こうと思いながらかけてなかったので。。。 今回はvalidate APIの紹介です。 背景 Elasticsear</description>
      <content:encoded><p>久しぶりに翻訳ではないブログを。書こうと思いながらかけてなかったので。。。</p>
<p>今回は<a href="http://www.elasticsearch.org/guide/ep/elasticsearch/reference/current/search-validate.html">validate API</a>の紹介です。</p>
<!-- more -->
<h2 id="背景">背景</h2>
<p>Elasticsearchのクエリは<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html">Query DSL</a>というJSONで
クエリを定義できるものを提供しています。
これは、様々な<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-queries.html">クエリ</a>、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filters.html">フィルタ</a>を定義するために必要です。</p>
<p>自分の望んでいる条件を記述するために、JSONのネストと格闘することも必要となります。。。
また、クエリ、フィルタには様々なパラメータが用意されています。
これらのパラメータをすべて覚えるのは無理でしょうし、タイプミスなどもありますよね。
タイプミスやカッコのミスマッチなどで格闘して1時間が経過してしまったなどもあると思います。</p>
<p>そんな時に便利なAPIとして用意されているのが<a href="http://www.elasticsearch.org/guide/ep/elasticsearch/reference/current/search-validate.html">validate API</a>です。</p>
<h2 id="利用方法">利用方法</h2>
<p>APIが用意されています。</p>
<pre><code>http://ホスト名:ポート番号/インデックス名/タイプ名/_validate/query
</code></pre><p><code>インデックス名</code>や<code>タイプ名</code>は省略可能ですが、マッピングが異なると思うので、タイプ名まで指定するほうが良いと思います。
上記のAPIに対してクエリを送信するだけです。</p>
<h3 id="クエリの確認">クエリの確認</h3>
<p>たとえば、<a href="https://gist.github.com/johtani/08dee5fb4da62037ef9e">こちらのGist</a>にあるようなマッピングのインデックスに対して
検索クエリを組み立てていて、エラーが出るとします。
※このクエリは<code>match_all</code>のところを<code>match_al</code>と、<code>l</code>が1文字足りないクエリになっています。</p>
<p><em><strong>検索クエリのリクエスト（エラーあり）</strong></em></p>
<pre><code>GET pref_aggs/_search
{
  &quot;query&quot;: {
    &quot;match_al&quot;: {}
  }
}
</code></pre><p><em><strong>実行結果のレスポンス</strong></em></p>
<pre><code>{
   &quot;error&quot;: &quot;SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[rwkb01chTZq2V7FD0Tlwrw][pref_aggs][0]: SearchParseException[[pref_aggs][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \&quot;query\&quot;: {\n    \&quot;match_al\&quot;: { }\n  }\n}\n]]]; nested: QueryParsingException[[pref_aggs] No query registered for [match_al]]; }{[rwkb01chTZq2V7FD0Tlwrw][pref_aggs][1]: SearchParseException[[pref_aggs][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \&quot;query\&quot;: {\n    \&quot;match_al\&quot;: { }\n  }\n}\n]]]; nested: QueryParsingException[[pref_aggs] No query registered for [match_al]]; }]&quot;,
   &quot;status&quot;: 400
}
</code></pre><p>とこんなかんじで、エラーが帰っては来るのですが、非常に読みづらいです。</p>
<p>そこで、<code>validate API</code>を利用します。
リクエスト先を<code>/_search</code>から<code>/_validate/query</code>に変更します。</p>
<p><em><strong>validate API</strong></em></p>
<pre><code>GET pref_aggs/_validate/query
{
  &quot;query&quot;: {
    &quot;match_al&quot;: {}
  }
}
</code></pre><p><em><strong>validate APIのレスポンス</strong></em></p>
<pre><code>{
   &quot;valid&quot;: false,
   &quot;_shards&quot;: {
      &quot;total&quot;: 1,
      &quot;successful&quot;: 1,
      &quot;failed&quot;: 0
   }
}
</code></pre><p>すると、非常にシンプルな結果が返ってきます。
<code>&quot;valid&quot;: false</code>となっているため、クエリに問題があることがわかります。</p>
<h3 id="エラーの詳細">エラーの詳細</h3>
<p>問題がある事自体はわかりましたが、エラーの内容も知りたいですよね？
その場合は、<code>explain</code>というパラメータを追加します。
（正しくは<code>explain=true</code>を追加しますが、<code>=true</code>を省略可能です。）</p>
<p><em><strong>validate API(explainあり、クエリ自体は省略)</strong></em></p>
<pre><code>GET pref_aggs/_validate/query?explain
{...}
</code></pre><p><em><strong>validate APIのレスポンス</strong></em></p>
<pre><code>{
   &quot;valid&quot;: false,
   &quot;_shards&quot;: {
      &quot;total&quot;: 1,
      &quot;successful&quot;: 1,
      &quot;failed&quot;: 0
   },
   &quot;explanations&quot;: [
      {
         &quot;index&quot;: &quot;pref_aggs&quot;,
         &quot;valid&quot;: false,
         &quot;error&quot;: &quot;org.elasticsearch.index.query.QueryParsingException: [pref_aggs] No query registered for [match_al]&quot;
      }
   ]
}
</code></pre><p><code>explanations</code>という項目が追加されました。
ここに<code>error</code>という項目として、エラーの詳細が返ってきます。<code>_search</code>の時よりも見やすいですね。
今回のエラーは、<code>match_all</code>が正しいクエリですの、<em><code>match_al</code>というクエリは登録されていないというエラー</em>でした。
では、クエリを修正して実行しましょう。</p>
<p><em><strong>validate API(エラー無し)</strong></em></p>
<pre><code>GET pref_aggs/_validate/query?explain
{
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  }
}
</code></pre><p><em><strong>validate APIのレスポンス</strong></em></p>
<pre><code>{
   &quot;valid&quot;: true,
   &quot;_shards&quot;: {
      &quot;total&quot;: 1,
      &quot;successful&quot;: 1,
      &quot;failed&quot;: 0
   },
   &quot;explanations&quot;: [
      {
         &quot;index&quot;: &quot;pref_aggs&quot;,
         &quot;valid&quot;: true,
         &quot;explanation&quot;: &quot;ConstantScore(*:*)&quot;
      }
   ]
}
</code></pre><p>今度はクエリに問題はありません。<code>&quot;valid&quot;: true</code>です。
そして、<code>explanations</code>の項目には、<code>error</code>の代わりに<code>explanation</code>という項目が返ってきました。
これが、実際にElasticsearch内部で実行されるクエリになります。</p>
<h3 id="実際のクエリに利用される単語の確認">実際のクエリに利用される単語の確認</h3>
<p>この機能はこの他に、クエリの解析にも利用できます。
思ったとおりに検索にヒットしない場合があって、困ったことはないですか？
フィールドに指定されたアナライザによっては、単語を変形したりするものが存在します。</p>
<p><em><strong>サンプルマッピング</strong></em></p>
<pre><code>PUT /validate_sample
{
  &quot;mappings&quot;: {
    &quot;several_analyzer&quot;: {
      &quot;properties&quot;: {
        &quot;title&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;body_ja&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;kuromoji&quot;},
        &quot;body_en&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;}
      }
    }
  }
}
</code></pre><p>例えば、このように<code>kuromoji</code>、<code>english</code>、デフォルト(<code>standard</code>)アナライザを利用したマッピングがあるとします。
このフィールドに対して<code>powerful</code>という単語で検索したとします。</p>
<p><em><strong>validate API</strong></em></p>
<pre><code>GET /validate_sample/_validate/query?explain
{
  &quot;query&quot;: {
    &quot;multi_match&quot;: {
      &quot;fields&quot;: [&quot;body_en&quot;,&quot;body_ja&quot;,&quot;title&quot;],
      &quot;query&quot;: &quot;powerful&quot;
    }
  }
}
</code></pre><p>この場合、レスポンスは次のとおりです。</p>
<p><em><strong>validate APIのレスポンス</strong></em></p>
<pre><code>{
   &quot;valid&quot;: true,
   &quot;_shards&quot;: {
      &quot;total&quot;: 1,
      &quot;successful&quot;: 1,
      &quot;failed&quot;: 0
   },
   &quot;explanations&quot;: [
      {
         &quot;index&quot;: &quot;validate_sample&quot;,
         &quot;valid&quot;: true,
         &quot;explanation&quot;: &quot;(title:powerful | body_en:power | body_ja:powerful)&quot;
      }
   ]
}
</code></pre><p><code>title</code>、<code>body_ja</code>については入力された単語がそのままクエリとして利用されています。
<code>body_en</code>については、<code>power</code>という単語に変換されて実行されています。
これは、<code>english</code>アナライザがステミングを行った結果がクエリとして利用されるという意味です。
また、<code>powerful</code>を<code>秋葉原</code>といった日本語に変更して実行すると次のようになります。
日本語は<code>standard</code>アナライザなどでは、1文字ずつ区切られてしまうことがわかります。</p>
<p><em><strong>validate APIのレスポンス</strong></em></p>
<pre><code>{
   &quot;valid&quot;: true,
   &quot;_shards&quot;: {
      &quot;total&quot;: 1,
      &quot;successful&quot;: 1,
      &quot;failed&quot;: 0
   },
   &quot;explanations&quot;: [
      {
         &quot;index&quot;: &quot;validate_sample&quot;,
         &quot;valid&quot;: true,
         &quot;explanation&quot;: &quot;((title:秋 title:葉 title:原) | (body_en:秋 body_en:葉 body_en:原) | ((body_ja:秋葉 body_ja:秋葉原) body_ja:原))&quot;
      }
   ]
}
</code></pre><p>このように、クエリの単語がどのような単語に変換されてクエリに利用されているかなども知ることが可能です。</p>
<p>また、クエリを組み立てて、ヒットするはずが、0件となってしまうという場合にも、どのようなクエリが組み立てられているかを確認するという点で、
<code>validate API</code>が役立ちます。
検索がヒットするが、望んだクエリになっていないのでは？という場合は<code>_search API</code>の<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-explain.html"><code>explain</code>パラメータ</a>を
利用すれば、クエリの構成がわかるのですが、検索結果が0件の場合はクエリの構成は表示されません。</p>
<h2 id="解決できない問題は">解決できない問題は？</h2>
<p>便利なvalidate APIですが、以下の問題に対しては残念ながら確認できません。</p>
<ul>
<li><code>query</code>以外の項目のvalidate不可
<ul>
<li>たとえば、<code>_search API</code>の<code>size</code>などの項目についてはチェックできないです。</li>
</ul>
</li>
<li>存在しないフィールドの指定
<ul>
<li>上記<code>validate_sample</code>のマッピングの例でクエリに<code>body_eng</code>という存在しないフィールドを指定してもエラーとはなりません。</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>書いたクエリがうまく動かない、JSONのタグがおかしいといった場合は、
まずはこの<code>validate API</code>で確認してみるのがオススメです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Sonatypeのバージョン番号で困ったので</title>
      <link>https://blog.johtani.info/blog/2014/10/15/versioning-of-sonatype/</link>
      <pubDate>Wed, 15 Oct 2014 15:26:08 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/10/15/versioning-of-sonatype/</guid>
      <description>Elasticsearch 1.4.0.Beta1がリリースされました。 個人でelasticsearch-extended-analyzeというプラグインを開発してま</description>
      <content:encoded><p><a href="http://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/">Elasticsearch 1.4.0.Beta1がリリース</a>されました。</p>
<p>個人で<a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analyze</a>というプラグインを開発してます。
こちらも1.4.0.Beta1に対応するべく作業をしてて、少し戸惑ったことがあったので、メモをば。</p>
<!-- more -->
<p>ここ最近はプラグインのバージョン番号をElasticsearchのバージョン番号と同じものを利用していました。
（プラグインの機能追加をサボってる？？）
その時に、<code>1.4.0.Beta1</code>という番号を指定したのですが、意味不明なエラーに悩まされてしまいまして。</p>
<p>プラグインのリリースでは、以下のコマンドを実行します。</p>
<pre><code>$ mvn release:prepare
$ mvn release:perform
</code></pre><p>最初のコマンド（prepare）で、パッケージングを実施し、Githubにリリースタグを打ったバージョンがpushされます。
次のコマンド（perform）で、パッケージングされたzipファイルがsonatypeのサイトに公開するためにアップロードされます。</p>
<p><code>1.4.0.Beta1</code>というバージョン文字列を利用した場合、prepareは問題なく実行できたのですが、
performで以下の様なエラーが返ってきました。</p>
<pre><code>Return code is: 401, ReasonPhrase: Unauthorized.
</code></pre><p>バージョン番号が<code>1.3.0</code>では特に問題はなかったのですが、、、
結局、バージョン番号を<code>1.4.0-beta1</code>に変更すると問題なくリリースが完了しました。</p>
<p>mike_neckさんと話をしていて、<a href="http://semver.org">Semantic Versioning</a>に関係しているのかなぁという話にはなったのですが、
詳しく調べていません。。。</p>
<p>そのうち調べようかなぁ。。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch 1.4.0.Beta1のリリース</title>
      <link>https://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/</link>
      <pubDate>Thu, 02 Oct 2014 19:14:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/</guid>
      <description>※この記事は次のブログを翻訳したものになります。 原文：elasticsearch 1.4.0.beta1 released 本日、Lucene 4.10.1をベースにした、Elast</description>
      <content:encoded><p>※この記事は次のブログを翻訳したものになります。</p>
<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">elasticsearch 1.4.0.beta1 released</a></p>
<p>本日、<em>Lucene 4.10.1</em>をベースにした、<em>Elasticsearch 1.4.0.Beta1</em>をリリースしました。
<a href="http://www.elasticsearch.org/downloads/1-4-0-Beta1">Elasticsearch 1.4.0.Beta1</a>からダウンロードできます。
また、すべての変更点に関してもこちらをご覧ください。</p>
<!-- more -->
<p>1.4.0のテーマは*resiliency(復元性、弾力性)*です。
<em>resiliency</em>とはElasticsearchをより安定し信頼性のあるものにすることを意味します。
すべての機能が正常に機能している場合は信頼することは簡単です。
予想外のことが発生した時に難しくなります：ノードでout of memoryの発生、スローGCや重いI/O、ネットワーク障害、不安定なデータの送信によるノードのパフォーマンス低下など。</p>
<p>本ベータリリースは、resiliencyの主な3つの改善を含んでいます。</p>
<ul>
<li><a href="#memory-mgmt">メモリ使用量の低下</a>によるノードの安定性向上</li>
<li>discoveryアルゴリズムの改善による<a href="#cluster-stability">クラスタの安定性</a>向上</li>
<li><a href="#checksums">チェックサム</a>の導入による破損したデータの検知</li>
</ul>
<p>分散システムは複雑です。
決して想像できないような状況をシミュレーションするために、ランダムなシナリオを作成する広範囲なテストスイートを持っています。
しかし、無数のエッジケース(特殊なケース)があることも認識しています。
1.4.0.Beta1はこれまで私たちが行ってきた改善のすべてを含んでいます。
これらの変更を実際にテストしていただき、<a href="https://github.com/elasticsearch/elasticsearch/issues">何か問題があった場合は私たちに教えてください</a>。</p>
<h2 id="a-namememory-mgmtメモリ管理a"><a name="memory-mgmt">メモリ管理</a></h2>
<p>ヒープ空間は限られたリソースです。
上限を32GBとし、利用可能なRAMの50%をヒープの上限にすることを推奨します。
この上限を超えた場合、JVMは圧縮したポインタを使用することができず、GCが非常に遅くなります。
ノードの不安定性の主な原因は遅いGCです。それは、次のようなことから発生します。</p>
<ul>
<li>メモリプレッシャー</li>
<li>スワップ(参照：<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/setup-configuration.html#setup-configuration-memory">memory settings</a>)</li>
<li>非常に大きなヒープ</li>
</ul>
<p>本リリースは、メモリ管理の改善し、（結果として）ノードの安定性を改善するいくつかの変更を含んでいます。</p>
<h3 id="doc-values">doc values</h3>
<p>メモリの利用の最も大きなものの1つは<strong>fielddata</strong>です
aggregation、ソート、スクリプトがフィールドの値に素早くアクセスするために、フィールドの値をメモリにロードして保持します。
ヒープは貴重なため、1ビットも無駄にしないためにメモリ内のデータは高度な圧縮と最適化を行っています。
これは、ヒープスペース以上のデータをもつまでは、非常によく動作します。
これは、多くのノードを追加することによって常に解決できる問題です。
しかし、CPUやI/Oが限界に達してしまうずっと前に、ヒープ空間の容量に到達します。</p>
<p>最近のリリースは、<strong>doc values</strong>によるサポートがあります。
基本的に、doc valuesはin-memory fielddataと同じ機能を提供します。
doc valuesの提供する利点は、それらが、非常に少量のヒープ空間しか使用しない点です。
doc valuesはメモリからではなく、ディスクから読み込まれます。
ディスクアクセスは遅いですが、doc valuesはカーネルのファイルシステムキャッシュの利点を得られます。
ファイルシステムキャッシュはJVMヒープとはことなり、32GBの制限による束縛がありません。
ヒープからファイルシステムキャッシュにfielddataを移行することによって、より小さなヒープを使うことができます。これは、GCがより早くなり、ノードが更に安定することを意味します。</p>
<p>本リリースより前は、doc valuesはin-memory fielddataよりもかなり遅かったです。
本リリースに含まれる変更は、パフォーマンスをかなり向上させ、in-memory fielddataとほぼ同じくらいの速度になっています。</p>
<p>in-memory fielddataの代わりにdoc valuesを利用するために必要なことは、次のように新しいフィールドをマッピングすることです。</p>
<pre><code>PUT /my_index
{
  &quot;mappings&quot;: {
    &quot;my_type&quot;: {
      &quot;properties&quot;: {
        &quot;timestamp&quot;: {
          &quot;type&quot;:       &quot;date&quot;,
          &quot;doc_values&quot;: true
        }
      }
    }
  }
}
</code></pre><p>このマッピングで、このフィールドに対するfielddataの利用は、メモリにフィールドをロードする代わりに、自動的にディスクからdoc valuesを利用します。
*注意：*現時点で、doc valuesはanalyzedな<code>string</code>フィールドはサポートしていません。</p>
<h3 id="request-circuit-breaker">request circuit breaker</h3>
<p>fielddata circuit breakerはfielddataによって利用されるメモリの上限を制限するために追加され、OOMEの最も大きな原因の1つを防ぎました。
そして、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-fielddata.html#request-circuit-breaker">リクエストレベルのcircuit-breaker</a>を提供するために、コンセプトを拡張しました。
これは、単一のリクエストによって使用されるメモリの上限を制限します。</p>
<h3 id="bloom-filters">bloom filters</h3>
<p><a href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a> はインデキシング(前のバージョンのドキュメントが存在するかどうかのチェックのため)や、
IDによるドキュメントの検索(ドキュメントを含むセグメントがどれかを決定するため)に関する重要な性能最適化を提供しました。
しかし、もちろんそれらはコスト（メモリ）を必要とします。
現在の改善は、bloom filterの必要性を取り除きました。
現在では、Elasticsearchはまだ、インデックス時にそれらを構築します(実世界の経験がテストシナリオにそぐわない場合に備えて)。
しかし、デフォルトではメモリにはロードされません。
すべてが予定通りに運べば、将来のバージョンで完全にこれらは除去します。</p>
<h2 id="a-namecluster-stabilityクラスタの安定性a"><a name="cluster-stability">クラスタの安定性</a></h2>
<p>クラスタの安定性向上のために私たちができる最も大きなことは、ノードの安定性の向上です。
もし、ノードが安定しておりタイミングよく反応すれば、クラスタが不安定になる可能性が大いに減少します。
私たちは不完全な世界に住んでいます。- 物事は予想外にうまく行きません。クラスタはデータを失うことなくこのような状況から回復できる必要があります。</p>
<p>私たちは、<code>improve_zen</code>ブランチ上で、Elasticsearchの障害からの復旧するための能力の向上に数ヶ月費やしてきました。
まず、複雑なネットワークレベルの障害を繰り返すためのテストを追加しました。
次に、各テストのための修正を追加しました。
そこには、より多くの行うことが存在します。しかし、私たちは、<a href="https://github.com/elasticsearch/elasticsearch/issues/2488">issue #2488</a>(&ldquo;分割が交差している場合、minimum_master_nodesはsplit-brainを防げない&rdquo;)に含まれる、ユーザが経験してきた大部分の問題を私たちは解決しました。</p>
<p>私たちはクラスタのresiliencyを非常に真剣に取り組んでいます。
私たちは、Elasticsearchが何ができるか、その上で何が弱点であるかを理解してほしいと思っています。
これを考慮して、私たちは<a href="http://www.elasticsearch.org/guide/en/elasticsearch/resiliency/current/index.html">Resiliency Status Document</a>を作成しました。
このドキュメントは、私たち(または私たちユーザ)が遭遇したresiliencyの問題の、何が修正済みで、何が修正されないまま残っているかを記録します。
このドキュメントを慎重に読み、あなたのデータを保護するために適切な方法を選択してください。</p>
<h2 id="a-namechecksumsデータ破損の検知a"><a name="checksums">データ破損の検知</a></h2>
<p>ネットワークをまたいだシャードリカバリのチェックサムは、圧縮ライブラリのバグを発見する助けとなりました。
それは、バージョン1.3.2で修正済みです。
それ以来、私たちはElasticsearchのいたるところにチェックサムとチェックサムの確認を追加しました。</p>
<ul>
<li>マージ中に、あるセグメント内すべてのチェックサムの確認(<a href="https://github.com/elasticsearch/elasticsearch/issues/7360">#7360</a>)</li>
<li>インデックス再オープン時に、あるセグメント内の最も小さなファイルの完全な確認と、より大きなファイルの軽量な打ち切りチェック(<a href="https://issues.apache.org/jira/browse/LUCENE-5842">LUCENE-5842</a>)</li>
<li>トランザクションログからイベントを再生するとき、各イベントはチェックサムを確認される(<a href="https://github.com/elasticsearch/elasticsearch/issues/6554">#6554</a>)</li>
<li>シャードのリカバリ中もしくは、スナップショットからのリストア中にElasticsearchはローカルファイルとリモートのコピーが同一であるか確認する必要がある。ファイルの長さとチェックサムのみを使うのは不十分であることが確認された。このため、現在はセグメントのすべてのファイルの同一性を確認(<a href="https://github.com/elasticsearch/elasticsearch/issues/7159">#7159</a>)</li>
</ul>
<h2 id="その他のハイライト">その他のハイライト</h2>
<p><a href="http://www.elasticsearch.org/downloads/1-4-0-Beta1">Elasticsearch 1.4.0.Beta1のchangelog</a>に本リリースの多くの機能、改善、バグフィックスについて読むことができます。
ここでは、特筆すべきいくつかの変更について述べます。</p>
<h3 id="groovyによるmvelの置き換え">groovyによるmvelの置き換え</h3>
<p>Groovyは現在、デフォルトのscripting languageです。
以前のデフォルトはMVELで、古くなってきており、サンドボックス内で実行できないという事実は、セキュリティ問題でした。
Groovyはサンドボックスであり(それは、ボックスの外へは許可が必要)、メンテナンスされており、速いです！
詳しくは<a href="http://www.elasticsearch.org/blog/scripting/">scriptingについてのブログ記事</a>をご覧ください。</p>
<h3 id="デフォルトでcorsはオフ">デフォルトでcorsはオフ</h3>
<p>Elasticsearchのデフォルト設定はクロスサイトスクリプティングに対して脆弱でした。
私たちはデフォルトで<a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS</a>をオフにすることで修正しました。
Elasticsearchにインストールされたサイトプラグインはこれまで同様に機能します。
しかし、CORSを再度オンにすることがない限り、外部のウェブサイトがリモートのクラスタにアクセスすることはできません。
ウェブサイトがあなたのクラスタにアクセス可能に制御できるように、さらに<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-http.html#_settings_2">CORS settings</a>を追加しました。
詳しくは<a href="http://www.elasticsearch.org/community/security">security page</a>をご覧ください。</p>
<h3 id="クエリキャッシュ">クエリキャッシュ</h3>
<p>新しい試験的な<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-shard-query-cache.html">shardレベルのクエリキャッシュ</a>は、静的なインデックスのアグリゲーションをほとんど即座に反応できます。
ウエブサイトのアクセスの日毎のページビュー数を見るダッシュボードを持っていると想像してみてください。
これらの数値は古いインデックスでは変更がありません。しかし、アグリゲーションはダッシュボードのリフレッシュのたびに再計算されます。
新しいクエリキャッシュを利用すると、シャードのデータが変更されない限り、アグリゲーションの結果はキャッシュから直接返却されます。
キャッシュから古い結果を決して取得することはありません。それは、常に、キャッシュされていないリクエストと同じ結果を返します。</p>
<h3 id="新しいaggregations">新しいaggregations</h3>
<p>3つの新しいaggregationsがあります。</p>
<ul>
<li>
<p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-filters-aggregation.html"><code>filters</code></a></p>
<ul>
<li>これは<code>filter</code> aggregationの拡張です。複数のバケットを定義し、バケット毎に異なるフィルタを利用できます。</li>
</ul>
</li>
<li>
<p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-children-aggregation.html"><code>children</code></a></p>
<ul>
<li><code>nested</code>アグリゲーションの親子版。<code>children</code> aggは親のドキュメントに属する子のドキュメントを集計できる</li>
</ul>
</li>
<li>
<p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-metrics-scripted-metric-aggregation.html"><code>scripted_metric</code></a></p>
<ul>
<li>このaggregationは、データによって計算されたメトリックを完全にコントロールできます。これは、初期化フェーズ、ドキュメント収集フェーズ、shardレベル結合フェーズ、global reduceフェーズを提供します。</li>
</ul>
</li>
</ul>
<h3 id="get-index-api">get /index api</h3>
<p>以前、ある1つのインデックスのaliases、mappings、settings、warmersを取得出来ました。しかし、それらを個別にです。
<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-get-index.html"><code>get-index</code> API</a> はこれらのすべてもしくは一部を、複数もしくはひとつのインデックスに対して一緒に取得できます。
これは、既存のインデックスと同一もしくはほぼ同一であるインデックスを作成したいときに非常に役に立ちます。</p>
<h3 id="登録と更新">登録と更新</h3>
<p>ドキュメントの登録と更新にいくつかの改善があります。</p>
<ul>
<li>現在、ドキュメントIDの自動生成のために<a href="http://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang">Flake ID</a>を使用しています。これは、プライマリキー探索時に素晴らしい性能向上を提供します。</li>
<li><code>detect_noop</code>に<code>true</code>を設定すると、ドキュメントに変更を与えない更新が軽量になります。この設定を有効にすると、<code>_source</code>フィールドのコンテンツを変更する更新リクエストだけ、ドキュメントの新しいバージョンを書き込みます。</li>
<li>更新はスクリプトから完全に操作できます。以前は、スクリプトはドキュメントがすでに存在しているときだけ実行可能で、それ以外は、<code>upsert</code>ドキュメントで登録しました。<code>script_upsert</code>パラメータでスクリプトから直接ドキュメントの作成が操作できます。</li>
</ul>
<h3 id="function-score">function score</h3>
<p>すでに非常に便利な<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/query-dsl-function-score-query.html"><code>function_score</code>クエリ</a>が、新しく<code>weight</code>パラメータをサポートします。
これは、それぞれの指定された関数の影響をチューニングするのに使われます。
これは、人気度よりも更新日時により重みをかけたり、地理情報よりも価格により重みをかけるといったことを可能にします。
また、<code>random_score</code>機能はセグメントマージによる影響を受けません。これにより、より一貫した順序が提供されます。</p>
<h2 id="試してみてください">試してみてください。</h2>
<p>ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-0-Beta1">Elasticsearch 1.4.0.Beta1</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第6回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2014/09/17/hold-on-6th-elasticsearch-jp/</link>
      <pubDate>Wed, 17 Sep 2014 13:22:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/09/17/hold-on-6th-elasticsearch-jp/</guid>
      <description>第6回Elsticsearch勉強会を開催しました。 スタッフの皆さん、スピーカーの皆さん、開場提供していただいたリクルートテクノロジーズさん</description>
      <content:encoded><p><a href="http://elasticsearch.doorkeeper.jp/events/13917">第6回Elsticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。
今回は、スタッフが私を含めて3，4名ということで、ドタバタしてしまってスミマセンでした。</p>
<!-- more -->
<p>今回はキャンセルが多く、最終的には90人弱の参加となりましたが、今回も多数の方にお集まりいただきありがとうございました。
同じ日に他の勉強会もあった影響でしょうか？</p>
<h2 id="aggregationあれこれelasticsearch-inc-jun-ohtani-johtani">「Aggregationあれこれ」Elasticsearch Inc. Jun Ohtani @johtani</h2>
<p>スライド：<a href="https://speakerdeck.com/johtani/aggregationarekore">Aggregationあれこれ</a></p>
<ul>
<li>ちょっと長かったですかね。。。</li>
<li>Aggregationの概要、内部動作、種類などを簡単に紹介してみました。</li>
<li>個々のAggregationもいろいろなオプションなどがあるので、色々と試してみていただければと思います。</li>
<li>アニメーション入りのスライドになってましたが、UpしてあるスライドはPDF版になります。</li>
</ul>
<h2 id="秒間3万の広告配信ログをelasticsearchでリアルタイム集計してきた戦いの記録-株式会社サイバーエージェント山田直行さんsatully">「秒間3万の広告配信ログをElasticSearchでリアルタイム集計してきた戦いの記録」 株式会社サイバーエージェント　山田直行さん　@satully</h2>
<p>スライド：<a href="http://www.slideshare.net/Satully/elasticsearch-study6threaltime20140916">秒間3万の広告配信ログをElasticSearchでリアルタイム集計してきた戦いの記録</a></p>
<ul>
<li>ディスプレイ広告配信DSPの話</li>
<li>システム: Fluentd、S3、Elasticsearch、Redis、MySQL</li>
<li>7月に秒間3万〜4万のリクエストをさばいている。</li>
<li>なぜElasticsearchを選んだのか、今の構成など</li>
<li>実際に苦労された点なども交えて話していただき面白かったです。</li>
<li>7月時点のお話ということで、現時点ではまた違う構成っぽかったので、また話を聞きたいなぁ。</li>
</ul>
<h2 id="elasticsearch-日本語スキーマレス環境構築とついでに多言語対応ナレッジワークス株式会社木戸国彦さん-9215">「Elasticsearch 日本語スキーマレス環境構築と、ついでに多言語対応」ナレッジワークス株式会社　木戸国彦さん @9215</h2>
<p>スライド：<a href="https://speakerdeck.com/kunihikokido/elasticsearch-ri-ben-yu-sukimaresuhuan-jing-gou-zhu-to-tuideniduo-yan-yu-dui-ying">Elasticsearch 日本語スキーマレス環境構築と、ついでに多言語対応</a></p>
<ul>
<li>Dynamic TemplateやIndex Templateの説明</li>
<li>日本語や多言語化するときのMappingのサンプルになりそうなものがゴロゴロ紹介されてました。</li>
<li>いくつかの例があって、後で見直したいなと。</li>
<li>途中で出てきた、fielddata（インデックスに入っている単語区切りのデータ）を見るのに使ってたクエリは<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fielddata-fields.html">field data fields</a>だったかな。</li>
</ul>
<h2 id="elasticsearchソースコードを読みはじめてみたfurandon_pig-さん">「elasticsearchソースコードを読みはじめてみた」@furandon_pig さん</h2>
<p>スライド：<a href="http://www.slideshare.net/furandon_pig/elasticsearch-39175134">elasticsearchソースコードを読みはじめてみた</a></p>
<ul>
<li>リクエストを受けて検索してる部分から読むといいって言われたらしいが、起動スクリプトから読み始めてみた。</li>
<li>時間かかりそうｗ</li>
<li>ただ、人がどんな感じでソースを読んだり理解してるかがわかりやすかったので面白かったです。</li>
<li>定期的に続きを聞いてみたいです。</li>
</ul>
<h2 id="lt">LT</h2>
<h3 id="reroute-apiを使用してシャード配置を制御する-株式会社富士通ソフトウェアテクノロジーズ-滝田聖己さん-pisatoshi">「reroute APIを使用してシャード配置を制御する」 株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん @pisatoshi</h3>
<p>スライド：<a href="https://speakerdeck.com/pisatoshi/elasticsearch-rerouteapiwoshi-tutasiyadopei-zhi-falsezhi-yu">reroute APIを使用してシャード配置を制御する</a></p>
<ul>
<li>シャードの再配置が自動で行われるので、それをオフにしないと、せっかく移動しても無駄になることがというあるあるネタ</li>
<li>Bonsaiロゴを作成するLT</li>
<li>実際にいくら掛かったのかが知りたかった。</li>
</ul>
<h3 id="検索のダウンタイム0でバックアップからindexをリストアする方法株式会社ドワンゴモバイル-西田和史さん">「検索のダウンタイム0でバックアップからIndexをリストアする方法」株式会社ドワンゴモバイル 西田和史さん</h3>
<p>スライド：<a href="http://www.slideshare.net/kbigwheel/0index-39143333">検索のダウンタイム0でバックアップからIndexをリストアする方法</a></p>
<ul>
<li>擬似無停止のやりかた。</li>
<li>aliasを活用して、かつ、Restoreで再構築するという方法。</li>
<li>aliasまで一緒にリストアされるので注意が必要っていうのは、実際にやってみたからわかることという感じですね。</li>
</ul>
<h2 id="その他感想などのブログ">その他、感想などのブログ</h2>
<p>適当に見つけたブログを列挙してあります。これもあるよ！などあれば、教えてください。</p>
<ul>
<li><a href="http://s-wool.blog.jp/archives/1009404632.html">第6回elasticsearch勉強会に行ってきましたのでそのメモ</a></li>
<li><a href="http://arika.hateblo.jp/entry/2014/09/17/100921">elasticsearch 勉強会 第6回</a></li>
</ul>
<h3 id="まとめ">まとめ</h3>
<p>今回も、ためになる話がいっぱい聞けたかなと。
個人的な印象としては、いつものメンバーよりも新しい方が多かった印象です。
また、ほとんどの方が、Elasticsearchをご存知でした。
そこそこ知名度は上がってきているようで嬉しい限りです。（東京以外での知名度なども知りたいかなと。）</p>
<p>あと、懇親会の部屋の案内が遅くなってしまってスミマセンでした。
さすがにスタッフ3名はきつかったです。。。</p>
<p>19時半開始にしてみましたが、懇親会の時間がやはり短めになってしまうなぁという印象でした。</p>
<p>次回ももちろん2ヶ月後くらいに行います。
スピーカー募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。
よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch.もうちょっと入門という話をしてきました #gihyo_efk</title>
      <link>https://blog.johtani.info/blog/2014/09/16/book-publication-event/</link>
      <pubDate>Tue, 16 Sep 2014 13:21:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/09/16/book-publication-event/</guid>
      <description>先日2014年9月9日(火)に『サーバ/インフラエンジニア養成読本 ログ収集〜可視化編』 出版記念！執筆者が語る大講演会！で、 「elastics</description>
      <content:encoded><p>先日2014年9月9日(火)に<a href="http://eventdots.jp/event/137658">『サーバ/インフラエンジニア養成読本 ログ収集〜可視化編』　出版記念！執筆者が語る大講演会！</a>で、
<a href="https://speakerdeck.com/johtani/elasticsearchmoutiyotutoru-men">「elasticsearch.もうちょっと入門」</a>というタイトルで発表してきました。
会場のGMOのみなさま、Treasure Data、技術評論社のみなさま、どうもありがとうございました。</p>
<p>書籍に興味のある方は、右のリンクから購入してもらえるとうれしいです。Kindle版も用意されています。</p>
<!-- more -->
<p>提供のTDの方に目をつぶってもらいながらLogstashについての発表となってしまいましたが、楽しんでいただけたかなぁと。
書籍では主にKibana3をメインにしたElasticsearchの使い方だったので、それ以外の機能ということで、Aggregationについて説明してみました。</p>
<p>そのあとは、おそらく初めてですが、パネルディスカッションにも参加しました。
<a href="https://twitter.com/naoya_ito">@naoya_ito</a>さんをモデレーターに、rebuild.fm風に進めていただき、話しやすかったかなと。
（少なくとも私は楽しめました！）
ただ、私だけバックグラウンドが少し異なることもあり、話をうまく繋げられなかったかもと気にしていたりもしますが。。。</p>
<p>パネルディスカッションでもありましたが、エンジニアが「趣味」で入れて試してみるのにはもってこいのツール群だと思います。
ちょっと入れてみて、可視化をしてみるといろいろと発見があると思います。
何かを発見するためにもまず試してみるのが何事も重要かなと最近思ってるのもあるので、気軽に試してみてもらえればと。</p>
<p>不明点などあれば、著者陣に気軽に聞いていただけると良いかと思います（いいですよね、みなさんｗ）。
Fluentd（もちろん、Logstashも）、Elasticsearch、Kibanaを利用して、データについて試行錯誤してもらって、
システムやビジネスに必要なものを探索して見てください。</p>
<h3 id="参考">参考</h3>
<p>他の方々のブログをメモとして。</p>
<ul>
<li><a href="http://suzuken.hatenablog.jp/entry/2014/09/11/210059">サービス改善とログデータ解析について発表してきました</a></li>
<li><a href="http://blog.harukasan.jp/entry/2014/09/12/144217">Kibanaではじめるダッシュボードについて発表してきました #gihyo_efk</a></li>
<li><a href="http://y-ken.hatenablog.com/entry/fluentd-system-design-pattern">Fluentdのお勧めシステム構成パターンについて発表しました</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearchのインデキシングに関するパフォーマンス検討</title>
      <link>https://blog.johtani.info/blog/2014/09/09/performance-considerations-for-elasticsearch-indexing/</link>
      <pubDate>Tue, 09 Sep 2014 17:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/09/09/performance-considerations-for-elasticsearch-indexing/</guid>
      <description>Elasticsearchのインデキシングに関するパフォーマンス検討 原文：performance considerations for elasticsearch indexing Elasticsearchユーザは様</description>
      <content:encoded><p>Elasticsearchのインデキシングに関するパフォーマンス検討</p>
<p>原文：<a href="http://www.elasticsearch.org/blog/performance-considerations-elasticsearch-indexing/">performance considerations for elasticsearch indexing</a></p>
<p>Elasticsearchユーザは様々な楽しいユースケースを持っています。小さなログを追加することから、Webスケールの大きなドキュメントの集合をインデキシングするようなことまでです。また、インデキシングのスループットを最大化することが重要で一般的な目標となります。
「典型的な」アプリケーションに対して良いデフォルト値を設定するようにしていますが、次のちょっとした簡単なベストプラクティスによってインデキシングのパフォーマンスをすぐに改善することができます。それらについて記述します。</p>
<!-- more -->
<p>第一に、制御できないならば、巨大なJavaヒープを使用しない：必要なサイズ（マシンの持つRAMの半分以下）のheapだけを設定しましょう。Elasticsearchの利用方法のために必要な全体量を設定します。これは、OSにIOキャッシュを制御するためのRAMを残すことを意味します。OSが<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html">javaプロセスをスワップアウト</a>していないことも確認しましょう。</p>
<p>最新バージョン（<a href="http://www.elasticsearch.org/downloads/1-3-2/">現時点では1.3.2</a>）のElasticsearchにアップグレードしましょう：多数のインデキシングに関連する問題点が最新リリースで修正されています。</p>
<p>詳細に入る前に警告：ここで述べるすべての情報は現時点での最新（<a href="http://www.elasticsearch.org/downloads/1-3-2/">1.3.2</a>）の情報です。しかし、Elasticsearchの更新は日々行われています。この情報をあなたが見た時点では最新ではなく、正確ではなくなっているかもしれません。自信がない場合は<a href="http://www.elasticsearch.org/community">ユーザメーリングリスト</a>で質問してください。</p>
<p>クラスタのインデキシングスループットをチューニングする場合、<a href="http://www.elasticsearch.org/overview/marvel">Marvel</a>は非常に有用なツールです：ここで述べている各設定を継続的に試し、変更の影響がクラスタの挙動をどのように変更されたかを簡単に可視化することが可能です。</p>
<h2 id="クライアントサイド">クライアントサイド</h2>
<p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html">bulk API</a>を常に使いましょう。1リクエストで複数のドキュメントをインデキシングでき、各バルクリクエストで送るのに良いドキュメント数を試しましょう。最適なサイズは多くの要因に依存しますが、最適サイズからずれるならば多すぎるよりも少なすぎる方が良いでしょう。クライアントサイドのスレッドで並列にbulkリクエストを使うか、個別の非同期リクエストを使ってください。</p>
<p>インデキシングが遅いと結論付ける前に、クラスタのハードウェアの性能を引き出せているかを確認して下さい：すべてのノードでCPUやIOが溢れていないかを確認するために<code>iostat</code>や<code>top</code>、<code>ps</code>といったツールを使いましょう。もし、溢れていなければ、より多くの並列なリクエストが必要です。しかし、javaクライアントからの<code>EsRejectedExecutionException</code>や、RESTリクエストのHTTPレスポンスとして<code>TOO_MANY_REQUESTS (429)</code>が返ってきた場合は並列リクエストを多く送りすぎています。もし<a href="http://www.elasticsearch.org/overview/marvel">Marvel</a>を利用しているなら、<a href="http://www.elasticsearch.org/guide/en/marvel/current/#_node_amp_index_statistics">Node Statistics Dashboard</a>の<code>THREAD POOLS - BULK</code>にリジェクトされた数が表示されます。bulkスレッドプールサイズ（デフォルト値はコア数）を増やすのは得策ではありません。インデキシングスループットを減少させるでしょう。クライアントサイドの並列度を下げるか、ノードを増やすのが良い選択です。</p>
<p>ここでは、1シャードに対してインデキシングスループットを最大化する設定に注目します。1つのLuceneインデックスのドキュメントの容量を測定するために、単一ノード（単一シャード、レプリカなし）で最初にテストをして最適化し、クラスタ全体にスケールする前にチューニングを繰り返します。これはまた、インデキシングスループットの要件を見つけるために、クラスタ全体にどのくらいのノードが必要かをラフに見積もるためのベースラインを与えてくれます。</p>
<p>単一シャードが十分機能したら、Elasticsearchのスケーラビリティの最大の利点や、クラスタでの複数ノードによるレプリカ数やシャード数の増加の利点が得られます。</p>
<p>結論を導き出す前に、ある程度の時間（60分）くらいクラスタ全体の性能を計測しましょう。このテストは、巨大なマージ、GCサイクル、シャードの移動、OSのIOキャッシュ、予期しないスワップの可能性などのイベントのライフサイクルをカバーできます。</p>
<h2 id="ストレージデバイス">ストレージデバイス</h2>
<p>当然ながらインデックスを保存するストレージデバイスはインデキシングの性能に多大な影響を及ぼします：</p>
<ul>
<li>SSDを利用する：これらは最も速いHDDよりも速いです。ランダムアクセスのための消費電力が低いだけでなく、シーケンシャルIOアクセスも高いです。また、同時に発生するインデキシング、マージや検索のための並列的なIOも高速です。</li>
<li>インデックスをリモートマウントされたファイルシステム（例：<a href="http://en.wikipedia.org/wiki/Network_File_System">NFS</a>や<a href="http://en.wikipedia.org/wiki/Server_Message_Block">SMB/CIFS</a>）上に配置しない：代わりにローカルストレージを使う</li>
<li>仮想化されたストレージ（Amazonの<a href="http://aws.amazon.com/ebs/">Elastic Block Storage</a>など）に注意：仮想化されたストレージはElasticsearchで十分に動作します。また、十分早く簡単に用意できることから魅力的です。しかし、残念なことに、ローカルストレージと比較すると本質的に遅いです。最近の非公式なテストでは、<a href="http://aws.amazon.com/ebs/details/#PIOPS">最高の性能を持つプロビジョニングされたIOPSのSSDオプションのEBS</a>でさえ、ローカルインスタンスにあるSSDよりも遅いです。ローカルインスタンスにあるSSDは物理マシン上のすべての仮想マシンから共有されてアクセスされます。もし他の仮想マシンが急にIOが集中した場合に不可解なスローダウンとなることがあることを覚えておいてください。</li>
<li>複数のSSDを<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-dir-layout.html">複数の<code>path.data</code>ディレクトリ</a>にインデックスをストライピング（<a href="http://en.wikipedia.org/wiki/RAID_0#RAID_0">RAID0</a>のように）：2つは同様で、ファイルブロックレベルでストライピングする代わりに、個別にインデックスファイルレベルでElasticsearchの&quot;stripes&quot;となります。これらのアプローチは、いづれかのSSDの故障によりインデックスが壊れるという、1シャードが故障する(IO性能を高速化することとトレードオフ)というリスクを増加させることに注意してください。これは、一般的に行うのに良いトレードオフです：単一シャードで最大のパフォーマンスを最適化し、異なるノード間でレプリカを追加すると、ノードの故障への冗長化ができます。また、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html">snapshotやrestore</a>を使って保険のためにインデックスのバックアップを取ることもできます。</li>
</ul>
<h2 id="セグメントとマージ">セグメントとマージ</h2>
<p>新しくインデキシングされたドキュメントは最初にLuceneの<code>IndexWriter</code>によってRAMに保存されます。RAMバッファがいっぱいになった時もしくは、Elasticsearchがflushもしくはrefreshを実行した時など定期的にこれらのドキュメントはディスクに新しいセグメントとして書き込まれます。最後に、セグメントが多くなった時に、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-merge.html">Merge PolicyとSucheduler</a>によってそれらがマージされます。このプロセスは連続的に生じます：マージされたセグメントはより大きなセグメントとなり、小さなマージが幾つか実行され、また、大きなセグメントにマージされます。これらがどのように動作するかを<a href="http://blog.mikemccandless.com/2011/02/visualizing-lucenes-segment-merges.html">わかりやすく可視化したブログはこちら</a>です。</p>
<p>マージ、特に大きなマージは非常に時間がかかります。これは、通常は問題ありません。そのようなマージはレアで全体のインデックスのコストと比べればささいなものです。しかし、マージすることがインデキシングについていけない場合、インデックスに非常に多くのセグメントがあるような深刻な問題を防ぐために、Elasticsearchはやってくるインデキシングリクエストを単一スレッド(1.2以降)に制限します。</p>
<p>もし、INFOレベルのログメッセージに<code>now throttling indexing</code>と表示されていたり、<a href="http://www.elasticsearch.org/guide/en/marvel/current">Marvel</a>でのセグメント数が増加しているを見た場合、マージが遅れているとわかります。Marvelは<a href="http://www.elasticsearch.org/guide/en/marvel/current/#_node_amp_index_statistics">Index Statistics dashboard</a>の<code>MANAGEMENT EXTENDED</code>の部分にセグメント数をプロットしており、それは、非常にゆっくりと指数対数的に増加しており、大きなマージが終了したところがのこぎりの歯のような形で見て取れます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="http://www.elasticsearch.org/content/uploads/2014/09/segmentCounts.png" />
    </div>
    <a href="http://www.elasticsearch.org/content/uploads/2014/09/segmentCounts.png" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>セグメント数</h4>
      </figcaption>
  </figure>
</div>

<p>なぜマージが遅れるのでしょう？デフォルトでElasticsearchはすべてのマージの書き込みのバイト数をわずか20MB/secに制限しています。スピニングディスク（HDD）に対して、これはマージによって典型的なドライブのIOキャパシティを飽和させず、並列に検索を十分に実行させることを保証します。しかし、もし、インデキシング中に検索をしない場合や、検索性能がインデキシングのスループットよりも重要でない場合、インデックスの保存にSSDを使用している場合などは、<code>index.store.throttle.type</code>に<code>none</code>を設定して、マージの速度制限を無効化するべきです（詳細は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html">こちら</a>をご覧ください）。なおバージョン1.2以前には<a href="https://github.com/elasticsearch/elasticsearch/issues/6018">期待以上のマージIO制限の発生</a>といったバグが存在します。アップグレードを！</p>
<p>もし、不幸にもスピニングディスク（それはSSDと同等の並列なIOを扱えません）をまだ使っている場合、<code>index.merge.scheduler.max_thread_count</code>に<code>1</code>を設定しなければなりません。そうでない場合は、（SSDを支持する）デフォルト値が多くのマージを同時に実行させるでしょう。</p>
<p>活発に更新が行われているインデックスで<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html"><code>optimize</code></a>を実行しないでください。それは、非常にコストの高い操作(すべてのセグメントをマージ)です。しかし、もし、インデックスにドキュメントを追加が終わった直後はオプティマイズのタイミングとしては良いタイミングです。それは、検索時のリソースを減らすからです。例えば、時間ベースのインデックスを持っており、新しいインデックスに日々のログを追加している場合、過去の日付のインデックスをオプティマイズするのは良い考えです。特に、ノードが多くの日付のインデックスを持っている場合です。</p>
<p>更にチューニングするための設定：</p>
<ul>
<li>実際に必要のないフィールドをオフにする。例えば<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-all-field.html"><code>_all</code>フィールドをオフ</a>。また、保持したいフィールドでは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html"><code>indexed</code>か<code>stored</code>かを検討する</a>。</li>
<li>もし、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-source-field.html"><code>_source</code>フィールドをオフ</a>にしたくなるかもしれないが、インデキシングコストは小さい(保存するだけで、インデキシングしない)、また、それは、将来の更新や、前のインデックスを再インデキシングするために非常に価値があり、それはディスク使用率の懸念事項がない限り、オフにする価値はあまりない。それは、ディスクが比較的安価であるので価値がない。</li>
<li>もし、インデックスされたドキュメントの検索までの遅延を許容できるなら、<code>index.refresh_interval</code>を<code>30s</code>に増やすか、<code>-1</code>を設定して、オフにする。これは、巨大なセグメントをフラッシュし、マージのプレッシャーを減らすことができる。</li>
<li><a href="http://www.elasticsearch.org/downloads/1-3-2/">Elasticsearch 1.3.2</a>(稀に、フラッシュ時に過度のRAMを使用するという<a href="https://github.com/elasticsearch/elasticsearch/issues/6443">問題</a>を<a href="https://github.com/elasticsearch/elasticsearch/issues/6379">修正した</a>)にアップグレードすることで、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-translog.html"><code>index.translog.flush_threshold_size</code></a>をデフォルト(200mb)から1gbに増加し、インデックスファイルのfsyncの頻度を減らす。
Marvelに<a href="http://www.elasticsearch.org/guide/en/marvel/current/#_node_amp_index_statistics"><code>Index Statistics dashboard</code></a>の<code>MANAGEMENT</code>にフラッシュの頻度がプロットされている。</li>
</ul>
<h2 id="インデックスバッファサイズ">インデックスバッファサイズ</h2>
<p>巨大なインデックスを構築中はレプリカ数を0にし、あとから、レプリカを有効にする。レプリカが0ということは、データを失った(ステータスがred)時に冗長性がないので、ノードの故障に注意すること。もし、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html"><code>optimize</code></a>(ドキュメントの追加をすることがないので)を計画するなら、インデキシングが終わったあとで、レプリカを作成する前に実行するのが良いでしょう。レプリカはオプティマイズされたセグメントをコピーするだけになります。詳細は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-update-settings.html">インデックス設定更新</a>を参照。</p>
<p>もし、ノードがヘビーなインデキシングを行っているだけなら、アクティブなシャードのインデキシングバッファに多くてい512MBを<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-indices.html">indices.memory.index_buffer_size</a>に与えてください。(超えてもインデキシングのパフォーマンスは一般的には改善されません。)Elasticsearchはその設定(Javaヒープのパーセンテージもしくはバイト数)を受けて、min_index_buffer_sizeとmax_index_buffer_sizeの値を前提にノードのアクティブシャードに均等に割り当てます；大きな値はLuceneが最初のセグメントをより大きくし、将来的なマージのプレッシャーを減らすことを意味します。</p>
<p>デフォルトは10%で、それで十分です；例えば、もし、5つのアクティブなシャードがノードにあり、ヒープが25GBの場合、各シャードは25GBの10%の1/5=512MB（すでに最大値）を持っています。ヘビーなインデキシングのあと、この設定をデフォルトに下げましょう。検索時のデータ構造のために十分なRAMを確保するために。この設定はまだ動的な設定変更はできません。<a href="https://github.com/elasticsearch/elasticsearch/issues/7045">Issueがここに</a>あります。</p>
<p>インデックスバッファによって現在利用されているバイト数は1.3.0の<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-stats.html">indices stats API</a>に追加されています。<code>indices.segments.index_writer_memory</code>の値を見ることができます。これはMarvelではまだプロットされていませんが、将来のバージョンで追加される予定です。しかし、自分でグラフに追加することもできます。(Marvelはデータは収集しています)</p>
<p>1.4.0では、<a href="https://github.com/elasticsearch/elasticsearch/issues/7440"><code>indices.segments.index_writer_max_memory</code></a>として、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-stats.html">indices stats API</a>にアクティブシャードにどのくらいのRAMバッファが割り当てられているかも表示されます。これらの値はインデックスのシャード事の値として見ることができ、<code>http://host:9200/&lt;indexName&gt;/_stats?level=shards</code>を使ってみることができます；これは、全シャードに対する合計と、各シャードごとのstatsを返すでしょう。</p>
<h2 id="オートidの利用もしくは良いidの利用">オートIDの利用もしくは良いIDの利用</h2>
<p>もし、ドキュメントの<code>ID</code>がなんでも良い場合、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#_automatic_id_generation%22">Elasticsearchで採番すること</a>ができます：これは、(1.2以降)ドキュメントIDをバージョンを探さずに保存できるように<a href="https://github.com/elasticsearch/elasticsearch/pull/5917">最適化</a>され、Elasticsearchの<a href="http://benchmarks.elasticsearch.org/">日毎のベンチマーク</a>で異なるパフォーマンスを見ることができます。(<code>Fast</code>と<code>FastUpdate</code>のグラフを比較)</p>
<p>もし、IDを自身が持っていて、自分の支配下で<a href="http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html">Luceneに対して素早く選ぼうとしている</a>なら、1.3.2にアップグレードしましょう、IDのルックアップが<a href="https://github.com/elasticsearch/elasticsearch/issues/6212">さらにオプティマイズ</a>されています。Javaの<a href="http://docs.oracle.com/javase/7/docs/api/java/util/UUID.html">UUID.randomUUID()</a>はやめましょう。それは、セグメントに対してどのようにIDを割り当てるかという予測やパターン性がないため、最悪のケースで<a href="http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html">セグメントごとのシーク</a>が発生します。</p>
<p><a href="http://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang/">Flake IDs</a>を利用した時の<a href="http://www.elasticsearch.org/overview/marvel">Marvel</a>によるインデックス性能の違い：</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="http://www.elasticsearch.org/content/uploads/2014/09/flakeIDsPerf.png" />
    </div>
    <a href="http://www.elasticsearch.org/content/uploads/2014/09/flakeIDsPerf.png" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>flakeIDsPerf</h4>
      </figcaption>
  </figure>
</div>

<p>ランダムUUIDを利用した場合：</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="http://www.elasticsearch.org/content/uploads/2014/09/uuidsPerf.png" />
    </div>
    <a href="http://www.elasticsearch.org/content/uploads/2014/09/uuidsPerf.png" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>uuidsPerf</h4>
      </figcaption>
  </figure>
</div>

<p>次の1.4.0では、ElasticsearchのID自動採番を<a href="https://github.com/elasticsearch/elasticsearch/issues/5941">UUIDからFlake IDに変更</a>します。</p>
<p>もし、Luceneのローレベル操作がインデックスに対してなにをやっているかについて興味があるなら、<a href="https://github.com/elasticsearch/elasticsearch/issues/5891"><code>lucene.iw</code>をTRACEログレベルで出力できるように</a>してみましょう(1.2から利用可能)。これは、多くの出力がありますが、Luceneの<code>IndexWriter</code>レベルで何が起きているかを理解するのに非常に役に立ちます。出力は非常にローレベルです：<a href="http://www.elasticsearch.org/guide/en/marvel/current">Marvel</a>がインデックスに何が起きているかをよりリアルタイムにグラフを描画してくれます。</p>
<h2 id="スケールアウト">スケールアウト</h2>
<p>我々は、単一シャード(Luceneインデックス)性能のチューニングに注目してきました。しかし、一旦それに満足できたならば、Elasticsearchはクラスタ全体にわたってインデキシングや検索を簡単にスケールアウトすることに長けています。シャード数(デフォルトでは5)を増やすのは可能です。それは、マシン全体に対して並列度、巨大なインデックスのサイズ、検索時のレイテンシの低下など得ることができます。また、レプリカを1位上にすることは、ハードウェア故障に対する冗長性を持つことを意味します。</p>
<p>最後に、このドキュメントを見ても問題解決しない場合は<a href="http://www.elasticsearch.org/community">コミュニティに参加</a>しましょう。例えば、<a href="https://groups.google.com/forum/?fromgroups#!forum/elasticsearch">ElasticsearchのユーザML</a>に投稿するなど。おそらく、修正すべきエキサイティングなバグがあるでしょう。(パッチも常に歓迎です！)</p>
</content:encoded>
    </item>
    
    <item>
      <title>サーバ/インフラエンジニア養成読本 ログ収集~可視化編 を手伝いました</title>
      <link>https://blog.johtani.info/blog/2014/08/04/release-magazine-book-of-log-aggs-and-viz/</link>
      <pubDate>Mon, 04 Aug 2014 21:54:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/08/04/release-magazine-book-of-log-aggs-and-viz/</guid>
      <description>懲りずにまた、執筆してみました。みなさん「買って」から感想をいただけるとうれしいです！ 本書について 共著者の方々のブログが詳しいので、そちらを</description>
      <content:encoded><p>懲りずにまた、執筆してみました。みなさん「買って」から感想をいただけるとうれしいです！</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&nou=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&o=9&p=8&l=as1&m=amazon&f=ifr&ref=tf_til&asins=4774169838" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<!-- more -->
<h2 id="本書について">本書について</h2>
<p>共著者の方々のブログが詳しいので、そちらを読んでもらいつつ。
実際にログを収集して解析されている方々と一緒に書かせていただくことで色々と勉強させていただいています。</p>
<h3 id="共著者の方々のブログ">共著者の方々のブログ</h3>
<ul>
<li><a href="https://twitter.com/suzu_v">@suzu_v</a>さん：<a href="http://suzuken.hatenablog.jp/entry/2014/07/18/084555">サーバ/インフラエンジニア養成読本 ログ収集~可視化編 を書きました</a></li>
<li><a href="https://twitter.com/yoshi_ken">@yoshi_ken</a>さん：<a href="http://y-ken.hatenablog.com/entry/published-elasticsearch-fluentd-kibana-book">ログ収集や可視化で話題のFluentd、Elasticsearch、Kibanaを徹底解説したムック本が発売となります</a></li>
<li><a href="https://twitter.com/harukasan">@harukasan</a>さん：<a href="http://blog.harukasan.jp/entry/2014/07/18/180351">書きました: サーバ/インフラエンジニア養成読本 ログ収集~可視化編</a></li>
</ul>
<h3 id="どの辺を書いたの">どの辺を書いたの？</h3>
<p>「特集３：Elasticsearch入門」（なんか、入門ばっかりだなぁ）を書かせていただきました。
データストア入門ということで、ほんとうに簡単な他のデータストアを説明し、Elasticsearchってどんなものかを単語の説明をしつつ紹介してみました。</p>
<p>Elasticsearch自体は多くの機能を持っており、それ単体で分厚い書籍がかけるので、ログ検索に関係ありそうな部分をピックアップしてみました。
あとは、運用時に気をつける点や便利なツール（Curatorなど）の紹介をしています。</p>
<p>また、Hadoopと合わせて利用してみたい、すでにHadoopにあるデータも活用してみたいという話もありそうだということで、<a href="https://github.com/elasticsearch/elasticsearch-hadoop">elasticsearch-hadoop</a>についても簡単ですが紹介してあります。</p>
<h2 id="その他感想">その他感想</h2>
<p>個人的に、忙しい時期<a href="http://blog.johtani.info/blog/2014/07/01/join-elasticsearch/">（参考記事）</a>だったので、あんまり力になれてないので大変申し訳なく思っています。。。
ただ、素晴らしい出来（カラーでKibanaの解説が日本語で読めたり、Fluentdの逆引きのリストがあったり、ログを貯めて可視化する意義を説明してあったり）です。</p>
<p>ぜひ、読んだ感想をいただければと！</p>
</content:encoded>
    </item>
    
    <item>
      <title>プロキシ環境でのpluginコマンドの実行</title>
      <link>https://blog.johtani.info/blog/2014/08/01/plugin-using-under-proxy-env/</link>
      <pubDate>Fri, 01 Aug 2014 15:24:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/08/01/plugin-using-under-proxy-env/</guid>
      <description>Proxy環境で働いている方も結構いると思います。 Twitter上で、Elasticsearchのpluginコマンドでプラグインがインスト</description>
      <content:encoded><p>Proxy環境で働いている方も結構いると思います。
Twitter上で、Elasticsearchのpluginコマンドでプラグインがインストールできなくて困っている方がいたので、
調べてみたのでメモしておきます。</p>
<!-- more -->
<h2 id="プラグインコマンド">プラグインコマンド</h2>
<p>Elasticsearchでは、プラグインという形でいくつかの便利な機能が公開されています。
<a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">形態素解析ライブラリのKuromoji</a>を使うためのプラグインや、<a href="https://github.com/lmenezes/elasticsearch-kopf">クラスタの管理がGUIで可能なkopf</a>プラグインなどがあります。
公式、サードパーティいろいろです。</p>
<p>これらのプラグインをElasticsearchにインストールする場合、以下のコマンドを実行すれば
自動的にダウンロードして<code>plugins</code>ディレクトリにインストールしてくれます。</p>
<pre><code>./bin/plugin -i elasticsearch/elasticsearch-analysis-kuromoji/2.3.0
</code></pre><p>ここで、<code>elasticsearch/elasticsearch-analysis-kuromoji/2.3.0</code>がプラグインのパスになります（例では、<code>提供元/プラグイン名/プラグインバージョン</code>となっています。）。</p>
<p>この<code>plugin</code>コマンドがダウンロード元にアクセスに行くのですが、プロキシ環境だとプロキシの設定が必要になります。</p>
<h2 id="プロキシの指定maclinuxとwindowsでの違い">プロキシの指定（Mac/LinuxとWindowsでの違い）</h2>
<h3 id="maclinuxshコマンド">Mac/Linux(shコマンド)</h3>
<p><a href="http://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji/">以前の記事</a>でプロキシのポート番号などの指定方法を
以下のように説明していました。
（※昔の記事のため、kuromojiプラグインのバージョンが古いです）</p>
<p>ElasticsearchのpluginコマンドはJavaで実装されています。（org.elasticsearch.common.http.client.HttpDownloadHelper）
プラグインのダウンロードには、java.net.URL.openConnection()から取得URLConnectionを使用しています。</p>
<p>ですので、pluginのインストールを行う際に、Proxy環境にある場合は以下のようにコマンドを実行します。</p>
<pre><code>./bin/plugin -DproxyPort=ポート番号 -DproxyHost=ホスト名 -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre><p>LinuxやMacの環境であれば、こちらのコマンドでプロキシの指定が可能です。
ただし、Windows環境ではうまくいきません。</p>
<p>Elasticsearchは、環境の違いにより、ダウンロードするファイルが異なります。
Windows環境の方は、zipファイルをダウンロードしてもらうようになっています。
elasticsearchコマンドおよびpluginコマンドがbat形式で提供されているのがzipファイルとなるからです。</p>
<h3 id="windowsbatコマンド">Windows(batコマンド)</h3>
<p>Windows環境では次のように指定します。</p>
<pre><code>set JAVA_OPTS=&quot;-DproxyHost=ホスト名 -DproxyPort=ポート番号&quot;
bin\plugin -i elasticsearch/elasticsearch-analysis-kuromoji/2.3.0
</code></pre><p>コマンドの実装方法が少し異なるために、このようになっています。</p>
<h2 id="まとめ">まとめ</h2>
<p>プロキシ環境で利用される場合は、プラグインコマンドは上記のように実行していただければと。</p>
<p>公式ガイドには、これらの情報を追記するPRを送る予定です。
また、WindowsのコマンドでもMac/Linuxと同様にできたほうがいい気がするので、Issueをあげようと思います。</p>
<p>不明点などあれば、コメントいただければと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 1.3.1 リリース（日本語訳）</title>
      <link>https://blog.johtani.info/blog/2014/07/29/elasticsearch-1-3-1-release/</link>
      <pubDate>Tue, 29 Jul 2014 12:22:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/07/29/elasticsearch-1-3-1-release/</guid>
      <description>原文：Elasticsearch 1.3.1 Releasedを日本語に翻訳したものです。 バグフィックス版のElasticsearch 1.3.1をリリー</description>
      <content:encoded><p><a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-1-released/">原文：Elasticsearch 1.3.1 Released</a>を日本語に翻訳したものです。</p>
<p>バグフィックス版のElasticsearch 1.3.1をリリースしました。
ダウンロードおよび変更履歴は<a href="http://www.elasticsearch.org/downloads/1-3-1/">Elasticsearch 1.3.1</a>からお願いいたします。</p>
<!-- more -->
<p>このリリースはインデックスリカバリ時の後方互換性バグ（<a href="https://github.com/elasticsearch/elasticsearch/pull/7055">#7055</a>）への対応です。
このバグは<strong>データの欠損は起こりません。</strong> Elasticsearch 1.3.1へアップグレードすることで問題を回避できます。
このバグは、以下のElasticsearchのバージョンで作成されたセグメントを含むインデックスを1.3.0へアップグレードしようとすると発生します。</p>
<ul>
<li>Elasticsearch 0.90.7</li>
<li>Elasticsearch 0.90.2</li>
<li>Elasticsearch 0.90.0以前のバージョン</li>
</ul>
<p>このバグは、これらの古いインデックスをレプリカからリカバリできなくします。
これらのバージョンのセグメントを持つインデックスが、レプリカは可能ですが、
ステータスがYellowのままGreenに決してなりません。
ログには次のようなExceptionが発生します。</p>
<blockquote>
<p>IllegalArgumentException[No enum constant org.apache.lucene.util.Version.x.x.x]</p>
</blockquote>
<p>Luceneの特定のバージョンではLuceneのマイナーバージョンを含んでおらず、誤ったバージョン番号がセグメントに記録されました。
<a href="https://issues.apache.org/jira/browse/LUCENE-5850">LUCENE-5850</a>のチケットがこの問題に対処するためにオープンされています。
この問題は我々の後方互換テストで見つかるべき問題ですが、Luceneで不足しているため発見されませんでした。
テストスイートは今後の可能性のために改良されます。</p>
<p>このリリースはその他に、Aggregationのマイナーバグフィックスも含まれています。
詳細は<a href="http://www.elasticsearch.org/downloads/1-3-1/">リリースノート</a>をご覧ください</p>
<p><a href="http://www.elasticsearch.org/downloads/1-3-1/">Elasticsearch 1.3.1</a>をダウンロードし、試してください。
もし問題を見つけた場合は<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHubのIssues</a>へご報告をお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Curator 1.2および1.1について</title>
      <link>https://blog.johtani.info/blog/2014/07/28/curator-2-0-and-1-1/</link>
      <pubDate>Mon, 28 Jul 2014 14:19:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/07/28/curator-2-0-and-1-1/</guid>
      <description>Curatorの1.2.0がリリースされました。 前回のCuratorの記事が古くなってしまった（1.1.0からコマンドのI/Fが変更された）</description>
      <content:encoded><p><a href="http://www.elasticsearch.org/blog/curator-1-2-0-released/">Curatorの1.2.0がリリース</a>されました。</p>
<p><a href="http://blog.johtani.info/blog/2014/01/24/curator-tending-your-time-series-indices-in-japanese/">前回のCuratorの記事</a>が古くなってしまった（1.1.0からコマンドのI/Fが変更された）ので
1.1.0および1.2.0に関する記事を翻訳しておきます。</p>
<p>ちなみに、<a href="https://github.com/elasticsearch/curator/">Curator</a>とは、Elasticsearchに時系列のインデックス（例：LogstashやFluentdでログを保存）を保存している場合にそれらのインデックスを管理（削除したり、クローズしたり）するための便利なツールです。
Curatorの概要については、<a href="https://github.com/elasticsearch/curator/">GitHubリポジトリ</a>か<a href="http://blog.johtani.info/blog/2014/01/24/curator-tending-your-time-series-indices-in-japanese/">前回の記事</a>をご覧ください。</p>
<!-- more -->
<h1 id="curator-110リリース-20140613公開a-namecurator_v110">Curator 1.1.0リリース (2014/06/13公開)<a name="curator_v110"/></h1>
<p>元記事：<a href="http://www.elasticsearch.org/blog/elasticsearch-curator-version-1-1-0-released/">elasticsearch curator - version 1.1.0 released</a></p>
<p>Elasticsearch 1.0.0がリリースされ、新しい機能、Snapshot &amp; Restoreが利用できるようになりました。
Snapshotはある時点でのインデックスの写真を撮るように、バックアップを作成することができます。
1.0.0が発表されてすぐに、この機能に関するリクエストが寄せられるようになりました。
「Curatorにスナップショットを追加して！」もしくは「いつCuratorでスナップショットが使えるようになる？」といった感じです。
これがあなたの要望なら、それはついに叶えられました。しかも他の追加機能も一緒にです。</p>
<h2 id="新機能">新機能</h2>
<p>Curatorの新機能は以下のとおりです。</p>
<ul>
<li>新CLI構造</li>
<li>スナップショット(Snapshot)</li>
<li>エイリアス(Aliases)</li>
<li>パターンによる除外インデックス指定</li>
<li>配置ルーティング(Allocation Routing)</li>
<li>インデックスとスナップショットの表示</li>
<li>リポジトリ管理(個別のスクリプトによる)</li>
<li><a href="https://github.com/elasticsearch/curator/wiki">ドキュメントWiki</a></li>
</ul>
<h3 id="新コマンドライン構造">新コマンドライン構造</h3>
<p><strong>注意</strong>：コマンドライン構造の変更とは、Curator 1.1.0以前のcron記述が動作しないことを意味します。Curator 1.1.0にアップグレードする場合はコマンドも修正が必要となるので注意してください。</p>
<p>シンプルにするために、<em>commands</em>という概念を追加しました。
また、ヘルプの出力もわかりやすくなっています。
前のバージョンと同じタスクをCuratorは実行できますが、異なるフォーマットを用いるようになりました。</p>
<p>旧コマンド：</p>
<pre><code>curator -d 30
</code></pre><p>新コマンド：</p>
<pre><code>curator delete --older-than 30
</code></pre><p>コマンドは、フラグとは異なりハイフンを前に付けないことに注意してください。
また、似たような名前のフラグがあることに気をつけてください。
例えば、<code>--older-than</code>フラグは多くのコマンドに利用できます。
指定される値は各ケースにおいて同一です。「指定された数よりも古いインデックス」となります。</p>
<p>新しいコマンドのリストは次のとおりです。</p>
<ul>
<li>alias</li>
<li>allocation</li>
<li>bloom</li>
<li>close</li>
<li>delete</li>
<li>optimize</li>
<li>show</li>
<li>snapshot</li>
</ul>
<p>コマンドのヘルプは次のコマンドで表示されます。</p>
<pre><code>curator [COMMAND] --help
</code></pre><p>コマンドに関係あるフラグがすべて表示されます。</p>
<h3 id="スナップショットsnapshots">スナップショット(snapshots)</h3>
<p><code>snapshot</code>コマンドで、存在しているリポジトリにインデックスのスナップショットを保存することができます。</p>
<p>Curatorはインデックス毎に1つのスナップショットを作成し、インデックスから名前をつけます。
例えば、インデックスの名前が<code>logstash-2014.06.10</code>の場合、スナップショットの名前は<code>logstash-2014.06.10</code>となります。
指定した条件を元に、シーケンシャルに、1つずつインデックスのスナップショットを作成していきます。</p>
<pre><code>curator snapshot --older-than 20 --repository REPOSITORY_NAME
</code></pre><p>このコマンドは、20日以上古いインデックスすべてのスナップショットを作成し、<code>REPOSITORY_NAME</code>で指定されたリポジトリに保存します。</p>
<p><code>es_repo_mgr</code>と呼ばれるリポジトリ作成を支援するスクリプトがCuratorには含まれています。
ファイルシステムおよびS3タイプのリポジトリ両方の作成を支援します。</p>
<p>さらに、古いインデックスのスナップショットを取ることができることに加えて、Curatorは最新のインデックスをアップロードする方法も提供します。
これは、<a href="http://www.elasticsearch.org/overview/marvel/">Elasticsearch Marvel</a>のインデックスをアップロードするときに便利です。
トラブルシューティングを目的として、パフォーマンスデータを他の人に見せる場合などです。</p>
<pre><code>curator snapshot --most-recent 3 --prefix .marvel- --repository REPOSITORY_NAME
</code></pre><p>このコマンドでは、最新の3つのMarvelインデックスのスナップショットを指定されたリポジトリに保存できます。</p>
<h3 id="エイリアスaliases">エイリアス(aliases)</h3>
<p>Curatorはすでに存在するエイリアスにインデックスを追加することも、削除することもできるようになりました。
ただし、エイリアスがすでに存在している必要があります。エイリアスの作成はできません。</p>
<p><code>last_week</code>という前の一週間のインデックスのエイリアスを保持していること想像してください。
この場合、次の2つのコマンドを利用することで、エイリアスを管理できます。</p>
<pre><code>curator alias --alias-older-than 7 --alias last_week
curator alias --unalias-older-than 14 --alias last_week
</code></pre><p>新しく作られたインデックスが<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#indices-templates">インデックステンプレート</a>によって
自動的にエイリアスの一部となるようにElasticsearchに設定しておくと、さらに便利です。
この場合、新しいインデックスが自動的に<code>this_week</code>というエイリアスの一部になるようにしてあれば、以下のコマンドのみとなります。</p>
<pre><code>curator alias --unalias-older-than 7 --alias this_week
</code></pre><p><code>this_week</code>と<code>last_week</code>のエイリアスのアップデートを保持できます。</p>
<h3 id="パターンによる除外exclude-pattern">パターンによる除外(exclude pattern)</h3>
<p>時には、指定したインデックスを操作から除外したくなる場合もあるでしょう。
ここまでは、プレフィックスや日付によって選択されたインデックスのみを対象にしてきました。
そこで、<code>--exclude-pattern</code>オプションです。これは、指定したインデックスを除いて処理を行うことができます。</p>
<p><code>logstash-2014.06.11</code>というインデックスを決して削除したくないとします。
この場合、次のコマンドのようになります。</p>
<pre><code>curator delete --older-than 15 --exclude-pattern 2014.06.11
</code></pre><p>Curatorはデフォルトで<code>logstash-</code>というプレフィックスにマッチしますが、<code>2014.06.11</code>というインデックスは対象外となります。</p>
<h3 id="配置ルーティングallocation-routing">配置ルーティング(allocation routing)</h3>
<p>Elasticsearchはノードにタグを付けることができます。
これらのタグはインデックスやシャードをクラスタのどこに配置するかをコントロールするために役立ちます。
一般的なユースケースだと、高性能なSSDドライブを持ったノードをインデキシングのために、ハードディスクを持った性能の低いマシンは検索頻度が低い古いインデックスを配置するといった場合です。
この場合、HDDノードには、<code>elasticsearch.yml</code>に<code>node.tag: hdd</code>、SSDノードには<code>node.tag: ssd</code>と設定されているべきです。
Curatorはこの時、インデックスをタグに基づいてオフピークの時間帯に再配置させることができます。</p>
<p>コマンド：</p>
<pre><code>curator allocation --older-than 2 --rule tag=hdd
</code></pre><p><code>index.routing.allocation.require.tag=hdd</code>という設定が２日よりも古いインデックスに適用されます。
これは、インデックスのシャードが<code>node.tag: hdd</code>というノードに再配置される必要があると、Elasticsearchに伝えます。</p>
<h3 id="インデックスとスナップショットの表示show-indices-and-snapshots">インデックスとスナップショットの表示(show indices and snapshots)</h3>
<p>これは、単にあなたの持っているインデックスやスナップショットがどんなものかを表示します。</p>
<pre><code>curator show --show-indices
</code></pre><p>これは、デフォルトプレフィックスの<code>logstash-</code>にマッチするすべてのインデックスを表示します。</p>
<pre><code>curator show --show-snapshots --repository REPOSITORY_NAME
</code></pre><p>これは、指定されたリポジトリにある、デフォルトプレフィックスの<code>logstash-</code>にマッチするすべてのスナップショットを表示します。</p>
<h3 id="リポジトリ管理repository-management">リポジトリ管理(repository management)</h3>
<p>前に説明したとおり、<code>es_repo_mgr</code>と呼ばれるヘルパースクリプトをCuratorは含んでいます。
現時点では、<code>fs</code>と<code>s3</code>タイプをサポートしています。
リポジトリを作る前に利用したいタイプのドキュメントを読むようにしてください。
例えば、<code>fs</code>タイプのリポジトリを各ノードで使う場合は、同じ共有ファイルシステムに、同じパスでアクセスできなければなりません。
パスの指定は<code>--location</code>です。</p>
<p><code>fs</code>タイプリポジトリの作成</p>
<pre><code>es_repo_mgr create_fs --location '/tmp/REPOSITORY_LOCATION' --repository REPOSITORY_NAME
</code></pre><p>削除</p>
<pre><code>es_repo_mgr delete --repository REPOSITORY_NAME
</code></pre><h3 id="ドキュメントwiki">ドキュメントWiki</h3>
<p><a href="https://github.com/elasticsearch/curator/wiki">Curatorのドキュメント</a>が更新され、オンラインにWiki形式でだれでも更新できるようになっています。
コマンドやフラグのより詳細の情報はこちらで見つけることができます。また、もし、興味があれば、ドキュメントを追加することもできます。</p>
<h2 id="インストールと更新">インストールと更新</h2>
<p>Curator 1.1.0は<a href="https://pypi.python.org/pypi?%3Aaction=pkg_edit&amp;name=elasticsearch-curator">PyPi</a>リポジトリにあります。
インストールは以下のとおりです。</p>
<pre><code>pip install elasticsearch-curator
</code></pre><p>バージョン1.0.0からアップグレードする場合は以下のとおりです。</p>
<pre><code>pip uninstall elasticsearch-curator
pip install elasticsearch-curator
</code></pre><p>バージョン1.0.0よりも古いバージョンからのアップグレードは以下のとおりです。</p>
<pre><code>pip uninstall elasticsearch-curator
pip uninstall elasticsearch
pip install elasticsearch-curator
</code></pre><p><code>pip uninstall elasticsearch</code>で、古いパイションモジュールをを削除します。
適切なバージョンが依存関係により再インストールされます。</p>
<h2 id="まとめ">まとめ</h2>
<p>Curatorの新機能は素晴らしいです！このリリースは大きな改善です。
もし、トラブルや足りないものを見つけた場合は<a href="http://github.com/elasticsearch/curator/issues">GitHub Issue</a>に報告してください。
また、Curatorが便利だと思ったら、私たちに伝えてください。<code>#elasticsearch</code>タグを付けてツイートしてください！</p>
<p>Curatorはまだ、始まったばかりです。Curator 2.0のロードマップを作業中です。ここまで読んでいただきありがとうございます。
Happy Curating!</p>
<hr>
<h1 id="curator-120リリース20140724">Curator 1.2.0リリース(2014/07/24)</h1>
<p>元記事：<a href="http://www.elasticsearch.org/blog/curator-1-2-0-released/">curator 1.2.0 released</a></p>
<p><a href="#curator_v110">Curator v1.1.0</a>のリリースから、数週間が経ちました。
私たちは、Curator 1.2.0をリリースしました。</p>
<h2 id="新機能new-features">新機能(new features)</h2>
<ul>
<li>ユーザ指定の日付パターン：長い間リクエストされていた機能</li>
<li>ウィークリーインデックスのサポート：これも長い間リクエストされていた機能</li>
<li>複数の<a href="https://github.com/elasticsearch/curator/wiki/Logformat">ログフォーマット</a>オプション：Logstashフォーマットが利用可能</li>
</ul>
<p>これらの変更は<a href="https://github.com/elasticsearch/curator/wiki">Curatorドキュメント</a>にも記載されています。</p>
<h2 id="更新updates">更新(updates)</h2>
<ul>
<li>ログ出力の整理：デフォルトのログ出力を整理しました。デバッグログはすべて表示されます。</li>
<li>ドライランのログ出力の詳細化：テスト実行時に何が起きたかをわかりやすくしました。</li>
</ul>
<h2 id="日付パターンと--timestringdate-patterns-and---timestring">日付パターンと<code>--timestring</code>(date patterns and &ndash;timestring)</h2>
<p>前のリリースで、セパレータ文字を利用して、インデックス名のエレメントを分離することで、日付を計算しました。
この設計の決定は、プログラムが管理するために設計されたLogstashのインデックスを使うのには簡単でした。
しかし、Curatorは時系列インデックス管理に成長しています。これは、異なる命名規則のインデックスを意味しています。</p>
<p>また、インターバルによって、日付の計算が必要になる場合もあります。
<code>--time-unit</code>オプションが残っており、<code>weeks</code>という単位を指定することもできます。
デフォルトの<code>--timestring</code>オプションは、以前のコマンドと同様の動作をしなければなりません。次のようになります。</p>
<table>
<thead>
<tr>
<th>Time Unit</th>
<th>Timestring</th>
</tr>
</thead>
<tbody>
<tr>
<td>days</td>
<td><code>%Y.%m.%d</code></td>
</tr>
<tr>
<td>hours</td>
<td><code>%Y.%m.%d.%H</code></td>
</tr>
<tr>
<td>weeks</td>
<td><code>%Y.%W</code></td>
</tr>
</tbody>
</table>
<p>これが意味するものは、もし、単位に<code>hours</code>をした場合、<code>--timestring</code>を指定しなかった場合は<code>%Y.%m.%d.%H</code>となります。
これは、<a href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">Pythonのstrftimeフォーマット</a>で&quot;年.月.日.時&quot;を意味します。
同様に、<code>weeks</code>を単位に指定した場合、Curatorはデフォルトの<code>--timestring</code>は<code>%Y.%W</code>となります。</p>
<p>この機能は、日付の間にセパレーター文字のないインデックスでも機能します。
例えば、<code>production-20140724</code>のような日時インデックスがある場合、2日よりも古いインデックスに対する<a href="https://github.com/elasticsearch/curator/wiki/Disable-Bloom-Filter-Cache">ブルームフィルタっキャッシュのオフ</a>のコマンドは次のようになります。</p>
<pre><code>curator bloom --prefix production- --older-than 2 --timestring %Y%m%d
</code></pre><p>この例で、デフォルトの単位は<code>days</code>であることに注意してください。<code>hourly-2014072414</code>のような時間インデックスの場合は次のようになります。</p>
<pre><code>curator bloom --prefix hourly- --older-than 2 --time-unit hours --timestring %Y%m%d%H
</code></pre><h2 id="--separatorの置き換え"><code>--separator</code>の置き換え</h2>
<p>もし、Curatorの前のバージョンでカスタムセパレータ文字を利用していた場合、次のように変更すべきです。
前のコマンドで<code>cerberus-2014-07-24</code>のようなインデックスがある場合、コマンドを<code>--separator -</code>の用に置き換える必要があります。
新しいコマンドは次のとおりです。</p>
<pre><code>curator delete --prefix cerberus- --older-than 30 --timestring %Y-%m-%d
</code></pre><p>年(<code>％Y</code>)と月(<code>%m</code>)と日('%d&rsquo;)の間にセパレータ文字を置くだけです。</p>
<p>これは、また、Curatorで以前は不可能であったことをできるようにもします。
異なるセパレータ文字の混在です。
<code>logs-2014.07.24-14</code>というようなインデックスを処理するときに<code>--timestring</code>は<code>%Y.%m.%d-%H</code>のようになります.</p>
<p><code>--timestring</code>の詳細は<a href="https://github.com/elasticsearch/curator/wiki/Timestring">Curatorのドキュメント</a>をご覧ください。</p>
<h2 id="フィードバック">フィードバック</h2>
<p>これらの新しい機能はユーザのコメントやリクエストから来ています。もし、機能のリクエストやバグを発見したら、<a href="https://github.com/elasticsearch/curator/issues">こちら</a>まで連絡してください。</p>
<p>また、Twitterでもお待ちしています。私たちのTwitter IDは<code>@elasticsearch</code>です。</p>
<p>Happy Curating!</p>
</content:encoded>
    </item>
    
    <item>
      <title>第5回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2014/07/19/hold-on-5th-elasticsearch-jp/</link>
      <pubDate>Sat, 19 Jul 2014 21:52:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/07/19/hold-on-5th-elasticsearch-jp/</guid>
      <description>第5回Elasticsearch勉強会を開催しました。 遅くなってしまいましたが、まとめてみました。 今回は、Elasticsearchに入って</description>
      <content:encoded><p><a href="http://elasticsearch.doorkeeper.jp/events/12028">第5回Elasticsearch勉強会</a>を開催しました。
遅くなってしまいましたが、まとめてみました。</p>
<p>今回は、Elasticsearchに入って初の勉強会でした。タイミングが良いことに、Honza、Igor、Shayの3名がトレーニングのために
来日していたため、特別回ということにして、話をしてもらいました。</p>
<p>そして、<a href="http://samuraism.com/">サムライズム</a>の<a href="https://twitter.com/yusuke">@yusuke</a>さんにテキスト翻訳してもらいました。
早くて正確なタイピング＋翻訳、本当にありがとうございました。</p>
<p>開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！
参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<blockquote class="twitter-tweet" lang="ja"><p>amazing turnout to the elasticsearch at Tokyo <a href="https://twitter.com/hashtag/elasticsearchjp?src=hash">#elasticsearchjp</a> <a href="http://t.co/Aa88eVf5dF">pic.twitter.com/Aa88eVf5dF</a></p>&mdash; Shay Banon (@kimchy) <a href="https://twitter.com/kimchy/statuses/488686274375843841">2014, 7月 14</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<!-- more -->
<p>動画があとで、アップされる予定です。お楽しみに。</p>
<h2 id="honzas-talk">Honza&rsquo;s talk</h2>
<ul>
<li>djangoの開発者！であり、ElasticsearchのPythonクライアント、Curatorの開発者</li>
<li>Python Clientを利用しながら、ライブコーディングのような形で説明する方法が新鮮</li>
<li>Aggregationの便利さについての説明</li>
<li>Python Clientがクエリを組み立てるのにすごく便利そうだった</li>
<li>Pythonユーザが結構いたので助かりましたｗ</li>
</ul>
<h2 id="igors-talk">Igor&rsquo;s talk</h2>
<p>スライド：<a href="https://speakerdeck.com/imotov/elasticsearch-data">elasticsearch data/</a></p>
<ul>
<li>Snapshot/Restoreの開発などを行っている開発者</li>
<li>Elasticsearchのデータ、ディレクトリ構造に関するお話</li>
<li>シャードの話から、ディレクトリ構造、メタデータに関する説明</li>
<li>transaction logの挙動の説明</li>
<li>検索のフェーズの説明</li>
</ul>
<p>Igorは、実は私がElasticsearch社の人とコンタクトがとれた最初の人だと思います。
第1回Elasticsearch勉強会が開催する当日に帰国されるという不運だったのですが、1年越しでトークしてもらえました！</p>
<blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/johtani">@johtani</a> I am so bummed! I am leaving Tokyo Thursday morning.</p>&mdash; Igor Motov (@imotov) <a href="https://twitter.com/imotov/statuses/372340973121986560">2013, 8月 27</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<h2 id="qa">QA</h2>
<p>ShayをメインにいくつかのQAをしてもらいました。
NetflixなどのMeetupの動画で見てたのですが、こんな形で日本でも実現できるとは。</p>
<ul>
<li>Q: なんで、ファイルデスクリプタの設定を大きくするの？
<ul>
<li>A: Luceneのインデックスは複数のセグメントから構成されている。メモリに作られたあと、ファイルにfsyncされる。</li>
</ul>
</li>
<li>Q: KibanaでAggregation使いたいんだけど？
<ul>
<li>A: Kibana 4で対応するよ！異なるフィールドの値を1つのグラフにすることも出来るよ！</li>
</ul>
</li>
<li>Q: なんでElasticsearch作ったの？
<ul>
<li>A: 暇だったからｗ奥さんのレシピ検索を作ってみようと思って作り始めて、Luceneを触って感動して。。。検索すげー、Compassってのを触ってこれもすごいと思いつつ、もっとLucene活用できるんじゃないかということでElasticsearch作ったんだ。奥さんのレシピ検索？まだ完成してないよｗ</li>
</ul>
</li>
<li>Q: 2000くらいスナップショット撮ったらパフォーマンスが悪くなっててなんで？
<ul>
<li>A: 差分でスナップショットを作るんだけど、差分の計算に昔のスナップショットを見るので、定期的に新しくしたほうがいい。もし、気になることがあったらIssue上げたりMLに投げてくれるとうれしい。<br>
（あとでちょっと聞いたけど、古いスナップショットを消すのも有効っぽい。差分でスナップショットを作るけど、昔のを消した場合は、新しいスナップショットが利用しているファイルは残る仕組みになっているから。）</li>
</ul>
</li>
<li>Q: Relevancyのチューニングってどうすればいい？ドキュメントが少なくない？
<ul>
<li>A: ドキュメンテーションは頑張ってるので、応援してねｗあとは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/index.html">definitive guide</a>も参考になるよ。スコアはfunction_scoreクエリがすごいのでいろいろ使ってね。MVELをGroovyに帰る予定。性能もだけど、サンドボックス的な意味もあります。</li>
</ul>
</li>
<li>Q: 次のVisionは？現時点は検索だけど。（最後の質問がとてもナイスで、助かりましたｗ私がしたほうがいい気がするｗｗ）
<ul>
<li>A: 今後はアナリティクスのプラットフォームに向かってる。Aggregationとかね。メモリ効率よくしたりしてるよ。あとは、Field-collapsionも実装中だよ。あと、マシンラーニングとかもね。データを探索するための機能を色々作ってくよ。障害性にも。チェックサム機能をLuceneに入れて、ESにも入れていく予定。Zenの機能も改善している。</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>今週は、トレーニングがあったり、いろいろな打ち合わせがあったりと、テンパってたので至らない点が多かったかもしれないですが。。。
楽しんでいただけと思います。
数日、Shay、Honza、Igorと行動を共にして、本当に情熱のあるチームでユーザのことを気にかけているなと感じることができました。
少しでもその片鱗を勉強会で感じてもらえたんじゃないかと。特に、QAでのShayによる情熱が伝わったんじゃないかと。</p>
<p>懇親会でも数人の方から、日本語のサポートを望んでいるという声も頂きました。
興味のある方は私までコンタクトいただければと。</p>
<p>あと、@yusukeさんのテキスト翻訳が素晴らしくて、参加してもらった方たちも絶賛してました。
次回も英語スピーカーの場合に助けてもらえると嬉しいです（私もそこまで出来るように頑張ります）</p>
<h2 id="その他のブログ">その他のブログ</h2>
<p>ブログ記事ありがとうございます！</p>
<ul>
<li><a href="http://arika.hateblo.jp/entry/2014/07/15/011241">第5回elasticsearch勉強会にいってきました - はやさがたりない。</a></li>
<li><a href="http://blog.yoslab.com/entry/2014/07/15/073000">感想戦：aggrigation から見える検索エンジンの次 - 第5回 Elasticsearch勉強会 - よしだのブログ</a></li>
<li><a href="http://uchimanajet7.hatenablog.com/entry/2014/07/15/114632">「第5回elasticsearch勉強会 #elasticsearch #elasticsearchjp」（2014年07月14日）の参加メモ - uchimanajet7のメモ</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>転職しました</title>
      <link>https://blog.johtani.info/blog/2014/07/01/join-elasticsearch/</link>
      <pubDate>Tue, 01 Jul 2014 11:30:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/07/01/join-elasticsearch/</guid>
      <description>ということで、転職しました。 どーしてもやりたいことが出てきたので、無理を言って転職することにしてみました。 サムライズムではなく、Elasti</description>
      <content:encoded><p>ということで、転職しました。
どーしてもやりたいことが出てきたので、無理を言って転職することにしてみました。</p>
<!-- more -->
<p><a href="http://samuraism.com">サムライズム</a>ではなく、<a href="http://www.elasticsearch.com">Elasticsearch</a>にジョインします。（というか、しました。）</p>
<blockquote class="twitter-tweet" lang="ja"><p>初出社 <a href="https://twitter.com/hashtag/%E3%82%B5%E3%83%A0%E3%83%A9%E3%82%A4%E3%82%BA%E3%83%A0?src=hash">#サムライズム</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/statuses/483793778541465600">2014, 7月 1</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>冗談でツイートしたのですが、その前に英語アカウントのツイートがRTされてしまっていまいちでした。。。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140701/schiphol.jpg" />
    </div>
    <a href="/images/entries/20140701/schiphol.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>スキポール空港</h4>
      </figcaption>
  </figure>
</div>

<p>先週、アムステルダムに行っていたのも退職前に休みをいただき、Elasticsearchの全社会議に参加していたためです。
とてもエキサイティングな経験（英語漬けとか）ができ、もっと精進しないとなという気持ちにもなり、ますます頑張らないとなと。</p>
<p>ということで、今後は日本中にElasticsearchやLogstash、Kibanaを広めるべく、いろいろな場所で話をしたいと思います。
興味のある方は、声をかけていただければと。</p>
<p>あと、東京で<a href="http://purchases.elasticsearch.com/class/elasticsearch/core-elasticsearch/tokyo/2014-05-20">ElasticsearchのCoreトレーニング</a>が行われます。
通常は2日間ですが、通訳の方が付く関係で3日間の開催となっています。
開発者2名がトレーナーとして来日します。開発者に質問をできる良い機会ですので、興味のある方は参加してみてはいかがでしょうか。
<strong>また、CTOのShay Banonも来日する予定です。</strong></p>
<p>トレーナー2名とShayは<a href="http://elasticsearch.doorkeeper.jp/events/12028">7/14の勉強会</a>でも話をしてくれます。
こちらも興味のある方は参加してみてください。（参加登録はこの後、すぐに開始します。）</p>
<p>まだまだ、勉強しなければいけないことだらけですが、ElasticsearchのいろいろなプロダクトやOSSについて広めていきたいと思いますので、よろしくお願いいたします。</p>
<h2 id="おまけ">おまけ</h2>
<p>ということで、一度やってみたかったのでリンクを貼ってみます。</p>
<p><a href="http://www.amazon.co.jp/registry/wishlist/29EMX20UN9P16">ほしい物リストはこちら</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>退職しました</title>
      <link>https://blog.johtani.info/blog/2014/06/30/resignation/</link>
      <pubDate>Mon, 30 Jun 2014 23:19:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/06/30/resignation/</guid>
      <description>退職しました。 色々と書きたいんですが、時差ボケで頭痛いので、このくらいです。 余裕出たら書くかも。 ほしい物リストはこちら 新天地についてはまた明</description>
      <content:encoded><p>退職しました。</p>
<!-- more -->
<p>色々と書きたいんですが、時差ボケで頭痛いので、このくらいです。
余裕出たら書くかも。</p>
<p><a href="http://www.amazon.co.jp/registry/wishlist/29EMX20UN9P16">ほしい物リストはこちら</a></p>
<p>新天地についてはまた明日。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch server 2nd editionのファーストインプレッション</title>
      <link>https://blog.johtani.info/blog/2014/06/16/first-impression-elasticsearch-server-2nd-edition/</link>
      <pubDate>Mon, 16 Jun 2014 17:47:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/06/16/first-impression-elasticsearch-server-2nd-edition/</guid>
      <description>Elasticsearch server 2nd editionが発売されています。 私が翻訳したのは前のバージョンですが。。。 まずは、目次を元にどのくらい変わってるかを見てみました。 （</description>
      <content:encoded><p><a href="http://bit.ly/1kbu5Xd">Elasticsearch server 2nd edition</a>が発売されています。</p>
<p>私が翻訳したのは前のバージョンですが。。。
まずは、目次を元にどのくらい変わってるかを見てみました。
（全部まだ読んでなくて。。。）</p>
<!-- more -->
<h2 id="1章-getting-started-with-the-elasticsearch-cluster">1章 Getting Started with the Elasticsearch Cluster</h2>
<p>冒頭に、全文検索とは、転置インデックスとはどんなものか、
Luceneの簡単なアーキテクチャの仕組みについて説明が追加されています。
検索の仕組みを知らない人が読んでもわかりやすくなっています。</p>
<p>インストール方法なども少し追記されています。
バージョニングと簡単なデータ登録と検索方法についてもここで触れられています。
検索結果の構造の説明もちょっとあります。
まず簡単に触ってみるというところまでが1章でまとめられた感じです。</p>
<h2 id="2章-indexing-your-data">2章 Indexing Your Data</h2>
<p>新しく、切りだされた形です。
前のバージョンでは1章で説明されていた、Mapping周りが切りだされています。
シャードやレプリカの説明もこちらです。</p>
<p>IPアドレスタイプ（IPv4のみ）と<code>token_count</code>タイプの説明も追加されてます。
similarityやpostingsフォーマットなどは新しく追記されています。
また、メタフィールドと呼ばれる<code>_type</code>などはこちらに移動しているようです。
マージ処理などの説明も追記されています。このあたりは、<a href="http://www.packtpub.com/mastering-elasticsearch-querying-and-data-handling/book">Mastering ElasticSearch</a>に
記載されているものが移植された感じでしょうか。</p>
<h2 id="3章-searching-your-data">3章 Searching Your Data</h2>
<p>前のバージョンでは2章だった章です。
クエリについては1.0で追加された<code>simple_query_string</code>などが追記されています。
<code>constant_score</code>や<code>dismax</code>などもです。</p>
<p>また、前のバージョンの3章で説明されていたハイライトや8章で触れられていた<code>validate API</code>についても
移動しています。</p>
<h2 id="4章-extending-your-index-structure">4章 Extending Your Index Structure</h2>
<p>前のバージョンの3章で触れられていた、データの構造に関する部分がこの章になります。
親子や配列、ネスト等のデータのインデックスや検索の方法です。</p>
<h2 id="5章-make-your-search-better">5章 Make Your Search Better</h2>
<p>スクリプティングや言語判定などの仕組みが記載されています。
また、ブーストについても同様です。Synonymについてもここです。
スパンクエリについては省略されたのかな？</p>
<h2 id="6章-beyond-full-text-searching">6章 Beyond Full-text Searching</h2>
<p>1.0の目玉機能の一つであるAggregationの説明から始まります。
その後、ファセットやPercolatorについてです。メモリに関する注意点もありそうです。
また、Geoについての説明がこちらに移動されていました。
<code>scroll API</code>についてもこちらで説明されています。</p>
<h2 id="7章-elasticsearch-cluster-in-detail">7章 Elasticsearch Cluster in Detail</h2>
<p>前の7章で記載されていたElasticsearchの分散の仕組み（Node Discovery）についての記載があります。
また、1.0で追加された<code>circuit breaker</code>やスレッドプール、インデックスのリフレッシュレートなど、<a href="http://www.packtpub.com/elasticsearch-server-second-edition/book">Mastering ElasticSearch</a>の
内容も追記されている気がします。</p>
<p>インデックスやマッピングのテンプレート機能についてもここで説明があるみたいです。</p>
<h2 id="8章-administrating-your-cluster">8章 Administrating Your Cluster</h2>
<p>1.0で追加された<code>snapshot/restore</code>の説明から始まります。
あとは、前のバージョンの7章で説明されていたクラスタ管理用のAPIについての説明です。
いくつか（例えば<code>cat API</code>）、1.0で追加されています。</p>
<p>また、シャードのリバランスの話も追加されているようです。
エイリアスやプラグインの話はこちらに移動してるみたいです。</p>
<h2 id="感想">感想</h2>
<p>ということで、とりあえず、駆け足で目次ベースで違いを見てみました。
<a href="http://www.packtpub.com/elasticsearch-server-second-edition/book">Mastering ElasticSearch</a>での
知見がフィードバックされ、しかも1.0（すでに1.3が出そうな勢いですが。。。）にバージョンアップされた内容になっています。
冒頭がわかりやすくなっているので、検索をやったことのない方にもおすすめな書籍になった気がします。
英語が苦にならなければ、おすすめの一冊だと思います。</p>
<p>来月から読み進めるつもりなので、また、面白い内容があったら感想を書いていこうと思います。
（また翻訳できるといいかもなー）</p>
</content:encoded>
    </item>
    
    <item>
      <title>可視化ツール現状確認会に参加してきました。#可視化</title>
      <link>https://blog.johtani.info/blog/2014/06/04/attending-visualize-study/</link>
      <pubDate>Wed, 04 Jun 2014 21:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/06/04/attending-visualize-study/</guid>
      <description>可視化ツール現状確認会に参加して、カジュアルウォーターじゃなくて可視化ツールの現状を確認してきました。 ということで、いつものメモです。 Mackerel と Graphite</description>
      <content:encoded><p><a href="http://www.zusaar.com/event/7437003">可視化ツール現状確認会</a>に参加して、<strike>カジュアルウォーターじゃなくて</strike>可視化ツールの現状を確認してきました。</p>
<p>ということで、いつものメモです。</p>
<!-- more -->
<h2 id="mackerel-と-graphite-について-y_uuk1さん">Mackerel と Graphite について （y_uuk1さん）</h2>
<h3 id="graphite">Graphite</h3>
<ul>
<li>時系列</li>
<li>工夫すればスケーラブル</li>
<li>SensuやCollectdと組み合わせたり</li>
<li>GrafanaとGrapheneでGUI</li>
<li>Mackerel素敵だよと。</li>
<li>架空のわかりやすいグラフが見れた</li>
</ul>
<h2 id="kibana--grafana--influga-hakoberaさん">Kibana &amp; Grafana &amp; Influga （hakoberaさん）</h2>
<ul>
<li>
<p>Kibana</p>
<ul>
<li>かっこいい。</li>
<li>JVM大変。</li>
<li></li>
</ul>
</li>
<li>
<p>Grafana</p>
<ul>
<li>Graphiteがカッコ悪いのでKibanaをフォーク</li>
<li>なぜか、ESが必要。</li>
<li>InfluxDBに浮気しそう</li>
</ul>
</li>
<li>
<p>Influga</p>
<ul>
<li>@haoberaさん作</li>
<li>InfluxDB Queryサポート</li>
</ul>
</li>
<li>
<p>迷ったら、Kibana入れとけ。</p>
</li>
<li>
<p>DistinctがKibana出できないけど、それ以外はある程度行けるらしい。</p>
<ul>
<li>aggregationがサポートされたら、できると思う。</li>
</ul>
</li>
<li>
<p>Kibana以外は、バックエンドがタイムシリーズなので、縛りがある</p>
</li>
</ul>
<h2 id="可視化ツール紹介仮-showyouさん">可視化ツール紹介（仮） （showyouさん）</h2>
<ul>
<li>誰がチャートを作るの？</li>
<li>誰が見るの？</li>
<li>一覧化されてて、あとで眺めたい</li>
<li>IPython Notebookのデモ</li>
<li>R shiny Serverのデモ</li>
<li>Pentaho CE+Saikuのデモはネットワークがダメだったので割愛</li>
<li>分析側の視点を見せたかったので。</li>
</ul>
<h2 id="可視化とは何だったのか-harukasanさん">可視化とは何だったのか （harukasanさん）</h2>
<ul>
<li>インフラ</li>
<li>モニタリング、アナライズ、可視化どれ？</li>
<li>可視化は目的ではないよねと。</li>
<li>熱く、ユーザとサービスプロバイダ間の距離のお話。
<ul>
<li>ログ</li>
<li>生ファイルで残すのが重要</li>
</ul>
</li>
</ul>
<p>トータルで必要とか考えないといけないことがわかって面白かったｗ</p>
<h2 id="あなたの知らないrrdtool-shoichimasuharaさん">あなたの知らないrrdtool （shoichimasuharaさん）</h2>
<ul>
<li>すげー！</li>
<li>いろいろできるらしい（ノートPCの前にいなかったｗ）</li>
</ul>
<h2 id="d3jsとジオマッピング-takaki_さん">D3.jsとジオマッピング （Takaki_さん）</h2>
<ul>
<li>時空間推進課！すげー！</li>
<li>ジオデータのおはなし。</li>
<li>TopoJSONとかGeoJSONとか</li>
</ul>
<h2 id="zipkinについて-synkさん">Zipkinについて （synkさん）</h2>
<ul>
<li>ものすごい数のJVMがThriftで殴りあってるらしい。</li>
<li>ZipkinはTwitterが作ったもの</li>
<li>Brave、HTraceというのが、PureJava用のZipkinトレーシングプラグイン</li>
<li>見積もりする必要がないくらいでかいHBaseがあるので、そこにためてる。</li>
<li>まとめ
<ul>
<li>LINEはエンジニアを大募集中</li>
</ul>
</li>
</ul>
<h2 id="graphiteについて-mickey19821-さん">Graphiteについて (mickey19821 さん)</h2>
<ul>
<li>ドリコムさんのGraphiteのおはなし</li>
<li>Metricsの収集はCollectd</li>
<li>Graphite-webがスケールできなくて困ってるらしい。</li>
<li>Graphiteで可視化ツラいｗ</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p><strong>RRDTool最高</strong></p>
<p>あと、クックパッドの技術力がすごいという発表なんかもありました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>最新インフラエンジニア技術勉強に参加しました。</title>
      <link>https://blog.johtani.info/blog/2014/05/23/attending-drecom-infra-study/</link>
      <pubDate>Fri, 23 May 2014 19:18:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/05/23/attending-drecom-infra-study/</guid>
      <description>今月2回目の目黒で、初のドリコムさんです。 「最新インフラエンジニア技術勉強～Fluentd, Elasticsearch,Chefの実践実例～</description>
      <content:encoded><p>今月2回目の目黒で、初のドリコムさんです。
「<a href="http://drecom-study.doorkeeper.jp/events/11137">最新インフラエンジニア技術勉強～Fluentd, Elasticsearch,Chefの実践実例～</a>」に参加してきました。
もちろん、Elasticsearchってキーワードがあったからです。</p>
<p>ざっくりメモです。</p>
<!-- more -->
<h2 id="ドリコムのinfrastructure-as-codeひらしーさん">ドリコムのInfrastructure as Code/ひらしーさん</h2>
<ul>
<li>CM：jojoss、トレクル、など</li>
<li>サーバ300台、クラウド○○台。月30〜50台の割合で増加中。</li>
<li>少人数でいかに回すか。</li>
</ul>
<h4 id="chef">Chef</h4>
<ul>
<li>Rubyが書ける人が多いから。</li>
</ul>
<h4 id="serverspec">serverspec</h4>
<ul>
<li>テストだよと。</li>
</ul>
<p>すみません、色々と聞き逃しました。。。</p>
<h2 id="winning-the-metrics-battlemickeyさん">Winning the metrics battle/mickeyさん</h2>
<ul>
<li>Graphiteとかを触っている。</li>
<li>1300台超えたら、色々大変だった。</li>
</ul>
<h4 id="失敗談">失敗談</h4>
<ul>
<li>Cactiを利用して、色々と運用が大変だった。DCが別なのでProxyとか。</li>
</ul>
<h4 id="成功例現行システム">成功例？現行システム？</h4>
<ul>
<li>最大値、平均値、最小値などをプロット</li>
<li>collectdを収集、送信に採用して、独自で開発？</li>
<li>受信して保存するのに、Graphite（carbon-relay、carbon-cache、DRBD、graphite-web）ってなってる。</li>
<li>1300台程度のサーバから、5分間隔で、問題ない。</li>
<li>Graphite良いツールだよ。</li>
</ul>
<p>Q：過去データはどのくらい？
A：5分間隔で1年分。</p>
<p>Q：移動平均とかを使ったグラフとか時間かかりませんか？100台だと
A：100台でもほとんど時間はかからない。</p>
<h2 id="fluentd-プラグイン開発講座外山-寛さん">Fluentd プラグイン開発講座/外山 寛さん</h2>
<ul>
<li>Fluentdプラグインを作ることができると威力倍増</li>
<li>Elasticsearchの勉強会の話までしてくれました！</li>
<li>勉強会スペース貸出しています。</li>
<li>未公開だけど、sedueのプラグインもあるらしい。</li>
<li>CHUNKとBUFFERとか覚えときましょう</li>
<li>プラグインの作り方的なのがなかった気がしたので、今回の発表です。</li>
<li>gem作らなくてもディレクトリにおけば使えるよと。</li>
<li>td-agent使ってる人が大多数だよね。（fluentdを素で使ってる人は会場にはいなかった）</li>
<li>エンジニア募集中</li>
</ul>
<p>Q：エラー処理どうしてますか？
A：今は、スルーしています</p>
<p>Q：単体テストの書き方は？
A：人によってバラバラみたいですね。</p>
<h2 id="mysqlと組み合わせて始める全文検索エンジンelasticsearchyoshi_ken">MySQLと組み合わせて始める全文検索エンジン「elasticsearch」/yoshi_ken</h2>
<p>スライド：http://www.slideshare.net/y-ken/introduce-elasticsearch-mysql-importer</p>
<ul>
<li>
<p>Elasticsearch歴は1年位です。</p>
</li>
<li>
<p>MySQLを使っていて、モダンな検索がほしいですよね？ね？</p>
</li>
<li>
<p>サジェスト、ファセット、位置情報、ネスト検索などなど。</p>
</li>
<li>
<p>GoogleトレンドだとSolrに迫る勢いと。</p>
</li>
<li>
<p>実データを用いて、手軽にElasticsearchと連携。</p>
</li>
<li>
<p>BinaryLogではなく、SQLの結果を同意する方式。yamabiko</p>
</li>
<li>
<p>今日は、新しいものを公開します。</p>
<ul>
<li><a href="https://github.com/y-ken/elasticsearch_mysql_importer">bulk import file generator as well as nested document from MySQL for elasticsearch bulk api</a></li>
</ul>
</li>
<li>
<p><a href="http://purchases.elasticsearch.com/class/elasticsearch/core-elasticsearch/tokyo/2014-05-20">東京トレーニング</a></p>
</li>
<li>
<p>Elasticsearch本については、右にあるリンクをクリックしてくれるとうれしいなぁ。</p>
</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>「よくわかるAmazon #CloudSearch 」に行ってきました！</title>
      <link>https://blog.johtani.info/blog/2014/05/15/amazon-cloud-seaarch-study-session/</link>
      <pubDate>Thu, 15 May 2014 17:50:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/05/15/amazon-cloud-seaarch-study-session/</guid>
      <description>どうやら、中身がSolrベース？Luceneベース？になったらしいということで、 今日は「AWSプロダクトシリーズ｜よくわかるAmazon Cl</description>
      <content:encoded><p>どうやら、中身がSolrベース？Luceneベース？になったらしいということで、
今日は<a href="http://kokucheese.com/event/index/168838/">「AWSプロダクトシリーズ｜よくわかるAmazon CloudSearch」</a>に行ってきました。</p>
<p><strong>※ElasticSearchではありません！</strong></p>
<!-- more -->
<p>ということで、いつものメモ。</p>
<h2 id="cloudsearch-overview">CloudSearch Overview</h2>
<h3 id="amazon-web-services-inc-pravin-muthukumarproduct-manager--vivek-sriram-business-development">Amazon Web Services, Inc. Pravin Muthukumar(Product Manager) / Vivek Sriram (Business Development)</h3>
<h4 id="introduction-to-search">Introduction to Search</h4>
<ul>
<li>検索の紹介。アイアンマンのDVD？のページにいろんな項目（フィールド）があるよねと。（もちろん、Amazonのページ）</li>
<li>ファセット、Geo、テキスト処理（Analysis処理）、Postings listとか。とかとか</li>
<li>ランキングも</li>
</ul>
<h4 id="amazon-cloudsearch">Amazon CloudSearch</h4>
<ul>
<li>独自実装orRDB拡張もある。</li>
<li>OSSもあるよね。</li>
<li><strong>Legacy</strong> Enterprise SearchとしてFASTとかもある。</li>
</ul>
<h4 id="building-with-cloudsearch">Building with CloudSearch</h4>
<ul>
<li>他のサービス同様、コンソールとかあるし、色々できるし、すぐ起動できるよと。</li>
</ul>
<p>自動で、データが増えたら、パーティションが増えると。</p>
<ul>
<li>日本語の形態素解析があるらしい。何を使ってるのかな？</li>
<li>ICUのノーマライズとかもやってくれるらしい。これかな？http://lucene.apache.org/core/4_8_0/analyzers-icu/index.html</li>
<li>ユーザが辞書を指定できるのかな？</li>
</ul>
<p>備えてる機能の説明</p>
<ul>
<li>ファセット</li>
<li>SimpleQuery</li>
<li>Autocomplete</li>
<li>Highlight</li>
</ul>
<p>などなど</p>
<ul>
<li>Multi-AZにも対応</li>
</ul>
<h4 id="qa">QA</h4>
<ul>
<li>Q：NGramありますか？
<ul>
<li>A：今はないです。</li>
</ul>
</li>
<li>Q：ユーザ辞書対応してますか？
<ul>
<li>A：今はないです。</li>
</ul>
</li>
<li>Q：lang-detectあるか？
<ul>
<li>A：今はないので、自分で判定して、適切なフィールドに入れてね。</li>
</ul>
</li>
</ul>
<h2 id="expectation-for-cloudsearch">Expectation for CloudSearch</h2>
<h3 id="apache-solr-contributor大須賀稔氏">Apache Solr contributor　大須賀　稔氏</h3>
<ul>
<li>Solr本の宣伝ありがとうございます！（右のアイコンから買ってもらえると更に嬉しいですｗ）</li>
<li>Kinesisとかとの組み合わせとか、自然言語処理とか、いろいろとあるAWSのコンポーネントと組み合わせる例が欲しいと。</li>
<li>すばらしい、最後はManifoldCFがらみに持っていくとは。ACLがらみのクローリングとかあるといいじゃないでしょうかと。</li>
</ul>
<h2 id="impression-of-using-cloudsearch">Impression of using CloudSearch</h2>
<h3 id="吉田匠氏yoshi0309httpblogyoslabcom">吉田　匠氏　(@yoshi0309　http://blog.yoslab.com/)</h3>
<p>スライド：https://speakerdeck.com/yoshi0309/impression-of-using-cloudsearch</p>
<ul>
<li>お見かけしたことある気がするなぁ。</li>
<li>全部置き換えできる！わけではなさそう。。。</li>
</ul>
<h4 id="いいところ">いいところ。</h4>
<ul>
<li>UIがいいし、セットアップが簡単</li>
<li>auto scaleがうれしい</li>
<li>マルチドメイン、マルチスキーマがいい</li>
<li>Luceneのdismaxサポートがいい。（edismaxじゃないのかな？）</li>
</ul>
<p>dismaxって書いてあるな。</p>
<p><a href="http://docs.aws.amazon.com/cloudsearch/latest/developerguide/search-api.html">http://docs.aws.amazon.com/cloudsearch/latest/developerguide/search-api.html</a></p>
<ul>
<li>
<p>フィードの仕方に気をつけて！</p>
<ul>
<li>バッチサイズで課金されるので、1件ずつじゃなくて、複数件送ったほうがいい。</li>
</ul>
</li>
<li>
<p>いきなりスケールアウトできるわけじゃない？</p>
</li>
<li>
<p>ウォームアップ機能がない。インスタンス上限がデフォルト50件</p>
</li>
<li>
<p>VPC対応してほしい。</p>
<ul>
<li>インターネット経由になってしまう</li>
<li>フィードのスピードが</li>
<li>セキュリティグループ機能が使えない</li>
</ul>
</li>
</ul>
<h2 id="cloudsearch-usecase---snapdish">CloudSearch UseCase - SnapDish</h2>
<h3 id="vuzz-inc清田史和氏">Vuzz Inc.　清田　史和氏</h3>
<ul>
<li>独自辞書をもって、Tokenizeは独自でやって、空白区切りでデータ登録している。</li>
<li>インデックス更新はSQSを使ってる。</li>
<li>古いAPIを使ってるらしい。</li>
<li>移行が結構大変らしい。</li>
</ul>
<h2 id="感想">感想</h2>
<p>使ったことないんですが、きめ細かい検索したい場合はちょっとテクニックが要るかもと思いました。
AWS初心者なんで、なんとも言えないんですが。。。</p>
<p>といあえず、テキスト処理（アナライズ処理）で、単語がどうやって区切られるのかってのがわからないのはキツイんじゃなかろうかと。
ただ、簡単に起動できて、オートスケールできるのは素晴らしいと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>外の世界を知るということ</title>
      <link>https://blog.johtani.info/blog/2014/05/14/spiritual-entry1/</link>
      <pubDate>Wed, 14 May 2014 01:44:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/05/14/spiritual-entry1/</guid>
      <description>酔っ払ってなんとなく書きたくなったスピリチュアルなブログなので、流していただければと。 最近、勉強会やTwitterで知り合いになって話をする</description>
      <content:encoded><p>酔っ払ってなんとなく書きたくなったスピリチュアルなブログなので、流していただければと。</p>
<!-- more -->
<p>最近、勉強会やTwitterで知り合いになって話をする人が増えたりで、
その方たちに色々と教えてもらうことが多いなぁと感じてます。
また、知り合いに恵まれてきたなぁとも。</p>
<p>そもそも私が勉強会に参加し始めたきっかけはSolr勉強会（第1回から参加してて、気づいたら主催者になってた）で、
そのころは、Solr初心者で色々とやってる人、知ってる人がいて無料で話が聞けるのすごいと興奮したのを覚えてます。
（もちろん、ぼっちでしたが）</p>
<p>Solr勉強会が定期的に開かれて、それに参加しているうちに少しずつ知り合いもできて、
面白そうな勉強会（Hadoopとか）が他にもあるんだ、参加してみようかって思うようになってと。</p>
<p>そこにプラスで、関口さんと知りあえてSolr本を書く話が出てきて、本を書いてみたら、
今度は勉強会で話してみませんかとなって。</p>
<p>スピーカーやると、顔を覚えてもらえるようで、また知り合いが増えてと。</p>
<p>人のつながりって大事だし、自分が発信することで教えてもらえることもあるしと。
前にも書いた気がするけど、Solr勉強会（当時の主催の方）や関口さんやElasticsearch勉強会のスタッフの方や
スピーカーやってくれた方、会場提供を心よくしてくれてるVOYAGE GROUP、リクルートテクノロジーズさん、という具合にどんどん頭が上がらなくなってきてるなぁと。</p>
<p>ということで、外の世界を知ると色々とつながりが出来て面白いですよという、個人的な感想でした。
ま、自分が思ってるだけで、違う意見もいっぱいあると思うけど。</p>
<p>あー、なんか、酔って勢いで書いてしまった。。。
脈略のない文章なきがするけど、エイヤで公開しちゃおう。</p>
</content:encoded>
    </item>
    
    <item>
      <title>アジャイルデータサイエンスが届きました</title>
      <link>https://blog.johtani.info/blog/2014/05/09/reading-agile-data-science/</link>
      <pubDate>Fri, 09 May 2014 00:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/05/09/reading-agile-data-science/</guid>
      <description>「アジャイルデータサイエンス」の見本をいただいてしまいました。 なので、感想文でも書いてみようかと。 どんな本？ 想像以上に薄かったです、物理的に</description>
      <content:encoded><p>「アジャイルデータサイエンス」の見本をいただいてしまいました。</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=4873116716&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<p>なので、感想文でも書いてみようかと。</p>
<!-- more -->
<h2 id="どんな本">どんな本？</h2>
<p>想像以上に薄かったです、物理的に。
なのに、とても幅広い範囲をカバーしています。</p>
<p>データの分析にどんな人が関わっているのか、アジャイルにデータを分析する目的から始まり、
データから何かを見つけるための手順、考え方などをさらっとですが、
システムに取り込む流れから取り込んだあとに改良するという流れまで紹介してくれます。
流れはこんな感じ。</p>
<ol>
<li>イベント：ログとかのイベントの発生</li>
<li>コレクタ：イベントの収集</li>
<li>バルクストレージ：イベントの一時保存</li>
<li>バッチ処理：ストレージにあるデータに対して処理</li>
<li>分散ストア：処理結果をWebアプリなどで表示するために保存</li>
<li>Webアプリ：処理結果をユーザに提供</li>
</ol>
<p>Webアプリでデータを見えるようにして、そこから更にフィードバックを受けることで、データの分析などを進めていくという流れです。</p>
<p>私はデータサイエンス系の人ではないのでこの流れで作業をされているかはわからないのですが、
検索のシステムについてもこの流れに似ているなと感じました。</p>
<p>検索システムでも、検索したいデータを収集して、加工（検索したい項目の洗い出しやファセットにする項目の選定とか）して、
インデックスを生成するといった処理が必要になるからです。</p>
<p>この本では、Python、Hadoop（Pig）、MongoDB、ElasticSearch（バージョンが0.90だから）、d3.jsなどが出てきます。
また、日本語版の付録では、Fluentd、Kibana+Elasticsearchといった組み合わせの紹介もあります。</p>
<p>コードも書かれていますが、すみません、そこまでちゃんと読んでません。。。</p>
<p>後半では、データを活用して行くためには順序があるという話が書かれています。</p>
<ol>
<li>レコード：レコード1件のアトミックな処理と表示</li>
<li>グラフ：レコードを元に集計したりグラフ作ったり</li>
<li>レポート：関係性やトレンドを抽出し、インタラクティブに探求</li>
<li>予測：構造を利用して、推論やレコメンドとか</li>
<li>行動：上記の結果を元にユーザに行動を促す</li>
</ol>
<p>前のステップがおろそかだと次のステップがうまくいかないという話です。
後半はこの流れに沿って、先ほどのシステムの流れを変更していく方法が展開されます。</p>
<h2 id="注意点">注意点</h2>
<p>ちょっとだけ気になる点があったので。</p>
<p>Elasticsearch周りについては少し注意が必要かと。（Pigとかは詳しくないので不明です。）
紹介されているElasticsearchのバージョンが0.90と少し古いのと、
Hadoopとの統合（Pigで処理したデータをElasticsearchに出力）に利用されているWondordogというツールも最近更新がされていないみたいです。
おそらく、1.0系では動かないのではないかと。</p>
<p>1.0系の場合は、Elasticsearchが提供している<a href="http://www.elasticsearch.org/overview/hadoop">elasticsearch-hadoop</a>を利用するのが良いと思います。</p>
<p>付録については1.0系で記載されているので問題ないかと。ただし、Elasticsearchは更新が早いので、公式サイトで最新情報を入手するのが良いと思います。</p>
<h2 id="感想">感想</h2>
<p>データを解析するシステムってどんなことやってるの？という全体像を
俯瞰するのに良い本なのではないかと思います。
また、データの解析を活かすために、必要な順序と疎かにできない点もあって面白かったです。</p>
<p>紹介されているツール類については、OSSのトレンドや開発速度が早いので、参考程度にとどめておいて、
自分たちで使いやすいものを探してきて検討するのがいいと思います。</p>
<p>付録がとても豪華です。FluentdやKibanaがわかりやすく解説されてます。</p>
<p>あと、薄いのでさらっと読めます。ｗ</p>
</content:encoded>
    </item>
    
    <item>
      <title>Aggregations - ファセットよりも柔軟な集計</title>
      <link>https://blog.johtani.info/blog/2014/05/07/aggregation-example/</link>
      <pubDate>Wed, 07 May 2014 18:57:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/05/07/aggregation-example/</guid>
      <description>こんなツイートを見つけたので、Aggregationのサンプルでも書こうかなと。（前から書こうと思ってたんですが。。。） @elasticsearch Hi, Would you please tell me the way to</description>
      <content:encoded><p>こんなツイートを見つけたので、Aggregationのサンプルでも書こうかなと。（前から書こうと思ってたんですが。。。）</p>
<blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/elasticsearch">@elasticsearch</a> Hi, Would you please tell me the way to do &quot;Pivot Faceting&quot; like Solr-4.0 in elasticsearch-1.1.1 or prior version? Thank you.</p>&mdash; Y.Kentaro (@yoshi_ken) <a href="https://twitter.com/yoshi_ken/statuses/462073860062322688">2014, 5月 2</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>ちなみに、Aggregationは1.0.0から導入された機能なので、ElasticSearch Server日本語版には掲載されていない機能になります。（ごめんなさい）</p>
<!-- more -->
<p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html">公式ガイドのAggregationsのページ</a>はこちらになりますが、実例があったほうがいいかなと。</p>
<p><a href="http://twitter.com/yoshi_ken">@yoshi_ken</a> さんから実例のサンプルの指定もいただいたので、ブログを書くのが非常に楽です。ありがとうございます。</p>
<h2 id="問題">問題</h2>
<p><a href="https://gist.github.com/y-ken/40d99c3a137247ba8eac">元ネタ（gist）</a></p>
<p>次のような不動産系のデータがあるとします。</p>
<ul>
<li>id</li>
<li>物件名</li>
<li>都道府県（東京、神奈川、&hellip;..）</li>
<li>物件種別（賃貸、売買、&hellip;..）</li>
</ul>
<p>この時、都道府県別に、物件種別ごとの件数を取得したいという趣旨です。</p>
<ul>
<li>東京
<ul>
<li>賃貸: xxx件</li>
<li>売買: yyy件</li>
</ul>
</li>
<li>神奈川
<ul>
<li>賃貸: xxx件</li>
<li>売買: yyy件 &hellip;</li>
</ul>
</li>
</ul>
<p>これを、Elasticsearchでどうやって取得するかという問題です。</p>
<h2 id="インデックスとデータの登録">インデックスとデータの登録</h2>
<p>まずは、インデックスを作ります。
あくまでもサンプルなので、全部not_analyzedにしてますが、そのへんは適宜変更してください。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">#</span> <span style="color:#960050;background-color:#1e0010">create</span> <span style="color:#960050;background-color:#1e0010">index</span>
<span style="color:#960050;background-color:#1e0010">PUT</span> <span style="color:#960050;background-color:#1e0010">/pref_aggs</span>
{
  <span style="color:#f92672">&#34;settings&#34;</span>: {
    <span style="color:#f92672">&#34;number_of_shards&#34;</span>: <span style="color:#ae81ff">2</span>
  },
  <span style="color:#f92672">&#34;mappings&#34;</span>: {
    <span style="color:#f92672">&#34;japan&#34;</span> : {
      <span style="color:#f92672">&#34;_id&#34;</span> : {
        <span style="color:#f92672">&#34;path&#34;</span> : <span style="color:#e6db74">&#34;id&#34;</span>
      },
      <span style="color:#f92672">&#34;properties&#34;</span>: {
        <span style="color:#f92672">&#34;id&#34;</span>: {<span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>, <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>},
        <span style="color:#f92672">&#34;name&#34;</span>: {<span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>, <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>},
        <span style="color:#f92672">&#34;pref&#34;</span>: {<span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>, <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>},
        <span style="color:#f92672">&#34;type&#34;</span>: {<span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>, <span style="color:#f92672">&#34;index&#34;</span>: <span style="color:#e6db74">&#34;not_analyzed&#34;</span>}
      }
    }
  }
}
</code></pre></div><p><code>_id</code>を使用して、データ登録時に<code>id</code>フィールドにある文字列をそのままIDとして登録できるように指定してあります。</p>
<p>登録するデータは次のようなものを適当に100件程度作ってりました。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{<span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;id0&#34;</span>, <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;name0&#34;</span>, <span style="color:#f92672">&#34;pref&#34;</span>: <span style="color:#e6db74">&#34;01_北海道&#34;</span>, <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;売買&#34;</span>}
{<span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;id1&#34;</span>, <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;name1&#34;</span>, <span style="color:#f92672">&#34;pref&#34;</span>: <span style="color:#e6db74">&#34;09_栃木県&#34;</span>, <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;売買&#34;</span>}
{<span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;id2&#34;</span>, <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;name2&#34;</span>, <span style="color:#f92672">&#34;pref&#34;</span>: <span style="color:#e6db74">&#34;38_愛媛県&#34;</span>, <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;賃貸&#34;</span>}
{<span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;id3&#34;</span>, <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;name3&#34;</span>, <span style="color:#f92672">&#34;pref&#34;</span>: <span style="color:#e6db74">&#34;40_福岡県&#34;</span>, <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;賃貸&#34;</span>}
{<span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;id4&#34;</span>, <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;name4&#34;</span>, <span style="color:#f92672">&#34;pref&#34;</span>: <span style="color:#e6db74">&#34;35_山口県&#34;</span>, <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;売買&#34;</span>}
{<span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;id5&#34;</span>, <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;name5&#34;</span>, <span style="color:#f92672">&#34;pref&#34;</span>: <span style="color:#e6db74">&#34;12_千葉県&#34;</span>, <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;賃貸&#34;</span>}
<span style="color:#960050;background-color:#1e0010">...</span>
</code></pre></div><p>データの登録には、前に紹介した方法「<a href="http://blog.johtani.info/blog/2014/04/24/usage-stream2es/">stream2esと複数データの登録</a>」を用いました。</p>
<h2 id="ファセット">ファセット</h2>
<p>このようなデータがある場合に、まず思いつくのはファセットによる取得です。
いささか強引ですが。。。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/pref_aggs/japan/_search</span>
{
  <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#f92672">&#34;query&#34;</span>: {
    <span style="color:#f92672">&#34;match_all&#34;</span>: {}
  },
  <span style="color:#f92672">&#34;facets&#34;</span>: {
    <span style="color:#f92672">&#34;type_賃貸&#34;</span>: {
      <span style="color:#f92672">&#34;terms&#34;</span>: {
        <span style="color:#f92672">&#34;order&#34;</span>: <span style="color:#e6db74">&#34;term&#34;</span>,
        <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;pref&#34;</span>,
        <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">50</span>
      }, <span style="color:#f92672">&#34;facet_filter&#34;</span>: {<span style="color:#f92672">&#34;term&#34;</span>: {<span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;賃貸&#34;</span> }}
    },
    <span style="color:#f92672">&#34;type_売買&#34;</span>: {
      <span style="color:#f92672">&#34;terms&#34;</span>: {
        <span style="color:#f92672">&#34;order&#34;</span>: <span style="color:#e6db74">&#34;term&#34;</span>,
        <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;pref&#34;</span>,
        <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">50</span>
      }, <span style="color:#f92672">&#34;facet_filter&#34;</span>: {<span style="color:#f92672">&#34;term&#34;</span>: {<span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;売買&#34;</span> }}
    }

  }
}
</code></pre></div><p><code>facet_filter</code>を使用して、<code>type</code>フィールドによる個別の絞込を行っています。
あとは、<code>pref</code>フィールドのファセットを取得すれば、出力は次のようになります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;took&#34;</span>: <span style="color:#ae81ff">6</span>,
   <span style="color:#f92672">&#34;timed_out&#34;</span>: <span style="color:#66d9ef">false</span>,
   <span style="color:#f92672">&#34;_shards&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;successful&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;failed&#34;</span>: <span style="color:#ae81ff">0</span>
   },
   <span style="color:#f92672">&#34;hits&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">100</span>,
      <span style="color:#f92672">&#34;max_score&#34;</span>: <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;hits&#34;</span>: []
   },
   <span style="color:#f92672">&#34;facets&#34;</span>: {
      <span style="color:#f92672">&#34;type_賃貸&#34;</span>: {
         <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;terms&#34;</span>,
         <span style="color:#f92672">&#34;missing&#34;</span>: <span style="color:#ae81ff">0</span>,
         <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">52</span>,
         <span style="color:#f92672">&#34;other&#34;</span>: <span style="color:#ae81ff">0</span>,
         <span style="color:#f92672">&#34;terms&#34;</span>: [
            {
               <span style="color:#f92672">&#34;term&#34;</span>: <span style="color:#e6db74">&#34;00_北海道&#34;</span>,
               <span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">1</span>
            },
            {
               <span style="color:#f92672">&#34;term&#34;</span>: <span style="color:#e6db74">&#34;01_青森県&#34;</span>,
               <span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">2</span>
            },
            {
               <span style="color:#f92672">&#34;term&#34;</span>: <span style="color:#e6db74">&#34;03_宮城県&#34;</span>,
               <span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">3</span>
            },
            <span style="color:#960050;background-color:#1e0010">...</span>
      <span style="color:#960050;background-color:#1e0010">}</span>,
      <span style="color:#e6db74">&#34;type_売買&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {
         <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;terms&#34;</span>,
         <span style="color:#f92672">&#34;missing&#34;</span>: <span style="color:#ae81ff">0</span>,
         <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">48</span>,
         <span style="color:#f92672">&#34;other&#34;</span>: <span style="color:#ae81ff">0</span>,
         <span style="color:#f92672">&#34;terms&#34;</span>: [
            {
               <span style="color:#f92672">&#34;term&#34;</span>: <span style="color:#e6db74">&#34;00_北海道&#34;</span>,
               <span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">2</span>
            },
            {
               <span style="color:#f92672">&#34;term&#34;</span>: <span style="color:#e6db74">&#34;02_岩手県&#34;</span>,
               <span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">1</span>
            },
            {
               <span style="color:#f92672">&#34;term&#34;</span>: <span style="color:#e6db74">&#34;04_秋田県&#34;</span>,
               <span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">1</span>
            },
            <span style="color:#960050;background-color:#1e0010">...</span>
<span style="color:#960050;background-color:#1e0010">}</span>
</code></pre></div><p>望んでいた形式とは少し異なりますが、<code>facet_filter</code>する回数を少なくするため、
ファセットは都道府県のフィールドを指定したためです。
アプリで頑張って入れ替えてください。。。</p>
<p>この場合、&lsquo;type'の個数がわかっているので、頑張ってこのような記述ができました。
ただ、<code>type</code>が増えた時にアプリの修正とかが必要になりますよね。</p>
<h2 id="aggregations">Aggregations</h2>
<p>ということで、Aggregationsの出番です。
ファセットよりも柔軟に、検索結果に対していろいろな集計が行える機能になります。
一見に如かずということで、クエリを紹介します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/pref_aggs/japan/_search</span>
{
  <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#f92672">&#34;query&#34;</span>: {
    <span style="color:#f92672">&#34;match_all&#34;</span>: {}
  },
  <span style="color:#f92672">&#34;aggs&#34;</span>: {
    <span style="color:#f92672">&#34;pref&#34;</span>: {
      <span style="color:#f92672">&#34;terms&#34;</span>: {
        <span style="color:#f92672">&#34;order&#34;</span>: {
          <span style="color:#f92672">&#34;_term&#34;</span>: <span style="color:#e6db74">&#34;asc&#34;</span>
        },
        <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;pref&#34;</span>,
        <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">50</span>
      },
      <span style="color:#f92672">&#34;aggs&#34;</span>: {
        <span style="color:#f92672">&#34;type&#34;</span>: {
          <span style="color:#f92672">&#34;terms&#34;</span>: {
            <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;type&#34;</span>,
            <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">10</span>
          }
        }
      }
    }
  }
}
</code></pre></div><p>ファセットよりもシンプルですし、<code>賃貸</code>といったような値を指定していません。
<code>aggs</code>というのが<code>aggregations</code>機能を指定している部分になります。
検索結果は次のように出力されます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;took&#34;</span>: <span style="color:#ae81ff">4</span>,
   <span style="color:#f92672">&#34;timed_out&#34;</span>: <span style="color:#66d9ef">false</span>,
   <span style="color:#f92672">&#34;_shards&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;successful&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;failed&#34;</span>: <span style="color:#ae81ff">0</span>
   },
   <span style="color:#f92672">&#34;hits&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">100</span>,
      <span style="color:#f92672">&#34;max_score&#34;</span>: <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;hits&#34;</span>: []
   },
   <span style="color:#f92672">&#34;aggregations&#34;</span>: {
      <span style="color:#f92672">&#34;pref&#34;</span>: {
         <span style="color:#f92672">&#34;buckets&#34;</span>: [
            {
               <span style="color:#f92672">&#34;key&#34;</span>: <span style="color:#e6db74">&#34;00_北海道&#34;</span>,
               <span style="color:#f92672">&#34;doc_count&#34;</span>: <span style="color:#ae81ff">3</span>,
               <span style="color:#f92672">&#34;type&#34;</span>: {
                  <span style="color:#f92672">&#34;buckets&#34;</span>: [
                     {
                        <span style="color:#f92672">&#34;key&#34;</span>: <span style="color:#e6db74">&#34;売買&#34;</span>,
                        <span style="color:#f92672">&#34;doc_count&#34;</span>: <span style="color:#ae81ff">2</span>
                     },
                     {
                        <span style="color:#f92672">&#34;key&#34;</span>: <span style="color:#e6db74">&#34;賃貸&#34;</span>,
                        <span style="color:#f92672">&#34;doc_count&#34;</span>: <span style="color:#ae81ff">1</span>
                     }
                  ]
               }
            },
            {
               <span style="color:#f92672">&#34;key&#34;</span>: <span style="color:#e6db74">&#34;01_青森県&#34;</span>,
               <span style="color:#f92672">&#34;doc_count&#34;</span>: <span style="color:#ae81ff">2</span>,
               <span style="color:#f92672">&#34;type&#34;</span>: {
                  <span style="color:#f92672">&#34;buckets&#34;</span>: [
                     {
                        <span style="color:#f92672">&#34;key&#34;</span>: <span style="color:#e6db74">&#34;賃貸&#34;</span>,
                        <span style="color:#f92672">&#34;doc_count&#34;</span>: <span style="color:#ae81ff">2</span>
                     }
                  ]
               }
            },
            <span style="color:#960050;background-color:#1e0010">...</span>
</code></pre></div><p>Aggregationsの結果は、望んでいた通りの出力になっています。</p>
<p>クエリの構成を見てみましょう。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#e6db74">&#34;aggs&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> {
  <span style="color:#f92672">&#34;pref&#34;</span>: { <span style="color:#960050;background-color:#1e0010">#1</span>
    <span style="color:#f92672">&#34;terms&#34;</span>: {
      <span style="color:#f92672">&#34;order&#34;</span>: {
        <span style="color:#f92672">&#34;_term&#34;</span>: <span style="color:#e6db74">&#34;asc&#34;</span>
      },
      <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;pref&#34;</span>,
      <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">50</span>
    },
    <span style="color:#f92672">&#34;aggs&#34;</span>: {  <span style="color:#960050;background-color:#1e0010">#2</span>
      <span style="color:#f92672">&#34;type&#34;</span>: {
        <span style="color:#f92672">&#34;terms&#34;</span>: {
          <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;type&#34;</span>,
          <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">10</span>
        }
      }
    }
  }
}
</code></pre></div><p>最初の#1の<code>pref</code>は出力を扱いやすくするためにつけているラベルになります。好きな名前をつけることが可能です。
次の<code>terms</code>がAggregationのタイプ（どのような集計をして欲しいか）になります。
今回は、<code>pref</code>フィールドにある単語(term)毎に、集計をしたいので、<code>terms</code>を指定します。
その他にどんなタイプがあるかは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html">公式ガイド</a>をご覧ください。</p>
<p>次に、さらに<code>type</code>フィールドで集計したいので、#2の部分で後続のAggregationを指定しています。
都道府県同様、<code>type</code>フィールドにある単語毎に集計するために、<code>terms</code>を指定します。</p>
<p>これで、先ほどのような結果が出力できます。
ちなみに、さらに<code>type</code>の中に他の種別で集計したいという場合は、さらに<code>aggs</code>を追加していけばOKです。</p>
<p>Aggregationは非常に柔軟な集計を可能にする機能です。ただし、検索結果に対して集計処理を行っているため、
メモリやCPUなどのリソースを消費するので注意が必要です。</p>
<p>Aggregationの説明については、<a href="https://www.found.no/foundation/elasticsearch-aggregations/">こちらのFound.noのブログ（英語）</a>がわかりやすかったので参考にしてみてください。</p>
<h2 id="まとめ">まとめ</h2>
<p>非常に簡単ですが、Aggregationsについて紹介しました。
その他にもAggregationsでできることがあるので、後日別のサンプルを用意して説明しようかと思います。</p>
<p>100件のデータやここまでの操作については、<a href="https://gist.github.com/johtani/08dee5fb4da62037ef9e">gist</a>にあるので、興味がある方はご覧いただければと。
stream2esの操作以外は、<a href="http://blog.johtani.info/blog/2014/01/29/simple-introduction-and-first-impression-es-marvel/">Marvelに付属のsense</a>を利用しています。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-kopfの紹介（aliases画面）</title>
      <link>https://blog.johtani.info/blog/2014/05/04/intro-elasticsearch-kopf-alias-percolator/</link>
      <pubDate>Sun, 04 May 2014 01:01:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/05/04/intro-elasticsearch-kopf-alias-percolator/</guid>
      <description>今日はelasticsearch-kopfのAnalysis画面の紹介です。 （簡単なところから。。。その３） ちょっとあいだが開いてしまいまし</description>
      <content:encoded><p>今日はelasticsearch-kopfのAnalysis画面の紹介です。</p>
<p>（簡単なところから。。。その３）</p>
<!-- more -->
<p>ちょっとあいだが開いてしまいましたが、再開です。
メニューの<code>aliases</code>を選択すると、次のような画面が表示されます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140504/kopf-aliases.jpg" />
    </div>
    <a href="/images/entries/20140504/kopf-aliases.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>Aliases画面</h4>
      </figcaption>
  </figure>
</div>

<p>Elasticsearchの<code>alias</code>を画面で確認できます。</p>
<p>エイリアスは、インデックスに別名をつけることができるElasticsearchの機能です。
１エイリアス＝１インデックスでも良いですが、１エイリアスに対して複数のエイリアスを付与することもできます。
この機能を利用することで、次のようなことが可能となります。</p>
<ul>
<li>インデックスの切り替えをアプリ側に意識させずに実施（アプリはエイリアス名に対して検索すればOKなので）</li>
<li>直近１週間のログを検索するためのエイリアスの作成（複数のインデックスを１つのエイリアスに割り当て可能）</li>
<li>特定のルーティングによる検索（特定のデータに対する検索だけに絞るためにfilterを指定する）</li>
</ul>
<p>エイリアスについて詳しく知りたい方は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html">公式ガイド</a>をご覧いただくのが良いかと。</p>
<p>画面は非常にわかりやすい作りになっているので、特に説明必要ないんですよね。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>stream2esと複数データの登録</title>
      <link>https://blog.johtani.info/blog/2014/04/24/usage-stream2es/</link>
      <pubDate>Thu, 24 Apr 2014 21:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/04/24/usage-stream2es/</guid>
      <description>kopfの記事の続きも書く必要があるんだけど、こんなツイートを見つけてしまったので。。。 ElasticsearchのBulk APIの仕様、J</description>
      <content:encoded><p>kopfの記事の続きも書く必要があるんだけど、こんなツイートを見つけてしまったので。。。</p>
<blockquote class="twitter-tweet" lang="ja"><p>ElasticsearchのBulk APIの仕様、JSONファイルをいい感じに加工して置かなければならないしハマりどころ多い。 <a href="http://t.co/hmfycqZlqk">http://t.co/hmfycqZlqk</a></p>&mdash; Kenta Suzuki (@suzu_v) <a href="https://twitter.com/suzu_v/statuses/459216999592124416">2014, 4月 24</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>前に思いついたけど、放ったらかしにしてた疑問が再浮上してきたので、せっかくだから調べてみようかなと。</p>
<!-- more -->
<p>複数JSONデータがある場合にもっと楽にデータを入れる方法ないかなぁと思って、これかな？というのがあったのですが、
そのまま手を動かさずに放置してたので、一念発起してブログ書いてます。</p>
<h2 id="bulk-apiって">Bulk APIって？</h2>
<p>ElasticsearchはURLにアクセスしてデータを登録できます。
基本的には次のように1件毎の登録になります。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">$</span> <span style="color:#960050;background-color:#1e0010">curl</span> <span style="color:#960050;background-color:#1e0010">-XPUT</span> <span style="color:#960050;background-color:#1e0010">http://localhost:</span><span style="color:#ae81ff">9200</span><span style="color:#960050;background-color:#1e0010">/bookshop/books/</span><span style="color:#ae81ff">1</span> <span style="color:#960050;background-color:#1e0010">-d</span>
<span style="color:#960050;background-color:#1e0010">&#39;</span>
{
  <span style="color:#f92672">&#34;book_id&#34;</span> : <span style="color:#ae81ff">1</span>,
  <span style="color:#f92672">&#34;title&#34;</span> : <span style="color:#e6db74">&#34;ElasticSearch Server Japanese Edition&#34;</span>,
  <span style="color:#f92672">&#34;price&#34;</span> : <span style="color:#ae81ff">3024</span>,
  <span style="color:#f92672">&#34;publisher&#34;</span> : <span style="color:#e6db74">&#34;KADOKAWA&#34;</span>
}<span style="color:#960050;background-color:#1e0010">&#39;</span>
</code></pre></div><p>これでもいいのですが、大量のデータを登録するときは、Elasticsearch側での効率が悪いです。
そこで、Elasticsearchは大量データを登録するために<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html#docs-bulk">Bulk API</a>というものを用意しています。</p>
<p>これは、次のような形式のJSONを作ってデータを登録します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{ <span style="color:#f92672">&#34;index&#34;</span> : { <span style="color:#f92672">&#34;_index&#34;</span> : <span style="color:#e6db74">&#34;bookshop&#34;</span>, <span style="color:#f92672">&#34;_type&#34;</span> : <span style="color:#e6db74">&#34;books&#34;</span>, <span style="color:#f92672">&#34;_id&#34;</span> : <span style="color:#e6db74">&#34;1&#34;</span> } }
{ <span style="color:#f92672">&#34;book_id&#34;</span> : <span style="color:#ae81ff">1</span>, <span style="color:#f92672">&#34;title&#34;</span> : <span style="color:#e6db74">&#34;ElasticSearch Server Japanese Edition&#34;</span>, <span style="color:#f92672">&#34;price&#34;</span> : <span style="color:#ae81ff">3024</span>, <span style="color:#f92672">&#34;publisher&#34;</span> : <span style="color:#e6db74">&#34;KADOKAWA&#34;</span>}
{ <span style="color:#f92672">&#34;index&#34;</span> : { <span style="color:#f92672">&#34;_index&#34;</span> : <span style="color:#e6db74">&#34;bookshop&#34;</span>, <span style="color:#f92672">&#34;_type&#34;</span> : <span style="color:#e6db74">&#34;books&#34;</span>, <span style="color:#f92672">&#34;_id&#34;</span> : <span style="color:#e6db74">&#34;2&#34;</span> } }
{ <span style="color:#f92672">&#34;book_id&#34;</span> : <span style="color:#ae81ff">2</span>, <span style="color:#f92672">&#34;title&#34;</span> : <span style="color:#e6db74">&#34;Introduction of Apache Solr&#34;</span>, <span style="color:#f92672">&#34;price&#34;</span> : <span style="color:#ae81ff">3888</span>, <span style="color:#f92672">&#34;publisher&#34;</span> : <span style="color:#e6db74">&#34;gihyo&#34;</span>}
</code></pre></div><p>これは、次のような構成になっています。</p>
<pre><code>コマンド
データ
コマンド
データ
...
</code></pre><p>これで効率よくデータが登録できるのですが、このようなJSONデータを別途作って上げる必要が出てきます。
結局、複数のJSONがあるのに、特殊なJSONを生成しないといけないということでプログラム書いて実行することになります。
これだと、Elasticsearchへのアクセスをプログラムで書くのとあまり大差がないかもしれません。</p>
<h2 id="stream2es">stream2es</h2>
<p>もっとお手軽に複数のJSONを登録できないかな？と目をつけていたのが、<a href="https://github.com/elasticsearch/stream2es">stream2es</a>です。</p>
<h3 id="どんなもの">どんなもの？</h3>
<p>Clojureで作られた、Elasticsearchにデータを流し込むためのツールです。
Java 7がインストールされていれば、ダウンロードしてくれば動作せることができます。</p>
<h3 id="インストール">インストール</h3>
<p>公式ページに載っている方法そのままです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ curl -O download.elasticsearch.org/stream2es/stream2es; chmod +x stream2es
</code></pre></div><p>実行したディレクトリにコマンドがコピーされます。
あとは、コマンドを実行すればOKです。</p>
<h3 id="実行">実行</h3>
<p>データは次のような形式で<code>sample.json</code>に保存してあるとします。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{ <span style="color:#f92672">&#34;book_id&#34;</span> : <span style="color:#ae81ff">1</span>, <span style="color:#f92672">&#34;title&#34;</span> : <span style="color:#e6db74">&#34;ElasticSearch Server Japanese Edition&#34;</span>, <span style="color:#f92672">&#34;price&#34;</span> : <span style="color:#ae81ff">3024</span>, <span style="color:#f92672">&#34;publisher&#34;</span> : <span style="color:#e6db74">&#34;KADOKAWA&#34;</span>}
{ <span style="color:#f92672">&#34;book_id&#34;</span> : <span style="color:#ae81ff">2</span>, <span style="color:#f92672">&#34;title&#34;</span> : <span style="color:#e6db74">&#34;Introduction of Apache Solr&#34;</span>, <span style="color:#f92672">&#34;price&#34;</span> : <span style="color:#ae81ff">3888</span>, <span style="color:#f92672">&#34;publisher&#34;</span> : <span style="color:#e6db74">&#34;gihyo&#34;</span>}
</code></pre></div><p>先ほどの<code>Bulk API</code>で利用したJSONよりも、スッキリしていますね。
1行1ドキュメント1JSONです。</p>
<p>あとは、次のコマンドを実行するだけです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ./stream2es stdin --target http://localhost:9200/bookshop/books &lt; sample.json
</code></pre></div><p>ファイルをstream2esに流し込んで、stream2esが1行ずつパースして、Elasticsearchに投げ込んでくれます。</p>
<p>登録されたデータは次のようになります。
IDは自動で付与されています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;bookstore&#34;</span>,
   <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;books&#34;</span>,
   <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;0Hvy4IJCRkKrvGb4Dgam_w&#34;</span>,
   <span style="color:#f92672">&#34;_version&#34;</span>: <span style="color:#ae81ff">1</span>,
   <span style="color:#f92672">&#34;found&#34;</span>: <span style="color:#66d9ef">true</span>,
   <span style="color:#f92672">&#34;_source&#34;</span>: {
      <span style="color:#f92672">&#34;book_id&#34;</span>: <span style="color:#ae81ff">1</span>,
      <span style="color:#f92672">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;ElasticSearch Server Japanese Edition&#34;</span>,
      <span style="color:#f92672">&#34;price&#34;</span>: <span style="color:#ae81ff">3024</span>,
      <span style="color:#f92672">&#34;publisher&#34;</span>: <span style="color:#e6db74">&#34;KADOKAWA&#34;</span>
   }
}<span style="color:#960050;background-color:#1e0010">,</span>
{
   <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;bookstore&#34;</span>,
   <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;books&#34;</span>,
   <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;b9M6TooFQzGYyJeix_t_WA&#34;</span>,
   <span style="color:#f92672">&#34;_version&#34;</span>: <span style="color:#ae81ff">1</span>,
   <span style="color:#f92672">&#34;found&#34;</span>: <span style="color:#66d9ef">true</span>,
   <span style="color:#f92672">&#34;_source&#34;</span>: {
      <span style="color:#f92672">&#34;book_id&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;Introduction of Apache Solr&#34;</span>,
      <span style="color:#f92672">&#34;price&#34;</span>: <span style="color:#ae81ff">3888</span>,
      <span style="color:#f92672">&#34;publisher&#34;</span>: <span style="color:#e6db74">&#34;gihyo&#34;</span>
   }
}
</code></pre></div><p>せっかく、<code>book_id</code>があるんだし、<code>_id</code>をインデックスの設定に指定します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ curl -XDELETE http://localhost:9200/bookshop
$ curl -XPUT http://localhost:9200/bookshop -d <span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">  &#34;mappings&#34;: {
</span><span style="color:#e6db74">    &#34;books&#34; : {
</span><span style="color:#e6db74">      &#34;_id&#34; : {
</span><span style="color:#e6db74">        &#34;path&#34;: &#34;book_id&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  }
</span><span style="color:#e6db74">}&#39;</span>
</code></pre></div><p>あとは、登録すれば<code>book_id</code>が<code>_id</code>に採用されます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;bookshop&#34;</span>,
   <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;books&#34;</span>,
   <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
   <span style="color:#f92672">&#34;_version&#34;</span>: <span style="color:#ae81ff">1</span>,
   <span style="color:#f92672">&#34;found&#34;</span>: <span style="color:#66d9ef">true</span>,
   <span style="color:#f92672">&#34;_source&#34;</span>: {
      <span style="color:#f92672">&#34;book_id&#34;</span>: <span style="color:#ae81ff">1</span>,
      <span style="color:#f92672">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;ElasticSearch Server Japanese Edition&#34;</span>,
      <span style="color:#f92672">&#34;price&#34;</span>: <span style="color:#ae81ff">3024</span>,
      <span style="color:#f92672">&#34;publisher&#34;</span>: <span style="color:#e6db74">&#34;KADOKAWA&#34;</span>
   }
}<span style="color:#960050;background-color:#1e0010">,</span>
{
   <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;bookshop&#34;</span>,
   <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;books&#34;</span>,
   <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;2&#34;</span>,
   <span style="color:#f92672">&#34;_version&#34;</span>: <span style="color:#ae81ff">1</span>,
   <span style="color:#f92672">&#34;found&#34;</span>: <span style="color:#66d9ef">true</span>,
   <span style="color:#f92672">&#34;_source&#34;</span>: {
      <span style="color:#f92672">&#34;book_id&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;Introduction of Apache Solr&#34;</span>,
      <span style="color:#f92672">&#34;price&#34;</span>: <span style="color:#ae81ff">3888</span>,
      <span style="color:#f92672">&#34;publisher&#34;</span>: <span style="color:#e6db74">&#34;gihyo&#34;</span>
   }
}
</code></pre></div><h3 id="複数ファイル">複数ファイル</h3>
<p>ディレクトリに複数のJSONファイルが有った場合は、次のようなコマンドでOK</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cat sample_data/*.json |./stream2es stdin --target http://localhost:9200/bookshop/books
</code></pre></div><p>まぁ、<code>cat</code>して流してるだけですが。。。</p>
<h3 id="ダメだったケース">ダメだったケース</h3>
<ul>
<li>
<p>JSONが複数行になっているようなデータだとエラーが出てしまいました。<br>
（<code>jq</code>コマンドで1行に整形したりできるかなぁ？）</p>
</li>
<li>
<p>また、1行に2つのJSONが書いてある場合は、1つ目のJSONをパースしたら、そこでおしまいみたいで、その後に記述されたデータは登録されませんでした。</p>
</li>
</ul>
<h3 id="インデックスがない場合">インデックスがない場合</h3>
<p>stream2esで登録するインデックスがElasticsearchに存在しない場合、stream2esがインデックスを作成してくれるのですが、
この時、シャード数などはstream2es内部に記述があるので注意が必要です。
以下がその設定です。</p>
<ul>
<li>index.number_of_shards : 2</li>
<li>index.number_of_replicas : 0</li>
<li>index.refresh_interval : 5s</li>
</ul>
<h2 id="課題">課題？</h2>
<p>内部的にはおそらく、<code>Bulk</code>でデータを登録していると思うのですが、まだよくわかっていません。
Clojureが読めないので、せっかくだから、Clojureの勉強も兼ねてちょっとソースを読んでみようかなと思います。
それほど量があるわけでもないので。</p>
<p>あとは、その他にWikipediaのデータやTwitterのデータ登録、
ElasticsearchからデータをScrollで読み出しつつ、別のElasticsearchに流しこむといったこともできそうなので、そちらも試してみようかと。
他にもオプションがいくつかありそうです。</p>
<p>今回は2件ほどでしたが、大量データを流し込んだ時にどうなるか（stream2esが悲鳴を上げるのか、Elasticsearchで詰まることがあったらどうなるか）なども
気になるので、なんか適当なデータで試してみるのもいいかなぁと。
（ということで、だれか、いろいろ試してみてもらえると楽できるなぁ。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>第4回Elasticsearch勉強会を開催しました。#elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2014/04/21/hold-on-4th-elasticsearch-jp/</link>
      <pubDate>Mon, 21 Apr 2014 19:44:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/04/21/hold-on-4th-elasticsearch-jp/</guid>
      <description>第4回Elsticsearch勉強会を開催しました。 今回から、遅刻厳禁にしてみました。 それほど困った人もいないと思うので、次回からも遅刻厳禁</description>
      <content:encoded><p><a href="http://elasticsearch.doorkeeper.jp/events/8865">第4回Elsticsearch勉強会</a>を開催しました。
今回から、遅刻厳禁にしてみました。
それほど困った人もいないと思うので、次回からも遅刻厳禁で。</p>
<p>ということで、今回も多数の方にお集まりいただきありがとうございました。</p>
<p>スタッフの皆さん、スピーカーの皆さん、プレゼント用に書籍を用意してくれたKADOKAWAさん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！
参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>
<p>さて、ブログですが司会業とかやってたので、あんまり書けてないけど。。。</p>
<!-- more -->
<p>134番までチケットがはけていたので＋スタッフで140〜150名くらいの参加者だったのではないかと思います。
懇親会まで残っていただいた方々も片付けなどありがとうございました。</p>
<p>さて、感想とか補足です。</p>
<h2 id="アナライズ処理の仕組みとクエリdsl株式会社シーマーク大谷純johtani">「アナライズ処理の仕組みとクエリDSL」株式会社シーマーク　大谷　純　@johtani</h2>
<p>スライド：<a href="/images/entries/20140421/Introduction_analysis_and_query_dsl_for_print.pdf">アナライズ処理の仕組みとクエリDSL</a>※スライドはPDFです。</p>
<p>プラグイン：<a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analyze</a><br>
プラグインの紹介記事：<a href="http://blog.johtani.info/blog/2013/10/25/developing-es-extended-analyze-plugin/">http://blog.johtani.info/blog/2013/10/25/developing-es-extended-analyze-plugin/</a></p>
<p>Marvel：<a href="http://www.elasticsearch.com/marvel/">http://www.elasticsearch.com/marvel/</a></p>
<p>日本語版メーリングリスト：<a href="https://groups.google.com/forum/#!forum/elasticsearch-jp">https://groups.google.com/forum/#!forum/elasticsearch-jp</a></p>
<p>なんか、宣伝（本＋プラグイン）ばっかりですみません。
「プラグインの紹介記事」に簡単な使い方が書いてあります。が、情報が古いので、Elasticsearchのバージョンに合わせたバージョンを使ってください。</p>
<p>まだまだ発表に慣れてないので、頑張ろ。</p>
<p>アンケート取ってみましたが、ログ検索と全文検索と半々くらいで興味がある人がいるみたいでした。
あと、有料トレーニングは人気ないっすね。。。</p>
<h2 id="elasticsearch-hadoopを使ってごにょごにょしてみる-株式会社マーズフラッグ-rd部やまかつ-さんyamakatu">「elasticsearch-hadoopを使ってごにょごにょしてみる」 株式会社マーズフラッグ R&amp;D部　やまかつ さん　@yamakatu</h2>
<p>スライド：<a href="http://www.slideshare.net/yamakatu/elasticsearchhadoop">elasticsearch-hadoopを使ってごにょごにょしてみる</a></p>
<p>elasticsearch-hadoop：<a href="https://github.com/elasticsearch/elasticsearch-hadoop">https://github.com/elasticsearch/elasticsearch-hadoop</a></p>
<p>QAとして、Elasticsearchにプライマリデータを保存するのは的な話が出てました。
ESにのみデータを入れるのは個人的には考えたことないかなぁ。
どうしても、ElasticsearchのWriteが遅いんじゃないかという懸念事項を持ってる人がいるなぁと。（実際ツラいという話もちらほら）</p>
<p>お腹痛い中の発表ありがとうございました。。。
次回はMapRの方に紹介してもらえそう（交渉中）なので楽しみです。やまかつさんの続きも聞きたいなぁ。</p>
<h2 id="couchbaseとelasticsearchが手を結んだら株式会社アットウェア-佐竹雅央さん-madgaoh-河村康爾さん-ijokarumawak">「CouchbaseとElasticsearchが手を結んだら」株式会社アットウェア 佐竹雅央さん @madgaoh 河村康爾さん @ijokarumawak</h2>
<p>スライド：<a href="http://www.slideshare.net/masahirosatake/elasticsearch-couchbaseelasticsearch">CouchbaseとElasticsearchが手を結んだら</a></p>
<p>CouchbaseのElasticsearchに関するページ：<a href="http://docs.couchbase.com/couchbase-elastic-search/">http://docs.couchbase.com/couchbase-elastic-search/</a></p>
<p>Couchbaseに入れたら、自動的にElasticsearchにもデータを入れてくれる。
デモがあるの、いいっすね。</p>
<p>最新版はmasterを落としてきてビルドしないとダメらしい。確かに、上のページには0.90.5って書かれてる。
ここでも、やはり、Elasticsearchが詰まった時にどうするの？みたいな話が出てました。
CouchbaseのXDCRだと、後ろが詰まってる時によしなに？データを流すのを制御してくれるってのがあるみたいですが、
Elasticsearchだと悲鳴を上げているのがわかりにくいと。</p>
<p>あと、Elasticsearchがインデキシングでキューを取りこぼしているのがログからわかりにくいってのも出てました。
（なにか、分かる方法があるかとかも調べてみようかなぁ。）</p>
<p><em>自分の宿題：Transportプラグインって何かについて調べてブログに書くこと。</em></p>
<h2 id="elasticsearch-at-wantedlyタイトルあってるか不安wantedly-inc-内田誠悟さん-spesnova">「Elasticsearch at Wantedly」（タイトルあってるか不安）　Wantedly, Inc 内田誠悟さん @spesnova</h2>
<p>スライド：<a href="http://speakerdeck.com/spesnova/elasticsearch-at-wantedly-inc">Elasticsearch at Wantedly</a></p>
<p>参考文献：<a href="http://code46.hatenablog.com/entry/2014/01/21/115620">Elasticsearchチュートリアル</a></p>
<p>参考文献：<a href="https://speakerdeck.com/dadoonet/elasticsearch-workshop">Elasticsearch Workshop</a></p>
<p>参考文献：<a href="https://github.com/elasticsearch/elasticsearch-rails">elasticsearch-rails</a></p>
<p>Wantedlyでどうやって使ってるのか。
あと、オートコンプリートでも使ってます。（この話は次回聞けるといいなぁｗ）</p>
<p>データ数は少ないので、参考にまだならないかも。</p>
<p>公式のサイト見難いですよねと。
ペンギン先生のブログが素晴らしかった！
マッピングすごいｗ</p>
<p>最後は、苦労して作ってもらったautocompleteの資料は放ったらかしにして、質疑応答してもらいました。
辞書とか、検索漏れとかの話は今後の課題っぽかったですね。</p>
<p>「Elasticsearchのみに決めてました！」ってセリフがカッコ良かったｗ</p>
<p>アクセスコントロール周りのノウハウもブログで共有してくれそうなので楽しみにしています！</p>
<p>あと、「tireがretire」の話が出てましたが（この発表だっけ？）参考文献にあげてある、elasticsearch-railsが今は本流なんじゃないかなぁ？</p>
<h2 id="lt">LT</h2>
<p>###「ElasticsearchのScripting」株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん @pisatoshi</p>
<p>スライド：<a href="https://speakerdeck.com/pisatoshi/elasticsearchdescripting">ElasticsearchのScripting</a></p>
<p>参考文献：<a href="https://github.com/imotov/elasticsearch-native-script-example">elasticsearch-native-script-example</a></p>
<p>色々とScriptがあるという話を説明してもらい感謝です。
もちろん、ElasticSearchServerにも書いてあるので、そちらも参考にしてください！</p>
<h3 id="elasticsearch-向け多言語解析プラグインベイシステクノロジー株式会社-江口天さん">「Elasticsearch 向け多言語解析プラグイン」ベイシス・テクノロジー株式会社 江口天さん</h3>
<p>スライド：<a href="http://www.slideshare.net/basistech/4-21-elasticsearch-meetup">Elasticsearch 向け多言語解析プラグイン</a></p>
<p>参考文献：<a href="http://www.basistech.jp/elasticsearch/">Elasticsearchで使えるRosette基本言語解析モジュール</a></p>
<p>参考文献：<a href="http://blog.johtani.info/blog/2013/09/23/intro-elasticsearch-inquisitor/">Elasticsearch-inquisitorプラグインの紹介</a></p>
<p>ベイシステクノロジさんが提供しているRosetteをElasticsearchで活用できるモジュールみたいです。
いまなら、無料で体験できるみたいなので、どんなものか触ってみると面白いかもしれません。</p>
<p>あと、デモで使用されていたプラグインについて、私が昔に書いた記事があるので、興味のある方は参考にしていただければと。
このプラグインはクエリがどのように内部でLuceneのクエリになっているか、どのフィールドでどうトークンが生成されるか？
といったものが見ることができるプラグインになっています。</p>
<h2 id="関連ブログ">関連ブログ</h2>
<p>適当に見つけたブログを列挙してあります。これもあるよ！などあれば、教えてください。</p>
<ul>
<li>
<p><a href="http://blog.yoslab.com/entry/2014/04/21/200601">勉強会メモ - 第4回elasticsearch勉強会 2014/04/21</a></p>
</li>
<li>
<p><a href="http://togetter.com/li/657879">togetter 第4回elasticsearch勉強会 #elasticsearchjp</a></p>
</li>
<li>
<p><a href="http://dev.classmethod.jp/server-side/4th-elasticsearchjp/">参加レポート:第4回elasticsearch勉強会 #elasticsearchjp</a></p>
</li>
<li>
<p><a href="http://qiita.com/maaru/items/0f82600dbc305715b23d">第4回elasticsearch勉強会に参加しました</a></p>
</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>しつこいくらい宣伝してしまいましたが、「ElasticSearch Server日本語版」よろしくお願いします！</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=4048662023&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<p>今回も楽しい話が聞けました。メモがちょっと少ないんですが。。。</p>
<p>次回は6末を目処に、MapRの方などと調整して開催しようと思います。
聞きたい話とか、発表したい方とかあれば、連絡くださいー！</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-kopfの紹介（analysis画面）</title>
      <link>https://blog.johtani.info/blog/2014/04/09/intro-elasticsearch-kopf-analysis/</link>
      <pubDate>Wed, 09 Apr 2014 11:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/04/09/intro-elasticsearch-kopf-analysis/</guid>
      <description>今日はelasticsearch-kopfのAnalysis画面の紹介です。 （簡単なところから。。。その２） メニューのanalysisを選択</description>
      <content:encoded><p>今日はelasticsearch-kopfのAnalysis画面の紹介です。</p>
<p>（簡単なところから。。。その２）</p>
<!-- more -->
<p>メニューの<code>analysis</code>を選択すると、次のような画面が表示されます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140409/kopf-analysis.jpg" />
    </div>
    <a href="/images/entries/20140409/kopf-analysis.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>Analysis画面</h4>
      </figcaption>
  </figure>
</div>

<p>Elasticsearchの<code>_analyze</code> APIを画面で確認できます。
画面で動作の確認ができるのは嬉しいですよね。</p>
<ol>
<li>入力文字列：入力となるドキュメントに含まれる文字列や検索キーワードを入力</li>
<li>フィールドの指定：対象とするインデックス名、タイプ名、フィールド名を選択</li>
<li>analyze：ボタンを押す</li>
<li>トークナイズされた結果：入力文字列がどのようなトークンに分割されるか
<ul>
<li>start、end：入力文字列中の文字列の位置</li>
<li>pos：トークンの位置</li>
</ul>
</li>
</ol>
<p>という形でElasticsearchが指定されたフィールドで入力文字をどのようにトークナイズしたかを確認することができます。</p>
<p>Elasticsearchは内部でこのトークナイズされた単語を元に転置インデックスを作成し、検索に利用します。
ですので、特定のデータが検索に上手くヒットしないときに、この画面でデータの文字列をトークナイズしてみるといった用途に使えます。</p>
<p>フィールドの設定がどのようにして入力文字列をトークンにしているかといった点については、今度のElasticsearch勉強会で話す予定です。</p>
<p>フィールドの設定を利用する以外に、アナライザを指定してどのようにトークナイズされるかを見ることもできます。
「ANALYZE BYANALYZER」をクリックすると利用できます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140409/kopf-analysis-analyzer.jpg" />
    </div>
    <a href="/images/entries/20140409/kopf-analysis-analyzer.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>ANALYZE BY ANALYZER</h4>
      </figcaption>
  </figure>
</div>

<p>トークナイズしたい文字列を入力し、インデックス名と、インデックスに設定されているアナライザ名を選択してanalyzeボタンを押すと
結果が表示されます。
（例では、kuromojiアナライザを利用して出力になっています。また、出力結果のposの表示位置がFIELD TYPEの時と違うのが少し気になりました。）</p>
<p>ただ、残念ながら、インデックスのマッピングで指定したアナライザしか利用できないみたいなので、
どのアナライザがどんな挙動かを調べたい場合は、以前紹介した<a href="http://blog.johtani.info/blog/2013/09/23/intro-elasticsearch-inquisitor/">elasticsearch-inquisitor</a>を
利用したほうが良さそうです。</p>
<p>ということで、今日はanalysis画面の説明でした。</p>
</content:encoded>
    </item>
    
    <item>
      <title>JVM Operation Casual Talksに参加しました #jvmcasual</title>
      <link>https://blog.johtani.info/blog/2014/04/07/attend-jvm-casual-1/</link>
      <pubDate>Mon, 07 Apr 2014 19:24:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/04/07/attend-jvm-casual-1/</guid>
      <description>JVM Operation Casual Talksに参加しました。 あんまり、運用とかやってないので、ついていけるか不安ですが、楽しそうだったので。 懇親会は予定されてないらしい</description>
      <content:encoded><p><a href="http://atnd.org/events/48999">JVM Operation Casual Talks</a>に参加しました。
あんまり、運用とかやってないので、ついていけるか不安ですが、楽しそうだったので。
懇親会は予定されてないらしい。</p>
<!-- more -->
<h2 id="stanaka15分でわかるjvmのメモリ管理">@stanaka	15分でわかるJVMのメモリ管理</h2>
<p>スライド：<a href="https://speakerdeck.com/stanaka/understanding-memory-management-of-javavm-in-15-minutes">https://speakerdeck.com/stanaka/understanding-memory-management-of-javavm-in-15-minutes</a></p>
<ul>
<li>
<p>JVMの基礎知識</p>
</li>
<li>
<p>某研究所で。。。</p>
</li>
<li>
<p>Elasticsearch、Solrの名前が！</p>
</li>
<li>
<p>JVM困りどころ。ネットの情報が古いのが多いとか。</p>
</li>
<li>
<p>使いこなすにはきちんと知識が必要。</p>
</li>
</ul>
<h3 id="memory-model">Memory Model</h3>
<ul>
<li>
<p>困ったらJava8に。。。</p>
</li>
<li>
<p>Heapのチューニングが重要</p>
</li>
<li>
<p>Heapの構成やGC処理の説明</p>
</li>
<li>
<p>CMSでもすべて並列ではない</p>
<ul>
<li>OlgGenがフラグメントして確保が遅くなっていく</li>
</ul>
</li>
<li>
<p>G1GCの概念図。YoungやOld、Survivorの区別がない？</p>
<ul>
<li>リージョン？</li>
</ul>
</li>
<li>
<p>リファレンスリストすばらしい。</p>
</li>
</ul>
<h2 id="oraniecassandra運用で実施しているjvm監視について">@oranie	Cassandra運用で実施しているJVM監視について</h2>
<p>スライド：<a href="http://www.slideshare.net/oranie/jvm-operation-casual-talks-33218856">http://www.slideshare.net/oranie/jvm-operation-casual-talks-33218856</a></p>
<ul>
<li>Cassandra(1.1)で運用中。</li>
<li>35TBとか</li>
<li>エンジニアブログ読め。</li>
<li>Cassandraの監視って何すればいいの？</li>
<li>GC頻度とかグラフで見れる</li>
<li>VisualVM便利って教えてもらった。
<ul>
<li>VisualVMでどんな値があるか見ながら調べて、取りたい値を考える。</li>
</ul>
</li>
<li>SNMPで頑張るの辛い＝JMXかな？＝Javaで叩くの？＝Jolokia</li>
<li>どんなことやってる？
<ul>
<li>Nagios+Jenkins＋。。。あとでスライド</li>
</ul>
</li>
<li>yohoushi+GrowthForecastも便利。</li>
</ul>
<h2 id="waysakusprayakka運用でjvmをcpuスケールさせるために行った事">@waysaku	Spray(Akka)運用でJVMをCPUスケールさせるために行った事</h2>
<p>スライド：<a href="http://www.slideshare.net/waysaku/jvm-operation-casual-talks">http://www.slideshare.net/waysaku/jvm-operation-casual-talks</a></p>
<ul>
<li><a href="http://spray.io">Spray</a></li>
<li>Akkaアプリケーションの運用を任されちゃった人</li>
<li>カットオーバー直前の負荷試験でびっくりするくらいスループットでない。。。
<ul>
<li>いろんなActorが特定のActorに処理を投げた状態で待ってるという状況。。。</li>
<li>スレッドダンプ追っかけてる様子を追体験してる。</li>
<li>ログ出力がBLOCKしてた</li>
<li>I/Oを伴う処理は非常にセンシティブ</li>
</ul>
</li>
</ul>
<h2 id="以下lt">以下、LT</h2>
<h2 id="oinume運用に効くjvmオプション三選">@oinume	運用に効く！JVMオプション三選</h2>
<p>スライド：<a href="http://www.slideshare.net/oinume/jvm-operationcasualtalks20140407">http://www.slideshare.net/oinume/jvm-operationcasualtalks20140407</a></p>
<ul>
<li>サーバに入らなくても見れるのは
<ul>
<li>VisualVM</li>
</ul>
</li>
<li>最終形Java Mission Control
<ul>
<li>Flight Recorder（商用だと有償）</li>
</ul>
</li>
</ul>
<h2 id="y_uuki1なにもわからないところから始めるjvmモニタリング">@y_uuki1	なにもわからないところから始めるJVMモニタリング</h2>
<p>スライド：<a href="https://speakerdeck.com/yuukit/nanimowakaranaitokorokarashi-merujvmmonitaringu">https://speakerdeck.com/yuukit/nanimowakaranaitokorokarashi-merujvmmonitaringu</a></p>
<p>ブログ：<a href="http://yuuki.hatenablog.com/entry/2014/04/08/074507">http://yuuki.hatenablog.com/entry/2014/04/08/074507</a></p>
<ul>
<li>なんでここにいるかわかりませんが。</li>
<li>色んな所記事とかのリンクがある。</li>
<li>Graphiteに放り込んでるらしい。</li>
</ul>
<h2 id="tagomorisnorikraのjvmチューンで苦労している話">@tagomoris	NorikraのJVMチューンで苦労している話</h2>
<p>スライド：<a href="http://www.slideshare.net/tagomoris/norikrajvm">http://www.slideshare.net/tagomoris/norikrajvm</a></p>
<ul>
<li>Norikraで困ったので開催したらみんなしゃべりに来てくれてありがたい世の中ですね。</li>
<li>デフォルト設定で運用。mx4096mで4ヶ月は元気に動いてた</li>
<li>年末に崩壊 -&gt; harukaさんのQiita見ながら設定してほったからした。</li>
<li>3月にまた崩壊OOM
<ul>
<li>SoftRefが悪さしてるのか？</li>
<li>nekopさんのブログにヒントが。</li>
</ul>
</li>
</ul>
<h2 id="shot6jvm的なナニカを話すgc戦略をそえて">@shot6	JVM的なナニカを話す、GC戦略をそえて</h2>
<p>スライド：<a href="http://www.slideshare.net/shot6/jvm-33248312">http://www.slideshare.net/shot6/jvm-33248312</a></p>
<ul>
<li>某A社からきました。</li>
<li>CMSを中心が良い</li>
<li>CPUコア少ないとCMS Incremental</li>
<li>G1GCは6GB以上とかもっと大きい場合のほうが良さそう（雑感）</li>
<li>基本的に、ドキュメントに書いてあるけど、つらいので、まとめてみました。</li>
<li>後で見たいな、このスライドも。</li>
</ul>
<h2 id="kazeburowebアプリケーションとメモリ">@kazeburo	Webアプリケーションとメモリ</h2>
<p>スライド：<a href="http://www.slideshare.net/kazeburo/jvm-casual">http://www.slideshare.net/kazeburo/jvm-casual</a></p>
<ul>
<li>Javaは1行も書いたことありません。</li>
<li>Java運用したけど、まぁ、ひどい目にあってます。</li>
<li>ということで、PerlとApacheの話？
<ul>
<li>bumpyなんとかってのは頭良くプロセスの処理回数とかをなんとかしてくれる</li>
</ul>
</li>
<li>JVMとかどうなの？
<ul>
<li>ということで、CA社の人が釣れたので良かったですね。</li>
</ul>
</li>
</ul>
<h2 id="tbdjvmの運用について話す会パネル">TBD	JVMの運用について話す会(パネル)</h2>
<p>なんでJVM？</p>
<ul>
<li>全文検索だとJVMだなぁ。SolrとかElasticsearchとか</li>
<li>入社したらあった。</li>
</ul>
<p>JVMこまったところは？</p>
<ul>
<li>突然Dioが出てくる</li>
</ul>
<p>JVMいいところは？</p>
<ul>
<li>うーん、あえて言うと。。。スレッド処理</li>
<li>型がある言語を調べるとJVM。というよりScala。</li>
<li>飛び入り参加。落ちないところ。人殺しは良くないｗ</li>
<li>簡単には殺せないところに使えるのは良いのでは。</li>
</ul>
<p>チューンはどこまでやる？</p>
<ul>
<li>7,8割でやめたほうが良い？</li>
<li>そんなに突き詰めなくてもいいのかな。</li>
<li>Cassandraだと、FullGCが走ってる時点で再起動しかない場合が多いです。</li>
</ul>
<p>言語のランタイムについてチューンすることってJVMの他ある？</p>
<ul>
<li>Rubyくらい？</li>
</ul>
<p>自分たちで何とかできないものを相手にしている人たちは苦労してるんじゃないか</p>
<ul>
<li>CaなんとかさんとかHBなんとかさんは困ってる人が多い</li>
</ul>
<p>最後に一言</p>
<ul>
<li>モニタリングツール参考にしたい</li>
</ul>
<h2 id="最後に">最後に</h2>
<p>「やりたくなったら誰かやってください。」ということみたいです。</p>
<p>基礎から監視まで結構幅広くキーワードが出てきたので楽しかったです。</p>
<p>その他に話に出てきてた方のブログもリンクしとこうかな。</p>
<p><a href="http://d.hatena.ne.jp/nekop/">nekopの日記</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-kopfの紹介（rest画面）</title>
      <link>https://blog.johtani.info/blog/2014/04/07/intro-elasticsearch-kopf-rest/</link>
      <pubDate>Mon, 07 Apr 2014 11:24:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/04/07/intro-elasticsearch-kopf-rest/</guid>
      <description>今日はelasticsearch-kopfのREST画面の紹介です。 （簡単なところから。。。） メニューのrestを選択すると、次のような画面</description>
      <content:encoded><p>今日はelasticsearch-kopfのREST画面の紹介です。</p>
<p>（簡単なところから。。。）</p>
<!-- more -->
<p>メニューの<code>rest</code>を選択すると、次のような画面が表示されます。</p>
<p>Elasticsearch自体が、さまざまな操作をRESTでできる仕組みになっています。
検索にも利用しますが、それ以外の設定などにつてもリクエストを送ればOKです。</p>
<p>ですので、リクエストや設定を自分で組み立てて送ることができる画面が用意されているととても便利です。
（もちろん、curlコマンドでもいいのですが、画面があると便利ですよね）</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140407/kopf-rest.jpg" />
    </div>
    <a href="/images/entries/20140407/kopf-rest.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>REST画面</h4>
      </figcaption>
  </figure>
</div>

<h2 id="history">History</h2>
<p>履歴表示画面です。
これまで、kopfのrest画面を利用して送信したリクエストが一覧で表示されます。</p>
<p><code>History</code>という文字をクリックすることで、表示/非表示の切り替えが可能です。（最初は非表示）
マウスオーバーすると、リクエストボディがポップアップで表示されます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140407/kopf-history.jpg" />
    </div>
    <a href="/images/entries/20140407/kopf-history.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>History</h4>
      </figcaption>
  </figure>
</div>

<p>履歴にあるURLはクリック可能で、クリックすると実行されます。
履歴は<code>localStorage</code>に保存されるみたいです。（ブラウザの仕様？あんまり詳しくないので。。。）
たぶん、30件が上限かと（ソースで確認しただけ）</p>
<h2 id="url">URL</h2>
<p>rest画面でリクエストを送信する先のURLを指定します。
メソッドは右側のSELECTで選択可能です。</p>
<p>リクエストパラメータも指定が可能です。</p>
<h2 id="リクエストボディ">リクエストボディ</h2>
<p>検索や設定のJSONを記述するところです。
一応、JSON的にエラーがある場合は行数の左側にバツ印が出てきておかしなところもわかるようになっています。</p>
<p>インデントなどは行ってくれますが、senseみたいな補完などはないので、少し辛いところです。</p>
<h2 id="レスポンス">レスポンス</h2>
<p>送信したリクエストに対するレスポンスが返ってきます。
インデントされた状態で表示されるので読みやすいかと。
また、入れ子になっているJSONについては、閉じたり開いたりすることも可能です。
（開始のカッコの右側に<code>-</code>が表示されていて、クリックすると閉じることができます。閉じると<code>+</code>に変わります）</p>
<p>簡単ですが、rest画面の説明でした。
KOPFを使っていて、ちょっとしたクエリを送ったりするのには便利だと思います。</p>
<p>複雑な検索クエリなどについては、やはりsenseを使うのが良いかと思いますが。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-kopfの紹介（概要）</title>
      <link>https://blog.johtani.info/blog/2014/04/05/intro-elasticsearch-kopf-1/</link>
      <pubDate>Sat, 05 Apr 2014 23:18:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/04/05/intro-elasticsearch-kopf-1/</guid>
      <description>なんだか、ドタバタしてて久しぶりの更新です。 ベルリンの旅行記みたいなのも書きたいのですが、まずはこちらかと。 elasticsearch-ko</description>
      <content:encoded><p>なんだか、ドタバタしてて久しぶりの更新です。
ベルリンの旅行記みたいなのも書きたいのですが、まずはこちらかと。</p>
<p><a href="https://github.com/lmenezes/elasticsearch-kopf">elasticsearch-kopf</a>プラグインの紹介です。</p>
<p>今回は概要の説明だけになります。機能が結構多いので。</p>
<!-- more -->
<h2 id="elasticsearch-kopfとは">elasticsearch-kopfとは？</h2>
<p><code>_site</code>プラグインの一つで、クラスタ管理用のプラグインになります。
<code>head</code>プラグインや<code>HQ</code>プラグインと同様です。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140405/kopf.jpg" />
    </div>
    <a href="/images/entries/20140405/kopf.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>プラグインの画面</h4>
      </figcaption>
  </figure>
</div>

<p>このようにシンプルな画面で、スッキリとしています。
緑を基調にした画面構成はElasticsearchの緑色を意識してるんでしょうか？</p>
<p>上記の画像に簡単なコメントを入れてあります。</p>
<h3 id="メニュー">メニュー</h3>
<ul>
<li>KOPF：KOPF自体の設定（接続先とリフレッシュインターバルの変更）</li>
<li>cluster：クラスタ管理、情報（デフォルト表示画面）</li>
<li>rest：RESTリクエスト送信、結果表示画面</li>
<li>aliases：エイリアス管理</li>
<li>analysis：<code>analysis API</code></li>
<li>percolator：パーコレータ管理</li>
<li>warmup：ウォームアップクエリ管理</li>
</ul>
<p>上記のようなメニューです。各メニューについては、今後のブログで少しずつ紹介しようかと。
このメニューの色が、クラスタの状態も表しています。ステータスがYELLOWなら黄色、REDなら赤色に変わります。</p>
<h3 id="インデックス">インデックス</h3>
<p>インデックスは列として表示されます。先ほどの画像では、2つのインデックスが表示されている状態です。
インデックス毎に、シャードも表示されます。これは、各ノードがどのシャードを保持しているかという情報です。
色の濃いシャードがプライマリでしょう。
インデックス名やシャードの箱はクリックできるようになっていて、それぞれの情報がJSONで表示されます。
その他にもドキュメント数、サイズなども表示されます。
インデックスの各種操作（closeやdeleteなど）もここからメニューが表示されます。（これも次回詳しく）</p>
<h3 id="ノード">ノード</h3>
<p>ノードの情報が行として表示されます。ノードが増えると下に追加されていきます。
<code>node1</code>というのが、ノード名です。（ヒーローの名前とかが出てくるやつです。）</p>
<p>その他に、IPアドレス、ポート番号、負荷、ヒープサイズなども表示されています。
電源ボタンはノードのシャットダウンを行うためのボタンです。（確認用のダイアログが表示される）</p>
<h3 id="その他">その他</h3>
<p>その他に、クラスタの概要として、ノード数、インデックス数、シャード数、ドキュメント数なども表示されます。
インデックスの作成などは、アイコンから操作が可能です。
大規模なクラスタを管理している場合、検索ボックスを利用することで、インデックス名やノード名による絞込もできるようになっています。</p>
<h2 id="感想">感想</h2>
<p>シンプルな構成の画面で、個人的には<code>head</code>よりも好きな画面です。
<code>HQ</code>よりもシャードの分散具合がわかりやすいので、今後はこのプラグインを利用していこうと考えています。</p>
<p>まずは、簡単な紹介です。今後、各画面についてもう少し説明をブログに書いていこうかと考えています。
待てない方は、触ってみてもらうのが良いかと。
もちろん、続きを書いてもらってもいいですよ！！</p>
</content:encoded>
    </item>
    
    <item>
      <title>ElasticSearch Server日本語版（電子版も）が発売されました</title>
      <link>https://blog.johtani.info/blog/2014/03/25/release-elasticsearch-server-ja-ebook/</link>
      <pubDate>Tue, 25 Mar 2014 13:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/03/25/release-elasticsearch-server-ja-ebook/</guid>
      <description>先日、「ElasticSearch Serverを翻訳しました」という記事を書きました。 この中で電子版も出ますよと書いていましたが、電子版も発</description>
      <content:encoded><p>先日、「<a href="http://blog.johtani.info/blog/2014/03/03/release-elasticsearch-server-japanese-edition/">ElasticSearch Serverを翻訳しました</a>」という記事を書きました。</p>
<p>この中で電子版も出ますよと書いていましたが、電子版も発売されたので、再告知も兼ねてブログを書いています。</p>
<p>なお、最近よく「ElasticsearchのSは小文字」とツイートしていますが、本書は原著のタイトルが「ElasticSearch Server」となっているため、Sは大文字になっています。原著が出版された時期にはまだSが小文字に統一されていなかったためです。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140325/ess.jpg" />
    </div>
    <a href="/images/entries/20140325/ess.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<!-- more -->
<p>紙の書籍はすでに書店に並んでいたり、Amazonでも発送されているようです。</p>
<p>私自身が電子書籍が場所を取らなくて好きというのもあり、電子版も出版していただけるようにお願いしていました。</p>
<p>電子版についてはAmazonでKindle版、達人出版会からEPUBとPDFが購入可能です。</p>
<ul>
<li><a href="http://tatsu-zine.com/books/elasticsearch-server">達人出版会のElasitcSearch Serverのページ</a></li>
<li><a href="http://www.amazon.co.jp/gp/product/B00J4KDYZU/ref=as_li_ss_tl?ie=UTF8&camp=247&creative=7399&creativeASIN=B00J4KDYZU&linkCode=as2&tag=johtani-22">AmazonのKindle版ページ</a><img src="http://ir-jp.amazon-adsystem.com/e/ir?t=johtani-22&l=as2&o=9&a=B00J4KDYZU" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /></li>
</ul>
<p>こちらのリンク（Amazonはアフィリンク）を参考にしていただければと。</p>
<p>書籍の写真入りツイートしたら、原著者の方からレスを頂きました。</p>
<blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/johtani">@johtani</a> Thanks for the great work out there :)</p>&mdash; Rafał Kuć (@kucrafal) <a href="https://twitter.com/kucrafal/statuses/447668482587779072">2014, 3月 23</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>また、原著者のサイトでも紹介してもらいました（結構うれしい）。
私が窓口をやっていた関係で、私の名前しか入っていませんが。。。
編集者と翻訳者の方々のお陰で良い本が出版できたと思っています。</p>
<blockquote class="twitter-tweet" lang="ja"><p>Thanks for sharing! / ElasticSearch Server book in Japanese | ElasticSearch Server Book Blog <a href="http://t.co/fhCP1vVBd3">http://t.co/fhCP1vVBd3</a></p>&mdash; Jun Ohtani (@johtani) <a href="https://twitter.com/johtani/statuses/448098766457815040">2014, 3月 24</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>ということで、Elasticsearchの導入の手助けになればと。
感想（賛否問わず）など、あればコメント、ツイート、ブログなど書いていただければうれしいです。
（その際に、連絡してもらえるとさらにうれしいです）</p>
</content:encoded>
    </item>
    
    <item>
      <title>GOTO Night elasticsearchに参加しました</title>
      <link>https://blog.johtani.info/blog/2014/03/18/attend-goto-night-elasticsearch/</link>
      <pubDate>Tue, 18 Mar 2014 23:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/03/18/attend-goto-night-elasticsearch/</guid>
      <description>GOTO Night elasticsearchに参加しました。 初の海外の勉強会です（海外自体が初だし）。 ベルリンにあるWoogaという会社で開催された勉強会</description>
      <content:encoded><p><a href="https://secure.trifork.com/berlin-2014/freeevent/index.jsp?eventOID=6151">GOTO Night elasticsearch</a>に参加しました。
初の海外の勉強会です（海外自体が初だし）。</p>
<p>ベルリンにあるWoogaという会社で開催された勉強会です。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140318/goto_night_elasticsearch.jpg" />
    </div>
    <a href="/images/entries/20140318/goto_night_elasticsearch.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<!-- more  -->
<blockquote class="twitter-tweet" lang="ja"><p>Full house at the <a href="https://twitter.com/search?q=%23gotonight&amp;src=hash">#gotonight</a> about <a href="https://twitter.com/elasticsearch">@elasticsearch</a> in the auditorium of <a href="https://twitter.com/wooga">@wooga</a> <a href="http://t.co/r2Vx2aMcEn">pic.twitter.com/r2Vx2aMcEn</a></p>&mdash; GOTO Berlin (@GOTOber) <a href="https://twitter.com/GOTOber/statuses/445987339924684801">2014, 3月 18</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>会場はこんな感じで、とってもおしゃれです。（なんか、写ってますね、最前列にｗ）
ベルリンの古い建物をリノベーションしたオフィスみたいで、そこにこういったひな壇を用意して発表するスペースにしているみたいです。</p>
<p>内容としては、Elasticsearch1.0の新機能の話として、<code>snapshot/restore</code>、<code>cat</code>、<code>aggregation</code>などの話題でした。簡単にどういったものかの説明です。</p>
<p>一人目の発表が終わったら、ブレイクタイムとして上のフロアに用意されている軽食＋ドリンクで軽く交流の時間が用意されていました。
ベーグルなどのサンドイッチとハイネケンやクラブマテ？と呼ばれるチープなレッドブルとかが飲めました。</p>
<p>ちょっと食べて談笑したあとに、次はLogstashとKibanaのお話でした。Logstashってどんなもの？という話がメインで、Kibanaは簡単な紹介という感じでしょうか。最後に、ライブデモがありました。
Kibanaを使ったライブデモはインパクトがあるなというのが感想です。
フィールド名の補完をしてくれたりと便利な機能が操作をしているところでわかるので。</p>
<p>Youtubeなどでも見てても思っていた感想ですが、こちらの勉強会は質問が結構出てきます。
今日参加した勉強会も質疑応答が結構されてました。</p>
<p>elasticsearchの方たちと少しだけ話しをできたので、かなり興奮気味でブログを書いています（ミーハー）。</p>
<p>ただ、やっぱり英語のヒアリングがまだまだだなぁとも実感出来ました。
場数踏むしかないと思うので少しずつ耳にしてなれるしかないかなぁと。
はぁ、ちゃんと高校とか大学の頃に単語を覚えとくんだったと軽く後悔。</p>
<p>こんなツイートもしてもらって、興奮気味です。明日早起きしないといけないので寝ないといけないのにｗ</p>
<blockquote class="twitter-tweet" lang="ja"><p>Meeting <a href="https://twitter.com/johtani">@johtani</a> finally in <a href="https://twitter.com/search?q=%23berlin&amp;src=hash">#berlin</a> watching <a href="https://twitter.com/spinscale">@spinscale</a> talking about <a href="https://twitter.com/search?q=%23elasticsearch&amp;src=hash">#elasticsearch</a></p>&mdash; Simon Willnauer (@s1m0nw) <a href="https://twitter.com/s1m0nw/statuses/446001849750269952">2014, 3月 18</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</content:encoded>
    </item>
    
    <item>
      <title>Berlinからおはようございます</title>
      <link>https://blog.johtani.info/blog/2014/03/16/post-from-berlin/</link>
      <pubDate>Sun, 16 Mar 2014 05:51:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/03/16/post-from-berlin/</guid>
      <description>Berlinからおはようございます。 ということで、人生で初めての海外に来ています。 人生初の海外なのに、一人で乗り換えがあるようなBerlin</description>
      <content:encoded><p>Berlinからおはようございます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140316/traffic_sign_in_Berlin.jpg" />
    </div>
    <a href="/images/entries/20140316/traffic_sign_in_Berlin.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>ということで、人生で初めての海外に来ています。</p>
<!-- more  -->
<p>人生初の海外なのに、一人で乗り換えがあるようなBerlinに来ています。
今年はチャレンジの年になりそう。初めてづくしですが、楽しみたいなと。
会社に感謝です。ESのトレーニングを受けに来たのがメインなのです。
ついでに、Berlinでの<a href="http://gotocon.com/berlin-2014/freeevent/index.jsp?eventOID=6151">勉強会</a>にも参加してみようかなと。</p>
<p>幸いにも時差ボケはなさそうなので、色々と見て回ろうと思います。
ただ、日曜日はお店が軒並みしまってそう。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>いつも入れているElasticsearchのプラグイン</title>
      <link>https://blog.johtani.info/blog/2014/03/11/es-plugin-installed-to-my-env/</link>
      <pubDate>Tue, 11 Mar 2014 14:23:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/03/11/es-plugin-installed-to-my-env/</guid>
      <description>elasticsearchに、このへん入れるときっと幸せになれるはず・たぶん。&amp;#10;elasticsearch/elasticsearc</description>
      <content:encoded><blockquote class="twitter-tweet" lang="ja"><p>elasticsearchに、このへん入れるときっと幸せになれるはず・たぶん。&#10;elasticsearch/elasticsearch-analysis-kuromoji/1.6.0&#10;oyrusso/elasticsearch-HQ&#10;mobz/elasticsearch-head</p>&mdash; toshi_miura (@toshi_miura) <a href="https://twitter.com/toshi_miura/statuses/441230280041304066">2014, 3月 5</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>こんなツイートを見かけたので、普段入れてるプラグインを簡単に紹介してみようかと。</p>
<!-- more -->
<p>ローカルの環境に普段入れているプラグインの紹介です。
ちゃんとクラスタを管理しているというよりは、最新版の動作などを確認するための環境になります。なので、ちょっと視点が異なるかもしれませんが参考になればと。</p>
<h2 id="elasticsearch-analysis-kuromoji">elasticsearch-analysis-kuromoji</h2>
<p>URL : <a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">elasticsearch-analysis-kuromoji</a></p>
<p>Kuromojiという日本語形態素解析のTokenizerなどを使えるようにするためのプラグインです。
今度、発売される<a href="http://www.amazon.co.jp/dp/4048662023?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=4048662023&amp;adid=072DC31D3GTPZCBQ6TYW&amp;&amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2Fblog%2F2014%2F03%2F03%2Frelease-elasticsearch-server-japanese-edition%2F">「ElasticSearch Server」日本語版</a>には付録として、利用方法を執筆しました。参考にしていただければと。
READMEにもサンプルは掲載されてるので、こちらを参考にするのもありですが。</p>
<h2 id="elasticsearch-extended-analyze">elasticsearch-extended-analyze</h2>
<p>URL : <a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analyze</a></p>
<p>私が開発しているプラグインです。
Elasticsearchには<code>analyze</code>というAPIが用意されています。
文章を渡すと指定した<code>analyzer</code>などでどのような単語に区切られるかがわかるAPIです。</p>
<p>ただ、<code>analyzer</code>の内部では<code>char filter</code>、<code>tokenizer</code>、<code>token filter</code>という個別のパーツがそれぞれ入力された文字列に対して処理を実施します。
この過程が<code>analyze</code> APIではわかりません。
それをわかるようにしてみたのが<code>elasticsearch-extended-analyze</code>プラグインになります。</p>
<p>詳細については<a href="http://blog.johtani.info/blog/2013/10/25/developing-es-extended-analyze-plugin/">過去の記事</a>を見ていただければと。
画面があると便利だよなぁと思いつつ、作ってない。。。</p>
<h2 id="polyfractalelasticsearch-inquisitor">polyfractal/elasticsearch-inquisitor</h2>
<p>URL : <a href="https://github.com/polyfractal/elasticsearch-inquisitor">elasticsearch-inquisitor</a></p>
<p>クエリのデバッグとかに便利なプラグイン。</p>
<p>こちらも詳細は<a href="http://blog.johtani.info/blog/2013/09/23/intro-elasticsearch-inquisitor/">過去の記事</a>を見ていただければと。</p>
<h2 id="mobzelasticsearch-head">mobz/elasticsearch-head</h2>
<p>URL : <a href="http://mobz.github.io/elasticsearch-head/">elasticsearch-head</a></p>
<p>クラスタ管理に便利なプラグインです。クラスタに存在するノードに対してインデックスのデータ（シャード）がどこに配置されているかなどが一目瞭然になる便利なプラグインです。
プライマリシャードやレプリカなどもわかります。
インデックスの削除もできるし、クエリを投げることもできるし、全部入りな感じのプラグインです。</p>
<p>私個人は、シャードの配置を見るのに主に利用しています。クエリを投げたりインデックスを消したりするのには殆ど使っていません。</p>
<h2 id="royrussoelasticsearch-hq">royrusso/elasticsearch-HQ</h2>
<p>URL : <a href="https://github.com/royrusso/elasticsearch-HQ">elasticsearch-HQ</a></p>
<p>これも管理系のプラグインです。こっちのほうが個人的にスッキリしていて好きなプラグインです。
インデックスの管理やノードの停止などはこちらを主に使用しています。
あくまでもローカルの簡易クラスタを管理する目的というのもあります。</p>
<h2 id="polyfractalelasticsearch-segmentspy">polyfractal/elasticsearch-segmentspy</h2>
<p>URL : <a href="https://github.com/polyfractal/elasticsearch-segmentspy">elasticsearch-segmentspy</a></p>
<p>こちらはモニタリングでしょうか。
ElasticSearch Serverで紹介されていたのが主な理由で、入れてますがあんまり見てないかも。
インデックスのSegment単位の情報が見ることが可能です。
あと、ちょっと更新されてない感じがしますね。</p>
<h2 id="elasticsearchmarvel">elasticsearch/marvel</h2>
<p>Elasticsearch社から提供されている、モニタリングなどに使えるプラグインです。
開発環境では無償提供という感じです。
渡しの場合、モニタリング目的ではなく、senseと呼ばれるクエリの補完をしてくれるツールの目的のために使用しています。
モニタリング部分を停止する方法とかないかなぁ。</p>
<p>詳細については<a href="http://blog.johtani.info/blog/2014/01/29/simple-introduction-and-first-impression-es-marvel/">過去の記事</a>を参考にしていただければと。</p>
<h2 id="まとめ">まとめ？</h2>
<p>ということで、簡単にローカルに入っているプラグインの紹介でした。
他にもいっぱいあるので、おすすめがあれば、教えてもらえると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ElasticSearch Serverを翻訳しました</title>
      <link>https://blog.johtani.info/blog/2014/03/03/release-elasticsearch-server-japanese-edition/</link>
      <pubDate>Mon, 03 Mar 2014 17:55:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/03/03/release-elasticsearch-server-japanese-edition/</guid>
      <description>第3回Elasticsearch勉強会で、軽く触れていましたが、ElasticSearch Server日本語版が発売されます。 ツイートなども</description>
      <content:encoded><p>第3回Elasticsearch勉強会で、軽く触れていましたが、<a href="http://www.amazon.co.jp/dp/4048662023?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=4048662023&amp;adid=1X58V7098G3T1N2ZTW61&amp;&amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2F">ElasticSearch Server日本語版</a>が発売されます。
ツイートなどもちらほらとして頂いているみたいで嬉しい限りです。</p>
<p>本書は、私自身、初の翻訳本となります。</p>
<p><strong>なお、ElasticSearchはAWSのサービスではなく、全文検索・解析サーバのOSSです</strong></p>
<!-- more -->
<h2 id="内容概要">内容、概要</h2>
<p>PacktPublishingから発売されている<a href="http://www.packtpub.com/elasticsearch-server-for-fast-scalable-flexible-search-solution/book">ElasticSearch Server</a>の日本語版となります。
以下の点が、原著とは異なる点になっています。</p>
<ul>
<li>0.90.xに対応（原著は0.20）</li>
<li>Kibana、Kuromojiに関して追記</li>
<li>もちろん日本語</li>
</ul>
<p>残念ながら、つい最近、Elasticsearchについては1.0がリリースされました。
1.0で追加された機能（SnapshotやRestore、Aggregatorなど）については触れていませんが、Elasticsearchの機能を網羅的にカバーした良書となっています。
どんな機能があるのか、どんなプラグインがあるのか、どういったことに使えるのかなど、幅広くまとめられた本になっていますので、
Elasticsearchに興味がある方はぜひ読んでいただければと思います。</p>
<p>また、現段階では予定ですが電子版の出版も予定されています。電子版が気になる方は、少しお待ちいただければと。</p>
<h4 id="elasticsearchelasticsearch">ElasticSearch？Elasticsearch？</h4>
<p>1.0.0がリリースされた現在は、Elasticsearch（SearchのSは小文字）が正式な名称となっています。
ただ、原著が発売された当初（2013年2月時点）では、まだSは小文字と大文字が混在した状況でした（コミットログなどを見るとわかります。）
このため、日本語版でもElasticSearchという表記に統一してあります。</p>
<h2 id="翻訳に関して">翻訳に関して</h2>
<p>初の翻訳書ということもあり、大変でした。英語に精通しているわけではないので（むしろ苦手）。。。
他の翻訳者の方々には大変助けていただきましたし、勉強になりました。
また、監修社である<a href="http://recruit-tech.co.jp">リクルートテクノロジーズ</a>にも色々とサポートしていただき、感謝の限りです。
（Elasticsearch勉強会の開場提供にも協力して頂いています。）</p>
<p>わかりにくい日本語となっている部分などありましたら、ご指摘いただければ今後の参考にさせていただきます。
英語やElasticsearchについて、学ぶという目的もあって、本書の翻訳を買って出たのが本音です。</p>
<h3 id="翻訳作業について">翻訳作業について</h3>
<p>Githubのリポジトリを編集の方に用意してもらい、翻訳原稿を管理、校正していきました。
Github自体をあまり触っていなかったので、作業をしながらGithubも覚えられ一石二鳥でした。
Issueやプルリクエストによる校正、チェックも便利ですね。
他の原稿を書くようなことがあれば、またこの経験を活かしていきたいなと。
（翻訳の進め方や原稿のチェックなどについてはまた後日何か書こうかと。）</p>
<h3 id="原著について">原著について</h3>
<p>原著のサイトが用意されています。
<a href="http://elasticsearchserverbook.com/elasticsearch-server-errata/">http://elasticsearchserverbook.com/elasticsearch-server-errata/</a></p>
<p>原著を翻訳するにあたって見つけた、誤植などを報告し、掲載して頂いています。
原著をお持ちの場合はこちらも参考にしていただければと思います。</p>
<h3 id="ご購入はこちらから">ご購入はこちらから</h3>
<p>ということで、簡単ですが書籍の紹介（というより宣伝！？）でした。
Elasticsearchに関する何かしらの助けになる書籍であれば嬉しい限りです。</p>
<p>「ElasticSearch Server日本語版」をよろしくお願いします。
（もちろん、購入は下のリンクからですよね！）</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=4048662023&nou=1&ref=qf_sp_asin_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
</content:encoded>
    </item>
    
    <item>
      <title>Nested Objectのフィールドの奇妙な動作</title>
      <link>https://blog.johtani.info/blog/2014/02/24/strange-behavior-of-field-in-nested-obj/</link>
      <pubDate>Mon, 24 Feb 2014 17:52:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/02/24/strange-behavior-of-field-in-nested-obj/</guid>
      <description>今年初の「突撃！隣のElasticsearch」ということで、Wantedlyさんにおじゃましました。 ※写真を自分でも撮ったのですが、画像が</description>
      <content:encoded><p>今年初の「突撃！隣のElasticsearch」ということで、Wantedlyさんにおじゃましました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140224/wantedly.jpg" />
    </div>
    <a href="/images/entries/20140224/wantedly.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>※写真を自分でも撮ったのですが、画像が壊れてたので、一緒に行ったペンギン先生の写真を拝借しました。</p>
<p>第3回のElasticsearch勉強会を開催中にES使ってるってツイートを見つけたので、アタックかけて遊びに行きました。
交渉に快諾いただきありがとうございました！</p>
<!-- more -->
<p>WantedlyさんがどのようにElasticsearchを使用されているかは<strike><strong>きっと、ブログを書いてくれる</strong>と思うので期待しておくとして、</strike>書いてくれました！！ <a href="http://engineer.wantedly.com/2014/02/25/elasticsearch-at-wantedly-1.html">「実践！Elasticsearch」</a>
そこで、<code>nested</code>でハイライトがなんかうまくいかないって話があったので、ちょっと調べてみました。
<em>（※まだ、調査中です）</em></p>
<h2 id="前提条件">前提条件</h2>
<p>再現する手順はgistにあります。（Senseに貼り付ければ動作します。ただし、elasticsearch-analysis-kuromojiが必要です。）https://gist.github.com/johtani/9184287</p>
<p><strong>なお、このマッピングやデータはWantedlyさんとは全く関係ありません。</strong></p>
<p><code>nested</code>フィールド内部のデータに対して、検索しハイライトしようとするとうまく動作しないという状況です。
マッピングは以下のとおり。</p>
<pre><code>  &quot;books&quot; : {
    &quot;properties&quot;: {
      &quot;book&quot; : {
        &quot;type&quot;: &quot;nested&quot;,
        &quot;properties&quot;: {
          &quot;title&quot; : { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;kuromoji&quot;, &quot;store&quot;: &quot;no&quot;},
          &quot;contents&quot; : {&quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;kuromoji&quot;, &quot;store&quot;: &quot;yes&quot;}
        }
      }
    }
  }
}
</code></pre><p>このマッピングの特徴は以下のとおり。</p>
<ul>
<li><code>_source</code>は保存される（デフォルト値）</li>
<li><code>book</code>が<code>nested</code>なオブジェクト</li>
<li><code>title</code>は<code>store : no</code></li>
<li><code>contents</code>は<code>store : yes</code></li>
</ul>
<p>動作の挙動をわかりやすくするため、<code>title</code>と<code>contents</code>の<code>store</code>属性に違いを持たせてあります。</p>
<h2 id="問題点">問題点</h2>
<p><code>nested</code>クエリを使って、検索した時にハイライトが返ってきません。
次のクエリを実行するとわかります。</p>
<pre><code>
GET /bookstore/books/_search
{
  &quot;_source&quot; : [&quot;book.title&quot;,&quot;book.contents&quot;],
  &quot;fields&quot;: [
    &quot;book.title&quot;,
    &quot;book.contents&quot;
  ], 
  &quot;query&quot;: {
    &quot;nested&quot;: {
      &quot;path&quot;: &quot;book&quot;,
      &quot;query&quot;: {
        &quot;query_string&quot; : {
          &quot;query&quot; : &quot;Solr&quot;,
          &quot;fields&quot; : [&quot;book.title&quot;, &quot;book.contents&quot;]
        }
      }
    }
  },
  &quot;highlight&quot;: {
    &quot;pre_tags&quot;: [&quot;&lt;b&gt;&quot;], 
    &quot;post_tags&quot;: [&quot;&lt;/b&gt;&quot;],
    &quot;fields&quot;: {
      &quot;*&quot;: {}
    }
  }
}
</code></pre><p>結果はこちら。
ハイライトがありません。</p>
<pre><code>{
   &quot;took&quot;: 3,
   &quot;timed_out&quot;: false,
   &quot;_shards&quot;: {
      &quot;total&quot;: 5,
      &quot;successful&quot;: 5,
      &quot;failed&quot;: 0
   },
   &quot;hits&quot;: {
      &quot;total&quot;: 1,
      &quot;max_score&quot;: 0.5,
      &quot;hits&quot;: [
         {
            &quot;_index&quot;: &quot;bookstore&quot;,
            &quot;_type&quot;: &quot;books&quot;,
            &quot;_id&quot;: &quot;1&quot;,
            &quot;_score&quot;: 0.5,
            &quot;_source&quot;: {
               &quot;book&quot;: {
                  &quot;title&quot;: &quot;Apache Solr入門&quot;,
                  &quot;contents&quot;: &quot;Apache Solrについて日本語で書かれた唯一の書籍です。SolrはLuceneをコアにした検索サーバです。&quot;
               }
            }
         }
      ]
   }
}
</code></pre><p>次に、ハイライトが帰ってくるパターン。
<code>nested</code>クエリではなく、<code>_all</code>を対象としたクエリを投げます。</p>
<pre><code>{
  &quot;_source&quot; : [&quot;book.title&quot;,&quot;book.contents&quot;],
  &quot;fields&quot;: [
    &quot;book.title&quot;,
    &quot;book.contents&quot;
  ], 
  &quot;query&quot;: {
    &quot;query_string&quot; : {
      &quot;query&quot; : &quot;Solr&quot;,
      &quot;fields&quot;: [
        &quot;_all&quot;
      ]
    }
  },
  &quot;highlight&quot;: {
    &quot;pre_tags&quot;: [&quot;&lt;b&gt;&quot;], 
    &quot;post_tags&quot;: [&quot;&lt;/b&gt;&quot;], 
    &quot;fields&quot;: {
      &quot;book.title&quot; : {},
      &quot;book.contents&quot;: {}
    }
  }
}
</code></pre><p>この場合の結果は次の通り。</p>
<pre><code>{
   &quot;took&quot;: 2,
   &quot;timed_out&quot;: false,
   &quot;_shards&quot;: {
      &quot;total&quot;: 5,
      &quot;successful&quot;: 5,
      &quot;failed&quot;: 0
   },
   &quot;hits&quot;: {
      &quot;total&quot;: 1,
      &quot;max_score&quot;: 0.27063292,
      &quot;hits&quot;: [
         {
            &quot;_index&quot;: &quot;bookstore&quot;,
            &quot;_type&quot;: &quot;books&quot;,
            &quot;_id&quot;: &quot;1&quot;,
            &quot;_score&quot;: 0.27063292,
            &quot;_source&quot;: {
               &quot;book&quot;: {
                  &quot;title&quot;: &quot;Apache Solr入門&quot;,
                  &quot;contents&quot;: &quot;Apache Solrについて日本語で書かれた唯一の書籍です。SolrはLuceneをコアにした検索サーバです。&quot;
               }
            },
            &quot;highlight&quot;: {
               &quot;book.title&quot;: [
                  &quot;Apache &lt;b&gt;Solr&lt;/b&gt;入門&quot;
               ]
            }
         }
      ]
   }
}
</code></pre><p>ハイライトが返ってきています。</p>
<h2 id="考察原因は未特定">考察（原因は未特定）</h2>
<p>残念ながら、まだ調査してません。
まずは、現象が理解できたというだけです。
問題点が実は２つありそうです。</p>
<h3 id="問題点１nestedクエリの場合にハイライトされない">問題点１：<code>nested</code>クエリの場合に、ハイライトされない。</h3>
<p><code>nested</code>クエリではハイライトが動作していないようです。
想像ですが、検索に利用されたクエリで指定されているフィールドをハイライタ（ハイライトを実行するモジュール）が認識できてないのではないかと。
なぜ認識できていないのかという点を調査する必要がありそうです。</p>
<h4 id="考察試してみたパターン">考察（試してみたパターン）</h4>
<p><code>nested</code>ではないクエリで、ハイライトが動作しているのですが、<code>      &quot;book.title&quot; : {&quot;require_field_match&quot; : true},</code>にした場合は、ハイライトが返ってこないです。
このオプションは、検索対象のフィールドでマッチした文字列だけがハイライトされるオプションになります。
したがって、<code>book.title</code>フィールドに対する検索で<code>Solr</code>という文字を検索していないことになります。
<code>_all</code>に対するクエリであるためです。
このため、例えば、<code>title</code>だけを検索対象にしたのに、<code>contents</code>に<code>Solr</code>という文字が入っていてもハイライトされてしまうという状況が発生します。</p>
<h3 id="問題点２-store--yesのデータがハイライトできない">問題点２ <code>store : yes</code>のデータがハイライトできない。</h3>
<p>GithubにIssueをあげました。https://github.com/elasticsearch/elasticsearch/issues/5245 （2014/02/25追記）</p>
<p><code>nested</code>オブジェクトにあるデータのうち、<code>store : no</code>のものだけがハイライト結果として返ってきました。</p>
<h4 id="考察">考察</h4>
<p>なぜ、<code>store : yes</code>のデータがハイライトされないかを調べるために、<code>fields</code>パラメータをリクエストに追加してみました。</p>
<pre><code>{
  &quot;_source&quot; : [&quot;book.title&quot;,&quot;book.contents&quot;],
  &quot;fields&quot;: [
    &quot;book.title&quot;,
    &quot;book.contents&quot;
  ], 
...
}
</code></pre><p>すると、<code>fields</code>の戻り値は次のとおりです。</p>
<pre><code>...
            &quot;fields&quot;: {
               &quot;book.title&quot;: [
                  &quot;Apache Solr入門&quot;
               ]
            },
...
</code></pre><p>このことから、<code>store : no</code>のデータの場合、<code>_source</code>から値を取得して返却しているというのがわかります。
ハイライトがされない原因も、<code>fields</code>で値が取れていないのも同じ原因であると思われます。
なぜなら、ハイライトは、保存された文字列を内部で取り出し利用して、ハイライトタグを埋め込むという動作をするためです。</p>
<h2 id="参考">参考？</h2>
<p>これらの問題点についてですが、次のIssueが関係あるかもしれません。</p>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/3022">Return matching nested inner objects per hit #3022</a></p>
<h2 id="今後">今後？</h2>
<p>残念ながら、現時点では、問題点がどんなものかというのを理解しただけとなります。
デバッグしたりソースを追っかけたりして何が問題なのかを調べて行ってみようかなぁと。</p>
<p>なにか、気づいたことなどあればコメントしてもらえると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第3回elasticsearch勉強会を開催しました！ #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2014/02/08/hold-3rd-elasticsaerch-meetup-in-tokyo/</link>
      <pubDate>Sat, 08 Feb 2014 00:42:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/02/08/hold-3rd-elasticsaerch-meetup-in-tokyo/</guid>
      <description>今回はたまたま日本にいたElasticsearchの人をスペシャルゲストに呼べたので、大満足ですｗ 英語の通訳とかちゃんと勉強しないとなぁ。。</description>
      <content:encoded><p>今回はたまたま日本にいたElasticsearchの人をスペシャルゲストに呼べたので、大満足ですｗ
英語の通訳とかちゃんと勉強しないとなぁ。。。</p>
<p>とりあえず、てきとーなメモですが、残しておきます。
参加者数は130人＋スタッフ＋リクルートテクノロジーズ社内の人。という感じでした。アンケート集計はもう少々おまちを。</p>
<!-- more -->
<p>スライドがそろったら、また、更新すると思いますが、第一報という感じで公開しておきます。
懇親会にも50名も参加していただけて、非常に楽しかったです。
話ができてない方が多数いるかもしれませんが、次回以降、声をかけていただければと。
（物覚え悪いんで、あれですが。。。）
盛り上がってきてて楽しいなぁ。
スタッフの人達の練度も上がってきてるので、すごく楽ができてます。</p>
<p>至らない点とかあれば、こちらにコメントしてもらったりしていただければと。</p>
<h2 id="geohashing-with-elasticsearch">Geohashing with Elasticsearch</h2>
<h3 id="florian-schilling-elasticsearch-inc">Florian Schilling, Elasticsearch Inc,</h3>
<p>スライド：https://speakerdeck.com/chilling/tokyo-es-study-session-iii-geohashes</p>
<ul>
<li>自己紹介
<ul>
<li>Geoのスタッフ</li>
</ul>
</li>
<li>Elsticsearchの概要
<ul>
<li>転置イデックスやREST APIなどの説明
マイクの調子が良くなくて申し訳なかったっす。。。</li>
</ul>
</li>
</ul>
<p>平賀さん、通訳ありがとう！
Solr本もよろしくお願いします！！</p>
<h2 id="awsで構築するsharding">AWSで構築するsharding</h2>
<h3 id="株式会社イプロス外山寛さんtoyama0919">株式会社イプロス　外山　寛さん　@toyama0919</h3>
<p>スライド：http://toyama0919.bitbucket.org/elasticsearch.html</p>
<ul>
<li>AWS対応の話</li>
<li>ルーティングが重要だよ。（宣伝ありがとうございますｗ）</li>
<li>type指定しないとルーティングできない。（内部でtypeも使ってハッシュ値取ってたかなぁ？）</li>
<li>苦労話とかいくつか。</li>
<li>tireはre-tire&hellip;</li>
</ul>
<h2 id="実サービスでのelasticsearch設定使用例仮">実サービスでのElasticsearch設定・使用例（仮）</h2>
<h3 id="株式会社じげん多田-雅斗さんtady_jp">株式会社じげん　多田 雅斗さん　@tady_jp</h3>
<p>スライド：https://speakerdeck.com/tadyjp/tesutoqu-dong-jian-suo-falsesusume-at-tady-jp</p>
<ul>
<li>検索とは的な話がわかりやすい。</li>
<li>全文検索のお話。ログ検索じゃないよと。</li>
<li>書籍ないですよねー（ふふふ）</li>
<li>specで検索条件記述しといて、ってのいいですよね。絶対必要だと思う</li>
</ul>
<p>Mapping変更した時にテストやり直す方法とかどうしてますか？
特にフレームワークは使ってないです。</p>
<h2 id="mysqlユーザ視点での小さく始めるelasticsearch">MySQLユーザ視点での、小さく始めるElasticsearch</h2>
<h3 id="株式会社リブセンス-吉田-健太郎さん-yoshi_ken">株式会社リブセンス 吉田 健太郎さん @yoshi_ken</h3>
<p>スライド：http://www.slideshare.net/y-ken/introducing-elasticsearch-for-mysql-users</p>
<ul>
<li>やっぱりkuromoji便利だよね</li>
<li>MySQLとかと連携したい。</li>
<li>river-pluginもいまいち安定しない</li>
<li>なので、<a href="https://github.com/y-ken/yamabiko">Yamabiko</a>作ってみました。</li>
<li>Geo検索とKuromojiの話をしてくれました。（作者とか開発者がいるってのを狙ってたのかすごいなぁ。）</li>
<li>Mappingとかはちゃんと指定したほうがいろいろいいですよ。</li>
</ul>
<h2 id="nodejsdynamodbelasticsearchで全社基盤を作った話">nodeJS+DynamoDB＋Elasticsearchで全社基盤を作った話</h2>
<h3 id="株式会社リクルートテクノロジーズ-相野谷-直樹さん-naokiainoya">株式会社リクルートテクノロジーズ 相野谷 直樹さん @naokiainoya</h3>
<p>スライド：http://www.slideshare.net/recruitcojp/elasticsearchnodejsdynamodb-7</p>
<ul>
<li>ちょっと変わった使い方のElasticsearchで面白いです。</li>
<li>Scroll/Scanについては、Solrでもない機能なので、そういう意味でもElasticsearchなのかもしれないですね。</li>
</ul>
<h2 id="参加していただいた方々のブログ">参加していただいた方々のブログ</h2>
<ul>
<li>
<p>第3回elasticsearch勉強会 [2014/02/07(Fri.)]に参加してきました - ほわいとぼーど<br>
<a href="http://a3no.hatenablog.com/entry/2014/02/09/022405">http://a3no.hatenablog.com/entry/2014/02/09/022405</a></p>
</li>
<li>
<p>第 3 回 elasticsearch 勉強会に行ってきた - ようへいの日々精進<br>
<a href="http://inokara.hateblo.jp/entry/2014/02/07/233057">http://inokara.hateblo.jp/entry/2014/02/07/233057</a></p>
</li>
<li>
<p><a href="http://www.smokeymonkey.net/2014/02/3elasticsearch.html">http://www.smokeymonkey.net/2014/02/3elasticsearch.html</a></p>
</li>
<li>
<p>第3回elasticsearch勉強会でトークしました #elasticsearchjp<br>
<a href="http://y-ken.hatenablog.com/entry/elasticsearch-meetup-vol3">http://y-ken.hatenablog.com/entry/elasticsearch-meetup-vol3</a></p>
</li>
<li>
<p>第3回elasticsearch勉強会にいってきました #elasticsearchjp<br>
<a href="http://blog.livedoor.jp/ashibuya0128/archives/52058766.html">http://blog.livedoor.jp/ashibuya0128/archives/52058766.html</a></p>
</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>すずけんさんのメモを元にVagrantでElasticsearchクラスタを起動してみた</title>
      <link>https://blog.johtani.info/blog/2014/02/06/es-cluster-start-using-vagrant-and-puppet/</link>
      <pubDate>Thu, 06 Feb 2014 23:09:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/02/06/es-cluster-start-using-vagrant-and-puppet/</guid>
      <description>すずけんさんがVagrant+puppet使って、VM起動してElasticsearchのクラスタを組んでる記事を書いているのを見て、試して</description>
      <content:encoded><p>すずけんさんがVagrant+puppet使って、VM起動してElasticsearchのクラスタを組んでる記事を書いているのを見て、試してみたくなりました。
ということで、VagrantとかPuppetなに？くらいの私ですが、クラスタを起動するところまで行ったので、その時のメモを残しておきます。</p>
<!-- more -->
<h2 id="元記事とか参考">元記事とか参考</h2>
<ul>
<li><a href="http://suzuken.hatenablog.jp/entry/2014/02/04/215959">Vagrant環境にpuppet moduleを利用してさくっとelasticsearchをインストールする</a></li>
<li><a href="http://suzuken.hatenablog.jp/entry/2014/02/05/232543">Vagrant環境にpuppetを利用してさくっとelasticsearchのclusterを作成する</a></li>
<li><a href="https://github.com/elasticsearch/puppet-elasticsearch">puppet-elasticsearch</a></li>
</ul>
<h2 id="なんとなくの理解">なんとなくの理解</h2>
<p>VagrantやPuppetについては、何度か勉強会で話を聞いてはいたのですが、
想像していたレベルだったので良い機会でした。
今のところの認識はこんな感じです。</p>
<h3 id="vagrant">Vagrant</h3>
<p>VMを起動したり、VM周りの設定をあれこれできるツール。
VMのネットワーク設定や、インスタンス名？などを指定できる。</p>
<h3 id="puppet">Puppet</h3>
<p>起動後のVM（VMとは限らないか。）のゲストOS側の設定周りやアプリのインストールなどを
実行できるツール。</p>
<h2 id="詰まった箇所">詰まった箇所</h2>
<p>すずけんさんのブログを元に作業をしましたが、自分がVagrantやPuppetに疎いため、以下の部分で躓いたので、備忘録のために残しておきました。</p>
<h3 id="その１puppetのファイルの場所">その１：Puppetのファイルの場所</h3>
<p><code>search01.vm.local</code>のVMを設定（というか、elasticsearchのインストール？）するときに、<code>manifests/search.app</code>と<code>roles/search/manifests/init.pp</code>ファイルが必要で作成します。</p>
<p>このファイルの配置場所は<code>/vagrant</code>配下に作成する必要がありました。
<code>ssh search01.vm.local</code>でVMにログインした場合は<code>/home/vagrant</code>にログインしており、この場所でファイルを作ってもPuppetがエラーを吐いたためです。</p>
<p><strong>と思ったのですが</strong>、あれ？これひょっとしてVagrantfileがあるところにディレクトリとファイル作ると勝手にVMにコピーしてくれるんですか？destroyして、upしたら、ファイルが勝手にコピーされてる。ひょっとして、<code>/vagrant</code>ってディレクトリはVagrantfileがあるディレクトリを共有してたりするのかな？そのうち、Vagrantについても調べてみようかな。</p>
<h3 id="その２ネットワーク周り">その２：ネットワーク周り</h3>
<pre><code>curl http://192.168.10.114:9200/
</code></pre><p>をホストOSから実行してみましたがうまく行きませんでした。。。
ネットワーク周りの設定だと思うんですが。
少なくとも「sshによるログイン」「ping」コマンドの応答は返ってきてます。</p>
<p>また、VM内でcurlコマンドを実行したらレスポンスが返ってきました。</p>
<p>なんで？ってツイートしたら各所から「iptables」という単語が飛んできて、
service止めたら大正解でした。まぁ、そうですよね。基本ですよね。。。</p>
<p>ということで、Puppetがよくわかっていませんが、ググって変更してみました。</p>
<p><code>manifests/search.app</code>に以下を追加</p>
<pre><code>include iptables
</code></pre><p><code>roles/iptables/manifests/init.pp</code></p>
<pre><code>class iptables {
  service { 'iptables':
    enable =&gt; false,
    ensure =&gt; stopped,
  }
}
</code></pre><p>iptablesを停止するmanifests？です（良くないことなんですが、よくわかってない）。</p>
<p>ということで、ローカルで1個のVM起動して、elasticsearchにアクセスできることは確認できました。</p>
<p>と、書いてるそばから、元記事が修正されてしまいましたｗ</p>
<h2 id="クラスタ編変更点">クラスタ編（変更点）</h2>
<p>クラスタを組むときに、追加でプラグインを入れたので<code>roles/search/manifests/init.pp</code>は次のようにしました。</p>
<pre><code>class search {
  class { 'elasticsearch':
    package_url =&gt; 'https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.0.0.RC2.noarch.rpm',
    java_install =&gt; true,
    config =&gt; {
      'cluster' =&gt; {
        'name' =&gt; 'test-es-cluster'
      },
      'network.host' =&gt; '_eth1:ipv4_',,
      'marvel.agent.exporter.es.hosts' =&gt; ['192.168.10.114:9200','192.168.10.115:9200']
    }
  }

  elasticsearch::plugin{'elasticsearch/marvel/latest':
    module_dir =&gt; 'marvel'
  }

  elasticsearch::plugin{'mobz/elasticsearch-head':
    module_dir =&gt; 'head'
  }

  elasticsearch::plugin{'royrusso/elasticsearch-HQ':
    module_dir =&gt; 'HQ'
  }

  elasticsearch::plugin{'elasticsearch/elasticsearch-analysis-kuromoji/2.0.0.RC1':
    module_dir =&gt; 'analysis-kuromoji'
  }

  elasticsearch::plugin{'info.johtani/elasticsearch-extended-analyze/1.0.0.RC1':
    module_dir =&gt; 'extended-analyze'
  }

  elasticsearch::plugin{'polyfractal/elasticsearch-inquisitor':
    module_dir =&gt; 'inquisitor'
  }
}
</code></pre><p>とりあえず、今日はクラスタ組んでMarvelやプラグインの動作確認でおしまいです。</p>
<h2 id="疑問点">疑問点</h2>
<p>いくつか疑問点が。試してみてもないんでなんとも言えませんが。気が向いたら、調べて追記するかも。</p>
<ul>
<li><code>:private_network</code>はVirtualBox内で完結する（Macから外には影響しない）ネットワークが構築される？たぶん、VagrantというよりはVM、仮想化周りの知識なんだろうけど</li>
<li>どこから再開可能？elasticsearch.ymlの設定を書き換えた場合に、最後のコマンドだけ実行するとちゃんとやりなしてくれたりするのかな？</li>
<li>VMのディスク増やすのもVagrantでできるんかな？まぁ、できると思うけど。</li>
<li><code>:forwarded_port</code>のauto_correctとかわかってない。</li>
<li>JVMをSunのJVMでかつ、7u25に変更したいのだがどうしたものか？(現時点での推奨バージョン)</li>
</ul>
<h2 id="感想">感想</h2>
<p>Vagrantって便利ですね。あれ？って思ったら、destroyして、やり直すのがすごく簡単です。
元記事があるので、なんとなくですが、構成とかどうすればいいかがわかるのは本当に助かりました。
これで、あれこれと検証する環境が簡単に構築できることがわかったので、色々と楽できるかも。ありがとうございます、すずけんさん！</p>
</content:encoded>
    </item>
    
    <item>
      <title>第13回Solr勉強会を開催しました</title>
      <link>https://blog.johtani.info/blog/2014/01/29/hold-to-japan-solr-meetup/</link>
      <pubDate>Wed, 29 Jan 2014 18:46:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/29/hold-to-japan-solr-meetup/</guid>
      <description>改訂新版Solr入門出版記念ということで、第13回Solr勉強会 #SolrJP 新Solr本出版記念を開催しました。 出版記念なので、技術評論社様より、プレ</description>
      <content:encoded><p>改訂新版Solr入門出版記念ということで、<a href="http://solr.doorkeeper.jp/events/7260">第13回Solr勉強会 #SolrJP 新Solr本出版記念</a>を開催しました。</p>
<p>出版記念なので、技術評論社様より、プレゼント用にSolr本を用意していただきました！ありがとうございます！！
書籍をゲット出来た方は、ツイートしたりブログ書いたり書評書いたりして、宣伝してください！！！</p>
<p>今回は、私は手を抜いて他の人に喋ってもらいました！</p>
<!-- more -->
<p>今回は、著者陣（関口さんは特別ゲスト）でスピーカーを固めてみました。
以下は、いつもの簡単なメモです。
スライドが集まったらまた更新していきます。</p>
<h2 id="1-はじめての検索エンジンsolr-株式会社nttデータccs鈴木-教嗣さん">1. 「はじめての検索エンジン＆Solr」 株式会社NTTデータCCS　鈴木 教嗣さん</h2>
<p>スライド：<a href="http://www.slideshare.net/suzu2525/solr-13">はじめての検索エンジン＆Solr 第13回Solr勉強会</a></p>
<p>鈴木さんの発表初めて聞きましたｗ。
趣味が多いなぁ。
ちょこちょこと、宣伝を入れてるのが流石ですｗ</p>
<ul>
<li>入門らしい概要</li>
<li>クエリの概要とかも。</li>
<li>スコア計算とか</li>
<li>導入するとうれしいところとか</li>
<li>Solr盛り上げましょう！</li>
</ul>
<h2 id="2-solr-searchcomponent-再訪-株式会社ロンウイット関口-宏司さん">2. 「Solr SearchComponent 再訪」 株式会社ロンウイット　関口 宏司さん</h2>
<p>スライド：公開待ち</p>
<ul>
<li>ベン図で検索の評価指標の説明</li>
<li>理論的なお話</li>
<li>Solrのサーチコンポーネントを使って何ができるか。ベン図で。</li>
<li>サーチコンポーネント以外にも
<ul>
<li>NGramTokenizerも</li>
<li>SynonymFilterも</li>
<li>パーソナライズ検索</li>
</ul>
</li>
</ul>
<p>いきなり話をふられたのでちょっとびっくりしましたｗ</p>
<h2 id="3-自動補完autocompleteともしかしてdid-you-mean-株式会社-ロンウイット大須賀-稔さん">3. 「自動補完(Autocomplete)ともしかして？(Did You Mean?)」 株式会社 ロンウイット　大須賀 稔さん</h2>
<p>スライド：<a href="http://www.slideshare.net/mosuka/solr-autocomplete-and-did-you-mean">Solr AutoComplete and Did You Mean?</a><br>
デモ：https://github.com/mosuka/solr-suggester-demo-ui</p>
<ul>
<li>職歴が相変わらずおもしろい</li>
<li>編集距離のお話</li>
<li>素晴らしいCM！</li>
</ul>
<p>候補のランキングを変更できる？
SpellcheckComponentのパラメータで指定できるものなら楽ですが。。。</p>
<h2 id="4-lucene-revolution-2013-dublin振り返り-楽天株式会社平賀-一昭さん">4. 「Lucene Revolution 2013 Dublin振り返り」 楽天株式会社　平賀 一昭さん</h2>
<p>スライド：公開待ち</p>
<ul>
<li>ダブリンどこ？（間違ってベルリンって言っちゃいましたｗ）</li>
<li>スタジアムで開催。グランドにも入れるのかなぁ？</li>
<li>まずはTwitter
<ul>
<li>Luceneの改良版</li>
<li>ちょっと特殊。１４０文字とか</li>
</ul>
</li>
<li>青いRさんのライバル。Careerbuilder
<ul>
<li>元FASTユーザ</li>
<li>企業向けに検索キーワードとかの解析画面を用意</li>
<li>検索精度の改良の話とか</li>
<li>転職で引っ越す意思があるかとか。</li>
</ul>
</li>
<li>最後はLinkedIn
<ul>
<li>Luceneのユーザ</li>
</ul>
</li>
</ul>
<h3 id="まとめ">まとめ</h3>
<p>ということで、スピーカーの方々のスライドにもありましたが、
<a href="http://www.amazon.co.jp/dp/4774161632?tag=johtani-22&amp;camp=243&amp;creative=1615&amp;linkCode=as1&amp;creativeASIN=4774161632&amp;adid=11S5FJFPHF9685VRE24M&amp;&amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2F">改訂新版Apache Solr入門</a>は良い本なので、購入していただけると嬉しいです。</p>
<p>感想、コメントなど、いつでもお待ちしています！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch Marvelの紹介と第一印象</title>
      <link>https://blog.johtani.info/blog/2014/01/29/simple-introduction-and-first-impression-es-marvel/</link>
      <pubDate>Wed, 29 Jan 2014 17:14:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/29/simple-introduction-and-first-impression-es-marvel/</guid>
      <description>昨晩、Elasticsearchから初のプロダクトとなるMarvelがリリースされました。ということで、さっそく触ってみて、簡単な紹介と感想</description>
      <content:encoded><p>昨晩、Elasticsearchから初のプロダクトとなるMarvelがリリースされました。ということで、さっそく触ってみて、簡単な紹介と感想を書いてみました。</p>
<!-- more -->
<h2 id="marvelって">Marvelって？</h2>
<p>Elasticsearch社が初のプロダクトとしてリリースした、Elasticsearchクラスタモニタリングツールです。
次のような特徴があります。</p>
<ul>
<li>plugin形式で提供</li>
<li>GUIがKibana
<ul>
<li>メトリックスはElasticsearchに保存</li>
</ul>
</li>
<li>SenseがChrome以外でも使える</li>
</ul>
<p>プロダクション環境で利用する場合は有料ですが、開発用途では無料で利用できます。
現時点(2014/01/29)では、<code>0.90.9</code>以上のバージョン(1.0.0.RC1含む)で利用が可能です。</p>
<h2 id="なにができるの">なにができるの？</h2>
<p>Elasticsearchクラスタに関するメトリックスを保存、可視化できるプロダクトです。
ドキュメント数やJVMの状況、クラスタの状態など、いろいろなメトリックスが保存されます。</p>
<p>保存先は、別のElasticsearchクラスタにすることも可能です。
お試しでインストールして見る場合は、同一クラスタにサービスに利用するインデックスとMarvel用のメトリックス保存先インデックスを入れても良いです。</p>
<p>ただ、プロダクション環境では、Marvel用インデックスはあくまでもモニタリングに使用するため、サービスのクラスタへの影響を最小にしたくなります。</p>
<p>このような場合、Marvelのプラグインの設定を変更することで、メトリックス送信用のエージェントとして動作させることができます。</p>
<p>詳しくは、Marvelのドキュメントにある<a href="http://www.elasticsearch.org/guide/en/marvel/current/#_installing_a_secondary_monitoring_cluster">installing a secondary monitoring cluster</a>を御覧ください。</p>
<blockquote>
<p><del>1/29 16時時点で、上記ドキュメントのエージェントの送信先の設定に関する部分に誤記がありました。</del>
<del>おそらく、<a href="http://www.elasticsearch.org/guide/en/marvel/current/#_statistics_exporting">configuration options</a>の記述が正だと思います。</del>
もう、なおってました。(1/30朝時点)</p>
</blockquote>
<h2 id="キャプチャいろいろ">キャプチャいろいろ</h2>
<p>日本語WikipediaのデータをRiverで登録しながら各画面の動作などを見てみました。</p>
<h4 id="marvel-overview">Marvel Overview</h4>
<p>日本語WikipediaをRiverで登録してる途中。Loadが高くなってることなどがわかります。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/Marvel_Overview_indexing_wikipedia_river.jpg" />
    </div>
    <a href="/images/entries/20140129/Marvel_Overview_indexing_wikipedia_river.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="overview-クラスタの状態が変化">Overview (クラスタの状態が変化)</h4>
<p>クラスタの状態が変化したところに、タグが付くみたいです。
ここでは、ノードの一つを停止、起動しました。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/tgged_event.jpg" />
    </div>
    <a href="/images/entries/20140129/tgged_event.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>インデックス終了後に、クラスタを再起動してしまい、クラスタ内のシャードの再配置が実行されてしまったため、クラスタの状態がYellowになってしまうとこんな感じ。ちょっとわかりにくいです。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/cluster_yellow.jpg" />
    </div>
    <a href="/images/entries/20140129/cluster_yellow.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="sense">Sense</h4>
<p>Chromeプラグインとしてリリースされていたクエリ実行コンソールがMarvelのサイトプラグインとして提供されています。これがあるだけで、Elasticsearchへのクエリの実行が格段に効率良くなります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/Marvel_sense.jpg" />
    </div>
    <a href="/images/entries/20140129/Marvel_sense.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="index-statistics">Index Statistics</h4>
<p>インデックスに関する情報のグラフが見れるページです。ドキュメント数の他に、容量やリクエスト数なども見れます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/Marvel_Index_Statistics.jpg" />
    </div>
    <a href="/images/entries/20140129/Marvel_Index_Statistics.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>インデックス終了後のグラフはこんな感じ。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/after_twice_indexing_ja_wikipedia.jpg" />
    </div>
    <a href="/images/entries/20140129/after_twice_indexing_ja_wikipedia.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>インデックス終了後のOverviewはこんなかんじです。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/after_indexing_ja_wikipedia.jpg" />
    </div>
    <a href="/images/entries/20140129/after_indexing_ja_wikipedia.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="cluster-pulse">Cluster Pulse</h4>
<p>クラスタで発生したイベントとイベントの詳細を見ることができるページです。各種インデックスがYELLOWからGREENに変わっていっているのがmessageで分かります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/after_reboot_cluster_pulse.jpg" />
    </div>
    <a href="/images/entries/20140129/after_reboot_cluster_pulse.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>すべて再配置が終わったらGREENになりました。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/finish_recovering_all_indices.jpg" />
    </div>
    <a href="/images/entries/20140129/finish_recovering_all_indices.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="node-statistics">Node Statistics</h4>
<p>各ノードに関する情報を見ることができる画面です。
ノードごとにグラフの色を分けることもできます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/two_node_in_nodes_stats.jpg" />
    </div>
    <a href="/images/entries/20140129/two_node_in_nodes_stats.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h4 id="その他">その他</h4>
<p>Marvelプラグインにブラウザから接続できなくなるとこんなメッセージが出ました。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/connect_error.jpg" />
    </div>
    <a href="/images/entries/20140129/connect_error.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>参考までに、<a href="https://github.com/mobz/elasticsearch-head">elasticsearch-head</a>の画面も。こちらのほうが、シャードの再配置中であるのがひと目で分かります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140129/elasticsearch-head.jpg" />
    </div>
    <a href="/images/entries/20140129/elasticsearch-head.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<h2 id="感想">感想</h2>
<p>綺麗です。まぁ、Kibanaが綺麗ですから。
クラスタ内で発生したイベントが時系列で保存されるため、あとからどんなことが発生したのかといった原因の追求などには非常に役に立ちそうです。</p>
<p>ただ、インデックスの状態や状況（クラスタ再起動やノード追加時にshard再配置などが実行されている状況とか）はelasticsearch-headのほうがわかりやすかったです。
インデックス単位でのStatusがMarvelの画面ではわからないため、shard再配置が完了したかどうかなどのタイミングがわかりにくかったです。</p>
<p>ある程度、多くのノードを利用したクラスタを利用する場合に、モニタリングツールとして利用するのは便利なのではないでしょうか？
時系列でログやイベントが保存されるので、ノードが追加されたり外れたりといった状況があとからでも追跡可能なのが便利です。</p>
<h2 id="疑問点">疑問点</h2>
<p>インデックスの情報などは、5s毎にMarvelのインデックスに保存されているようです。ただ、GUI上では5分毎のデータしか表示されません。
どうやって変更するんだろう？</p>
<p>また、Marvelのクラスタへの接続が切れた時のデータはどうなるのか？という部分も気になります。Marvelのクラスタを更新している時や、ネットワークが遮断されてしまった場合のデータがどうなるのかという点です。</p>
<h2 id="疑問点への回答20140130追記">疑問点への回答(2014/01/30追記)</h2>
<p>疑問点に対して中の人から回答を頂いたので、追記です。</p>
<ul>
<li>Q：GUI上で5分毎のデータしか表示されないんですが？
<ul>
<li>A：ブラウザの負荷を高くしないようにするために、1つのグラフに20のプロットしてるだけです。ズームしたりすると、もっと細かなデータが見れますよ。</li>
</ul>
</li>
<li>Q：Marvelのクラスタへの接続が切れた時のデータはどうなるんだろう？
<ul>
<li>A：接続が切れた場合は、ローカルに保存されるけどデータは無視されます。接続が戻ると、戻った後のデータは記録されていきます。将来的には改善するかも。</li>
</ul>
</li>
</ul>
<p>ちなみに、昨日試してた環境が、足元Linux環境（監視対象のクラスタ）＋手元Mac環境（Marvelモニタリングデータ格納クラスタ）という環境でした。
確かに、出社してから、手元Mac環境を起動すると、データが流れてくるようになりました。
ただ、監視対象のクラスタでは、socket timeoutのログがずっと出てましたが。</p>
<h2 id="参考文献">参考文献</h2>
<ul>
<li><a href="http://www.elasticsearch.com/blog/introducing-elasticsearch-marvel-native-monitoring-deployments/">リリースブログ</a></li>
<li><a href="http://www.elasticsearch.com/marvel">プロダクトページ</a></li>
<li><a href="http://www.elasticsearch.org/guide/en/marvel/current/">ドキュメント</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 4.6.1のリリースに関する注意点</title>
      <link>https://blog.johtani.info/blog/2014/01/28/release-lucene-gosen-4-dot-6-1/</link>
      <pubDate>Tue, 28 Jan 2014 12:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/28/release-lucene-gosen-4-dot-6-1/</guid>
      <description>Lucene/Solr 4.6.1がリリースされそう(バイナリ配布待ち)lucene-gosenの4.6.1対応版をリリースしました。 ライブラリのインタフェースな</description>
      <content:encoded><p>Lucene/Solr 4.6.1がリリースされそう(バイナリ配布待ち)<a href="https://code.google.com/p/lucene-gosen/">lucene-gosen</a>の4.6.1対応版をリリースしました。</p>
<p>ライブラリのインタフェースなどは特に変更はないのですが、ライブラリのダウンロード先が変更になっているため、注意喚起です。</p>
<!-- more -->
<p>Google Project Hostingの仕様変更により、Downloadsに新規ファイルがアップロードできなくなっています。（2014年から）</p>
<p>このため、プロジェクトの選択肢としては以下の3点となっています。</p>
<ol>
<li>Google Driveにファイルをアップロードしてダウンロードしてもらう</li>
<li>他のソースコード管理サイトなどを利用する。</li>
<li>他のダウンロードサイトを利用する</li>
</ol>
<p>1.と3.は場所が違うだけで、方法は一緒です。
今回は、暫定的に1.を利用してダウンロードするように対応しました。</p>
<p>ダウンロード先はプロジェクトのページにリンクが有りますが、わかりにくいのでキャプチャを撮ってみました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140128/project_home.jpg" />
    </div>
    <a href="/images/entries/20140128/project_home.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>ダウンロード先</h4>
      </figcaption>
  </figure>
</div>

<p>これまでの<code>Featured - Downloads</code>とは異なり、<code>Links - External links</code>の下に
<a href="https://drive.google.com/folderview?id=0B0xz3tf1TTPnYTlSNExkTzBhWnc&amp;usp=sharing">Downloads lucene-gosen 4.6.1</a>というリンクを用意してあります。</p>
<p>フォルダとなっており、各種jarファイルがリストされていますので、こちらからダウンロードをお願いします。
今後は、この下にダウンロードリンクを追加していく予定です。</p>
<p>ただし、2.で述べたように「別のソースコード管理サイト」も検討中です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Curator: 時系列インデックスの管理(日本語訳)</title>
      <link>https://blog.johtani.info/blog/2014/01/24/curator-tending-your-time-series-indices-in-japanese/</link>
      <pubDate>Fri, 24 Jan 2014 14:48:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/24/curator-tending-your-time-series-indices-in-japanese/</guid>
      <description>Elasticsearchのcuratorのブログ記事を読んで、日本語でツイートしたところ、Aaron Mildensteinさんから日本語（</description>
      <content:encoded><p>Elasticsearchのcuratorの<a href="http://www.elasticsearch.org/blog/curator-tending-your-time-series-indices/">ブログ記事</a>を読んで、日本語でツイートしたところ、Aaron Mildensteinさんから日本語（ローマ字）で返信を頂きました。
せっかくなので、ブログ記事を翻訳してもいいかを尋ねたところ、快くOKを頂いたので、翻訳してみました。参考になればと。（誤訳など見つけたらコメントください。）</p>
<blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/johtani">@johtani</a> Kore no hou ga ii. Nihongo de no Curator RT, arigatou gozaimasu! <a href="https://twitter.com/search?q=%23elasticsearch&amp;src=hash">#elasticsearch</a> <a href="https://twitter.com/search?q=%23curator&amp;src=hash">#curator</a> <a href="https://twitter.com/search?q=%23logstash&amp;src=hash">#logstash</a></p>&mdash; Aaron Mildenstein (@theuntergeek) <a href="https://twitter.com/theuntergeek/statuses/426009968513277952">2014, 1月 22</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<!-- more -->
<h2 id="curator-時系列インデックスの管理">curator: 時系列インデックスの管理</h2>
<p>原文：<a href="http://www.elasticsearch.org/blog/curator-tending-your-time-series-indices/">curator: tending your time-series indices</a></p>
<h2 id="背景">背景</h2>
<p>数年前、Elasticsearch、Logstash、Kibana(ELK)を管理し、ここ30日よりも古いインデックスを自動的に削除する方法を必要としていました。
APIドキュメントを読み、#logstashや#elasticsearchのIRCチャネルのコミュニティの助けを借りて、簡単なスクリプトとcronを用意するのが簡単であることを知りました。</p>
<pre><code>curl -XDELETE 'localhost:9200/logstash-2014.01.01?pretty'
</code></pre><p>もちろん、これも動作しますが、日付を生成するのがめんどくさいのでもっとエレガントな方法が欲しかったです。</p>
<h2 id="最初に">最初に</h2>
<p>pythonでスクリプトを書き始めました。特定の日数のインデックスを管理するだけのコマンドラインクリーナーを書いてコミュニティにシェアしました。他の人が、新しい機能を追加してくれました。私は、古いインデックスをoptimizeすることができる他のスクリプトも書きました。これは、シャードごとにnセグメント以上存在しないように各シャードのセグメントをマージすることです。これらのスクリプトで1つになるようにマージしたりエンハンスし、古いインデックスを管理する助けになるツールです。</p>
<h2 id="curatorの紹介">curatorの紹介</h2>
<p>Curatorで可能なインデックスオペレーション</p>
<ul>
<li>削除（日付もしくは、トータル容量による制限）</li>
<li>インデックスのクローズ(Close)</li>
<li>bloom filter cacheの無効化</li>
<li>Optimize(LuceneのforceMerge)</li>
</ul>
<h2 id="curatorのインストール">curatorのインストール</h2>
<p>この記事を書いている時点で、Curator は0.5.1がリリースされ、0.90.10に対応しています。Curatorはまた、Elasticsearchの1.0(現在はRC1)へも対応しています。各リリースへの互換性の保証のためのテストも行っています。</p>
<p>現時点では、gitリポジトリで配布しています。近い将来、pipによるインストール可能なパッケージにする予定です。利用することを恐れないでください。もし、pythonとpipがあなたのマシンにインストールされていれば、次のようにインストールは簡単です。</p>
<pre><code>git clone https://github.com/elasticsearch/curator.git
pip install -r requirements.txt
</code></pre><p>インストール後の確認は次のコマンドです。</p>
<pre><code>$ ./curator.py -v
curator.py 0.5.1
</code></pre><h3 id="利用方法とサンプル">利用方法とサンプル</h3>
<p>サンプルを示す前に、<a href="#arguments">オプションを見ておくとよいでしょう</a>。このリストは長いですが（この記事の最後に含まれています）、どのようなことがコントロールできるかを説明しています。デフォルトがどうなっているかに注意してください。もし、デフォルト値で良い場合は、フラグを指定する必要はありません。</p>
<p>では、簡単なサンプルを見ながら、CuratorがELKスタックをどうやって管理するかを見て行きましょう。</p>
<h3 id="削除delete">削除(delete)</h3>
<p>90日以上のインデックスを保存したくないとしましょう。コマンドは次のようになります。</p>
<pre><code>$ curator.py --host my-elasticsearch -d 90
</code></pre><p><code>-d</code>で日数を指定しているだけです。簡単でしょ？</p>
<h4 id="容量による削除delete-by-space">容量による削除(delete by space)</h4>
<p>これは、指定したギガバイト数を超えたインデックスを場合に(最も古いものから)削除を行う特殊なケースです。</p>
<pre><code>$ curator.py --host my-elasticsearch -C space -g 10024
</code></pre><p><code>-C</code>でspaceによるcurationであること、<code>-g</code>でギガバイト数(10024、10TB)であることを指定しているのがわかります。<code>-g</code>は1.5や0.5という数値を指定できます。</p>
<p><em>その他のCuratorオプションは<code>space</code>による削除と組み合わせて使用できないことに注意してください。</em></p>
<h3 id="クローズclose">クローズ(close)</h3>
<p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-open-close.html">Open/Close Index API</a>により、インデックスをクローズすることができます。</p>
<blockquote>
<p>open/close index APIを利用すると、インデックスをクローズしたり、あとでオープンしたりすることができます。クローズされたインデックスはクラスタのオーバヘッドにほとんどならず(メタデータの管理を除く)、読み書き操作の妨げにもなりません。クローズされたインデックスは、リカバリプロセス時に、オープンされます。</p>
</blockquote>
<p>インデックスをクローズすることは、存在はするが検索できないという意味です。何が便利なのでしょう？</p>
<p>90日のインデックスを保存する義務があるが、検索は過去30日のインデックスを対象にする以外は稀であるような場合を想像してください。このような状況で、価値のあるリソース(ヒープスペースなど)を節約するためにインデックスをクローズすることができます。これは、クラスタに検索やインデキシングのためのメモリを与えることができることを意味します。そして、もし、クローズしたインデックスのデータが必要になったら、APIを呼び出してインデックスをオープンすれば検索できます。</p>
<p><em>このような場合、今オープンしているインデックスが再び、クローズされないように、一時的にCuratorのスケジュール実行をオフにしておくのが懸命です。</em></p>
<pre><code>$ curator.py --host my-elasticsearch -c 30 -d 90
</code></pre><p>先ほど説明した例の実行方法です。これは、30日よりも古いインデックスはクローズし、90日より古いインデックスを削除します。本当に簡単でしょ？</p>
<h3 id="bloom-filterの無効化">bloom filterの無効化</h3>
<p><a href="https://github.com/elasticsearch/elasticsearch/issues/4525">これは、0.90.9以降のバージョンで利用可能な機能です。(リンク先はIssue #4525)</a></p>
<p>心配しないでください。このスクリプトは操作を行う前に、elasticsearchが利用可能なバージョンであるかをチェックします。</p>
<p>bloom filterとは何でしょう？なぜ、無効化したくなるのでしょう？</p>
<p>bloom filterはインデキシング操作を高速化するためにリソースを割り当てられます。時系列データで、インデキシングしている間もこれは有用です。インデックスは2日後には、日付が変わると新しいデータはおそらくインデックスされません。そのインデックスにはもはや必要のないリソースをbloom filterはまだ持っています。Curatorはこれらのリソースを開放することができます！</p>
<pre><code>$ curator.py --host my-elasticsearch -b 2 -c 30 -d 90
</code></pre><p>これで、bloom filterのリソースは少なくとも2日(1にもできます)よりも古いインデックスについては利用せず、30日より古いインデックスはクローズし、90より古いインデックスは削除します。</p>
<h3 id="optimizeというよりもforcemerge">optimizeというよりもforcemerge</h3>
<p>コマンドの説明をする前に、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html">Elasticsearch APIのoptimize</a>を見ることは、生きているインデックスや&quot;cold&quot;インデックス(インデキシングがアクティブではないという意味)に実行する必要があるということを理解するために重要です。実際、<a href="http://blog.trifork.com/2011/11/21/simon-says-optimize-is-bad-for-you/">optimizeはLuceneではforceMergeと名前が変えられ</a>、インデックスを改善するためにoptimizeを呼び出す必要はなくなりました。Elasticsearchのセグメントをマージすることは利点がありますが、coldインデックス全てに対してoptimizeを開始する前に、コストを理解する必要があります。</p>
<p>forceMerge操作はインデックスにある各シャードのセグメントの数を少なくします。各セグメントはオーバヘッドがあるため、セグメントが多いということは、より多くのリソースを使うという意味です。良さそうですね？リソースが少ない？</p>
<p>それは、可能ですが、merge操作を実行するには多くのディスクやネットワークI/Oが必要で、ディスクやクラスタの通常の書き込み操作に悪影響を及ぼします。もし、これが必要なら私のアドバイスを良く考えてください。(数％ほど)検索を速くし、リソースの使用量も減らすことができます。また、管理しているセグメント数が小さくなるということは、クラスタのリカバリを速くすることにもなります。1つのインデックスをoptimizeするためにはおそらく1時間以上の時間がかかります。「使用する前に目立たない場所で試してください」というクリーニングボトル(訳注：洗剤とか漂白剤かな？)の注意書きと同様に、ディスクI/Oが低い時にテストし、もし操作とリソースがあなたのクラスタのユースケースにあっているかを見てください。デフォルトでは、シャードごとに2つのセグメントにマージしますが、<code>--max_num_segments</code>フラグで変更可能です。</p>
<p>ここまでのサンプルは次のようなコマンドになります。</p>
<pre><code>$ curator.py --host my-elasticsearch -b 2 -o 2 -c 30 -d 90
</code></pre><p>これで、bloom filterは2日より古いインデックスでは向こうにし、2日より古いインデックスは&quot;optimize&quot;し、30日より古いインデックスはクローズし、90日より古いインデックスは削除されます。</p>
<h3 id="操作の順序">操作の順序</h3>
<p>スクリプトは操作が衝突するのを防ぐために次の順序で実行されます。なぜ、クローズされたインデックスはoptimizeしないのでしょう？なぜ、削除予定のインデックスはクローズされないのでしょう？</p>
<ol>
<li>Delete (by space or time)</li>
<li>Close</li>
<li>Disable bloom filters</li>
<li>Optimize</li>
</ol>
<h3 id="使用の検討">使用の検討</h3>
<p>最後の例で、3つの操作を1つのコマンドで実行していますが、それらが連続ですべて実行されるのを望んでいないかもしれません。</p>
<pre><code>$ curator.py --host my-elasticsearch -b 2 -o 2 -c 30 -d 90
</code></pre><p>これは、次の操作と同様です。</p>
<pre><code>$ curator.py --host my-elasticsearch -d 90
$ curator.py --host my-elasticsearch -c 30
$ curator.py --host my-elasticsearch -b 2
$ curator.py --host my-elasticsearch -o 2
</code></pre><p>これらのコマンドを異なる時間に実行したり、異なるその他のオプション(特に、optimize実行で<code>--timeout 3600</code>を追加したり)を指定して実行するのは簡単です。</p>
<p>また、デフォルトの<code>logstash-</code>とは異なるプレフィックスのインデックスを持っているかもしれません。</p>
<pre><code>$ curator.py --host my-elasticsearch --prefix logstash- -d 30
$ curator.py --host my-elasticsearch --prefix othername- -d 30
</code></pre><h2 id="最後に">最後に</h2>
<p>Curatorは時系列インデックスの保存ポリシーを管理するのに役立ちます。豊富な設定オプションがインデックスを管理することを簡単にします。クラスタに存在するノードの数に関係なく。<a href="https://github.com/elasticsearch/curator">https://github.com/elasticsearch/curator</a>へのフィードバックやコントリビューションをお待ちしています！</p>
<h4 id="a-namearguments参考全引数とオプション"><a name="arguments">参考(全引数とオプション)</h4>
<pre><code>$ curator.py -h
usage: curator.py [-h] [-v] [--host HOST] [--port PORT] [-t TIMEOUT]
                  [-p PREFIX] [-s SEPARATOR] [-C CURATION_STYLE]
                  [-T TIME_UNIT] [-d DELETE_OLDER] [-c CLOSE_OLDER]
                  [-b BLOOM_OLDER] [-g DISK_SPACE]
                  [--max_num_segments MAX_NUM_SEGMENTS] [-o OPTIMIZE] [-n]
                  [-D] [-l LOG_FILE]

Curator for Elasticsearch indices. Can delete (by space or time), close,
disable bloom filters and optimize (forceMerge) your indices.

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         show program version number and exit
  --host HOST           Elasticsearch host. Default: localhost
  --port PORT           Elasticsearch port. Default: 9200
  -t TIMEOUT, --timeout TIMEOUT
                        Elasticsearch timeout. Default: 30
  -p PREFIX, --prefix PREFIX
                        Prefix for the indices. Indices that do not have this
                        prefix are skipped. Default: logstash-
  -s SEPARATOR, --separator SEPARATOR
                        Time unit separator. Default: .
  -C CURATION_STYLE, --curation-style CURATION_STYLE
                        Curate indices by [time, space] Default: time
  -T TIME_UNIT, --time-unit TIME_UNIT
                        Unit of time to reckon by: [days, hours] Default: days
  -d DELETE_OLDER, --delete DELETE_OLDER
                        Delete indices older than n TIME_UNITs.
  -c CLOSE_OLDER, --close CLOSE_OLDER
                        Close indices older than n TIME_UNITs.
  -b BLOOM_OLDER, --bloom BLOOM_OLDER
                        Disable bloom filter for indices older than n
                        TIME_UNITs.
  -g DISK_SPACE, --disk-space DISK_SPACE
                        Delete indices beyond n GIGABYTES.
  --max_num_segments MAX_NUM_SEGMENTS
                        Maximum number of segments, post-optimize. Default: 2
  -o OPTIMIZE, --optimize OPTIMIZE
                        Optimize (Lucene forceMerge) indices older than n
                        TIME_UNITs. Must increase timeout to stay connected
                        throughout optimize operation, recommend no less than
                        3600.
  -n, --dry-run         If true, does not perform any changes to the
                        Elasticsearch indices.
  -D, --debug           Debug mode
  -l LOG_FILE, --logfile LOG_FILE
                        log file
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>Extended-Analyze 1.0.0RC1をリリースしました</title>
      <link>https://blog.johtani.info/blog/2014/01/22/release-extended-plugin-for-1-0-0rc1/</link>
      <pubDate>Wed, 22 Jan 2014 00:16:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/22/release-extended-plugin-for-1-0-0rc1/</guid>
      <description>あけましておめでとうございます。今年もSolrやElasticsearchについて色々と頑張っていく所存です。 とまぁ、お決まりの挨拶はおいて</description>
      <content:encoded><p>あけましておめでとうございます。今年もSolrやElasticsearchについて色々と頑張っていく所存です。
とまぁ、お決まりの挨拶はおいておいてと。(もう、新年も22日ですが。。。)</p>
<p>Elasticsearchの1.0.0RC1がリリースされました。
ということで、私が作っている<a href="https://github.com/johtani/elasticsearch-extended-analyze">Extended-Analyzeプラグイン</a>も1.0.0RC1向けに修正してリリースしました。</p>
<!-- more -->
<h2 id="100rc1向けに修正したこと">1.0.0RC1向けに修正したこと</h2>
<p>コミットログを見てもらえば、いいのですが、ロジック自体は変更しなくても良かったです。</p>
<p>ただ、正式に、Elasticsearchのつづりが決定したようで、クラス名が「ElasticSearchほげほげ」から、「Elasticsearchほげほげ」と、SearchのSが小文字になりっています。
この影響で、例外クラスなどの名称を幾つか変更しました。
また、バージョン番号を1.0.0RC1とし、0.x系をElasticsearchの0.90系向けのバージョンにしていく予定です。</p>
<p>今後は、UIを追加したいと思っているので、Elasticsearchのバージョン番号とはずれてくるとは思いますが。。。</p>
<h2 id="elasticsearch-100rc1を利用してみて">Elasticsearch 1.0.0RC1を利用してみて</h2>
<p>1点だけですが。
これまでは、<code>-f</code>オプションを指定すると、デーモンではない動作で起動できていました。（デフォルトがデーモン起動）</p>
<p>これが、1.0.0から（0.90の最新もかな？詳しく見ていない）デフォルトの挙動が変更され、デーモン起動ではなくなりました。
代わりに、<code>-d</code>オプションを指定することで、デーモン起動ができるようになりました。</p>
<p>これで、手元でうっかりデーモン起動することがなくなって、ひと安心です。（他の人は困るかもしれないけど）</p>
<h2 id="ということで">ということで</h2>
<p>1.0.0RC1が出たので、少しずつ1.0系で追加されたAPIや機能について、ブログで紹介していけたらと思います。</p>
<p>こんなこと調べてよ？、これわかんないんだけど？などありましたら、コメントいただければと。
気が向いたら記事を書くので。</p>
<p>あと、<a href="https://github.com/johtani/elasticsearch-extended-analyze">Extended-Analyzeプラグイン</a>の感想などもお待ちしています！</p>
<p>今年もよろしくお願いします！</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2013）</title>
      <link>https://blog.johtani.info/blog/2013/12/30/looking-back-2013/</link>
      <pubDate>Mon, 30 Dec 2013 21:20:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/30/looking-back-2013/</guid>
      <description>昨年は、大晦日に書いてました。 ちょっと進歩したかも。 振り返り(2012年に書いた抱負から) ということで、まずは昨年書いた抱負からの振り返りで</description>
      <content:encoded><p>昨年は、<a href="http://johtani.jugem.jp/?eid=115">大晦日に書いてました</a>。
ちょっと進歩したかも。</p>
<!-- more -->
<h2 id="振り返り2012年に書いた抱負から">振り返り(2012年に書いた抱負から)</h2>
<p>ということで、まずは昨年書いた抱負からの振り返りです。</p>
<ul>
<li>IntelliJ IDEAをメインに使う</li>
<li>JIRAを継続して活用</li>
<li>ブログの継続</li>
<li>elasticsearch</li>
<li><strike>Luceneのソースコードリーディング</strike></li>
<li>何かOSSのソースを読む</li>
<li>Macでもっと開発</li>
<li>読書と英語を継続</li>
</ul>
<p>お。思ったよりもできてるかも。
IntelliJ IDEAについては、メインになりました。Eclipseを開くことはまずないです。
Macがメインの開発マシンにもなってきてるので。(開発してないんじゃないかという話も。。。)</p>
<p>JIRAは一応、継続しているという感じです。</p>
<p>唯一Luceneのソースコードリーディングができてないですね。。。
言い訳をすると、明示的にソースコードリーディングをしているわけではないですが、Luceneのソース自体は時々見ています。
SolrやElasticsearchを調べるとそのままLuceneにたどり着くことがあるので。</p>
<p>ポモドーロのタスク管理用に使っていますが、時々忘れていたり。あと、振り返りがまだちゃんとできてないので、そこをやらないとかなぁと。
読書と英語は今後も継続です。もっと習慣づけないと。</p>
<h2 id="振り返り今年あった出来事など">振り返り(今年あった出来事など)</h2>
<ul>
<li>CROSSでモデレータやりました</li>
<li>Xperia Zに機種変</li>
<li>メニエール病</li>
<li>リクルートテクノロジーズさんのお手伝い</li>
<li>Lucene In Action輪読やってる</li>
<li>MIR輪読も継続</li>
<li>Solr勉強会を不定期開催</li>
<li>Elasticsearch勉強会を始めた</li>
<li>Solr本の改訂版を執筆</li>
<li>AWSちょっと触った</li>
<li>Elasticsearchプラグイン作ってみた</li>
<li>Githubの活用</li>
<li>Octopressでブログ</li>
</ul>
<p>こんなかんじです。</p>
<p><a href="http://2013.cross-party.com/programs/?p=366">CROSSでは、モデレータ</a>をやらせていただきました。すこしは検索を面白いと思ってもらえたかなと思います。
来年の<a href="http://www.cross-party.com">CROSS 2014</a>ではスタッフとして盛り上げていく予定ですので、スタッフに興味ある方は声をかけてください！
面白い話がいっぱい聞けますし、おいしい物も飲み食いできるかも！？</p>
<p>Xperia Zは良い買い物でした。ただ、すでに2回電源部分が故障してますが。。。
それ以外は非常に快適です。</p>
<p>2月末から3月は少し休んでました。難聴→めまい→メニエール病という流れで、ちょっとしんどかったです。
ほぼ回復しましたが、原因は不明みたいなので、長く付き合ってくのかなぁと。</p>
<p>4月からリクルートテクノロジーズさんの仕事を手伝っています(Solr本の著者紹介にも書いてます)。
色々と面白いことをしている<a href="http://atl.recruit-tech.co.jp">ATLという部署</a>で面白い人達と仕事させて頂いてます。
来年もよろしくお願いします！(もっと価値を出さないとなぁ。。。)</p>
<p>その一環で、Lucene In Actionの輪読を社内でやっていたりも。バージョンが3.x系で書かれているので、
ちょっと大変ですが、4だとどう違うかなどの話を交えつつ少しずつ読んでいます。</p>
<p>輪読会といえば、MIR(Modern Information Retirieval)の輪読も続いています。
私は深いところまでわからないのですが、色々と詳しい方たちとボチボチ読んでます。教えてもらってばかりですが、来年も頑張りますよと。</p>
<p>Solr勉強会も、引き継いでぼちぼちやっています。来年は<a href="http://solr.doorkeeper.jp/events/7260">1/29に開催</a>します。Solrの入門的な話をしてもらうので、
ぜひ、触ったことがない方や興味がある人に参加していただきたいと。</p>
<p><a href="http://elasticsearch.doorkeeper.jp">Elasticsearch勉強会</a>も主催し始めました。興味ある人がいるだろうとは思ったのですが、想像以上でびっくりしてます。
自分が理解を深め、発表するためにも、2ヶ月程度のスパンで来年も開催する予定です。
スピーカーに興味のある方は、<a href="https://groups.google.com/forum/#!forum/elasticsearch-jp">elasticsearch-jpのML</a>や私にコンタクトしてください。</p>
<p>Solr入門のSolr4対応版も執筆しました。今回は全体のコーディネートもやらせていただきました。
Solr活用のお役に立てていただければと。まだ購入してない方はぜひ、以下のリンクから！(<a href="https://gihyo.jp/dp/ebook/2013/978-4-7741-6240-9">PDF版もあります。</a>)
質問などあれば、ブログにコメントをいただくか、ツイートしていただくか、技術評論社のサポートページに問い合わせていただければと思います。</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&nou=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&o=9&p=8&l=as1&m=amazon&f=ifr&ref=qf_sp_asin_til&asins=4774161632" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<p>AWSをちょっとだけ触りました。
S3にバックアップ取ってるだけですが。。。来年はもう少し。。。
JIRA（さくらVPSに立ててる）のバックアップなどをやってます。</p>
<p>あとは、Elasticsearchの勉強会も始めたこともあり、Elasticsearchを色々と触っています。
<a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">elasticsearch-analysis-kuromojiプラグインのREADME</a>を記述してコントリビュートしてみたり、
<a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analysisプラグイン</a>を作ってみたり。</p>
<p>勉強会のスピーカー探しも兼ねて、いろんなところに、「突撃!隣のElasticsearch」と称してElasticsearchの話を聞きに行きたいと思っていますので、使ってるよ！とか使いたいんだけど、どうすればいい？みたいな話があれば、声をかけてください。Twitterでツイート見たら勝手に、アタックすることもあるかもですが、その時はよろしくお願いします。</p>
<p>来年はSolrやElaticsearchにもっと貢献できればと。</p>
<p>Github(git)もようやくまともに触り始めました。
某データの管理やプラグインの開発などでやっと触り始めました。
まだ、チーム開発ってほどではないので、チームで開発するときのやり方なども少しずつ勉強していこうかと。
プルリクとかも少しずつやってみたりしてます。</p>
<p>あとは、Octopressでブログ始めました。その前はjugemだったのですが、なんとなく。
Markdownに慣れるためというのもありOctopressにしてみました。
(Solr本の原稿もMarkdownで書いてました)
昔のブログもこっちにコピーするプログラムも書いてみようかな。</p>
<h2 id="来年の抱負">来年の抱負</h2>
<ul>
<li>Elasticsearch勉強会、Solr勉強会の継続＋ミックスした検索勉強会の開催</li>
<li>IDEAのさらなる活用</li>
<li>もっと開発(プラグインとか)</li>
<li>AWSをもう少し活用</li>
<li>海外のイベントに行ってみたい</li>
<li>読書と英語を継続</li>
</ul>
<p>勉強会の開催は今後も継続していく予定です。会場を毎回提供して頂いているVOYAGE GROUPさん、リクルートテクノロジーズさんには、
今後もお手数をお掛けしますが、よろしくお願い致します。勉強会の開催を通じて、色々な人のノウハウがうまく共有できて、
もっと本質的な作業（サービスの改善とか）に注力できる環境ができると面白いなぁと思ってるので。</p>
<p>あと、Elasticsearch、Solrと別々の勉強会になっていますが、検索エンジン勉強会という形でそれぞれのメリット・デメリットを
共有できる勉強会も面白いかもと思っているので、春とかに開催したいなぁと考えてもいます。（まだ考えているだけで何もネタがないですが）</p>
<p>IDEAは、年貢(13のサブスクリプション)を収めたので、もっと活用しないとという意味で。プラグインの開発、gitやsvnのクライアントとしては
活躍していますが、もっと開発に活用していかないとなぁと。まだ、10%くらいしか活用できてない気がするので。</p>
<p>もっと開発しないとなと。毎年言ってますが。今年は書籍に注力したのもあり、ソースを読んだり英語を読んだりが多くなってました。
偉そうにしてるためか、開発する機会が減ってきているので、細かなことでもいいので、すこしずつコードを書いていこうかと。
（もっと自分で問題点を見出してそれを補完するようなコードなりプラグインなり書けばいいんだろうな。）</p>
<p>開発と合わせて、AWSをもう少し触りたいなと。これも数年言っていますができてないです。。。
ネタ探すのが下手なのかなぁ。</p>
<p>海外イベントにも行ってみたいです。まだ、海外行ったことないんですが。。。
Lucene Revolutionとか。その訓練も兼ねて英語でメール書いたり、プラグインのREADME書いたりしてみてます。</p>
<p>最後は、毎年恒例ですが、読書と英語です。
Packtのキャンペーンで買ってしまった英語の本とか貯まっているので、読まないと。
nasneが便利で通勤時間にドラマ見てるからなぁ。ま、必要なときにボチボチと読んでいく予定です。</p>
<p>最後は、いつものようにですが、来年も勉強会やいろんなイベントに参加する予定です。
ブログも週1程度で書く努力しないとなぁ。
こんな話を書いてくださいとかのリクエストもお待ちしています。</p>
<p>今年もまだ、大晦日が残っていますが、色々とお世話になりました。
この場を借りてお礼申し上げます。</p>
<p>来年も、いろんな方に絡んでいくとは思いますが、よろしくお願いいたします。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 0.90.8がリリースされました＆注意点（2013/12/20追記）</title>
      <link>https://blog.johtani.info/blog/2013/12/20/release-elasticsearch-0-90-8/</link>
      <pubDate>Fri, 20 Dec 2013 16:24:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/20/release-elasticsearch-0-90-8/</guid>
      <description>昨夜、Elasticsearchの0.90.8がリリースされました。 リリースされた内容などについては、本家のブログ「0.90.8 releas</description>
      <content:encoded><p>昨夜、Elasticsearchの0.90.8がリリースされました。</p>
<p>リリースされた内容などについては、本家のブログ「<a href="http://www.elasticsearch.org/blog/0-90-8-released/">0.90.8 released</a>」をご覧いただくこととして。
1点注意したほうが良い点があります。</p>
<!-- more -->
<p><a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">elasticsearch-analysis-kuromoji</a>を利用している場合は、0.90.8に対応したバージョンがリリースされるのを待つ必要があります。</p>
<p>elasticsearch 0.90.8はLuceneのバージョンが4.6.0に変更されています。
Lucene 4.6.0では、TokenStreamというTokenizerのI/Fに変更があり、Tokenizerの実装を変更する必要があります。</p>
<p>現時点（2013年12月19日現在）のelasticsearch-analysis-kuromojiの1.6.0にはlucene-analyzers-kuromoji-4.5.1.jarが含まれており、この部分でI/Fが異なるためエラーが発生してしまいます。
プラグインをインストールする時点ではエラーは発生せず、実際にKuromojiのTokenizerやAnalyzerを利用するタイミングでエラーが出ます。
以下、0.90.8にanalysis-kuromojiの1.6.0をインストールした状態で<code>_analyze</code>を実行した時のエラー。</p>
<pre><code>curl -XPOST 'localhost:9200/_analyze?tokenizer=kuromoji_tokenizer&amp;filters=kuromoji_baseform&amp;pretty' -d '寿司が美味しかった'
{
  &quot;error&quot; : &quot;IllegalStateException[TokenStream contract violation: reset()/close() call missing, reset() called multiple times, or subclass does not call super.reset(). Please see Javadocs of TokenStream class for more information about the correct consuming workflow.]&quot;,
  &quot;status&quot; : 500
}
</code></pre><p>ということで、1.7.0がリリースされるのを待つか、自分で<code>mvn package</code>してビルドする必要があります。
他にも独自でTokenizerなどを造られている方は注意が必要かと。</p>
<p>たぶん、すぐにリリースされるんじゃないかなぁと。</p>
<p><strong>2013/12/20追記</strong></p>
<p>とりあえず、masterブランチが0.90.8に変更されたみたいです。(と書いてるそばから、1.7.0がリリースされました)
ということで、0.90.8では1.7.0を使うとエラーが出ないです。
（あと、踊り字対応のcharfilterも追加されたみたいです）</p>
</content:encoded>
    </item>
    
    <item>
      <title>Solrへのプラグインの配置方法について</title>
      <link>https://blog.johtani.info/blog/2013/12/19/add-jar-file-to-solr/</link>
      <pubDate>Thu, 19 Dec 2013 19:09:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/19/add-jar-file-to-solr/</guid>
      <description>Solr本が出てから、質問を受けてブログ書くと言いながら書いてなかったことを思い出しました。。。 プラグインの配置方法についてこんな質問を受け</description>
      <content:encoded><p>Solr本が出てから、質問を受けてブログ書くと言いながら書いてなかったことを思い出しました。。。</p>
<p>プラグインの配置方法についてこんな質問を受けてたので、それっぽいエントリを書いておきます。（想像と違ってたらツッコミ入れてください）</p>
<!-- more -->
<blockquote class="twitter-tweet" lang="ja"><p><a href="https://twitter.com/johtani">@johtani</a> 追加でプラグインの配置方法とかあると便利かなと思いました</p>&mdash; Tsubosaka (@tsubosaka) <a href="https://twitter.com/tsubosaka/statuses/407395766471110656">2013, 12月 2</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>改定前のSolr本では、日本語の形態素解析器をjarファイルとして追加する方法が書かれていました。
ただ、改定後のSolr本では、KuromojiがLuceneで実装されているためサンプルとしてjarファイルを追加するような方法の記載が明確にはありません。</p>
<p>19ページのcollection1の説明ですこしだけ、libディレクトリについて触れています。</p>
<p>独自のTokenizer（lucene-gosenなど）はjar形式でSolrに追加し、schema.xmlなどに利用するFactoryを指定してから利用します。</p>
<p>このとき、追加のjarファイルを配置する先がlibディレクトリです。</p>
<p>libディレクトリは2つの種類のスコープのディレクトリが存在します。</p>
<ul>
<li>Solr全体で利用可能なlibディレクトリ</li>
<li>コア単位で利用可能なlibディレクトリ</li>
</ul>
<h2 id="solr全体で利用するlibディレクトリ">Solr全体で利用するlibディレクトリ</h2>
<p>これは、起動しているSolrにある全てのコアで利用するようなjarファイルを配置するディレクトリになります。
場所は<code>$SOLR_HOME/lib</code>です。ここにjarファイルを配置することで、この<code>$SOLR_HOME</code>を利用するすべてのコアで同じjarファイルを利用することができるようになります。</p>
<p>ですので、例えば、lucene-gosenはすべてのコアで利用するという場合にはここに配置すれば、1つのjarファイルを配置するだけで済むことになります。</p>
<h2 id="コア単位で利用するlibディレクトリ">コア単位で利用するlibディレクトリ</h2>
<p>これは、コアごとにlibディレクトリを用意する場合です。
19ページにも記載されていますが、<code>$SOLR_HOME/コアディレクトリ名/lib</code>となります。</p>
<p>特定のコアのみで利用するライブラリについてはこちらに配置する形になります。
他のコアで利用してほしくないjarファイルなどを配置するのに利用すればよいかと。</p>
<p>簡単ですが、補足記事でした。
UIMAやlangidの利用方法などもあるとうれしですかね？
そのうち気が向けば書くかもしれません。（他の人に書いてもらうのもありかも。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-extended-analyzeプラグインをMavenとSonatypeにリリース</title>
      <link>https://blog.johtani.info/blog/2013/12/17/release-es-extended-analyze-plugin-to-maven-and-sonatype/</link>
      <pubDate>Tue, 17 Dec 2013 12:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/17/release-es-extended-analyze-plugin-to-maven-and-sonatype/</guid>
      <description>ども。 プラグインのインストールに長いURL入れるの辛いですよね？ね？ ということで、MavenでリリースしてMaven Repositoryから</description>
      <content:encoded><p>ども。
プラグインのインストールに長いURL入れるの辛いですよね？ね？</p>
<p>ということで、MavenでリリースしてMaven Repositoryからダウンロードできるようにしてみました。</p>
<!-- more -->
<p>流れとしては</p>
<ol>
<li>Sonatypeにリリースできるように申請する</li>
<li>Sonatypeにリリースする</li>
<li>SonatypeからMavenにSyncしてもらう</li>
</ol>
<p>という流れです。</p>
<p>Sonatypeにリリースするための方法はイケメンの人(@yusuke)がブログに簡単ですが残してくれてました。
あと、こちらの@vvakameさんのブログも参考にしながら作業しました。</p>
<ul>
<li><a href="http://samuraism.jp/diary/2012/05/03/1336047480000.html">【最新版】Maven Central Repository へのライブラリ登録方法 #maven</a></li>
<li><a href="http://d.hatena.ne.jp/vvakame/20120507#1336411831">JsonPullParser が Maven Central Repository に入るようです</a></li>
</ul>
<p>pom.xmlについては、プラグインのpom.xmlを参考にしてもらえればと。
1.の作業が終わったら、リリースを実行します。</p>
<p>この時、<code>&lt;scm&gt;</code>タグにgithubの情報が記載されているため（？）、githubにタグを打つ作業もmavenコマンドがやってくれるみたいです。</p>
<pre><code>mvn release:prepare
</code></pre><p>を実行すると、リリースするバージョンやタグ名などを聞いてくれます。
それらに答えると、pom.xmlにバージョンを指定してcommit&amp;pushしてくれ、タグも打ってくれます。（なんて便利）</p>
<p>その後、<code>release:perform</code>にてSonatypeへのリリースが完了します。
あとは、Sonatypeの画面で作業したら、Mavenのリポジトリにそのうち同期してくれます。</p>
<p>ということで、次のコマンドを実行すればプラグインがインストールできるようになりました。0.6.0と0.7.0の違いは実装には差異はありません。リリース方法が変更されただけということになります。</p>
<pre><code>bin/plugin -i info.johtani/elasticsearch-extended-analyze/0.7.0
</code></pre><p>これで少しは活用してもらえるようになるかなぁ？
（どのくらいの人が使ってくれてるのかは不明。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>改訂版Solr入門のPDF版も発売</title>
      <link>https://blog.johtani.info/blog/2013/12/09/release-introduction-solr-ebook/</link>
      <pubDate>Mon, 09 Dec 2013 11:08:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/09/release-introduction-solr-ebook/</guid>
      <description>少し遅くなってしまいましたが、12/05に電子書籍も発売されました。 技術評論社の電子書籍サイトから購入可能です。 書籍のページへのリンク PDF</description>
      <content:encoded><p>少し遅くなってしまいましたが、12/05に電子書籍も発売されました。</p>
<!-- more -->
<p>技術評論社の電子書籍サイトから購入可能です。</p>
<ul>
<li><a href="https://gihyo.jp/dp/ebook/2013/978-4-7741-6240-9">書籍のページへのリンク</a></li>
</ul>
<p>PDF版となっております。
購入の際は、技術評論社の電子書籍サイトに会員登録後購入可能となります。</p>
<p>個人的には電子書籍が便利なので、こちらを普段活用しようと思っています。</p>
<p>もちろん、紙の書籍も発売中です！購入の際は右の書影をクリックしていただければと！</p>
</content:encoded>
    </item>
    
    <item>
      <title>改訂版Solr入門を執筆しました</title>
      <link>https://blog.johtani.info/blog/2013/11/26/introduction-to-solr-new-edition/</link>
      <pubDate>Tue, 26 Nov 2013 12:27:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/26/introduction-to-solr-new-edition/</guid>
      <description>勉強会で宣伝もしましたが、改めて。 Solr入門の改訂版を執筆しました。 考えてみれば、もう3年も前なんですね、Solr入門は。 Solr勉強会な</description>
      <content:encoded><p>勉強会で宣伝もしましたが、改めて。</p>
<p>Solr入門の改訂版を執筆しました。
考えてみれば、もう3年も前なんですね、<a href="http://gihyo.jp/book/2010/978-4-7741-4175-6">Solr入門</a>は。
Solr勉強会などでも何度も新しいのは出ないのですか？と聞かれていましたが、やっと出ました。（お待たせしました。）</p>
<p>時が立つのは早いものです。前回のSolr入門はバージョン1.4にて執筆していましたが、今回は4.4をベースにし、4.5.1への対応を行っています。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20131126/intro_solr.jpg" />
    </div>
    <a href="/images/entries/20131126/intro_solr.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>月曜日には手元に見本が届き、今週金曜日に発売予定です！</p>
<!-- more -->
<p>SolrCloud、SoftCommit、Spatial、Joinなど、多彩な機能についても記載してあります。
また、<a href="http://manifoldcf.apache.org/ja_JP/index.html">ManifoldCF</a>というSolrにデータを登録するのに
利用できるコネクタフレームワークについても書いてあります。</p>
<p>より多彩になったSolrの機能を活用するための一助となれればと思います。
（電子版も出る予定です。詳細についてはもう少々お待ちください）</p>
<p>また、出版を記念して少し時期が先になりますが、Solr勉強会を開催しようと思います。</p>
<ul>
<li>日時：2014年01月29日</li>
<li><a href="http://solr.doorkeeper.jp/events/7260">第13回Solr勉強会 #SolrJP 新Solr本出版記念</a></li>
</ul>
<p>今回はせっかくのSolr入門の書籍の出版記念ということで入門的な話をしてもらう予定です。
Solr初心者の方、Solrに興味のある方などに来ていただきたいと思っています。
（プレゼントも用意できるかも！？）</p>
<p>ということで、「改訂版Apache Solr入門」をよろしくお願いします。
（もちろん、購入は下のリンクからですよね！）</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&o=9&p=8&l=as1&asins=4774161632&nou=1&ref=tf_til&fc1=000000&IS2=1&lt1=_blank&m=amazon&lc1=0000FF&bc1=000000&bg1=FFFFFF&f=ifr" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-extended-analyzeを公開？</title>
      <link>https://blog.johtani.info/blog/2013/11/14/release-elasticsearch-extended-analyze-0-dot-5/</link>
      <pubDate>Thu, 14 Nov 2013 17:55:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/14/release-elasticsearch-extended-analyze-0-dot-5/</guid>
      <description>どーも。以前の記事で開発中としていたプラグインですが、とりあえず、pluginコマンドでインストール出来る形にしてみました。 インストールなど</description>
      <content:encoded><p>どーも。以前の記事で開発中としていたプラグインですが、とりあえず、pluginコマンドでインストール出来る形にしてみました。</p>
<p>インストールなどについては、<a href="https://github.com/johtani/elasticsearch-extended-analyze">READMEに記載した</a>のでそちらを参照してもらうことにして、試行錯誤した話をメモとして残しておきます。</p>
<!-- more -->
<p>プラグインの開発はしいてたのですが、やっぱりpluginコマンドでインストール出来ないと使ってもらえないよなということで、勉強会も終わったのでちょっと調べてました。</p>
<h2 id="プラグインコマンド">プラグインコマンド</h2>
<p>コマンドが用意されてますが、実態はJavaで実装されてて、通常はこんなかんじでプラグインをインストールします。</p>
<pre><code>./bin/plugin -i elasticsearch/elasticsearch-analysis-kuromoji/1.6.0
</code></pre><p>この「<code>elasticsearch/elasticsearch-analysis-kuromoji/1.6.0</code>」という文字列ですが、「<code>ユーザ名/リポジトリ名/バージョン</code>」という意味になります。</p>
<p>で、ダウンロードするURLは以下のものの中から選ばれます。</p>
<ul>
<li>elasticsearch.orgのダウンロード用サイト</li>
<li>search.maven.org</li>
<li>oss.sonatype.org</li>
<li>Githubのarchive</li>
</ul>
<p>これらのサイトに先ほどのユーザ名、リポジトリ名、バージョンを利用したURLを組み立てて、ダウンロードしてくれるという仕組みになっています。</p>
<p>elasticsearch.orgについては、本家の人しかアップロードできないと思うので、なし。<br>
maven、sonatypeについては、Mavenのリポジトリにリリースする必要があるんじゃないかなと。
で、昔<strike>調べて</strike>ググって途中で挫折したんですが、挫折してます。手順が結構手間で。。。
（参考記事：<a href="http://samuraism.jp/diary/2012/05/03/1336047480000.html">【最新版】Maven Central Repository へのライブラリ登録方法 #maven</a>）</p>
<p>ということで、Githubにアップしたらなんとかなるんじゃん？ということで色々と調査して試してみました。（結果はイマイチなんですが。。。）</p>
<h2 id="その１mvn-releaseprepare">その１：mvn release:prepare</h2>
<p>せっかくGithubだし、せっかくMavenなんだしなんか、pom.xmlに便利な設定したらコマンド一発でリリースできるんじゃない？という甘い気持ちで<strike>調査した</strike>ググったらそれっぽい記事が見つかりました。
「<a href="http://www.kanasansoft.com/weblab/2009/11/integration_between_maven_and_github.html">MavenとGitHubの連携</a>」って記事です。<br>
で、pom.xmlの設定にも他のプラグインを真似してコピペしたものに<code>&lt;scm&gt;</code>ってタグがあったなぁと。このコマンドでついでにGithubにアップロードできるんじゃないの？ということで、試してみました。</p>
<pre><code>mvn release:prepare
</code></pre><p>このコマンドを叩くと、記事にあるとおりにいくつか質問をされます。
タグについては、<code>プロジェクト名-バージョン番号</code>という文字列がデフォルトだと指定されているので、<code>v0.5</code>と変更して実施してみると、Githubのreleaseにv0.5ってのができてるじゃないですか。<br>
※<code>plugin</code>コマンドはGithubを見に行く時に次のファイルをダウンロードしに行きます。</p>
<pre><code>https://github.com/ユーザ名/リポジトリ名/archive/vバージョン名.zip
</code></pre><p>やった！と思い、早速pluginコマンドを実行してみましたが、エラーが出ました。。。</p>
<pre><code>Trying https://github.com/johtani/elasticsearch-extended-analyze/archive/v0.5.zip...
Downloading ...DONE
Installed johtani/elasticsearch-extended-analyze/0.5 into /Users/johtani/projects/tmp/ess_env/second_node/elasticsearch-0.90.7/plugins/extended-analyze
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
...省略...

Message:
   Error while installing plugin, reason: IllegalArgumentException: Plugin installation assumed to be site plugin, but contains source code, aborting installation.
</code></pre><p>あらら、なんで？と。<br>
で、実際にgithubにアップされてたzipファイルをダウンロードしてみたら、githubのリポジトリにあるディレクトリ構成がそのまま入ってるじゃないですか。。。
そうですか、そうですよね。prepareだし、タグ打ってzipにかためてくれるだけなんですねと。。。</p>
<p>おそらく、siteプラグイン<em>だけ</em>の場合はこの方法でpluginコマンド叩けばOKなんでしょうが、私がダウンロードしてもらいたいのは.jarファイルが入ったzipファイルなんです。</p>
<p>ということで、断念しました。（タグ消したりをgitコマンドで叩いて綺麗にし直すとか虚しい作業をしてました）</p>
<h2 id="その２githubcomのwebでリリース">その２：github.comのWebでリリース</h2>
<p>おとなしく、Sonatypeのサイトにアップロードする方向でがんばればいいんですが、とりあえず使えるようにするのが先だと思い、
github.comのページでアップロードしてしまおうと。</p>
<p>「release」というタブをクリックすると、画面からアップロードできるようになります。<br>
zipファイルを作ってアップロードしました。（zipファイル自体は<code>mvn package</code>コマンドを実行したら<code>target/release</code>というディレクトリに作成されてる）</p>
<p>これで行けるだろということで、またpluginコマンドを実行すると</p>
<pre><code>Trying https://github.com/johtani/elasticsearch-extended-analyze/archive/v0.5.zip...
Downloading ...DONE
Installed johtani/elasticsearch-extended-analyze/0.5 into /Users/johtani/projects/tmp/ess_env/second_node/elasticsearch-0.90.7/plugins/extended-analyze
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
...省略...

Message:
   Error while installing plugin, reason: IllegalArgumentException: Plugin installation assumed to be site plugin, but contains source code, aborting installation.
</code></pre><p>あれ？同じエラー？なんで？jar入りのzipファイルアップロードしたのに？？？</p>
<p>と。で、<code>https://github.com/johtani/elasticsearch-extended-analyze/releases</code>にreleaseのページができてたので見てみると、あら。
アップロードしたファイルについては次のようなURLになってるじゃないですか。</p>
<pre><code>https://github.com/johtani/elasticsearch-extended-analyze/releases/download/v0.5/v0.5.zip
</code></pre><p>で、よく見ると「Source code(zip)」というボタンもあるぞ？このリンクは？</p>
<pre><code>https://github.com/johtani/elasticsearch-extended-analyze/archive/v0.5.zip
</code></pre><p>。。。あぁ。そうですか。そういうことですか。理解してない私が悪いんですねと。</p>
<h2 id="結論">結論？</h2>
<p>ということで、とりあえず、releaseにjar入りファイルはアップロードできた（手動で）ので
<code>-u</code>オプションで直接URL指定すればインストールできるだろ！と諦めました。
いい勉強になりました。。。</p>
<p>README見ていただくとインストール方法が分かりますが、長いです。。。</p>
<p>時間をとって本腰入れてSonatypeにMavenコマンドでアップロードできるようにしようかな。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第2回elasticsearch勉強会を開催しました！ #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2013/11/12/elasticsearch-japan-user-meetup-no2/</link>
      <pubDate>Tue, 12 Nov 2013 18:16:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/12/elasticsearch-japan-user-meetup-no2/</guid>
      <description>第2回を開催しました！ すごい、140人くらいくらいの参加登録者（参加者は１００人ちょっと！）がいて、びっくりです。 ステキな会場を提供していた</description>
      <content:encoded><p>第2回を開催しました！
すごい、140人くらいくらいの参加登録者（参加者は１００人ちょっと！）がいて、びっくりです。
ステキな会場を提供していただいた、<a href="http://recruit-tech.co.jp/recruitment/">リクルートテクノロジーズ</a>さん、運営していただいた方々、スピーカーの皆さん、参加者の皆さん本当にありがとうございました。
今回も素敵な看板ありがとうございます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20131112/es_signboard.jpg" />
    </div>
    <a href="/images/entries/20131112/es_signboard.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>今回もしっかり楽しめたので、次回も頑張ります！</p>
<p>今回は、<a href="https://groups.google.com/forum/#!forum/elasticsearch-jp">elasticsearch-jp</a>MLの紹介とかをできたのでよかったかなぁと。
ぜひ、活用してください！どんな質問でもいいので。</p>
<p>あと、スライドに入ってた例の本もよろしくです。</p>
<!-- more -->
<p>ということで、懇親会も盛り上がったし楽しかったです。
今後も場の提供＋自分の勉強のトリガーとして、開催していくので、ご協力お願いします！
聞きたい話など、MLや@ツイートしていただければと。</p>
<h2 id="elasticsearchのrouting機能株式会社シーマーク大谷純johtani">elasticsearchのRouting機能：株式会社シーマーク　大谷　純　（@johtani）</h2>
<p>スライド：<a href="/images/entries/20131112/About_es_routing.pdf">Routing機能</a>※スライドはPDFです。</p>
<p>ド緊張で、大した発表ではなかったですが。。。
どちらかと言うとSolr本の紹介だったかもなぁ。スミマセン。</p>
<p>※スライドが一部文字が消えてるので、作りなおすかも。</p>
<h2 id="elasticsearchを使ったbaas基盤の開発仮株式会社富士通ソフトウェアテクノロジーズ-滝田聖己さんpisatoshi">ElasticSearchを使ったBaaS基盤の開発(仮)：株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん（@pisatoshi）</h2>
<p>スライド：<a href="https://speakerdeck.com/pisatoshi/elasticsearch-trial-and-error">https://speakerdeck.com/pisatoshi/elasticsearch-trial-and-error</a></p>
<p>本日はお越しいただきありがとうございました！しかも静岡から！今後もよろしくお願い致します。</p>
<ul>
<li>
<p>EnchantMoonでシステム構成ｗ</p>
</li>
<li>
<p>0.17.0から利用されていると。（スゴイ）</p>
</li>
<li>
<p>プライマリのデータストア！ただし、登録元データはMySQLにもある。</p>
</li>
<li>
<p>階層も深く、大きめのドキュメント。</p>
</li>
<li>
<p>レプリカ１、インデックスのバックアップも取ってないと。。。</p>
</li>
<li>
<p>ルーティングの機能</p>
</li>
<li>
<p>DynamicMappingの問題点</p>
</li>
<li>
<p>マッピング定義が肥大、型がコンフリクト。。。苦労しっぱなし</p>
</li>
<li>
<p>データ登録は１台にして、１台で一気に登録してから再配置</p>
</li>
<li>
<p>実際に運用とかされてるので、いろんなノウハウがまだまだありそう！</p>
</li>
</ul>
<h2 id="kibana入門水戸祐介さんy_310">Kibana入門：水戸祐介さん（@y_310）</h2>
<p>スライド：<a href="https://speakerdeck.com/y310/kibanaru-men">https://speakerdeck.com/y310/kibanaru-men</a></p>
<p>（やっぱりru-menになってるｗ）</p>
<p>実は、押しかけて話してもらうように説得したのでした。今後もよろしくです。</p>
<ul>
<li>COOKPADの方によるKibanaのお話。</li>
<li>Kibanaの利点とかなんで？とか。</li>
<li>画面構成の説明から</li>
<li>ダッシュボードは必ず保存して！リロードしたら悲しい思いをしてしまうので。</li>
<li>sparkline便利そうだなぁ。ほんとに、データサイエンティスト系のツールを目指してるのかな</li>
<li>一通り、ダッシュボードに配置できるパネルの説明してもらえたのですごく参考になりました！</li>
<li>Tips周りが役に立ちそう。not_analyzedは重要ですよね。</li>
</ul>
<h2 id="lt">LT</h2>
<h3 id="データ集計用ダッシュボードブラウザとしても使えるelasticsearchkibana-v3を利用する際の運用ノウハウ紹介株式会社リブセンス-ykentaro-さん-yoshi_ken-さん">「データ集計用ダッシュボードブラウザとしても使えるElasticSearch＋Kibana v3を利用する際の運用ノウハウ紹介」：株式会社リブセンス Y.Kentaro さん (@yoshi_ken) さん</h3>
<p>スライド：<a href="http://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips">http://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips</a></p>
<ul>
<li>Kibanaの紹介とかFluentdの紹介。</li>
<li>Tips満載すばらしい。</li>
<li>JDBC riverは0.90.6ではうまく動かないので、気をつけてと。</li>
</ul>
<h3 id="fluentd-as-a-kibanarepeatedly-さん">「Fluentd as a Kibana」：@repeatedly さん</h3>
<p>スライド(gist)？：<a href="https://gist.github.com/repeatedly/7427856">https://gist.github.com/repeatedly/7427856</a></p>
<p>Kibanaがfluentdの中で動くと！？</p>
<h3 id="authプラグインでアクセスコントロール株式会社エヌツーエスエム-菅谷信介さん-shinsuke_sugaya">「Authプラグインでアクセスコントロール」：株式会社エヌツーエスエム 菅谷信介さん (@shinsuke_sugaya)</h3>
<p>スライド：<a href="http://www.slideshare.net/shinsuke/es-auth-plugin">http://www.slideshare.net/shinsuke/es-auth-plugin</a></p>
<p>API毎？インデックスごと？にアクセス制御ができるプラグイン</p>
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera World Tokyo 2013に参加しました！ #cwt2013 </title>
      <link>https://blog.johtani.info/blog/2013/11/07/cloudera-world-tokyo-2013/</link>
      <pubDate>Thu, 07 Nov 2013 10:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/07/cloudera-world-tokyo-2013/</guid>
      <description>Cloudera World Tokyo 2013に参加してきました。 午前中はあいにくの雨でしたが、それでも結構な人数が最初の基調講演から参加されてました。 私が参加したセッショ</description>
      <content:encoded><p><a href="http://www.cloudera.co.jp/jpevents/cwt2013/index.html">Cloudera World Tokyo 2013</a>に参加してきました。</p>
<p>午前中はあいにくの雨でしたが、それでも結構な人数が最初の基調講演から参加されてました。
私が参加したセッションは大盛況な感じでした。</p>
<p>おみやげとしてカステラも頂いちゃいました！</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20131107/kasutera.jpg" />
    </div>
    <a href="/images/entries/20131107/kasutera.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>また、色々なセッションに現れたこんなメッセージ画像も見つけました！</p>
<blockquote class="twitter-tweet"><p>昨日の写真データの整理をしていたら、こんなものが・・・ <a href="https://twitter.com/shiumachi">@shiumachi</a> さんよ・・・ <a href="https://twitter.com/search?q=%23cwt2013&amp;src=hash">#cwt2013</a> <a href="http://t.co/S0JsxSYXIx">pic.twitter.com/S0JsxSYXIx</a></p>&mdash; Kenichiro HAMANO (@hamaken) <a href="https://twitter.com/hamaken/statuses/398613935399510016">November 8, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<!-- more -->
<p>やっぱり、スーツの人が多いなという印象。</p>
<p>名刺を毎回回収されるのはちょっとつらかったです。なにか、いい方法ないですかねぇ。</p>
<p>以下はいつもの個人メモです。</p>
<h2 id="ビッグデータプラットフォームとして進化するhadoop">「ビッグデータプラットフォームとして進化するHadoop」</h2>
<h4 id="cloudera株式会社代表取締役ジュセッペ小林氏">Cloudera株式会社　代表取締役　ジュセッペ小林氏</h4>
<ul>
<li>Costcoなどの写真を元にビッグデータを可視化</li>
</ul>
<h3 id="bigdataとhadoopの関係">BigDataとHadoopの関係</h3>
<p>検索、SQL、機会学習、数理処理、データ管理などにもHadoopの活用されつつある。
セキュリティ、データ管理、クラスタ上でのツールの実行なども増えてきてる。</p>
<p>「今日ビッグデータは明日のスモールデータ」</p>
<h4 id="アーキテクチャとしてのビッグデータ">アーキテクチャとしてのビッグデータ</h4>
<p>多種多様なデータを一箇所に集約し、生データを直接活用できる。
OSSとしての責任も。</p>
<h3 id="データサイエンス">データサイエンス</h3>
<p>Opsだけでないデータ解析にも活用</p>
<h2 id="clouderaのビッグデータプラットフォーム戦略仮">「Clouderaのビッグデータプラットフォーム戦略」（仮）</h2>
<h4 id="講師cloudera-inc-cto-dramr-awadallah">講師：Cloudera, Inc. CTO Dr.Amr Awadallah</h4>
<p>レガシーな情報アーキテクチャ→スケールできない、可視化の限界、硬直したスキーマなどなど。</p>
<p>エンタープライズデータハブとしてのHadoopとか。</p>
<h2 id="ビッグデータの歴史と将来展望">ビッグデータの歴史と将来展望</h2>
<h4 id="講師国立情報学研究所アーキテクチャ科学研究系教授佐藤一郎氏">講師：国立情報学研究所　アーキテクチャ科学研究系　教授　佐藤一郎氏</h4>
<h3 id="ビッグデータの歴史的経緯とか">ビッグデータの歴史的経緯とか</h3>
<ul>
<li>
<p>最初の事例はアメリカの1880年国勢調査。</p>
</li>
<li>
<p>「ビッグデータがコンピュータを生み出した」。コンピュータがビッグデータを生み出したんじゃない。</p>
</li>
<li>
<p>少量データにもHadoopを</p>
<ul>
<li>バッチ処理のリアルタイム化とか（一晩から10分へ）</li>
</ul>
</li>
<li>
<p>原点は検索データのインデクシング</p>
</li>
<li>
<p>Hadoopを使うのが目的じゃないんだから、構築には手を掛けないのがいいよね。</p>
</li>
<li>
<p>プラットフォームと発展している</p>
</li>
</ul>
<h3 id="分散システム研究者から見たhadoop">分散システム研究者から見たHadoop</h3>
<ul>
<li>分散ししテムの難しさを、処理範囲を限定することで巧みに回避</li>
<li>データの近くで処理</li>
<li>研究レベルではリアルタイム化や逐次処理化が活発</li>
<li>全工程で逐次・リアルタイムが必要とは限らない</li>
<li>聞いてばかりじゃなくて、動かしてみましょう。</li>
</ul>
<h2 id="データサイエンス超並列分散処理を活用した新たなビジネス価値の創出">データサイエンス：超並列分散処理を活用した新たなビジネス価値の創出</h2>
<h4 id="講師アクセンチュア株式会社工藤卓哉氏">講師：アクセンチュア株式会社　工藤卓哉氏</h4>
<ul>
<li>「日経BPのビッグデータ総覧2013」に記事書いてる。</li>
<li>多様化するデータ（社外のデータも）をどうやってうまく活用していくか。</li>
<li>データが教えてくれたこと→まず、データありき、まずデータためましょう。それから解析とかすればいいのでは？というはなし？</li>
<li>競合他社さんはNGだけど、ブースでデモ？実機？が見れますと。</li>
</ul>
<h2 id="hadoopデータプラットフォーム">Hadoopデータプラットフォーム</h2>
<h4 id="cloudera株式会社嶋内翔氏">Cloudera株式会社　嶋内　翔氏</h4>
<h3 id="まずは宣伝">まずは宣伝</h3>
<ul>
<li>Cloudera Implaraのフリーブックの日本語版</li>
<li>Hadoop Operationの書籍でるよ</li>
</ul>
<h3 id="プラットフォームを構成するもの">プラットフォームを構成するもの</h3>
<ul>
<li>Flume</li>
<li>Sqoop</li>
<li>HBase</li>
<li>Hive</li>
<li>Impala</li>
</ul>
<h3 id="データ登録してbiアナリストのお仕事にどうやって役立てる">データ登録してBIアナリストのお仕事にどうやって役立てる？</h3>
<ul>
<li>外部テーブル：Hiveからはテーブルのように見える仕組み。元ファイルは消えない</li>
<li>SerDe（さーでぃー）：データをHiveレコードに変換する仕組み</li>
<li>生データを少し加工しましょう
<ul>
<li>圧縮したりファイル結合したりはしときましょう。</li>
</ul>
</li>
<li>Hadoop活用のポイント
<ul>
<li>富豪的プログラミング。リソースケチるな。</li>
<li>ローカルでできることはローカル。むりにHadoopでやんなくてもいいですよねと。バランス重要</li>
</ul>
</li>
<li>スケジューリング実行などはOozie使うと便利。（日次集計とか）</li>
<li>Cloudera Searchで元データにインデックス貼れるぞと。検索しながら分析ができる</li>
</ul>
<h3 id="クラスタ管理とか">クラスタ管理とか</h3>
<ul>
<li>Cloudera manager便利ですよ</li>
<li>ストレージリソースの管理。
<ul>
<li>声掛け、管理者が容量チェック、Cloudera Managerのレポート</li>
</ul>
</li>
<li>少数精鋭でHadoop使おう＝手が回らなくなる。</li>
<li>みんなで使おう＝Kerberos認証とか管理をちゃんと考えないと。けど、文化が根付けば強力。Sentry、Cloudera Navigatorとか。</li>
<li>Hadoopシステムの全体構成図。データの流れと各製品のつながり。</li>
<li>We are hiring!ということで、興味のある方は@shiumachiさんにコンタクトをとりましょうとのこと。</li>
</ul>
<h2 id="sqlで実現するバッチ処理とストリーム処理">SQLで実現するバッチ処理とストリーム処理</h2>
<h4 id="line株式会社田籠-聡氏">LINE株式会社　田籠 聡氏</h4>
<p>資料：<a href="http://www.slideshare.net/tagomoris/batch-and-stream-processing-with-sql">Batch and Stream processing with SQL</a></p>
<ul>
<li>
<p>LINEのキャラがちらほら出てきた。</p>
</li>
<li>
<p>SQL好きですか？</p>
</li>
<li>
<p>ログの量とか。2.1TB/Day</p>
</li>
<li>
<p>バッチ処理とストリーム</p>
<ul>
<li>速い集計のためにHadoopが重要</li>
</ul>
</li>
<li>
<p>エラー系のログとかはストリームで処理したい</p>
</li>
<li>
<p>アーキテクチャ説明</p>
</li>
<li>
<p>データ解析する人って色々。</p>
<ul>
<li>管理者</li>
<li>プログラマ</li>
<li>サービスディレクタ</li>
<li>経営陣</li>
</ul>
</li>
<li>
<p>みんなが集計用処理を理解、編集ができるほうがいい。</p>
</li>
<li>
<p>顔あげたらHiveアイコンだらけだったｗ</p>
</li>
<li>
<p>Shibとか。</p>
</li>
<li>
<p>なんでHiveに限るの？</p>
<ul>
<li>Hiveに着目したバージョンアップだけを考えれば良くなる。</li>
</ul>
</li>
<li>
<p>スケジュールクエリが増えてきて、つらい。</p>
<ul>
<li>TimeWindowを固定して集計処理をすることで、回避できる。</li>
</ul>
</li>
</ul>
<h3 id="norikra">Norikra!!</h3>
<ul>
<li>
<p>スキーマレス</p>
</li>
<li>
<p>OSS。Esperベース。</p>
</li>
<li>
<p>インストールが楽</p>
</li>
<li>
<p>クエリの動作のお話。</p>
</li>
<li>
<p><a href="http://norikra.github.io">http://norikra.github.io</a></p>
</li>
<li>
<p>We Are Hiring!</p>
</li>
</ul>
<h2 id="hadoop-コミュニティと-yarn-の現状">Hadoop コミュニティと YARN の現状</h2>
<h4 id="日本電信電話株式会社小沢-健史氏">日本電信電話株式会社　小沢 健史氏</h4>
<h3 id="なんでhadoop">なんでHadoop？</h3>
<ul>
<li>PostgreSQLでやってたけど、大きなデータにはHadoopを使おうという感じになってきた。</li>
<li>なんで使い分けるの？
<ul>
<li>スキーマ後付け</li>
</ul>
</li>
<li>NTTDocomoのモバイル位置情報の統計処理とか？</li>
<li>技術的な話をするので、HiveTに着替えます！ｗ</li>
<li>YARNのなにが嬉しいの？
<ul>
<li>ImpalaとMapReduceが同時に動くような環境の時に、リソースをうまく管理できないのがV1</li>
<li>そこでYARN</li>
</ul>
</li>
<li>Apache Mesosとだいたい一緒。</li>
<li>Apache MesosとYARNの比較</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Riak Meetup #3 #riakjp に参加しました。</title>
      <link>https://blog.johtani.info/blog/2013/11/06/riak-meetup-tokyo-no3/</link>
      <pubDate>Wed, 06 Nov 2013 23:01:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/06/riak-meetup-tokyo-no3/</guid>
      <description>またまた、Riak Meetup Tokyo #3に参加してきました。Riak2.0のYokozunaの話があると聞いたので。 ということで、いつものように個人メモで</description>
      <content:encoded><p>またまた、<a href="http://connpass.com/event/3600/">Riak Meetup Tokyo #3</a>に参加してきました。Riak2.0のYokozunaの話があると聞いたので。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20131107/bakusoku.jpg" />
    </div>
    <a href="/images/entries/20131107/bakusoku.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>ということで、いつものように個人メモです。</p>
<!--  more -->
<p>今日は自重して、懇親会はアルコールなしにしました。思いがけずfluentd＋elasticsearch+kibanaという組み合わせの話も聞けて大満足です。
Yokozunaのデモが見れなかったのがちょっと残念だったかなぁ。</p>
<p>少しだけSolr本の宣伝もしてきちゃいました。</p>
<h2 id="古城さんmixi-riakcsとmixi-プライベートクラウド環境">古城さん@Mixi 「RiakCSとmixi プライベートクラウド環境」</h2>
<h3 id="プライベートクラウドとは">プライベートクラウドとは？</h3>
<ul>
<li>計算リソースが安い（ストレージは微妙）</li>
<li>開発者が好きに使える環境＋運用側のチケットに追われる毎日からの開放</li>
</ul>
<h3 id="riak-csの概要">Riak CSの概要</h3>
<ul>
<li>S3互換の分散ファイルストレージ</li>
</ul>
<h3 id="検証とか">検証とか</h3>
<ul>
<li>
<p>検証時はRiak CS 1.3</p>
</li>
<li>
<p>5ノードで2Mファイルを40万個</p>
</li>
<li>
<p>削除は遅延で削除される</p>
</li>
<li>
<p>PUT性能とか。</p>
</li>
<li>
<p>Riak CSの死活監視にはRiakのstatsとか</p>
</li>
<li>
<p>障害対応とか</p>
</li>
</ul>
<h3 id="クラスタ">クラスタ</h3>
<ul>
<li>クラスタは3つ。ログ、サービス（画像） 、バックアップ保存用</li>
<li>ログの収集にfluentd、解析に<strong>elasticsearch</strong>や<strong>kibana</strong>を使っているらしい。</li>
</ul>
<h2 id="篠原basho-riak-20-分散データ型セキュリティそしてより容易な運用へ">篠原＠Basho 「Riak 2.0: 分散データ型、セキュリティ、そしてより容易な運用へ」</h2>
<ul>
<li>riak 2.0 pre5をリリース</li>
<li>アプリ向け機能強化、運用の容易性</li>
</ul>
<h3 id="設計ポリシーとか1xとか">設計ポリシーとか、1.xとか</h3>
<ul>
<li>運用、高可用性、水平拡張性</li>
<li>Capabilityネゴシエーションとか</li>
</ul>
<h3 id="20の機能強化">2.0の機能強化</h3>
<h4 id="バケットタイプ">バケットタイプ</h4>
<ul>
<li>キーの名前空間としてバケットがあったけど、同種のバケットをまとめて管理とかしたくなった。</li>
<li>データ型の導入（CRDT）</li>
<li>Setデータ型</li>
<li>セキュリティとか</li>
<li>強い整合性&hellip;</li>
</ul>
<h2 id="鈴木basho-yokozuna-riak-20の新しい全文検索機能">鈴木@Basho 「Yokozuna: Riak 2.0の新しい全文検索機能」</h2>
<h3 id="yokozunaやsolrの概要">YokozunaやSolrの概要</h3>
<ul>
<li>1ノードにRiakとSolrが1プロセスずつ</li>
<li>SolrプロセスはRiakが管理してくれる</li>
<li>SolrCloudは使ってません。</li>
<li>Riakのノードに届いたデータをSolrに裏で書き込んでくれる。</li>
<li>検索時にはRiakのノードが分散検索をしてくれる</li>
<li>X-Riak-Metaもインデクシングしてくれる。jsonやxmlの各要素をSolrのフィールドとして認識してくれる。</li>
<li>Extractor</li>
</ul>
<p>Solrは4.4で、JVMは1.7をユーザが入れるらしい。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-extended-analyzeの改良</title>
      <link>https://blog.johtani.info/blog/2013/11/04/improve-output-extended-analyze/</link>
      <pubDate>Mon, 04 Nov 2013 22:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/04/improve-output-extended-analyze/</guid>
      <description>開発中ですと書きました、elasticsearch-extended-analyzeですが、改良しました。 改良と変更は以下のとおりです。 ソー</description>
      <content:encoded><p><a href="/blog/2013/10/25/developing-es-extended-analyze-plugin/">開発中です</a>と書きました、<a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analyze</a>ですが、改良しました。</p>
<!-- more -->
<p>改良と変更は以下のとおりです。</p>
<ul>
<li>ソースのパッケージを<code>org.elasticsearch</code>から<code>info.johtani</code>に。MLで気になったので質問したら、変えたほうがいいよとのこと。ダウンロード化については、もう少々お待ちを。</li>
<li>出力形式を変更。可能な限りCharFilter、Tokenizer、TokenFilterそれぞれが出力する内容を返すようにしました。
<ul>
<li>ただし、既存のAnalyzer（JapaneseAnalyzerクラスとか）に関しては、現時点では出力しません。CharFilterなどを取得するI/Fが見えないためです。（改良できるかの調査は未着手）</li>
</ul>
</li>
</ul>
<p>現時点でできてないのは以下の項目</p>
<ul>
<li>pluginコマンドでインストール</li>
<li>出力したいAttributeの指定</li>
<li>TokenizeChainで変更されたTokenの追跡（現状はどのTokenがStopFilterで消されたかなどが不明）</li>
<li>画面の用意（簡単に確認できる画面）</li>
</ul>
<p>ということで、README.mdに出力サンプルは貼り付けてるので、興味のある方は試してみてください。
不明点などあれば、コメントかIssueかツイートでも。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-extended-analyzeプラグインを開発中</title>
      <link>https://blog.johtani.info/blog/2013/10/25/developing-es-extended-analyze-plugin/</link>
      <pubDate>Fri, 25 Oct 2013 19:06:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/10/25/developing-es-extended-analyze-plugin/</guid>
      <description>お久しぶりです。 気づいたらまた、結構ブログを書いてなかったです。。。 今回は、今開発しているElasticsearchのプラグインに関するお話</description>
      <content:encoded><p>お久しぶりです。
気づいたらまた、結構ブログを書いてなかったです。。。</p>
<p>今回は、今開発しているElasticsearchのプラグインに関するお話です。</p>
<p>いやぁ、名前決めるの難しいですね。これで英語的に合ってるか不安ですが、<a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analyze</a>というプラグインを作っています。</p>
<!-- more -->
<h2 id="どんなもの">どんなもの？</h2>
<p>Solrの管理画面のanalysisに相当する機能が欲しくて作り始めました。</p>
<p>Elasticsearchには<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-analyze.html">analyze API</a>というAPI（名前あってるのかなぁ?）が存在します。<br>
これは、文字列を投げると、指定したアナライザやトークナイザでどのようなトークンに分割されるかを調べることができるAPIです。</p>
<p>例えば、<a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">elasticsearch-analysis-kuromoji</a>をインストールしたElasticsearchに対して、以下のcurlコマンドを実行します。</p>
<pre><code>curl -XPOST 'localhost:9200/_analyze?tokenizer=kuromoji_tokenizer&amp;filters=kuromoji_baseform&amp;pretty' -d '寿司が美味しい'
</code></pre><p>すると、トークナイズされた結果が次のようなJSONで返ってきます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;tokens&#34;</span> : [ {
    <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;寿司&#34;</span>,
    <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">0</span>,
    <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">2</span>,
    <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
    <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">1</span>
  }, {
    <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;が&#34;</span>,
    <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">2</span>,
    <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">3</span>,
    <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
    <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">2</span>
  }, {
    <span style="color:#f92672">&#34;token&#34;</span> : <span style="color:#e6db74">&#34;美味しい&#34;</span>,
    <span style="color:#f92672">&#34;start_offset&#34;</span> : <span style="color:#ae81ff">3</span>,
    <span style="color:#f92672">&#34;end_offset&#34;</span> : <span style="color:#ae81ff">7</span>,
    <span style="color:#f92672">&#34;type&#34;</span> : <span style="color:#e6db74">&#34;word&#34;</span>,
    <span style="color:#f92672">&#34;position&#34;</span> : <span style="color:#ae81ff">3</span>
  } ]
}
</code></pre></div><p>トークナイズの結果がわかるのは嬉しいのですが、どんな品詞なのかといったKuromoji固有のTokenの属性情報がなくなってしまいます。</p>
<p>Solrでは、こんな画面が用意されていて、品詞情報とかが出力されます。あとは、各TokenFilterでどのトークンがなくなっているかなどもわかるようになっています。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20131025/solr_admin_analysis.jpg" />
    </div>
    <a href="/images/entries/20131025/solr_admin_analysis.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>これって結構役立つと思うんですよ。
ということで、Pluginも作ってみたかったので、いい機会だから作ってみようかと。</p>
<h2 id="出力サンプル">出力サンプル</h2>
<p>まずは、その他のAttribute（品詞とか）を表示するところを実装してみました。</p>
<pre><code>curl -XPOST 'localhost:9200/_extended_analyze?tokenizer=kuromoji_tokenizer&amp;filters=kuromoji_baseform&amp;pretty' -d '寿司が美味しい'
</code></pre><p>先ほどとほぼ一緒のcurlコマンドを実行します。違う点は**「_analyze」**が**「_extended_analyze」**となっている点です。<br>
で、実行結果はこんな感じです。（長いですがそのまま載せてます。続きの文章がしたにあります。）</p>
<pre><code>{
  &quot;tokens&quot; : [ {
    &quot;token&quot; : &quot;寿司&quot;,
    &quot;start_offset&quot; : 0,
    &quot;end_offset&quot; : 2,
    &quot;type&quot; : &quot;word&quot;,
    &quot;position&quot; : 1,
    &quot;extended_attributes&quot; : [ {
      &quot;org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute#bytes&quot; : &quot;[e5 af bf e5 8f b8]&quot;
    }, {
      &quot;org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute#positionLength&quot; : 1
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.BaseFormAttribute#baseForm&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.PartOfSpeechAttribute#partOfSpeech&quot; : &quot;名詞-一般&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.PartOfSpeechAttribute#partOfSpeech (en)&quot; : &quot;noun-common&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#reading&quot; : &quot;スシ&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#reading (en)&quot; : &quot;sushi&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#pronunciation&quot; : &quot;スシ&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#pronunciation (en)&quot; : &quot;sushi&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionType&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionType (en)&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionForm&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionForm (en)&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.tokenattributes.KeywordAttribute#keyword&quot; : false
    } ]
  }, {
    &quot;token&quot; : &quot;が&quot;,
    &quot;start_offset&quot; : 2,
    &quot;end_offset&quot; : 3,
    &quot;type&quot; : &quot;word&quot;,
    &quot;position&quot; : 2,
    &quot;extended_attributes&quot; : [ {
      &quot;org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute#bytes&quot; : &quot;[e3 81 8c]&quot;
    }, {
      &quot;org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute#positionLength&quot; : 1
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.BaseFormAttribute#baseForm&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.PartOfSpeechAttribute#partOfSpeech&quot; : &quot;助詞-格助詞-一般&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.PartOfSpeechAttribute#partOfSpeech (en)&quot; : &quot;particle-case-misc&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#reading&quot; : &quot;ガ&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#reading (en)&quot; : &quot;ga&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#pronunciation&quot; : &quot;ガ&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#pronunciation (en)&quot; : &quot;ga&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionType&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionType (en)&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionForm&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionForm (en)&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.tokenattributes.KeywordAttribute#keyword&quot; : false
    } ]
  }, {
    &quot;token&quot; : &quot;美味しい&quot;,
    &quot;start_offset&quot; : 3,
    &quot;end_offset&quot; : 7,
    &quot;type&quot; : &quot;word&quot;,
    &quot;position&quot; : 3,
    &quot;extended_attributes&quot; : [ {
      &quot;org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute#bytes&quot; : &quot;[e7 be 8e e5 91 b3 e3 81 97 e3 81 84]&quot;
    }, {
      &quot;org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute#positionLength&quot; : 1
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.BaseFormAttribute#baseForm&quot; : null
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.PartOfSpeechAttribute#partOfSpeech&quot; : &quot;形容詞-自立&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.PartOfSpeechAttribute#partOfSpeech (en)&quot; : &quot;adjective-main&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#reading&quot; : &quot;オイシイ&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#reading (en)&quot; : &quot;oishii&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#pronunciation&quot; : &quot;オイシイ&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.ReadingAttribute#pronunciation (en)&quot; : &quot;oishii&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionType&quot; : &quot;形容詞・イ段&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionType (en)&quot; : &quot;adj-group-i&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionForm&quot; : &quot;基本形&quot;
    }, {
      &quot;org.apache.lucene.analysis.ja.tokenattributes.InflectionAttribute#inflectionForm (en)&quot; : &quot;base&quot;
    }, {
      &quot;org.apache.lucene.analysis.tokenattributes.KeywordAttribute#keyword&quot; : false
    } ]
  } ]
}
</code></pre><p>先ほどの結果に**「extended_attributes」**という配列のオブジェクトが追加された形になっています。
ちょっと長くなってしまいましたが。。。</p>
<p>Solrの処理を真似して作ったので大したことはやってないんですが、少しは便利になるかもなぁと。</p>
<p>現時点では、最終的な結果しか取得できないですが、今後は次のような機能を作っていこうかと思っています。
できるかどうかは、やってみてって感じですが。</p>
<ul>
<li>pluginコマンドでインストール
<ul>
<li>pom.xmlはありますが、まだMavenとかに登録はされていません。ですので、<code>mvn package</code>してからjarファイルをpluginsフォルダに配置しないといけません。pluginコマンドでインストールできるともっと使ってもらえるはず？</li>
</ul>
</li>
<li>出力したいAttributeの指定
<ul>
<li>リクエストパラメータで、出力したいAttribute名を指定するとか。</li>
</ul>
</li>
<li>出力形式の変更
<ul>
<li>今は、Solrの真似をしていますが、せっかくJSONだったりするので、もう少し検討しようかと（同じAttributeの異なる値も1オブジェクトとして出力されてる）</li>
</ul>
</li>
<li>TokenizeChainの出力
<ul>
<li>Solr同様、CharFilter、Tokenizer、TokenFilterが動作して、最終的なTokenがインデックスに登録されます。ですので、各処理の直後のTokenがどうなっているかもわかったほうが嬉しいと思うので、それらも取得できるようにしたいなぁと</li>
</ul>
</li>
<li>画面の用意
<ul>
<li>せっかくプラグインなんだし、画面で見れると嬉しいかなと。これは当分先になっちゃうと思いますが、Webページで確認できるような画面を作ると確認しやすくなるかなぁと。上記対応が終わってから取替かかると思いますが。</li>
</ul>
</li>
</ul>
<p>とりあえず、思いつくのはこんなかんじです。</p>
<p>Elasticsearchの_analyze APIを真似しただけのコードだし、テストも実装もまだまだですが、とりあえず公開してみました。</p>
<p>要望などあれば、コメント、Issue、ツイート（もちろん、テストコードなども！）なんでも受け付けてますので、お気軽に。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第12回Solr勉強会を主催しました。#SolrJP</title>
      <link>https://blog.johtani.info/blog/2013/10/10/solr-meetup-memo/</link>
      <pubDate>Thu, 10 Oct 2013 11:35:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/10/10/solr-meetup-memo/</guid>
      <description>不定期開催ですが第12回Solr勉強会を主催しました。 今回は、前回ほどの過熱ぶりでは無かったですが、70人ほどの参加者の方がいらっしゃったか</description>
      <content:encoded><p>不定期開催ですが<a href="http://atnd.org/events/43532/">第12回Solr勉強会</a>を主催しました。</p>
<p>今回は、前回ほどの過熱ぶりでは無かったですが、70人ほどの参加者の方がいらっしゃったかと。
ありがとうございます！</p>
<p>今回は聞きたかったYokozunaの話をしてもらいました。あと、リベンジManifoldCF。
<em>一部、追記しました。Bashoさんからツッコミがあったので。あと、4.5.1の話とか。</em></p>
<!-- more -->
<h2 id="manifoldcfのとsolrの組み合わせ仮株式会社-ロンウイット大須賀稔さん">ManifoldCFのとSolrの組み合わせ（仮）株式会社 ロンウイット　大須賀　稔さん</h2>
<p>前回お休みだったのでリベンジですw。</p>
<p>英語だ。。。やっぱ英語がいいですか、スライド。。。<br>
ManifoldCFの概要から。
最新版は1.3です。色々サポートしてるなぁ。</p>
<p>デモもありました。（やっぱりちゃんと動かないので、鬼門みたいですが）</p>
<h4 id="デモ">デモ</h4>
<p>ManifoldCFのGUIで操作しながら。
いまいちちゃんと動かなかった。。。</p>
<h4 id="qa">QA</h4>
<ul>
<li>Q:Zipはうまく動かなかった</li>
<li>A:Solr側で処理してくれてる。</li>
<li>Q:Notes対応するの？</li>
<li>A:いまのところない。</li>
<li>Q:ExcelとかPDFはTika？</li>
<li>A:Tika次第です。</li>
<li>Q:認証周りどこから取ってくるの？</li>
<li>A:クローラ側にはなくて、SharePointとかの権限をみてる。</li>
<li>Q:Web系の認証は？</li>
<li>A:まだないのでは。。。（調査します）</li>
</ul>
<p>あー、デモの続き忘れてましたね。。。</p>
<h2 id="solrを組み込んだriak-20の全文検索機能--yokozuna--bashoジャパン株式会社鈴木一弘さん">Solrを組み込んだRiak 2.0の全文検索機能 -Yokozuna- Bashoジャパン株式会社　鈴木　一弘さん</h2>
<p>Riak色々使われてるよ！アングリーバードとか、Y!とか。
Riakで提供されている1機能としてのYokozuna。単独製品ではないですよと。</p>
<p>Riakの説明。スケールするよ、いつでもRead/Writeできるよ、運用にフォーカスしてるよと。
マスターレスですよ。
Riak2.0のリリースは2013年末。Yokozunaもかな？</p>
<p>ダイナミックフィールド使ってるので、Yokozunaをonにするだけで簡単に使えるよ。</p>
<p>RiakがSolrのプロセスを管理。</p>
<p>インデックスの不整合の検知とかってどうやってるのかなぁ？
インデックス比較用のハッシュツリーをノード間でコピーしつつ検査してる。（Active Anti-Entropy）</p>
<p>(デモには魔物がいるようだ。。。)</p>
<h4 id="qa-1">QA</h4>
<ul>
<li>Q:JSONの属性を元にしてフィールドにインデックス可能か？</li>
<li>A:可能です。IIJさんの発表で話が出ます。</li>
<li>Q:ProtocolBufferでSolrにアクセス可能？</li>
<li>A:<strike>そのうちできそうです。</strike>リリース時にはできるようになっています。</li>
<li>Q:コアのスワップは？スキーマの変更は？</li>
<li>A:事前に設定するのは可能。</li>
<li>Q:RiakのデータとSolrでデータがずれるってのはあるの？</li>
<li>A:可能性はありますが、<strike>極力ずれ</strike>AAEで修復。</li>
<li>Q:復旧中のインデックスにアクセスが行かないようにする仕組みなどはある？</li>
<li>A:今はないです。</li>
</ul>
<h2 id="yokozuna-ベンチマークしました株式会社インターネットイニシアティブ曽我部崇さん田中-義久さん">Yokozuna ベンチマークしました　株式会社インターネットイニシアティブ　曽我部　崇さん、田中 義久さん</h2>
<p>いいとこ取りで楽だなぁと。いうことで、試してみてます。
デモが動いてる。</p>
<p>extractorでXMLやJSONをパースできる。
ベンチマーク結果。</p>
<p>Riak Meetup Tokyo #2の時のQAも入ってるので助かります。素晴らしい。</p>
<h4 id="qa-2">QA</h4>
<ul>
<li>Q:スナップショットは両方取れるの？</li>
<li>A:Riakは取れますが、インデックスは今は無理です。</li>
<li>フォロー:0.8はYokozunaにボトルネックがあったので、0.9以降だともっと性能が出るはずですとのこと。また次回とかに発表してもらうのもありですかねぇ。</li>
</ul>
<h2 id="solr-45の新機能など-johtani">Solr 4.5の新機能など @johtani</h2>
<p><a href="/images/entries/20131009/Solr4_5_Changes.pdf">発表資料のPDF</a>です。</p>
<p>ツイート見てて誤解を招いたなと思ったのですが、7u40は4.5限定ではなく、すべてのバージョンと考えてください。
チケットを見ると分かりますが、影響バージョンの記載はありません。</p>
<p>※あ、4.5のChangesを紹介しましたが、4.5.1が出るかも。このへんが困ってるらしいです。</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SOLR-5306">SOLR-5306: can not create collection when have over one config</a></li>
<li><a href="https://issues.apache.org/jira/browse/SOLR-5317">SOLR-5317: CoreAdmin API is not persisting data properly</a></li>
<li><a href="https://issues.apache.org/jira/browse/LUCENE-5263">LUCENE-5263: Deletes may be silently lost if an IOException is hit and later not hit (e.g., disk fills up and then frees up)</a></li>
</ul>
<h2 id="lt">LT</h2>
<h3 id="haruyama-さん">@haruyama さん</h3>
<p>資料：<a href="http://haruyama.github.io/solr_20131009/#(1)">http://haruyama.github.io/solr_20131009/#(1)</a></p>
<p>記号が捨てられるTokenizer困るので、捨てないのを作ってみました。</p>
<p>Kuromojiの困ったこと。全角数字を分解しちゃう。→MappingCharFilterFactoryで全角から半角にしましょう。
lucene-gosenデフォで半角記号が未知語になってしまい、半角カナと混ざるので、記号を全角にしましょう。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-inquisitorプラグインの紹介</title>
      <link>https://blog.johtani.info/blog/2013/09/23/intro-elasticsearch-inquisitor/</link>
      <pubDate>Mon, 23 Sep 2013 12:27:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/09/23/intro-elasticsearch-inquisitor/</guid>
      <description>今日は、ElasticSearchのMLで見つけたelasticsearch-inquisitorプラグインの紹介です。 ElasticSea</description>
      <content:encoded><p>今日は、ElasticSearchのMLで見つけた<a href="https://github.com/polyfractal/elasticsearch-inquisitor">elasticsearch-inquisitor</a>プラグインの紹介です。</p>
<p>ElasticSearchはREST API形式で簡単にコマンドラインからいろいろな処理を実行できて便利ですが、
GUIがあったほうが楽なこともまた事実です。
今回紹介する、inquisitorプラグインもSiteプラグイン（Webブラウザでアクセスできるプラグイン）の1つです。
（ただし、ローカルにインストールしてローカルのElasticSearchにしか接続できませんが。。。）</p>
<!-- more -->
<h2 id="インストール">インストール</h2>
<p>プラグインですので、以下のコマンドでインストールが出来ます。インストール後はElasticSearchの再起動が必要です。</p>
<pre><code>bin/plugin -install polyfractal/elasticsearch-inquisitor
</code></pre><p>ElasticSearch再起動後に、以下のURLにアクセスすればOKです。
※ローカルでのみ動作可能なプラグインです。（内部で呼び出しているJSにlocalhostと記載があるため）</p>
<pre><code>http://localhost:9200/_plugin/inquisitor/#/
</code></pre><h2 id="何ができるの">何ができるの？</h2>
<p>自分の書いたQueryが正しく動作するかや、Analyzerによって文章がどのように、Term（Token）に分割されるかといった挙動をWebブラウザ上で確認することができます。用意されている画面は「Queries」「Analyzers」「Tokenizers」の3種類です。</p>
<h3 id="queries">Queries</h3>
<p>クエリの確認、実行が可能な画面です。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130923/queries_sample.jpg" />
    </div>
    <a href="/images/entries/20130923/queries_sample.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>「Index」「Type」はプルダウンになっており、現在ElasticSearchに存在しているものが選択可能です。
その下のテキストエリアがクエリを入力する画面です。</p>
<p>クエリを入力していると、入力しているクエリがValidかどうかをクエリのコンソール部分（右側上部）に表示してくれます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130923/query_error.jpg" />
    </div>
    <a href="/images/entries/20130923/query_error.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>少し残念なことに、Tabを押すと、フォームのフォーカスが切り替わってしまうので、クエリを入力するのがちょっと面倒です。。。（私は通常の検索には、<a href="https://chrome.google.com/webstore/detail/sense/doinijnbnggojdlcjifpdckfokbbfpbo">ChromeプラグインのSense</a>というものを利用してます。）</p>
<p>クエリに問題がない場合は、「Query」ボタンを押すことで実際の検索が実行されます。
この時、画面真ん中のブルーのテーブル（内部で実行されるクエリ）の部分に、QueryがElasticSearch内部で解釈されたあとの、Luceneで実行されるレベルのクエリに変換されたクエリが表示されます。</p>
<p>これが便利です。JSONで記述したり、色々なタイプのクエリがElasticSearchでは実行できますが、望んだ形に単語が区切られているかなどを確認することができるため、非常に便利です。</p>
<p>ElasticSearchのQuery DSLでは<a href="http://www.elasticsearch.org/guide/reference/api/search/explain/">explain</a>をtrueにすることで、ヒットしたドキュメントのスコア計算に用いられた単語などがわかるのですが、そもそもヒットしないクエリの場合は、explainでは単語の区切られ方などがわかりません。</p>
<p>その場合に、このプラグインで確認すると、想定と違う単語の区切られ方やクエリの造られ方がわかるかと思います。</p>
<h3 id="analyzers">Analyzers</h3>
<p>Analyzerによる文章のアナライズ結果の確認が出来る画面です。
ElasticSearchやSolrにあまり詳しくない場合、どんなAnalyzerが文章をどのように単語に区切って、転置インデックスのキーワードとして利用しているかがわからないと思います。</p>
<p>このAnalyzerが文章をどのように単語に区切っているかを確認することができるのがAnalyzers画面です。
こんなかんじの画面になります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130923/analyzers_sample.jpg" />
    </div>
    <a href="/images/entries/20130923/analyzers_sample.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>一番上のテキストエリアが文章を入力する場所です。
文章を入力していくと、その下のテーブルの「Analyzed Text」の部分が変化していくのが分かります。
このグレーの単語が転置インデックスのキーワードとなります。</p>
<p>予め用意されているAnalyzer以外に、用意されているTokenzier＋Filterの組み合わせも簡単ですが確認可能です。（Tokenizer、Filtersとあるテーブル）
ただし、ここまでのどちらも細かな設定は画面上ではできません（Filterの細かな引数の指定など）</p>
<p>一番下の部分が、ElasticSearchに存在しているインデックスごとに定義されたAnalyzerやフィールドを元にした解析結果を表示することができる領域です。</p>
<p>自分でマッピングを記述してフィールド定義したものの動作確認や、インデックスを適当に作ったけど、うまくヒットしない場合など、ここで、単語の区切れ方を確認することで、検索になぜヒットしないのかといった問題のヒントを得ることができると思います。</p>
<p>Analyzerによっては、インデックス対象の文字として扱わない文字があったりしますので。
先ほどのQueries画面のLuceneに投げられる直前のクエリと、Analyzersでの単語の区切られ方を確認することで、検索がうまくヒットしていないことが判明すると思います。</p>
<h3 id="tokenizers">Tokenizers</h3>
<p>最後はTokenizers画面です。Analyzersとほぼ同様ですが、ちがいは、デフォルトで用意されているTokenizerの挙動の確認ができるというだけになります。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130923/tokenizers_sample.jpg" />
    </div>
    <a href="/images/entries/20130923/tokenizers_sample.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>簡単な確認ならここで可能かと。</p>
<h2 id="注意点は">注意点は？</h2>
<p>まだ開発途中のようで、つぎの部分が課題かと。</p>
<ul>
<li>ローカルでのみ実行可能</li>
<li>Queries画面の結果の「Explain Result」リンクが未実装</li>
<li>Queries画面のクエリ入力が使いにくい（タブが打てないので）</li>
<li>カスタム登録のAnalyzersはインデックスを用意しないと確認できない。（Kuromojiのプラグインを登録しただけでは確認できなかった）</li>
<li>細かな設定のフィールドも用意しないと、Analyzers画面では利用できない</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>ということで、Inquisitor（読みがわからない）プラグインの簡単な説明でした。
検索にうまくヒットしないという理由は大体の場合、
クエリに入力した文字列が単語に区切られたものと、登録したデータが単語に区切られたものが異なるために検索にヒットしないというものです。</p>
<p>そのクエリ、データの単語の区切られ方を確認するのに役に立つプラグインじゃないでしょうか。</p>
<p>ちなみに、このプラグイン自体はHTML＋JSで作成されており、実際にはElasticSearchが持っているREST APIをキックしているだけになります。
ですので、Web画面なんか要らないという方は、このプラグインが実際に送信しているリクエストを参考にするとcurlコマンドでどういったリクエストを投げればいいかというのがわかると思います。</p>
<p>私は軟弱者なので画面があったほうがいいですが。</p>
</content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-river-wikipediaの疑問点</title>
      <link>https://blog.johtani.info/blog/2013/09/12/question-river-wikipedia/</link>
      <pubDate>Thu, 12 Sep 2013 02:38:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/09/12/question-river-wikipedia/</guid>
      <description>river-wikipediaの前々回の記事で書きましたが、bulk_sizeに関連して登録件数がやけにきりが良いのが気になると書いていまし</description>
      <content:encoded><p><a href="/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch/">river-wikipediaの前々回の記事</a>で書きましたが、bulk_sizeに関連して登録件数がやけにきりが良いのが気になると書いていました。</p>
<p>で、Riverの仕組みを勉强がてら、<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia">elasticsearch-river-wikipedia</a>のソース（1.2.0）を読んでみました。</p>
<!-- more -->
<h2 id="riverの作り">Riverの作り</h2>
<p>Riverはorg.elasticsearch.river.Riverというinterfaceを実装することで作らています。
ただ、Riverがinterfaceとなっていますが、o.e.river.AbstractRiverComponentというクラスを継承して作られています。</p>
<p>AbstractRiverComponentにはRiverの名前や設定などが用意されています。
ま、ここはそれほど重要じゃないので、軽く流してと。</p>
<p>Riverの設定関連は実装したRiverクラス（ここでは、WikipediaRiverクラス）のコンストラクタで、設定値の読み取りなどの記述を記載します。
このコンストラクタが、<code>_river/hogehoge/_meta</code>をPUTした時のJSONを元にElasticSearchから呼ばれて、Riverのインスタンスが作成されます。（たぶん、<a href="https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/river/RiversService.java">このへんがその処理</a>だと思う。。。このあたりはまた今度）</p>
<p>実際のRiverの処理はWikipediaRiverクラスのstart()メソッド内部に記述されています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java">    <span style="color:#a6e22e">@Override</span>
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">start</span><span style="color:#f92672">()</span> <span style="color:#f92672">{</span>
        logger<span style="color:#f92672">.</span><span style="color:#a6e22e">info</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;starting wikipedia stream&#34;</span><span style="color:#f92672">);</span>
        <span style="color:#66d9ef">try</span> <span style="color:#f92672">{</span>
<span style="color:#960050;background-color:#1e0010">①</span>            client<span style="color:#f92672">.</span><span style="color:#a6e22e">admin</span><span style="color:#f92672">().</span><span style="color:#a6e22e">indices</span><span style="color:#f92672">().</span><span style="color:#a6e22e">prepareCreate</span><span style="color:#f92672">(</span>indexName<span style="color:#f92672">).</span><span style="color:#a6e22e">execute</span><span style="color:#f92672">().</span><span style="color:#a6e22e">actionGet</span><span style="color:#f92672">();</span>
        <span style="color:#f92672">}</span> <span style="color:#66d9ef">catch</span> <span style="color:#f92672">(</span>Exception e<span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
            <span style="color:#66d9ef">if</span> <span style="color:#f92672">(</span>ExceptionsHelper<span style="color:#f92672">.</span><span style="color:#a6e22e">unwrapCause</span><span style="color:#f92672">(</span>e<span style="color:#f92672">)</span> <span style="color:#66d9ef">instanceof</span> IndexAlreadyExistsException<span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
                <span style="color:#75715e">// that&#39;s fine
</span><span style="color:#75715e"></span>            <span style="color:#f92672">}</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">if</span> <span style="color:#f92672">(</span>ExceptionsHelper<span style="color:#f92672">.</span><span style="color:#a6e22e">unwrapCause</span><span style="color:#f92672">(</span>e<span style="color:#f92672">)</span> <span style="color:#66d9ef">instanceof</span> ClusterBlockException<span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
                <span style="color:#75715e">// ok, not recovered yet..., lets start indexing and hope we recover by the first bulk
</span><span style="color:#75715e"></span>                <span style="color:#75715e">// TODO: a smarter logic can be to register for cluster event listener here, and only start sampling when the block is removed...
</span><span style="color:#75715e"></span>            <span style="color:#f92672">}</span> <span style="color:#66d9ef">else</span> <span style="color:#f92672">{</span>
                logger<span style="color:#f92672">.</span><span style="color:#a6e22e">warn</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;failed to create index [{}], disabling river...&#34;</span><span style="color:#f92672">,</span> e<span style="color:#f92672">,</span> indexName<span style="color:#f92672">);</span>
                <span style="color:#66d9ef">return</span><span style="color:#f92672">;</span>
            <span style="color:#f92672">}</span>
        <span style="color:#f92672">}</span>
<span style="color:#960050;background-color:#1e0010">②</span>        currentRequest <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span><span style="color:#a6e22e">prepareBulk</span><span style="color:#f92672">();</span>
<span style="color:#960050;background-color:#1e0010">③</span>        WikiXMLParser parser <span style="color:#f92672">=</span> WikiXMLParserFactory<span style="color:#f92672">.</span><span style="color:#a6e22e">getSAXParser</span><span style="color:#f92672">(</span>url<span style="color:#f92672">);</span>
        <span style="color:#66d9ef">try</span> <span style="color:#f92672">{</span>
<span style="color:#960050;background-color:#1e0010">④</span>            parser<span style="color:#f92672">.</span><span style="color:#a6e22e">setPageCallback</span><span style="color:#f92672">(</span><span style="color:#66d9ef">new</span> PageCallback<span style="color:#f92672">());</span>
        <span style="color:#f92672">}</span> <span style="color:#66d9ef">catch</span> <span style="color:#f92672">(</span>Exception e<span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
            logger<span style="color:#f92672">.</span><span style="color:#a6e22e">error</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;failed to create parser&#34;</span><span style="color:#f92672">,</span> e<span style="color:#f92672">);</span>
            <span style="color:#66d9ef">return</span><span style="color:#f92672">;</span>
        <span style="color:#f92672">}</span>
<span style="color:#960050;background-color:#1e0010">⑤</span>        thread <span style="color:#f92672">=</span> EsExecutors<span style="color:#f92672">.</span><span style="color:#a6e22e">daemonThreadFactory</span><span style="color:#f92672">(</span>settings<span style="color:#f92672">.</span><span style="color:#a6e22e">globalSettings</span><span style="color:#f92672">(),</span> <span style="color:#e6db74">&#34;wikipedia_slurper&#34;</span><span style="color:#f92672">).</span><span style="color:#a6e22e">newThread</span><span style="color:#f92672">(</span><span style="color:#66d9ef">new</span> Parser<span style="color:#f92672">(</span>parser<span style="color:#f92672">));</span>
        thread<span style="color:#f92672">.</span><span style="color:#a6e22e">start</span><span style="color:#f92672">();</span>
    <span style="color:#f92672">}</span>
</code></pre></div><p>内部では</p>
<ol>
<li>インデックスの作成</li>
<li>バルクアップデート用クライアントの設定</li>
<li>WikiXMLのパーサの初期化</li>
<li>ページごとにキックされるコールバック処理の登録</li>
<li>デーモンスレッドの起動と起動</li>
</ol>
<p>といった処理の流れになっています。</p>
<p>で、このスレッドの起動後は、4.で用意したparser.parse()処理がグルグル回ります。</p>
<p>1ページがパースされるたびに、<code>WikipediaRiver.PageCallback</code>クラスの<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L166"><code>proess()</code>メソッド</a>が呼ばれます。
このメソッドの最後で、<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L198"><code>processBulkIfNeeded()</code>メソッド</a>が呼ばれています。ここで、実際にパースしたページをインデックスに登録する処理が実行されます。</p>
<p><a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L205">このメソッドの1行目</a>が鍵でした。
bulkSize以上の件数がバルクのリクエストに貯まった時だけ、実際にインデックスに登録する処理が実行されます。
このため、スレッドが回っている間は、bulkSize以上のデータが貯まらないと、インデックスへの登録は行われないわけです。</p>
<p>次に、このスレッドを止めるには、前々回書いたように、_riverにPUTした、Riverの設定をDELETEするしかありません。（あとは、ElasticSearchを停止するとかでしょうか。）</p>
<p>で、DELETEが実行される呼ばれるのが、<code>WikipediaRiver</code>クラスの<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L135"><code>close()</code>メソッド</a>です。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java">
    <span style="color:#a6e22e">@Override</span>
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">close</span><span style="color:#f92672">()</span> <span style="color:#f92672">{</span>
        logger<span style="color:#f92672">.</span><span style="color:#a6e22e">info</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;closing wikipedia river&#34;</span><span style="color:#f92672">);</span>
        closed <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span><span style="color:#f92672">;</span>
        <span style="color:#66d9ef">if</span> <span style="color:#f92672">(</span>thread <span style="color:#f92672">!=</span> <span style="color:#66d9ef">null</span><span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
            thread<span style="color:#f92672">.</span><span style="color:#a6e22e">interrupt</span><span style="color:#f92672">();</span>
        <span style="color:#f92672">}</span>
    <span style="color:#f92672">}</span>
</code></pre></div><p>見ていただくと分かりますが、スレッド止めて終了です。</p>
<h3 id="問題点は">問題点は？</h3>
<p>ということで、</p>
<ul>
<li>WikipediaのXMLを読み込んでもRiverは停止しない</li>
<li>Riverの停止を行ってもスレッドが止められるだけ。</li>
<li>bulkSize以下の件数が<code>currentRequest</code>に残っているけど、破棄される</li>
</ul>
<p>とまぁ、こんな流れになっているので、最後の端数のドキュメントがインデックスに登録されないようです。
（まだ、ちゃんと確認してないんですが、備忘録のため先に書いちゃいました。。。）</p>
<p>じゃあ、全部うまく登録するにはどうしたもんかなぁと。
いまのところ思いついたのはこんな感じです。
他にいい案があったら教えて下さい。</p>
<ul>
<li>案１：close()処理の中で、スレッド停止後に、<code>currentRequest</code>に貯まっているデータをインデックスに登録しちゃう</li>
<li>案２：bulkSize以外に、定期的（指定された時間）で登録処理を実行してしまう。</li>
</ul>
<p>簡単なのでとりあえず、案１を実装してみるかなぁと。
（さっさとコード書けよって話ですね。。。スミマセン）
その前にMLで質問ですかねぇ、英語で。</p>
<p>WikipediaのRiverをざっと眺めてみた感じですが、わかりやすい作りだなぁと。
他のRiverがどうなってるかをちゃんと見てませんが、他にもbulkSize指定をするRiverの場合は、このように件数がbulkSizeに満たない状態ではデータが登録されないといったことがあるかもしれません。</p>
<p>ElasticSearchのソースを読み始める取っ掛かりとしては面白いかと思いますので、興味ある方は読んで作ってみるといいかもしれません。（私は読んだだけですがｗ）</p>
<h2 id="追記20130913-2100">追記（2013/09/13 21:00）</h2>
<p>MLで質問してみました。とりあえず、案1を。</p>
<p><a href="https://groups.google.com/forum/#!topic/elasticsearch/hqU-LF5aTy4">river-wikipedia does not index all pages</a></p>
<p>他のRiverでは対応してるしバグだね、Issue上げてとのことで、あげときました。
ついでにプルリクも出せばいいんでしょうが、プルリクまだやったことないヘタレです。。。</p>
<p>あと、案2についても同じトピックで質問してます。
どうやら、BulkProcessorにその機能があるよと。
<code>flushinterval</code>というプロパティがありそうです。どうやって設定して、どうやって動くのかとか見てないので、
調査してブログorLTかな。</p>
<p><a href="http://www.elasticsearch.org/guide/reference/api/bulk-udp/">bulk udp</a>にはその値を設定できそうなのがあるんだよなぁ。</p>
<h2 id="追記その２20130916-2350">追記その２（2013/09/16 23:50）</h2>
<p>さっそく<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/commit/3719ac5cd3cd5f0e4e57edaa72f5d4ca0b45ca5d">修正版がコミット(コミットログ)</a>されてました。
結構変わってます。BulkProcessorに<code>flush_interval</code>の設定をすれば、よしなにやってくれる仕組みがすでに実装されているようです。
<code>bulkSize</code>についても同様に、BulkProcessorに設定すれば良いようです。
Riverの仕組みが結構スッキリしています。
もともと実装されていた、bulkSizeごとの処理も消されています。
確かに、BulkProcessorの仕組みとして実装されている方がしっくりきますね。</p>
<p>ということで、考える暇もなくコミットされてしまいました。
こうやって質問しつつ、少しずつソースを読んでいこうかなと思ってるとこです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>日本語Wikipediaをインデクシング（Kuromojiバージョン）</title>
      <link>https://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji/</link>
      <pubDate>Tue, 03 Sep 2013 01:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji/</guid>
      <description>前々回紹介した、日本語Wikipediaのデータをインデックス登録する記事の続きです。 今回は、Kuromojiのアナライザを利用してインデッ</description>
      <content:encoded><p><a href="/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch">前々回紹介した、日本語Wikipediaのデータをインデックス登録する記事</a>の続きです。</p>
<!-- more -->
<p>今回は、Kuromojiのアナライザを利用してインデックス登録してみます。</p>
<h2 id="余談proxy環境でのプラグインインストール">余談（Proxy環境でのプラグインインストール）</h2>
<p>ElasticSearchのpluginコマンドはJavaで実装されています。（<a href="https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/plugins/PluginManager.java#L315">org.elasticsearch.plugins.PluginManager</a>）
プラグインのダウンロードには、java.net.URL.openConnection()から取得URLConnectionを使用しています。</p>
<p>ですので、pluginのインストールを行う際に、Proxy環境にある場合は以下のようにコマンドを実行します。</p>
<pre><code>./bin/plugin -DproxyPort=ポート番号 -DproxyHost=ホスト名 -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre><h2 id="elasticsearch-analysis-kuromojiのインストール">elasticsearch-analysis-kuromojiのインストール</h2>
<p>WikipediaのデータをKuromojiを使って、形態素解析ベースの転置インデックスを作成していきます。
まずは、Kuromojiを利用するために、<a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">Analysisプラグイン</a>のインストールです。
ElasticSearchのバージョンに対応したプラグインのバージョンがあります。（プラグインのページに対応したバージョンの記載あり）
今回はElasticSearchの0.90.3を利用しているため、1.5.0をインストールします。</p>
<pre><code>./bin/plugin -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre><p>インストール後は再起動しておきます。
なお、Kuromojiを利用して、Wikipediaのデータを登録するばあい、デフォルトの設定では、ヒープが足りなくなるおそれがあります。
ElasticSearchの起動時に以下のオプションを指定して、最大ヒープサイズを2Gとしておきます。</p>
<pre><code>export ES_HEAP_SIZE=2g;./bin/elasticsearch
</code></pre><h2 id="indexの作成デフォルトでkuromojiのanalyzerを利用する">Indexの作成（デフォルトでKuromojiのAnalyzerを利用する）</h2>
<p>Wikipediaのデータを登録する際に、Kuromojiのアナライザを利用したいのが今回の趣旨でした。
一番ラクな方法として、Wikipediaデータのインデックスの設定として、デフォルトのアナライザをKuromojiにしてしまいます。
（きちんと設計する場合は、必要に応じてフィールドごとに指定しましょう）</p>
<pre><code>curl -XPUT 'localhost:9200/ja-wikipedia-kuromoji' -d '{
    &quot;settings&quot;: {
        &quot;analysis&quot;: {
            &quot;analyzer&quot;: {
                &quot;default&quot; : {
                    &quot;type&quot; : &quot;kuromoji&quot;
                }
            }
        }
    }
}'
</code></pre><p>これでkuromojiのアナライザがデフォルトで利用される形となります。
あとは、Riverを起動して登録するだけです。</p>
<h2 id="riverの実行">Riverの実行</h2>
<p>前回と一緒です。
インデックス名（<strong>_river/&lt;インデックス名&gt;/_meta</strong>）だけは、先ほど作成した「<code>ja-wikipedia-kuromoji</code>」に変更してください。</p>
<pre><code>curl -XPUT localhost:9200/_river/ja-wikipedia-kuromoji/_meta -d '
{
    &quot;type&quot; : &quot;wikipedia&quot;,
    &quot;wikipedia&quot; : {
        &quot;url&quot; : &quot;file:/home/johtani/src/jawiki-latest-pages-articles.xml&quot;
    },
    &quot;index&quot; : {
        &quot;bulk_size&quot; : 10000
    }
}'
</code></pre><p>あとは、インデックスされるのを待つだけです。</p>
<h2 id="データ量とか">データ量とか</h2>
<p>5.8gbになりました。Kuromojiを利用したため、形態素解析により単語にきちんとトークないずされた結果でしょう。
Uni-gramだと、転置インデックスのボキャブラリも単語に対してヒットするドキュメント数も大きくなるため、
インデックスサイズも大きくなっているのかと。</p>
<p>検索クエリのサンプルなどはまた後日。（夜遅いので。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>第1回ElasticSearch勉強会を開催しました！ #elasticsearchjp</title>
      <link>https://blog.johtani.info/blog/2013/08/30/hold-first-elasticsearch-meetup-in-japan/</link>
      <pubDate>Fri, 30 Aug 2013 02:42:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/08/30/hold-first-elasticsearch-meetup-in-japan/</guid>
      <description>ElasticSearch勉強会 第1回を主催しました。 昨年のpyfesでなんちゃって資料で喋って、1年たちました。 ElasticSearch</description>
      <content:encoded><p>ElasticSearch勉強会 第1回を主催しました。
昨年のpyfesでなんちゃって資料で喋って、1年たちました。</p>
<p>ElasticSearchの書籍（英語）も出てきて、今年はElasticSearchが面白くなりそうだし、使ってる人たちから話も聞きたいなぁということで、主催しました。</p>
<!-- more -->
<p>思った以上に興味のある方がいらっしゃったようで、100人応募のところ、チケットがすぐ完売してしまうほど。。。
しかも、当日もほぼ満員ということで、大変な盛況ぶりでした。</p>
<p>スピーカーの皆様、参加された皆様、会場を提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズ</a>さん、ありがとうございました！（たぶん、90人くらいいらっしゃってたかと。）</p>
<p>こんなステキな案内板も用意してもらいました。スタッフのみなさんありがとうございます！

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130830/es_signboard.jpg" />
    </div>
    <a href="/images/entries/20130830/es_signboard.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p><a href="http://togetter.com/li/556140">トゥギャっても頂きました</a>。まとめていただいてありがとうございます！</p>
<p>自分の発表や個々の発表に関する感想は以下のメモに。</p>
<h2 id="elasticsearch入門-johtani">ElasticSearch入門 @johtani</h2>
<p>スライド：<a href="/images/entries/20130830/IntroductionES20130829.pdf">ElasticSearch入門</a>※スライドはPDFです。</p>
<ul>
<li>緊張しまくりでわかりにくかったですかね。。。</li>
<li>とりあえず、AWSのサービスじゃないってのだけでも覚えて帰っていただければ満足です。</li>
<li>途中で見せた<a href="https://chrome.google.com/webstore/detail/sense/doinijnbnggojdlcjifpdckfokbbfpbo">ChromeプラグインのSense</a></li>
<li>わからなかった点や質問、ご意見などは、当ブログのコメント、私宛の＠ツイート、なんでもいいので、反応ください。なんでもいいので反応があると、今後の励みになりますので！</li>
<li>ということで、発表でも言いましたが、わからないことがアレば、<a href="http://twitter.com/johtani">@johtani</a>まで投げてもらえれば、「知らない」「ブログのネタにします」「ソレはこんなかんじですかねぇ？（テキトー）」みたいに答えると思います。</li>
<li>Elasticsearch in Actionは帰宅中に、4章追加されたよというメールが届きました。</li>
</ul>
<h4 id="宿題">宿題</h4>
<ul>
<li>クラスタへのノードの追加の処理方法とか、シャーディングの実装とか。</li>
</ul>
<h2 id="elasticsearchプラグイン入門--mocksolrpluginでsolrと入れ替えてみよう-菅谷さん">elasticsearchプラグイン入門 ～ mocksolrpluginでSolrと入れ替えてみよう 菅谷さん</h2>
<p>スライド：<a href="http://www.slideshare.net/shinsuke/es-study1">elasticsearchプラグイン入門</a>　slideshare</p>
<ul>
<li>プラグイン構成とか（公式でまとまってるの見つけられないので助かります。）</li>
<li>作る上でのポイントうれしいです。
<ul>
<li>パッケージ名変えられるとか、つらい。。。</li>
</ul>
</li>
<li>Solr APIプラグインについて（Solrの振りしてくれる便利なヤツ）</li>
</ul>
<h3 id="感想">感想</h3>
<p>まだ、プラグインを書いたことがないのですが、ちゃんとプラグインはどういう構成で書くんですよというまとまった資料って本家のサイトにもない気がしています。
なので、スライドが公開されたらすごく役に立つかと。
そのまえに、何かプラグイン書いてみます。。。</p>
<h2 id="debugging-and-testing-es-systems-chris-birchallさん">Debugging and testing ES systems Chris Birchallさん</h2>
<p>スライド：<a href="http://www.slideshare.net/cb372/debugging-and-testing-es-systems">Debugging and testing ES systems</a>　slideshare</p>
<ul>
<li>テストの方法とか便利なお話（最後のほうしか聞いてなかったですが。。。。）</li>
<li>クエリのデバッグは色々とやらないと、なんでヒットしないのってよくあるので。とくに形態素解析を利用した検索の場合、短い文章（クエリ）と長い文章（ドキュメント）で切れ目が変わってうまくヒットしないとかありますよね。</li>
<li>最後に少しだけ話しましたが、n-gramとKuromojiを組み合わせてOR検索とかすると良い場合があります。
<ul>
<li>インデックスが大きくなったり、OR検索なので遅くなったりというデメリットもありますが。</li>
</ul>
</li>
</ul>
<h3 id="感想-1">感想</h3>
<p>実際に使われているノウハウを元に話をしていただいたので助かりました。
最初に席を外してたのですが、戻ってきて日本語で普通に発表されててほんとにびっくりしましたｗ
テスト用プラグインがあるのとかは知らなかったです。
あと、使われてたIDEが<a href="http://samuraism.com/products/jetbrains/intellij-idea">IntelliJ IDEA</a>でしたね！私も使ってます！</p>
<h2 id="ニコニコ動画データセット-25億件を検索可能にしてみよう-penguinana_">ニコニコ動画データセット 25億件を検索可能にしてみよう @PENGUINANA_</h2>
<p>スライド：<a href="http://www.slideshare.net/penguinana/ss-25714442">ニコニコ動画を検索可能にしてみよう</a>　slideshare</p>
<ul>
<li>kibana@cookpadのお話</li>
<li>どういう挙動するかをやってみればいいじゃんってことで、やってみるのカッコイイ！</li>
<li><a href="http://goo.gl/FYtO5T">http://goo.gl/FYtO5T</a></li>
<li>bigdeskとか。プラグインいろいろ。</li>
</ul>
<h3 id="感想-2">感想</h3>
<p>さすがです、ペンギン先生。大きなデータセットつかって、構築した環境に関する数値も書かれてる資料ができて素晴らしすぎです。
思った以上にサクサク動いてて、4hでインデクシングできるのもすごいなぁと。
私も見習ってこのくらいがサクッとできるようになりたい。。。
あと、発表後にムチャぶりしましたが、次回はぜひ検索側の性能とかも話してもらえたらと。</p>
<h2 id="反省点">反省点</h2>
<ul>
<li>イベントアテンドだと、キャンセル待ちができない＋どのくらいの方が興味をもっているのかわからない。</li>
<li>イベントページの主催者は複数指定できる方がいい。（土壇場の登録の人が管理できない。私が司会やってたから）</li>
<li>マイクが聞こえにくかった（音量調節とかちゃんと調べないと）</li>
<li>懇親会は立食のほうがやはり動きやすい。</li>
<li>懇親会が1時間ちょっとしかできなかった</li>
</ul>
<h2 id="雑感">雑感</h2>
<p>ということで、ES勉強会、主催の私が一番楽しめました。ありがとうございます。
（あと、気前よく調べて答えますと言ってしまいました。。。まぁ、いいトリガーになるので、ウェルカムですが）</p>
<p>やっぱりKibanaについて興味を持ってる人も多いのかなぁという感触がしたので、次回はぜひKibana3の話をしてもらえるように頑張ります。
あと、<a href="http://mobz.github.io/elasticsearch-head/">elasticsearch-head</a>を使われてるんだなぁと。私は<a href="http://www.elastichq.org/gettingstarted.html">elasticsearch-HQ</a>を入れて使ってみてます。大きなクラスタ管理まではまだやってないので。</p>
<p>あとは、やはりいろんな人に助けられてるなぁと実感しつつ、今後も開催するので助けてください！ということで。
（もちろん、Solr勉強会もがんばりますよー）</p>
<h2 id="関連ブログ">関連ブログ</h2>
<ul>
<li><a href="http://repeatedly.github.io/ja/2013/08/elasticsearch-meetup-1st/">ElasticSearch勉強会 第1回 - Go ahead!</a></li>
<li><a href="http://mt.orz.at/archives/2013/09/elasticsearch-e.html">タムタムの日記 - ElasticSearch勉強会 の参加メモ #elasticsearchjp</a></li>
<li><a href="http://samuraism.jp/diary/2013/09/03/1378216200000.html">第1回ElasticSearch勉強会に行ってきた！ #elasticsearchjp - #侍ズム</a></li>
<li><a href="http://seratch.hatenablog.jp/entry/2013/09/03/234712">第1回 ElasticSearch 勉強会に参加 #elasticsearchjp - seratch</a></li>
</ul>
<p>他にもブログを書かれた方がいらっしゃいましたたら、リンクしたいので連絡いただければ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ElasticSearchにプラグインで日本語Wikipediaデータを入れてみました</title>
      <link>https://blog.johtani.info/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch/</link>
      <pubDate>Fri, 23 Aug 2013 12:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch/</guid>
      <description>久々のブログはElasticSearchネタです。勉強会開催する予定だったりすので、もう少し触っておきたいなと。 お手軽に検索するデータとして</description>
      <content:encoded><p>久々のブログはElasticSearchネタです。勉強会開催する予定だったりすので、もう少し触っておきたいなと。
お手軽に検索するデータとして、よくWikipediaのデータを使っています。
ElasticSearchには<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia">elasticsearch-river-wikipedia</a>という便利なプラグインがあり、Wikipediaのデータを簡単に検索可能な状態にできます。このRiverを利用して日本語のWikipediaのデータを入れたので、メモを取っておきます。
まずは、river-wikipediaで日本語のデータをインデクシングしてみるまでの説明です。
日本語特有の設定（Kuromojiを利用したインデクシング）などはまた後日。</p>
<!-- more -->
<h2 id="プラグインのインストール">プラグインのインストール</h2>
<p>対象とするElasticSearchは現時点で最新版の0.90.3とします。
最新版でRiver動かないなぁとつぶやいた影響かどうかはわかりませんが、2013/08/19に最新版のElasticSearchで動作するプラグインが公開されました。</p>
<p>まずはインストールです。
HPにも書いてありますが、以下のコマンドを実行すればインストールされます。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">$ ./bin/plugin -install elasticsearch/elasticsearch-river-wikipedia/1.2.0
-&gt; Installing elasticsearch/elasticsearch-river-wikipedia/1.2.0...
Trying http://download.elasticsearch.org/elasticsearch/elasticsearch-river-wikipedia/elasticsearch-river-wikipedia-1.2.0.zip...
Downloading ..........DONE
Installed river-wikipedia into /opt/elasticsearch/plugins/river-wikipedia
</code></pre></div><p>ElasticSearchが起動している場合はプラグインをインストール後、認識させるためにElasticSearchを再起動します。</p>
<h2 id="日本語wikipediaのインデクシング">日本語Wikipediaのインデクシング</h2>
<p>通常は英語のWikipediaがインデクシングされますが、対象となるファイルを変更することで日本語のWikipediaもインデクシング可能です。
手元に日本語Wikipediaのダンプファイルがあるものとします。（<a href="http://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89">ダウンロードはWikipediaデータベースダウンロード</a>のページにあるpages-articles.xml.bz2のファイルです）</p>
<p>ファイルを指定してインデクシングするには、つぎのcurlコマンドを実行します。
コマンドを実行するとすぐにインデクシングが始まりますので注意が必要です。</p>
<pre><code>curl -XPUT localhost:9200/_river/ja-wikipedia/_meta -d '
{
    &quot;type&quot; : &quot;wikipedia&quot;,
    &quot;wikipedia&quot; : {
        &quot;url&quot; : &quot;file:/home/johtani/src/jawiki-latest-pages-articles.xml&quot;
    },
    &quot;index&quot; : {
        &quot;bulk_size&quot; : 1000
    }
}'
</code></pre><p>ここでURLに含まれる「ja-wikipedia」がインデックス名になります。
また、JSONの&quot;url&quot;にはファイルの場所を指定するため、<code>file:</code>で開始するパスを指定します。
例では、bz2を解凍したファイルを指定していますが、bz2のままのファイルでもOKです。</p>
<p>上記コマンドを実行すると、<code>_river</code>というインデックスにつぎのようなエントリが増えています。
(<code>curl -XGET 'localhost:9200/_river/ja-wikipedia/_search?pretty</code>)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;took&#34;</span>: <span style="color:#ae81ff">5</span>,
   <span style="color:#f92672">&#34;timed_out&#34;</span>: <span style="color:#66d9ef">false</span>,
   <span style="color:#f92672">&#34;_shards&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">1</span>,
      <span style="color:#f92672">&#34;successful&#34;</span>: <span style="color:#ae81ff">1</span>,
      <span style="color:#f92672">&#34;failed&#34;</span>: <span style="color:#ae81ff">0</span>
   },
   <span style="color:#f92672">&#34;hits&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">2</span>,
      <span style="color:#f92672">&#34;max_score&#34;</span>: <span style="color:#ae81ff">1</span>,
      <span style="color:#f92672">&#34;hits&#34;</span>: [
         {
            <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;_river&#34;</span>,
            <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;ja-wikipedia&#34;</span>,
            <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;_status&#34;</span>,
            <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">1</span>,
            <span style="color:#f92672">&#34;_source&#34;</span>: {
               <span style="color:#f92672">&#34;ok&#34;</span>: <span style="color:#66d9ef">true</span>,
               <span style="color:#f92672">&#34;node&#34;</span>: {
                  <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;gdyvwpiAR52lqUCcRhVwsg&#34;</span>,
                  <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;Blitzschlag, Baron Von&#34;</span>,
                  <span style="color:#f92672">&#34;transport_address&#34;</span>: <span style="color:#e6db74">&#34;inet[/192.168.100.7:9300]&#34;</span>
               }
            }
         },
         {
            <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;_river&#34;</span>,
            <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;ja-wikipedia&#34;</span>,
            <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;_meta&#34;</span>,
            <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">1</span>,
            <span style="color:#f92672">&#34;_source&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;wikipedia&#34;</span>,
               <span style="color:#f92672">&#34;wikipedia&#34;</span>: {
                  <span style="color:#f92672">&#34;url&#34;</span>: <span style="color:#e6db74">&#34;file:/home/johtani/src/jawiki-latest-pages-articles.xml&#34;</span>
               },
               <span style="color:#f92672">&#34;index&#34;</span>: {
                  <span style="color:#f92672">&#34;bulk_size&#34;</span>: <span style="color:#ae81ff">100</span>
               }
            }
         }
      ]
   }
}
</code></pre></div><p><code>&quot;_id&quot;: &quot;_meta&quot;</code>というエントリがさきほど登録したWikipediaのRiverに関する設定です。
<code>&quot;_id&quot;: &quot;_status&quot;</code>というエントリが起動したRiverの状態になります。</p>
<h2 id="riverの停止">Riverの停止</h2>
<p>日本語Wikipediaは結構サイズが大きく、手元のAirでインデクシングするのに30分程度かかりました。（bz2圧縮されていないファイルで、何もしていない状態）</p>
<p>途中でRiverを停止したくなった場合は、以下のcurlコマンドを実行します。</p>
<pre><code>$ curl -XDELETE 'localhost:9200/_river/ja-wikipedia'
</code></pre><p>先ほど設定した<code>_river/ja-wikipedia</code>の情報を削除すると、エントリが削除されたのを検知してRiverが停止します。ログにはつぎのようなメッセージが表示されます。</p>
<pre><code>[2013-08-26 18:26:50,130][INFO ][cluster.metadata         ] [Blitzschlag, Baron Von] [[_river]] remove_mapping [ja-wikipedia]
[2013-08-26 18:26:50,130][INFO ][river.wikipedia          ] [Blitzschlag, Baron Von] [wikipedia][ja-wikipedia] closing wikipedia river
</code></pre><p>Riverを停止してもそれまでインデクシングされたデータは検索できます。
データはちょっとだけで良いという場合は、先ほどの<code>_river</code>のデータを削除してください。
（◯件だけ登録したいとかできるかは調べてないです。）</p>
<h2 id="サイズとかマッピングとか">サイズとかマッピングとか</h2>
<h3 id="サイズ">サイズ</h3>
<p>インデックス前のXMLのサイズが5.7Gのとき、ElasticSearchのインデックスサイズ（Optimize後）は7.2Gとなりました。すこし古いファイルを利用しているため、最新版とはサイズが異なるかもしれません。</p>
<p>あと、データ数が、1540000件とやけにきりがいいのがちょっと気になっています。。。
bulkのサイズを10000で指定してインデックスしたので、切れてるのかなぁと。</p>
<p>ということは、データが欠落しているような気がするのでRiverの作りの問題なのか、ElasticSearchの問題なのかはちょっと調べてみないとわからないなと。</p>
<h3 id="マッピング">マッピング</h3>
<p>出来上がったインデックスのマッピング（Solrでいうスキーマみたいなもの）は次のようになっています。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;ja-wikipedia&#34;</span>: {
      <span style="color:#f92672">&#34;page&#34;</span>: {
         <span style="color:#f92672">&#34;properties&#34;</span>: {
            <span style="color:#f92672">&#34;category&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
            },
            <span style="color:#f92672">&#34;disambiguation&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;boolean&#34;</span>
            },
            <span style="color:#f92672">&#34;link&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
            },
            <span style="color:#f92672">&#34;redirect&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;boolean&#34;</span>
            },
            <span style="color:#f92672">&#34;special&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;boolean&#34;</span>
            },
            <span style="color:#f92672">&#34;stub&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;boolean&#34;</span>
            },
            <span style="color:#f92672">&#34;text&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
            },
            <span style="color:#f92672">&#34;title&#34;</span>: {
               <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>
            }
         }
      }
   }
}
</code></pre></div><p>Wikipediaの各種データが上記のフィールドに入っています。
また、マッピングタイプはデフォルトで「page」というタイプになっています。</p>
<h2 id="検索">検索</h2>
<p>先ほどのマッピングを元に検索すればOKです。例えばつぎのような感じです。</p>
<pre><code>curl -XPOST 'localhost:9200/ja-wikipedia/_search?pretty' -d '
{
    &quot;size&quot; : 3,
    &quot;script_fields&quot;: {
       &quot;title_only&quot;: {
          &quot;script&quot;: &quot;_source.title&quot;
       }
    }, 
    &quot;query&quot; : {
        &quot;query_string&quot;: {
            &quot;default_field&quot;: &quot;title&quot;,
            &quot;query&quot; : &quot;千葉&quot;
        }
    }
}'
</code></pre><p>結果はこんな感じ。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#f92672">&#34;took&#34;</span>: <span style="color:#ae81ff">51</span>,
   <span style="color:#f92672">&#34;timed_out&#34;</span>: <span style="color:#66d9ef">false</span>,
   <span style="color:#f92672">&#34;_shards&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">5</span>,
      <span style="color:#f92672">&#34;successful&#34;</span>: <span style="color:#ae81ff">5</span>,
      <span style="color:#f92672">&#34;failed&#34;</span>: <span style="color:#ae81ff">0</span>
   },
   <span style="color:#f92672">&#34;hits&#34;</span>: {
      <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">8616</span>,
      <span style="color:#f92672">&#34;max_score&#34;</span>: <span style="color:#ae81ff">5.8075247</span>,
      <span style="color:#f92672">&#34;hits&#34;</span>: [
         {
            <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;ja-wikipedia&#34;</span>,
            <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;page&#34;</span>,
            <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;3582&#34;</span>,
            <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">5.8075247</span>,
            <span style="color:#f92672">&#34;fields&#34;</span>: {
               <span style="color:#f92672">&#34;title_only&#34;</span>: <span style="color:#e6db74">&#34;千葉&#34;</span>
            }
         },
         {
            <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;ja-wikipedia&#34;</span>,
            <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;page&#34;</span>,
            <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;2352241&#34;</span>,
            <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">4.94406</span>,
            <span style="color:#f92672">&#34;fields&#34;</span>: {
               <span style="color:#f92672">&#34;title_only&#34;</span>: <span style="color:#e6db74">&#34;千葉千枝子&#34;</span>
            }
         },
         {
            <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;ja-wikipedia&#34;</span>,
            <span style="color:#f92672">&#34;_type&#34;</span>: <span style="color:#e6db74">&#34;page&#34;</span>,
            <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;14020&#34;</span>,
            <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">4.8754807</span>,
            <span style="color:#f92672">&#34;fields&#34;</span>: {
               <span style="color:#f92672">&#34;title_only&#34;</span>: <span style="color:#e6db74">&#34;千葉千恵巳&#34;</span>
            }
         }
      ]
   }
}
</code></pre></div><p>結果を見やすくするため、タイトルだけを「title_only」という表示にしています。
ただ、この検索だと、一見「千葉」できちんとヒットしているように見えますが、ElasticSearchのフィールドの定義はstring型になっています。なので、実は「千」や「葉」だけのデータもヒットしています。
マルチバイト文字は1文字ずつインデックスされてしまい、query_stringというクエリでは、フレーズ検索などができていないためです。</p>
<h2 id="まとめ">まとめ</h2>
<p>プラグインいれて、XMLファイルがあれば、検索できるデータが出来上がるので、
暇があったら、お試しで触ってみるデータを簡単に入れてみてはどうでしょうか。</p>
<p>ただ、いくつか気になる点も。</p>
<ul>
<li>日本語が検索しにくい（string型のフィールドなのでuni-gramっぽくなっている）</li>
<li>bulk_sizeの影響で端数が登録できてない（バグ？どうなの？）</li>
</ul>
<p>ということで、ちょっと使いにくいかもなぁということで、つぎはKuromojiを利用してインデックスしてみてみようかなと。次回のエントリで書く予定です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>MorphlinesのloadSolrをちょっとだけ調べてみた</title>
      <link>https://blog.johtani.info/blog/2013/08/02/morphlines-loadsolr/</link>
      <pubDate>Fri, 02 Aug 2013 18:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/08/02/morphlines-loadsolr/</guid>
      <description>宿題その2？かな。Solr勉強会でCloudera Searchのスキーマ周りってどうなってるの？という質問が出てて、 なんか調べることになって</description>
      <content:encoded><p>宿題その2？かな。Solr勉強会でCloudera Searchのスキーマ周りってどうなってるの？という質問が出てて、
なんか調べることになってたので、関係しそうなMorphlinesの<a href="https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/LoadSolrBuilder.java">LoadSolr</a>コマンドを調べてみました。
こいつが、Solrへの書き込みを実行するコマンドみたいだったので。<br>
（※Cloudera Searchのスキーマの設定方法とかは調べてないです。）<br>
（※めんどくさかったので、パッケージ名すっ飛ばしてクラス名書いてます。githubへのリンクを代わりに貼ってます。）</p>
<!-- more -->
<h2 id="recordsolrのドキュメント">Record＝Solrのドキュメント</h2>
<p>convert()メソッドにて、MorphlinesのRecord（コマンドの処理するデータの１単位）に格納されているKey-ValueデータをSolrInputDocumentクラスのフィールドとして格納しています。
Recordにもフィールドという概念があり、Recordのフィールド＝Solrのフィールドという事みたいです。</p>
<p>ということで、Solrのフィールドは事前に定義しておき、Morphlinesの処理内部でSolrのフィールド名に値を詰めていく感じでしょうか。
別途、<a href="https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/SanitizeUnknownSolrFieldsBuilder.java">sanitizeUnknownSolrFilds</a>というコマンドが用意されていて、Solrのスキーマにないものはこのコマンドを使って、無視するフィールド名に変えたり、雑多なデータを入れるためのフィールド名にするといった処理ができるようです。このコマンド内部で、Solrのスキーマ設定を元に、Solrのフィールドに合致する物があるかをチェックして処理しています。</p>
<h2 id="solrへの登録処理は">Solrへの登録処理は？</h2>
<p>Solrへの登録処理自体はLoadSolrクラス内部でDocumentLoaderというクラスのload()メソッドを呼び出しているだけでした。ということで、<a href="https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/DocumentLoader.java">DocumentBuilder</a>クラスを少し調査。</p>
<h3 id="documentloader">DocumentLoader</h3>
<p>IFでした。。。実クラスは次の条件</p>
<ol>
<li>SolrMorphlineContextにDocumentLoaderがあればそちらを採用（他の種類はなにがあるんだろ？）</li>
<li>なければ、<a href="https://github.com/cloudera/cdk/blob/b6f98cff4a027af04f97fdec9abf729785d74cf5/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/SolrServerDocumentLoader.java">SolrServerDocumentLoader</a>をnewしたものを利用</li>
</ol>
<p>2.の場合がおそらくMapReduceではないパターンのloadSolrだと思われます。SolrServerDocumentBuilderはSolrJのAPIを利用して、Solrへデータ登録していく普通のアプリです。（対象とするSolrは外部に起動しているもののはず＝FlumeのSolrSinkではこちらを採用かな？）<br>
Solrへの接続情報とか設定ファイルとかSolrCloud用のZooKeeperとかは<a href="https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/SolrLocator.java">SolrLocatorクラス</a>に設定される内容が利用されます。</p>
<p>1.のパターンは、どうやら、<a href="https://github.com/cloudera/search/blob/master/search-mr/src/main/java/org/apache/solr/hadoop/morphline/MorphlineMapper.java">Cloudera SearchのMapReduceIndexerToolのクラス</a>にあるMyDocumentLoaderかなぁと。
こちらは、MapReduceを利用する場合に、利用されてるっぽいです（ちゃんと見てないけど）
内部処理は、HadoopのContext.writeメソッドにでSolrInputDocument（＝MorphlinesのRecord）を書きだして、ReducerでSolrOutputFormatでインデックス作成の流れかなと。たぶん、<a href="https://github.com/cloudera/search/blob/master/search-mr/src/main/java/org/apache/solr/hadoop/morphline/MorphlineMapRunner.java">MorphlineMapRunner</a>あたりを読みこめば解読できるかと。
ちなみに、こちらは、2.とは異なり、SolrLocatorの設定は無視されそう。</p>
<h2 id="感想妄想">感想＋妄想？</h2>
<p>ということで、Morphlinesのデータ流れを考える上で、現時点ではSolrのスキーマを頭の片隅に置きつつ、
Recordの中にあるデータをゴニョゴニョしてデータを形成していくって感じになりそうです。
うまく処理できなかったものとかのカウントとかもとれたりするのかなぁ？とか、また色々と気になるところが出てきますが、一旦ここまでで。。。（だれか、続きを調べて書いてみてくれてもいいんですよ！コマンドもいっぱいあるし！）</p>
<p>とまぁ、こんなかんじでMorphlinesをちょっとだけ読みました。
よくよく考えたら、こんなの作ったことあるなぁと（こんなに汎用的じゃないけど）。
みんな同じ事考えるんですねぇ。
コマンドパターン？みたいな感じで、I/F決めてSolrとか別のシステムとかにデータ入れる処理を順番に記述できる的なバッチ処理良くかいてます（書いてましたのほうが正解かなぁ）。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Xperia Zが故障したので交換したのでメモ</title>
      <link>https://blog.johtani.info/blog/2013/08/01/replace-xperia-z/</link>
      <pubDate>Thu, 01 Aug 2013 22:38:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/08/01/replace-xperia-z/</guid>
      <description>今朝起きたら、Xperia Zが故障してしまいました。 電源ボタンを押してもスリープにならないんです。しかも、スリープからの復帰もできない。。。</description>
      <content:encoded><p>今朝起きたら、Xperia Zが故障してしまいました。
電源ボタンを押してもスリープにならないんです。しかも、スリープからの復帰もできない。。。</p>
<!-- more -->
<p>ということで、ドコモショップに行って来ました。対応も良かったです。
やっぱり、ボタンがおかしく再起動してもNGでした。
ということで、トントン拍子で交換品に変更してもらえました。</p>
<p>2、3回繰り返し言われましたが、データなくなりますよ＆交換品なので、復旧はご自分でとのこと。
ドコモショップ行く前に「Sony Bridge for Mac」というSony製のアプリでバックアップ取ってたし問題ないですと回答して、交換してもらいました。
最後に使ってないオプションとか要らないのでやめたほうが安くなりますよなどの提案もいただき満足でした。
さらに、前の機種に貼ってたフィルムもきちんと張り替えてもらえたし。時間はちょっとかかったけど。</p>
<p>で、先ほど帰宅して、朝とっておいたバックアップから復旧しようとしたら。</p>
<p>**「そんなバックアップありません」**みたいなメッセージが表示されて復旧出来ません。。。</p>
<p>え？なにそれ？と思い、とりあえず現時点のデータをバックアップ取ってみるかと。
バックアップ取ったあとに、再度復旧しようとしたらちゃんとリストがでてくるじゃないですか。<br>
なんだこれ？ということで、ファイル探しです。。。</p>
<p>「Sony Bridge for Mac」バックアップファイルは次の場所に保存されるみたいです。</p>
<pre><code>/Users/ユーザ名/Library/Application Support/Sony Ericsson Bridge for Mac/Backups/製品番号みたいな文字列/日付
</code></pre><p>この最後のディレクトリが製品番号？ごとのディレクトリっぽく、２つありました。
どうせ初期化された状態だしということで、古い日付フォルダを新しい製品番号のフォルダにコピーしたら復元は可能でした。
壁紙の設定は再起動したら復活しましたが、復活してないものも。。。
復活してないのは以下のものです。</p>
<ul>
<li>インストールしてたアプリ</li>
<li>ホーム画面の配置</li>
<li>アカウント色々（Google Playとか）</li>
</ul>
<p>ということで、スマホでインストールするのもだるくなってきたので、PCで黙々とGoogle Playのサイトからインストールしてますが、これもだるいですね。。。</p>
<p>必要なアプリインストールし直したら、またSonyのアプリで復旧してみて、設定が復活するのを祈って。。。</p>
<h4 id="続き">続き</h4>
<p>インストールしなおして、ホーム画面での配置をがんばりました。
Google Playのサイトから既存アプリをインストールするときに、ブラウザバックで戻ってたのですが、
一覧から別タブにインストールしたいアプリのページを開きまくってから、インストールボタン押すのが楽でした（半分くらい終わった所で気づいた。。。）</p>
<p>一通りインストールしたあとに、ホーム画面の配置をやり直して、「Sony Bridge for Mac」もう一回復元。
twiccaやブクログなど、個別にアカウントとか設定を管理しているアプリについては復活しました。</p>
<p>けど、Androidがアカウントを管理するもの（例：Facebookなど）については、残念ながら、一部のものはアカウントを再登録しないといけませんでした。</p>
<p>写真や音楽についてはもともとSDカードに入れていたのもあったので、まぁ、被害は少なかったかなぁと。</p>
<p>ただ、iPadやiPhoneの用に簡単にバックアップ（アプリの情報、配置まで）ができて復元できると助かるなぁと思いました。（今度はホーム画面のキャプチャ撮ってから修理に出すかなぁ。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>Morphlines入門？</title>
      <link>https://blog.johtani.info/blog/2013/07/31/introduction-morphlines/</link>
      <pubDate>Wed, 31 Jul 2013 19:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/31/introduction-morphlines/</guid>
      <description>Morphlinesについてちょっとだけ、さらに調べました。 誤解 Solr勉強会でなんとなく私の認識を話しましたが、ちょっと誤解してたみたいで</description>
      <content:encoded><p>Morphlinesについてちょっとだけ、さらに調べました。</p>
<h2 id="誤解">誤解</h2>
<p>Solr勉強会でなんとなく私の認識を話しましたが、ちょっと誤解してたみたいです。スミマセン。</p>
<!-- more -->
<h3 id="誤解morphlineというプラットフォームミドルウェアがありそうなイメージ">誤解：Morphlineというプラットフォーム/ミドルウェアがありそうなイメージ</h3>
<p>まぁ、書いてあるのでちゃんと読めって話ですが、Morphlineはあくまでライブラリだということでした。
私はなんとなくManifoldCFのようなミドルウェアorプラットフォームが存在して、
そこにFlumeのSinkとかMapReduceによるIndexerが動作するのかと思ってました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130731/wrong_image.jpg" />
    </div>
    <a href="/images/entries/20130731/wrong_image.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>まぁ、これが間違いでした。正解のイメージはこっちですね。</p>


<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130731/correct_image.jpg" />
    </div>
    <a href="/images/entries/20130731/correct_image.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>各プラットフォーム（FlumeとかHadoopとか）に組み込んむライブラリで、
それぞれ組み込んだ先でMorphlineの設定を記述することで、パイプライン処理ができるっぽいです。</p>
<p>Flumeについては、MorphlineSolrSinkというクラスでMorphlineの設定ファイルを読み込み、いろいろ処理出来ます。</p>
<p>Map/ReduceだとCloudera Searchに含まれてる<a href="https://github.com/cloudera/search">MapReduceIndexerTool</a>がMorphlineの設定を読み込んでコマンド実行してくれるみたいです。
MapReduceIndexerToolはまだちゃんと読んでないのですが、MapperとしてMorphlineのコマンドが実行されるのかなぁ？という感じです。
（結構入り組んでるので、ちゃんと読まないとわからない。。。）</p>
<p>ということで、Morphlineというプラットフォームがあって、一元的にFlumeやMap/Reduceに対するコマンドをパイプライン化するという話でありませんでした。</p>
<p>※ちなみに、上の画像ですが、愛用しているNUBoardを使って書いてます。
考えをまとめるのにすごく役立つ一品です。持ち運び可能なノート型ホワイトボードです。</p>
<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS1=1&nou=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&o=9&p=8&l=as1&m=amazon&f=ifr&ref=qf_sp_asin_til&asins=B00A08IVT4" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>
<h3 id="疑問点">疑問点</h3>
<p>ただ、読んでてまだ不明な点があります。まぁ、ぼちぼち調べるかなぁと。。。</p>
<ul>
<li>Solrのschemaはどーなってんの？</li>
<li>MorphlineにSolrへロードするコマンド（loadSolr）があるけど、FlumeのMorphlineSolrSinkってのもSolrに書き込みそうだけど？</li>
<li>Map/ReduceでSolrに書き込むもMorphlineのコマンドとの違いは？（前にソースを見たときはSOLR-1301がベースになっていて、SolrOutputFormatってクラスがEmbeddedSolrServer起動してインデクシングしてた）</li>
<li>GoLiveってなんだろ？（MapReduceIndexerToolに入ってて、M/Rでインデックス作ったあとにSolrのクラスタに配布＋マージするやつっぽい）</li>
<li>どんなコマンドがあるの？（<a href="http://cloudera.github.io/cdk/docs/0.4.1/cdk-morphlines/morphlinesReferenceGuide.html">Cloudera Morphlines Ref Guide</a>）</li>
</ul>
<p>以下は、参考資料と参考資料にあるSlideshareの資料を一部訳したものになります。</p>
<h3 id="参考資料">参考資料</h3>
<ul>
<li><a href="http://www.slideshare.net/cloudera/using-morphlines-for-onthefly-etl">Using Morphlines for On-the-Fly ETL(slideshare)</a></li>
<li><a href="https://github.com/cloudera/cdk/tree/master/cdk-morphlines">cloudera/cdk/cdk-morphlines(github)</a></li>
</ul>
<h2 id="メモ">メモ</h2>
<h3 id="現在のコマンドライブラリスライド-18-19ページ">現在のコマンドライブラリ（スライド 18-19ページ）</h3>
<ul>
<li>Solrへのインテグレートとロード</li>
<li>フレキシブルなログファイル解析</li>
<li>1行、複数行、CSVファイル</li>
<li>正規表現ベースのパターンマッチと展開</li>
<li>Avro、JSON、XML、HTMLのインテグレーション</li>
<li>Hadoop シーケンスファイルのインテグレーション</li>
<li>SolrCellとApache Tikaパーサすべてのインテグレーション</li>
<li>Tikaを利用したバイナリデータからMIMEタイプの自動判別</li>
<li>動的Javaコードのスクリプティングサポート</li>
<li>フィールドの割り当て処理、比較処理</li>
<li>リストやセット書式のフィールド処理</li>
<li>if-then-else条件分岐</li>
<li>簡易ルールエンジン（tryRules）</li>
<li>文字列とタイムスタンプの変換</li>
<li>slf4jロギング</li>
<li>Yammerメトリックとカウンター</li>
<li>ネストされたファイルフォーマットコンテナの解凍</li>
<li>などなど</li>
</ul>
<h3 id="プラグインコマンドスライド20ページ">プラグインコマンド（スライド　20ページ）</h3>
<ul>
<li>簡単に新しいI/Oや変換コマンドが追加可能</li>
<li>サードパーティや既存機能のインテグレード</li>
<li>CommandインタフェースかAbstractCommandのサブクラスを実装</li>
<li>Javaクラスパスに新規作成したものを追加</li>
<li>登録処理などは必要ない</li>
</ul>
<h3 id="新しいプラグインコマンドとして考えられるもの22ページ">新しいプラグインコマンドとして考えられるもの（22ページ）</h3>
<ul>
<li>RDBやKVSやローカルファイルなどの外部データソースをレコードにjoin</li>
<li>DNS名前解決とか短縮URLの展開とか</li>
<li>ソーシャル・ネットワークからリンクされたメタデータのフェッチ（？？）</li>
<li>レコードの感情分析とアノテーション？</li>
</ul>
<p>31ページの図がわかりやすいかも</p>
<p>以上。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第11回Solr勉強会を主催しました。#SolrJP</title>
      <link>https://blog.johtani.info/blog/2013/07/29/study-of-solr/</link>
      <pubDate>Mon, 29 Jul 2013 23:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/29/study-of-solr/</guid>
      <description>不定期開催ですが第11回Solr勉強会を主催しました。 今回も大入り90人くらい？の参加者の皆さんがいらっしゃいました。ありがたいことです！（</description>
      <content:encoded><p>不定期開催ですが<a href="http://atnd.org/events/41368/">第11回Solr勉強会</a>を主催しました。</p>
<p>今回も大入り90人くらい？の参加者の皆さんがいらっしゃいました。ありがたいことです！（20時時点で最終的に補欠17人でした。）</p>
<p><strike>とりあえず、第一報です。このあと懇親会なので。</strike></p>
<p>ということで、帰りの電車でいくつか感想を（忘れちゃうから）。</p>
<!--  more -->
<p>小林さんの苦労話は細かいですが、結構はまりがちな点を共有していただいたので良かったかなぁと。
Solrのexampleの設定とか、ManifoldCFとかちょっとずつ罠があったりするので、あるあるネタはありがたいと思いますｗ</p>
<p>Cloudera Searchについては、安定の嶋内さんの喋りに圧巻でした。検索だけの視点とは異なる観点についての
話は私には足りないしてんだったりするので参考になります。
なんか、気づいたらMorphlineやスキーマ周りを調べてブログ書くことになっちゃったけど。。。
一つ質問しそこねたのがあって、Cloudera社は基本的に公開したOSSについてのトレーニングも立ち上げているイメージです。Cloudera Searchについてもトレーニングが立ち上がるのかなぁと密かに期待をしてみたり（予算の関係上参加できるかは不明ですが。。。）</p>
<p>牧野さんの話は画像系について、私は詳しくないので、また関口さんのalikeと比較とかしてもらえると面白いかなぁと。とりあえず、青いロボットがちゃんと検索できるようになるといいですねｗｗ</p>
<p>秀野さんの空間検索は緯度経度以外のPOLYGONなどを利用した検索で、実は私も知らない機能でしたｗ<br>
なとなくは知ってたんですが、そこまでちゃんと検索できるとは！地図以外にも活用できるような気がします（想像つかないんだけど。。。）</p>
<p>最後は私の発表で、簡単な資料ですみませんでした。しかも発表よりも宣伝が。。。（ブログの宣伝だったりとか。。。）
最後に宣伝した「<a href="http://www.ipsj.or.jp/dp/cfp/copy_of_copy_of_dp0502s.html">「ビッグデータ活用を支えるOSS」特集への論文投稿のご案内</a>」もご検討ください！</p>
<p>懇親会も楽しかったです。また思いついたら開催しますー<br>
最後に、今回の発表者の皆様、会場提供していただいたVOYAGE GROUPの皆様ありがとうございました！</p>
<p>以下はいつものメモです。</p>
<h2 id="manifoldcfのとsolrの組み合わせ仮株式会社-ロンウイット大須賀さん">ManifoldCFのとSolrの組み合わせ（仮）株式会社 ロンウイット　大須賀さん</h2>
<p>残念ながら、発熱のため発表は次回に持ち越しに。</p>
<p>##社内ファイル及びWEBコンテンツの検索システム構築時に苦労したこと ソフトバンクBB㈱　小林さん</p>
<ul>
<li>ManifoldCF＋Solrを使って社内ファイルの検索システム構築</li>
<li>約1000万ドキュメント</li>
<li>さまざまなDCにドキュメントがある</li>
</ul>
<h4 id="クロールジョブのハング">クロールジョブのハング。。。</h4>
<ul>
<li>ログをDEBUGにしたら。。。ログファイル150GB。。。</li>
<li>一定時間ごとにAgentをリスタートするバッチを。。。（力技）</li>
</ul>
<h4 id="mcfエラーによるジョブの停止">MCFエラーによるジョブの停止</h4>
<ul>
<li>CONNECTORS-590</li>
<li>エラーが発生して止まったジョブを起動するバッチをcronで。。。</li>
</ul>
<h4 id="自作リアルタイムインデクシングの問題">自作リアルタイムインデクシングの問題</h4>
<ul>
<li>MCF使わないでSlaveにインデックス</li>
<li>openSearcher=falseだとautoCommitが実行されてもSearcherを再起動しないので検索にでてこない</li>
</ul>
<h4 id="リプリケーションのnw負荷">リプリケーションのNW負荷</h4>
<ul>
<li>別DCからのレプリケーションが複数が一度に実施される→ネットワーク負荷が。。。</li>
<li>cronで別々にレプリすることでNW負荷を分散できてるかな。。。
　　</li>
</ul>
<h2 id="cloudera-search-入門仮-cloudera-株式会社嶋内さん">Cloudera Search 入門(仮) Cloudera 株式会社　嶋内さん</h2>
<ul>
<li>マサカリ画像がｗ</li>
<li>SolrのコミッターMark Millerさんもジョインしてる</li>
</ul>
<h4 id="clouderaとhadoop入門とか">ClouderaとHadoop入門とか。</h4>
<ul>
<li>いろいろあるよ、エコシステム</li>
<li>4つの分類。
<ul>
<li>データの取り込み</li>
<li>データの保存</li>
<li>データの活用</li>
</ul>
</li>
</ul>
<h4 id="search">Search</h4>
<ul>
<li>検索エンジンなら数十億人が使い方を知ってる（Clouderaのチャールズ・ゼドルースキ）</li>
</ul>
<h4 id="cloudera-search">Cloudera Search</h4>
<ul>
<li>
<p>Hadoopのためのインタラクティブな検索</p>
</li>
<li>
<p>CDHとSolrの統合</p>
</li>
<li>
<p>OSS！</p>
</li>
<li>
<p>利点とか。</p>
<ul>
<li>データ解析にも使えるよね、検索</li>
<li>非構造化データの検索にもいいよね</li>
<li>単一プラットフォームによるコスパ</li>
</ul>
</li>
</ul>
<h4 id="cloudera-searchの事例">Cloudera Searchの事例</h4>
<ul>
<li>バイオテクノロジー企業で画像検索とか</li>
<li>医療系企業でいろんなログイベントの管理とか</li>
</ul>
<h4 id="cloudera-searchのアーキテクチャ">Cloudera Searchのアーキテクチャ</h4>
<ul>
<li>Flumeでストリーミングで登録</li>
<li>HBaseデータの登録</li>
<li>M/Rでバッチ登録</li>
<li>HueのWebインタフェース</li>
</ul>
<p>Morphlines、HBaseはLinyプロジェクトのもの</p>
<p>Solr使うならCDH！！</p>
<h4 id="qa">QA</h4>
<ul>
<li>
<p>Q：デモで使われたTwitterの検索のデータ料とかは？</p>
</li>
<li>
<p>A：デモ環境ですので小さい。</p>
</li>
<li>
<p>Q：スキーマってどうするの？</p>
</li>
<li>
<p>A：スキーマは。。。私が書こうかなぁ、ブログ。。。</p>
</li>
</ul>
<h2 id="コンピュータビジョン株式会社-curious-vehicle牧野さん">コンピュータビジョン　株式会社 Curious Vehicle　牧野さん</h2>
<ul>
<li>色々やってます</li>
<li>コンピュータビジョンの説明（某ネコ型ロボットのいろんな画像がｗ）</li>
</ul>
<h4 id="画像検索の流れ">画像検索の流れ</h4>
<ol>
<li>特徴情報の抽出</li>
<li>特徴情報のクラスタリングによるword化</li>
<li>Solrによる画像情報の検索</li>
</ol>
<h5 id="1-特徴情報の抽出">1. 特徴情報の抽出</h5>
<ul>
<li>SIFT特徴点解析</li>
<li>グレースケールしてからSIFT</li>
<li>注意！SIFTは商用ライセンスが必要です</li>
</ul>
<h5 id="2-特徴情報のクラスタリングによるword化">2. 特徴情報のクラスタリングによるword化</h5>
<ul>
<li>K-meansでクラスタリング</li>
<li>クラスタ情報をヒストグラム化してSolrへ</li>
</ul>
<h5 id="3-solrによる画像情報の検索">3. Solrによる画像情報の検索</h5>
<ul>
<li>物体認識ベンチマークセット（ケンタッキー大）を使って。</li>
<li>やっぱり良し悪しある。データセットに特化したチューニングしてます。</li>
</ul>
<h4 id="つぎのステップ">つぎのステップ</h4>
<ul>
<li>文字認識とか顔認識</li>
<li>つぎはドラえもんじゃねぇ、検索とかも。。。</li>
</ul>
<h5 id="ガウシアンによる画像ぼかしの例">ガウシアンによる画像ぼかしの例</h5>
<h4 id="qa-1">QA</h4>
<p>マイク回しててメモ取れず。。。</p>
<h2 id="国土交通省のデータをsolrで検索株式会社ネクスト秀野さん">国土交通省のデータをSolrで検索　株式会社ネクスト　秀野さん</h2>
<p><a href="https://speakerdeck.com/ryo0301/guo-jiao-sheng-falsedetawosolrdejian-suo">スライドはこちら</a></p>
<ul>
<li>評価の関係で。。。</li>
<li>Spatial検索の話</li>
</ul>
<h4 id="デモの想定機能">デモの想定機能</h4>
<ul>
<li>地図上の小学校を起点に物件検索</li>
<li>地図上をクリックしたところを中心に検索</li>
</ul>
<h4 id="デモ環境">デモ環境</h4>
<ul>
<li>Solr4.3.0、PostGIS 2.0.3、東京都のデータ</li>
</ul>
<h4 id="事前知識">事前知識</h4>
<ul>
<li>ジオメトリーデータ（点、線、面がある）</li>
<li>WKB/WKT、Intersects（しらなかった。こんなのもあるのか）</li>
</ul>
<h4 id="環境">環境</h4>
<ul>
<li>EC2上にPostGIS＋Solrで構築</li>
<li>WKT形式でDIHでインポートできるらしい。</li>
<li>Solr＋S3をJSでGoogleMapへ</li>
</ul>
<h2 id="solr-44新機能をちょっと紹介johtani">Solr 4.4新機能をちょっと紹介　@johtani</h2>
<p>紹介というよりも宣伝。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>『プログラミング Hive』 『Hadoop 第3版』刊行記念セミナーに参加しました！ #oreilly0724</title>
      <link>https://blog.johtani.info/blog/2013/07/25/hadoop-hive-publication-party/</link>
      <pubDate>Thu, 25 Jul 2013 02:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/25/hadoop-hive-publication-party/</guid>
      <description>Hadoopとか離れちゃってるし、Hive触ったこと無いにもかかわらず参加しました！ （たまたま近くにいるからって理由なのは内緒で） 玉川さんの</description>
      <content:encoded><p>Hadoopとか離れちゃってるし、Hive触ったこと無いにもかかわらず参加しました！<br>
（たまたま近くにいるからって理由なのは内緒で）<br>
玉川さんの四方山話を聞くのが主目的で参加しました。（ちょっと翻訳が気になってるので）</p>
<p>イベントページは<a href="http://connpass.com/event/2944/">こちら</a><br>
刊行記念イベントにも関わらず、想像以上の人の入りでびっくりしました。Hadoop、Hive界隈はまだまだ人気なんだなぁと。<br>
プレゼントじゃんけん大会もあったのですが、そうそうに負けてしまったのが悔やまれます。。。<br>
Team Geek欲しかったなぁ。もちろん、懇親会まで参加しました。</p>
<p>以下、いつものメモです。</p>
<!--  more -->
<h2 id="hiveの正しい使い方cloudera-嶋内さん">Hiveの正しい使い方（Cloudera 嶋内さん）</h2>
<p>残念ながら、マサカリは持ってなかったです。</p>
<ul>
<li>スライドの各所に本の章番号が書いてあるのがうれしい。</li>
<li>Hiveロゴが回ってたのでスライドの時に集中できなかったｗ</li>
<li>Impalaの話も出てきた。
<ul>
<li>速いけど、色々足りない。Hiveの置き換えじゃないよと。</li>
</ul>
</li>
</ul>
<h2 id="hiveとimpalaのおいしいとこ取りセラン須田さん">HiveとImpalaのおいしいとこ取り（セラン　須田さん）</h2>
<p>スライド：<a href="http://www.slideshare.net/sudabon/20130724-oreilly-org">http://www.slideshare.net/sudabon/20130724-oreilly-org</a></p>
<ul>
<li>オンプレだとCDH便利だよと教えてもらう</li>
<li>いくつかSlideshareにImpalaの性能評価の資料を上げてある（必要になったら検索で。。。）</li>
<li>リリースされたその日に性能評価やってレポート書くとかすごすぎ！</li>
</ul>
<h2 id="翻訳の四方山話玉川さん">翻訳の四方山話（玉川さん）</h2>
<ul>
<li>翻訳＝写経です</li>
<li>締め切り駆動勉強法ｗ</li>
<li>4page/day</li>
<li>自分から電突してオライリーさんに翻訳させてくださいと。</li>
<li>他の方の本が読めない（チェックしちゃうのでｗ）</li>
<li>動機があるから読めるってのはあるだろうなぁ。</li>
<li>選び方：わくわくするもの、仕事に活きるもの</li>
<li>今年もあと2冊やる予定（Hadoop Operations、Vagrantを翻訳中）</li>
<li>来年の候補（Chefとか）</li>
</ul>
<h2 id="高可用性hdfsのご紹介cloudera-小林さん">高可用性HDFSのご紹介（Cloudera 小林さん）</h2>
<ul>
<li>スライドにどの版で書いてあったかがわかりやすく書いてある。</li>
<li>3段階の開発フェーズを経てる</li>
<li>QJMのお話</li>
</ul>
<h2 id="cloudera-universityとhadoop認定試験cloudera-川崎さん">Cloudera UniversityとHadoop認定試験（Cloudera 川崎さん）</h2>
<ul>
<li>Clouderaデータアナリスト向けトレーニング（3日間、10月日本語で開催予定）
<ul>
<li>Hive、Pig、Impalaなど</li>
</ul>
</li>
<li>Data Science入門コースも準備中</li>
<li>出版記念！
<ul>
<li>8月管理者向け先着20 or 30名にHadoop第3版贈呈予定</li>
<li>先着20名にプログラミングHive贈呈予定</li>
</ul>
</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Yokozunaの気になる点というかなんというか</title>
      <link>https://blog.johtani.info/blog/2013/07/11/yokozuna-check-point/</link>
      <pubDate>Thu, 11 Jul 2013 01:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/11/yokozuna-check-point/</guid>
      <description>Yokozunaの気になる点というか、自分だったらこのへん調べるだろうなって観点を上げてみます。 別に調べるわけじゃないので、完全に自己満足な</description>
      <content:encoded><p>Yokozunaの気になる点というか、自分だったらこのへん調べるだろうなって観点を上げてみます。
別に調べるわけじゃないので、完全に自己満足なメモですけど。<br>
ちなみに、分散システムとかRiakの仕組みは詳しくないので、ズレてる点がいっぱいあるかも。<br>
というか、分散システムでテストというか、検討する点とかってまとまってる資料とかあるのかなぁ？</p>
<!-- more -->
<ul>
<li>スキーマ変更時の挙動
<ul>
<li>フィールド型変更とか、フィールド追加とか</li>
</ul>
</li>
<li>既存RiakクラスタにYokozunaの機能を追加する方法と制限
<ul>
<li>タイムラグとかも</li>
</ul>
</li>
<li>Riak＋Yokozunaクラスタに対してノード追加時に発生するオーバーヘッド（ネットワークとかディスクIOとか）</li>
<li>性能検証のためのシナリオ（どっちが先に悲鳴をあげるかとか）
<ul>
<li>Riakメインで、Yokozunaはおまけ程度に検索するというシナリオ</li>
<li>Yokozunaメインで使うシナリオ</li>
<li>更新が多い場合のシナリオ</li>
</ul>
</li>
<li>Riakのみ、Riak＋Yokozunaの各種統計情報（CPU、メモリ、ディスクサイズ、ネットワークIO）</li>
<li>運用系（監視とか）の手法とか機能？とか</li>
<li>バージョンアップなどの対応方法</li>
<li>Solrがコケた時とかの対処</li>
</ul>
<p>とりあえず、こんな感じかなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Riak Meetup Tokyo #2に参加しました。#riakjp</title>
      <link>https://blog.johtani.info/blog/2013/07/10/riak-meetup-tokyo-no2/</link>
      <pubDate>Wed, 10 Jul 2013 18:57:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/10/riak-meetup-tokyo-no2/</guid>
      <description>先日、Bashoさんにおじゃましたのもあり、Riak Meetup Tokyo #2に参加しました。 Yokozunaの話も聞けるということで。 懇親会も参加しました。</description>
      <content:encoded><p>先日、Bashoさんにおじゃましたのもあり、<a href="http://connpass.com/event/2656/">Riak Meetup Tokyo #2</a>に参加しました。<br>
Yokozunaの話も聞けるということで。
懇親会も参加しました。Vさん＆リピさんと話し込んじゃってあんまり他の人と話せなかったけど。。。</p>
<p>以下はいつものメモです。</p>
<!-- more -->
<h2 id="freakout-久森さん-riak環境をプロダクションで構築運用してみた仮">FreakOut 久森さん 「Riak環境をプロダクションで構築＆運用してみた（仮）」</h2>
<h3 id="freakoutとrtb">FreakOutとRTB</h3>
<ul>
<li>ディスプレイ広告の新しい配信の枠の話</li>
<li>この人には何出すの？いくらで？みたいな感じ</li>
<li>純広告：表示保証、期間保証など</li>
<li>RTB：1回の広告表示ごとに買い付け</li>
<li>DSP（デマンド・サイト・プラットフォーム）</li>
<li>広告表示は大体0.1秒で表示しないといけない。この間に色々やってる。
<ul>
<li>50ms or die.で戦ってます。</li>
</ul>
</li>
<li>RTBはCPUバウンド
<ul>
<li>多コアを安く並べたい</li>
<li>Tokyoなんとかとか使ってた。
<ul>
<li>スケーラビリティがキツイ（クライアント側でアルゴリズム分散してる）</li>
<li>データ解析もしたいけど、検索ができない</li>
</ul>
</li>
<li>RTBに適したRiakがうまくハマるのではと。</li>
</ul>
</li>
</ul>
<h3 id="構成とかとか">構成とかとか</h3>
<ul>
<li>アプリはPerlなので、PerlでRiakクライアントが必要。Memcached互換とかあると嬉しい。</li>
<li>ProtobufサポートもPurePerlしかなかった。</li>
<li>ないなら、作ろうと。<a href="https://github.com/myfinder?tab=repositories">githubに上がってます。このへんかな？</a></li>
<li>監視はcloudforecastとかでやってる。</li>
</ul>
<h3 id="課題">課題</h3>
<ul>
<li>Redirectがつらい（haproxy？がつらい？）</li>
<li>Setが詰まるとつらい（ケースがまだわからない）</li>
<li>対策１
<ul>
<li>memcached＋Riak</li>
</ul>
</li>
<li>対応２（案）
<ul>
<li>hashからpartitionに直接取りに行くとか</li>
</ul>
</li>
</ul>
<h3 id="まとめ">まとめ</h3>
<ul>
<li>素のままRiakはちょっとつらい</li>
</ul>
<h4 id="qa">QA</h4>
<p>聞き取れたやつだけ</p>
<ul>
<li>Q：1台いくら位ですか？
<ul>
<li>A：10万から11万くらい</li>
</ul>
</li>
<li>Q：どのくらいの性能ですか？
<ul>
<li>A：同時1000くらいをさばいてる？</li>
</ul>
</li>
<li>Q：50ms以下を出すのに、ネットワーク周りで近さとかを考えることありますか？
<ul>
<li>A：国内だと10msあればなんとかなる。それよりもアプリ側のチューニングのほうがまだ重要</li>
</ul>
</li>
<li>Q：Cassandraとか候補に挙がらなかったんですか？
<ul>
<li>A：苦しんでる人が知人にいるので。。。あと、用途的に違うので。</li>
</ul>
</li>
<li>Q：バックエンドとしてはなにを？
<ul>
<li>A：bitcaskにしてる</li>
</ul>
</li>
<li>Q：サーバ構成、ネットワーク構成がどうなってる？
<ul>
<li>A：。。。</li>
</ul>
</li>
<li>Q：Redirectとは？RiakがやってるRedirect？
<ul>
<li>A：はい。</li>
</ul>
</li>
<li>Q：他に候補にあがったのは？
<ul>
<li>A：<a href="http://www.aerospike.com">商用のaerospike（これかな？）</a>がスケールできそうだったけど、クライアントがいまいち。。。</li>
</ul>
</li>
</ul>
<h3 id="感想">感想</h3>
<p>広告業界のことをよくわかってないので、微妙にピンときてなかったりもするのですが、以下に素早く返すかって観点でどこに注力して、問題点を潰していくのかってのは面白そうだなぁと。
リクエスト処理の性能がクリアできたらつぎはスケールの観点（ノード追加時の挙動とか）で検証していくんだろうなと。次回の話も聞いてみたい感じです。</p>
<h2 id="iij-曽我部さん田中さん-yokozuna-日本語検索性能を評価しました">IIJ 曽我部さん、田中さん 「Yokozuna 日本語検索性能を評価しました」</h2>
<h3 id="yokozunaって">Yokozunaって？</h3>
<ul>
<li>Riak＋Solrでいいとこ取り</li>
<li>データの登録とかはRiakのAPIで。</li>
<li>SolrのAPIが使える。</li>
<li>YokozunaがSolrの分散検索の部分を隠してくれる。</li>
</ul>
<h3 id="yokozunaのインストールとか">Yokozunaのインストールとか。</h3>
<ul>
<li>SolrのAPIっぽい形で検索できるし、戻りもSolrのXMLっぽいのが出てくるよ。</li>
</ul>
<h3 id="wikipediaデータってstoreの性能とか">Wikipediaデータってstoreの性能とか。</h3>
<ul>
<li>Riakのノード32台。（Xeon、メモリ24GB、HDD。。。）</li>
<li>yz_extractor：Riakのコンテンツタイプを見てSolrにデータを入れる処理が書いてある。</li>
<li>自分でschema.xmlを書いてYokozunaに指定することもできる。
<ul>
<li>スキーマの変更とか登録とか。
<ul>
<li>すでに指定済みスキーマを変更した場合の挙動ってどうなるの？</li>
</ul>
</li>
</ul>
</li>
<li>デモではSolrからid取って、Riakからその他のデータを取り出していた。</li>
</ul>
<h4 id="rubyでの性能評価">Rubyでの性能評価</h4>
<ul>
<li>ベンチマークプログラム側の問題が先に影響が出てしまった。</li>
</ul>
<h3 id="qa-1">QA</h3>
<ul>
<li>Q：Riak単体とYokozunaつかった時でディスク容量がどのくらい増えた？
<ul>
<li>A：ちゃんと調べてないが、10%くらい増えた気がする。</li>
</ul>
</li>
<li>Q：Solr側の設定でstored=trueだけど、falseにしてもいいんじゃないの？
<ul>
<li>A：デモはfalseにしてます。</li>
</ul>
</li>
<li>Q：スキーマってあとから変更できるんですかね？
<ul>
<li>A：まだ良くわかってないです。</li>
</ul>
</li>
<li>Q：ノードの追加、削除時の挙動とかも気になります。</li>
</ul>
<h3 id="感想-1">感想</h3>
<p>今回はStore性能に関してでしたが、今後は検索性能やシナリオによる性能（KVSの処理メインで、時々全文検索とか、全文検索の処理も結構あるパターンとか）の測定とか、耐障害性とかの観点で調査を進めてもらってSolr勉強会で話をしてもらえると面白そうだなぁと勝手に思ってみたり。
Solr勉強会へのコンタクトお待ちしてます！ｗ</p>
</content:encoded>
    </item>
    
    <item>
      <title>スキーマレスモード？（SOLR-4897）を調べて見ました。</title>
      <link>https://blog.johtani.info/blog/2013/07/04/schemaless-example/</link>
      <pubDate>Thu, 04 Jul 2013 01:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/04/schemaless-example/</guid>
      <description>Solr 4.4に取り込まれる予定のチケットで、気になるものを見つけたのでいつものごとく調べてみました。 元となるチケットはこちら。SOLR-4897</description>
      <content:encoded><p>Solr 4.4に取り込まれる予定のチケットで、気になるものを見つけたのでいつものごとく調べてみました。</p>
<p>元となるチケットは<a href="https://issues.apache.org/jira/browse/SOLR-4897">こちら。SOLR-4897</a>。</p>
<!-- more -->
<h2 id="スキーマレス">スキーマレス？</h2>
<p>Solrはschema.xmlにデータの定義（フィールドタイプやフィールドなど）を記述して、データを登録する全文検索システムです。
これまでのSolrではこの設定ファイルを元にデータを登録するフィールド名を決定しており、
変更を行う場合はSolrのコアを再起動するなどの手順が必要でした。（※ダイナミックフィールドはすこし特殊）</p>
<p>それだと、Solrを管理するのがめんどくさいですね？という感じで現れたのが<a href="http://wiki.apache.org/solr/SchemaRESTAPI">SchemaREST API</a>です。（たぶん。）</p>
<h2 id="schema-rest-api">Schema REST API</h2>
<p>Solr 4.2から導入されたSolrのスキーマに関する情報を提供するためのREST APIです。
4.2で導入されたのはあくまでもschema.xmlの情報を取得するためのAPIでした。
たとえば、Fieldの一覧を取得するとか。</p>
<p>4.4から、フィールドの追加（変更、削除はできない）ができるようになりました。あくまでも、フィールドの追加で、フィールドタイプなどの追加はまだできません。（できるようになるのかもわからないですが。）
フィールドの追加方法などは<a href="http://wiki.apache.org/solr/SchemaRESTAPI?highlight=%28managed%29#Adding_fields_to_a_schema">Wiki</a>に記載がありました。</p>
<p>ということで、簡単に試してみることに。</p>
<h2 id="起動方法">起動方法</h2>
<p>exampleディレクトリの下にexample-schemalessというディレクトリが新設されています。
ここに、スキーマレスモード用の設定がされているファイルが入っているので、こちらを利用します。</p>
<pre><code>cd $SOLR/example
java -Dsolr.solr.home=example-schemaless/solr -jar start.jar
</code></pre><p>ログにいくつかWARNが出ますが、影響の内パス設定ミスなので無視してOKです。</p>
<p>最初に定義されているフィールドは「id」と「_version_」のみになります。（Schema Browserなどで確認できます。あ、REST APIでもいいですね。<a href="http://localhost:8983/solr/schema/fields">http://localhost:8983/solr/schema/fields</a>）</p>
<h2 id="スキーマの更新">スキーマの更新</h2>
<p>さて、フィールドを追加してみます。
PUTを利用すると1フィールドの追加が可能です。
「fugatext」というフィールド名でフィールドを追加しています。今のところJSONのみ対応みたいです。</p>
<pre><code>$ curl -X PUT http://localhost:8983/solr/schema/fields/fugatext -H 'Content-Type: application/json' -d '{&quot;type&quot;:&quot;text_ja&quot;,&quot;stored&quot;:false,&quot;multiValued&quot;:true}'
{
  &quot;responseHeader&quot;:{
    &quot;status&quot;:0,
    &quot;QTime&quot;:18}}
</code></pre><p>追加できたかどうかもREST APIで取得してみます。</p>
<pre><code>$ curl http://localhost:8983/solr/schema/fields
{
  &quot;responseHeader&quot;:{
    &quot;status&quot;:0,
    &quot;QTime&quot;:0},
  &quot;fields&quot;:[{
      &quot;name&quot;:&quot;_version_&quot;,
      &quot;type&quot;:&quot;long&quot;,
      &quot;indexed&quot;:true,
      &quot;stored&quot;:true},
    {
      &quot;name&quot;:&quot;fugatext&quot;,
      &quot;type&quot;:&quot;text_ja&quot;,
      &quot;multiValued&quot;:true,
      &quot;stored&quot;:false},
    {
      &quot;name&quot;:&quot;id&quot;,
      &quot;type&quot;:&quot;string&quot;,
      &quot;multiValued&quot;:false,
      &quot;indexed&quot;:true,
      &quot;required&quot;:true,
      &quot;stored&quot;:true,
      &quot;uniqueKey&quot;:true}]}
</code></pre><p>追加できました。
ちなみに、同じフィールド名を追加しようとするとエラーが帰ってきます。</p>
<pre><code>$ curl -X PUT http://localhost:8983/solr/schema/fields/fugatext -H 'Content-Type: application/json' -d '{&quot;type&quot;:&quot;text_ja&quot;,&quot;stored&quot;:false,&quot;multiValued&quot;:true}'
{
  &quot;responseHeader&quot;:{
    &quot;status&quot;:400,
    &quot;QTime&quot;:1},
  &quot;error&quot;:{
    &quot;msg&quot;:&quot;Field 'fugatext' already exists.&quot;,
    &quot;code&quot;:400}}
</code></pre><h2 id="設定の違い">設定の違い</h2>
<p>example-schemalessのsolrconfig.xmlは以下の設定が通常のexampleとは異なるようです。</p>
<h4 id="schemafactoryの設定">schemaFactoryの設定</h4>
<p>schemaをAPIから変更可能にする設定です。これまでの変更しない設定の場合は<code>ClassicIndexSchemaFactory</code>を指定します。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">...
  <span style="color:#f92672">&lt;schemaFactory</span> <span style="color:#a6e22e">class=</span><span style="color:#e6db74">&#34;ManagedIndexSchemaFactory&#34;</span><span style="color:#f92672">&gt;</span>
    <span style="color:#f92672">&lt;bool</span> <span style="color:#a6e22e">name=</span><span style="color:#e6db74">&#34;mutable&#34;</span><span style="color:#f92672">&gt;</span>true<span style="color:#f92672">&lt;/bool&gt;</span>
    <span style="color:#f92672">&lt;str</span> <span style="color:#a6e22e">name=</span><span style="color:#e6db74">&#34;managedSchemaResourceName&#34;</span><span style="color:#f92672">&gt;</span>managed-schema<span style="color:#f92672">&lt;/str&gt;</span>
  <span style="color:#f92672">&lt;/schemaFactory&gt;</span>
...
</code></pre></div><h4 id="updatechainの設定">update.chainの設定</h4>
<p>更新処理（update関連のリクエストハンドラ「/update」とか）には次のような設定が追加されていました。（1006行目あたり）</p>
<pre><code>  &lt;requestHandler name=&quot;/update&quot; class=&quot;solr.UpdateRequestHandler&quot;&gt;
    &lt;!-- See below for information on defining 
         updateRequestProcessorChains that can be used by name 
         on each Update Request
      --&gt;
    &lt;lst name=&quot;defaults&quot;&gt;
      &lt;str name=&quot;update.chain&quot;&gt;add-unknown-fields-to-the-schema&lt;/str&gt;
    &lt;/lst&gt;
  &lt;/requestHandler&gt;
</code></pre><p>「add-unknown-fields-to-the-schema」というupdate.chainが指定されています。このchainの定義自体は1669行目くらいに存在します。
（長い。。。）</p>
<pre><code>  &lt;!-- Add unknown fields to the schema 
  
       An example field type guessing update processor that will
       attempt to parse string-typed field values as Booleans, Longs,
       Doubles, or Dates, and then add schema fields with the guessed
       field types.  
       
       This requires that the schema is both managed and mutable, by
       declaring schemaFactory as ManagedIndexSchemaFactory, with
       mutable specified as true. 
       
       See http://wiki.apache.org/solr/GuessingFieldTypes
    --&gt;
  &lt;updateRequestProcessorChain name=&quot;add-unknown-fields-to-the-schema&quot;&gt;
    &lt;processor class=&quot;solr.RemoveBlankFieldUpdateProcessorFactory&quot;/&gt;
    &lt;processor class=&quot;solr.ParseBooleanFieldUpdateProcessorFactory&quot;/&gt;
    &lt;processor class=&quot;solr.ParseLongFieldUpdateProcessorFactory&quot;/&gt;
    &lt;processor class=&quot;solr.ParseDoubleFieldUpdateProcessorFactory&quot;/&gt;
    &lt;processor class=&quot;solr.ParseDateFieldUpdateProcessorFactory&quot;&gt;
      &lt;arr name=&quot;format&quot;&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss.SSSZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss,SSSZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss.SSS&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss,SSS&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm:ssZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm:ss&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mmZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd'T'HH:mm&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm:ss.SSSZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm:ss,SSSZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm:ss.SSS&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm:ss,SSS&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm:ssZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm:ss&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mmZ&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd HH:mm&lt;/str&gt;
        &lt;str&gt;yyyy-MM-dd&lt;/str&gt;
      &lt;/arr&gt;
    &lt;/processor&gt;
    &lt;processor class=&quot;solr.AddSchemaFieldsUpdateProcessorFactory&quot;&gt;
      &lt;str name=&quot;defaultFieldType&quot;&gt;text_general&lt;/str&gt;
      &lt;lst name=&quot;typeMapping&quot;&gt;
        &lt;str name=&quot;valueClass&quot;&gt;java.lang.Boolean&lt;/str&gt;
        &lt;str name=&quot;fieldType&quot;&gt;booleans&lt;/str&gt;
      &lt;/lst&gt;
      &lt;lst name=&quot;typeMapping&quot;&gt;
        &lt;str name=&quot;valueClass&quot;&gt;java.util.Date&lt;/str&gt;
        &lt;str name=&quot;fieldType&quot;&gt;tdates&lt;/str&gt;
      &lt;/lst&gt;
      &lt;lst name=&quot;typeMapping&quot;&gt;
        &lt;str name=&quot;valueClass&quot;&gt;java.lang.Long&lt;/str&gt;
        &lt;str name=&quot;valueClass&quot;&gt;java.lang.Integer&lt;/str&gt;
        &lt;str name=&quot;fieldType&quot;&gt;tlongs&lt;/str&gt;
      &lt;/lst&gt;
      &lt;lst name=&quot;typeMapping&quot;&gt;
        &lt;str name=&quot;valueClass&quot;&gt;java.lang.Number&lt;/str&gt;
        &lt;str name=&quot;fieldType&quot;&gt;tdoubles&lt;/str&gt;
      &lt;/lst&gt;
    &lt;/processor&gt;
    &lt;processor class=&quot;solr.LogUpdateProcessorFactory&quot;/&gt;
    &lt;processor class=&quot;solr.RunUpdateProcessorFactory&quot;/&gt;
  &lt;/updateRequestProcessorChain&gt;
</code></pre><p>使ってるUpdateProcessorはこんな感じみたいです。最後の2つはこれ用じゃないので省略。</p>
<table>
<tr><th>プロセッサ名</th><th>説明</th></tr>
<tr><td>RemoveBlankFieldUpdateProcessorFactory</td><td>値がないフィールドは除去</td></tr>
<tr><td>ParseBooleanFieldUpdateProcessorFactory</td><td>スキーマに定義されていないフィールドで、値がBooleanとしてパースできたら、Boolean型とする。</td></tr>
<tr><td>ParseLongFieldUpdateProcessorFactory</td><td>スキーマに定義されていないフィールドで、値がLongとしてパースできたら、Long型とする。</td></tr>
<tr><td>ParseDoubleFieldUpdateProcessorFactory</td><td>スキーマに定義されていないフィールドで、値がDoubleとしてパースできたら、Double型とする。</td></tr>
<tr><td>ParseDateFieldUpdateProcessorFactory</td><td>スキーマに定義されていないフィールドで、値がDateとしてパースできたら、Date型とする。（パースの形式がformatで列挙されてる）</td></tr>
<tr><td>AddSchemaFieldsUpdateProcessorFactory</td><td>入力されたドキュメントの中でスキーマに定義されていないフィールド（静的、動的両方）を見つけた時に、フィールドの値の型を元にフィールド型をマッピングする。</td></tr>
</table>
<p>とここまで見てきたところで、スキーマレスという名前の意図がちょっとわかったかも。</p>
<h2 id="定義されてないフィールドを持ったデータを登録">定義されてないフィールドを持ったデータを登録</h2>
<p>起動時には定義されてないフィールドをもったデータを登録してみます。
boolean型で試してみることに。以下のデータを管理画面のデータ登録画面から登録します。（http://localhost:8983/solr/#/collection1/documents）
（タイトルでbooleanってわかりにくいですが）</p>
<pre><code>{&quot;id&quot;:&quot;change.me&quot;,&quot;title&quot;:true, &quot;price&quot;:1.25, &quot;fuga&quot;:&quot;100,200&quot;}
</code></pre><p>エラーは出ません。で、またフィールド一覧を取得すると。</p>
<pre><code>$ curl http://localhost:8983/solr/schema/fields
{
  &quot;responseHeader&quot;:{
    &quot;status&quot;:0,
    &quot;QTime&quot;:1},
  &quot;fields&quot;:[{
      &quot;name&quot;:&quot;_version_&quot;,
      &quot;type&quot;:&quot;long&quot;,
      &quot;indexed&quot;:true,
      &quot;stored&quot;:true},
    {
      &quot;name&quot;:&quot;fuga&quot;,
      &quot;type&quot;:&quot;tlongs&quot;},
    {
      &quot;name&quot;:&quot;fugatext&quot;,
      &quot;type&quot;:&quot;text_ja&quot;,
      &quot;multiValued&quot;:true,
      &quot;stored&quot;:false},
    {
      &quot;name&quot;:&quot;id&quot;,
      &quot;type&quot;:&quot;string&quot;,
      &quot;multiValued&quot;:false,
      &quot;indexed&quot;:true,
      &quot;required&quot;:true,
      &quot;stored&quot;:true,
      &quot;uniqueKey&quot;:true},
    {
      &quot;name&quot;:&quot;price&quot;,
      &quot;type&quot;:&quot;tdoubles&quot;},
    {
      &quot;name&quot;:&quot;title&quot;,
      &quot;type&quot;:&quot;booleans&quot;}]}
</code></pre><p>おー、最後にtitleが追加されてます。他にもfugaやpriceも。（日付は手を抜きました。。。）</p>
<h2 id="感想">感想</h2>
<p>詳細までは追いかけてないですが、こんなかんじです。
フィールド追加が可能になるのはいいんじゃないでしょうか。SolrCloudの機能との関連もあるのかもしれません。ZooKeeperへの出力も実装されてそうなので。</p>
<p>ただ、機械的に出力されたschema.xml（exampleだとmanaged-schemaというファイル）には_「DO NOT EDIT」_との記述があるので、修正するとなにかおきてしまうかもしれないですねぇ。
現時点では、フィールドタイプの変更やフィールドの更新、削除に関してはSolrCoreの再起動などの手順が必要です。
あと、変なデータ（タイプミスとか）が登録されたりしないかってのは気になりますね。</p>
<p>※ちなみに、別の人が気づいたんですが、ちょっとバグが有ったみたいで、代わりにチケットつくったらキリ番（<a href="https://issues.apache.org/jira/browse/SOLR-5000">SOLR-5000</a>）
ゲットしましたｗ</p>
</content:encoded>
    </item>
    
    <item>
      <title>Solrの管理画面でデータ登録</title>
      <link>https://blog.johtani.info/blog/2013/06/27/upload-docs-solr-admin/</link>
      <pubDate>Thu, 27 Jun 2013 16:28:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/27/upload-docs-solr-admin/</guid>
      <description>SolrのチケットをML経由で眺めてるんですが、便利そうなチケットが流れてきたのでブログを書いてみみようかと。 元になってるチケットはこちらで</description>
      <content:encoded><p>SolrのチケットをML経由で眺めてるんですが、便利そうなチケットが流れてきたのでブログを書いてみみようかと。
元になってるチケットは<a href="https://issues.apache.org/jira/browse/SOLR-4921">こちら</a>です。昨日だか、今朝にtrunkとbranch_4xにコミットされたみたいです。試してみたい方は、branch_4xかtrunkをチェックアウトすると触ることができます。</p>
<!-- more -->
<h2 id="データ登録用の画面json">データ登録用の画面（JSON）</h2>
<p>branch_4xをチェックアウトしてexampleを起動し、Solrにアクセスします。<br>
管理画面に「Dcuments」という項目が追加されてます。開くとこんなかんじです。</p>
<p>
  <img src="/images/entries/20130626/add_default_page.jpg" alt="デフォルトのデータ登録画面">

</p>
<p>なんと、デフォルトはJSONになってます。これも時代の流れでしょうかｗ<br>
Solrでは、これまで設定ファイルやデータ登録もXMLがメインになっていました。（<a href="http://www.amazon.co.jp/gp/product/4774141755/ref=as_li_ss_tl?ie=UTF8&amp;camp=247&amp;creative=7399&amp;creativeASIN=4774141755&amp;linkCode=as2&amp;tag=johtani-22">Apache Solr入門</a>もXMLを基本に書いてます。このころはまだデフォルトでは対応してなかったので）</p>
<p>登録するデータをテキストエリアに記述して、「Submit Document」をクリックすればデータは登録されます。基本的には単件登録の画面でしょうか。
（登録されたデータを確認するには「Query」画面を利用すればいいです。）
また、JSONのデータ形式は<a href="http://wiki.apache.org/solr/UpdateJSON">SolrのWiki</a>を参照してください。</p>
<h3 id="csvやxmlも">CSVやXMLも</h3>
<p>この管理画面ではJSON以外の形式でもデータの登録が可能です。
「Document Type」の項目をクリックすると以下のように選択肢があられます。</p>
<p>
  <img src="/images/entries/20130626/select_document_type.jpg" alt="Document Typeの選択">

</p>
<p>CSV、XMLについては、先ほどのJSONの画面の用に、テキストエリアが表示されます。
テキストエリアにCSV（データの形式は<a href="http://wiki.apache.org/solr/UpdateCSV">こちら</a>）やXML（データの形式は<a href="http://wiki.apache.org/solr/UpdateXmlMessages">こちら</a>）を入力してボタンを押せば登録できます。</p>
<h3 id="solr-command形式もjosnかxml">Solr Command形式も（JOSNかXML）</h3>
<p>Solr Command というのはXMLやJSONで登録、コミット、削除などを実行するための画面になります。
JSONのコマンドは<a href="http://wiki.apache.org/solr/UpdateJSON#Update_Commands">こちら</a>、XMLのコマンドは<a href="http://wiki.apache.org/solr/UpdateXmlMessages">こちら</a>をご覧ください。</p>
<p>あと、便利なのがファイルアップロードです。
こんなかんじで、ファイルを選んでSubmitすればデータが登録出来ます。</p>
<p>
  <img src="/images/entries/20130626/file_upload.jpg" alt="File Uploadの画面">

</p>
<p>ファイルのサイズが大きいとちょっと時間がかかりますが、コマンドを打つより簡単かもしれません。
post.jarツールと違って、デフォルトでコミットをしてくれるわけではないので、「Extracting Req. Handler Params」に「commit=true」をつけないと、データが登録されてない？と思ってしまうかもしれませんが。</p>
<h3 id="組立もできるみたいdocument-builder">組立もできるみたい（Document Builder）</h3>
<p>最後に紹介するのが「Document Builder」というタイプです。</p>
<p>
  <img src="/images/entries/20130626/document_builder_desc.jpg" alt="Document Builderの画面説明">

</p>
<p>もっと簡易にデータを記述できるようにということで用意されているようです。
フィールドの情報はSolrに接続して利用できるフィールド？（ダイナミックはないのかな？）が表示されます。</p>
<p>
  <img src="/images/entries/20130626/document_builder_fields.jpg" alt="Document Builderでフィールド表示">

</p>
<p>追記していくとこんなかんじになります。</p>
<p>
  <img src="/images/entries/20130626/document_builder_.jpg" alt="出来上がったドキュメント">

</p>
<p>日本語のデータもちゃんと登録できました。
ただ、まだ、開発中なんでしょうがないかもしれませんが、以下の様な制約があるようです。</p>
<ul>
<li>multiValuedなフィールドに値を追加できない（上書きされる）</li>
<li>改行が入ったデータをテキストエリアにいれると「Add Field」を押しても反応しない</li>
<li>ダイナミックフィールドは自分で書きましょう</li>
</ul>
<p>ただ、これまでXMLでファイルを作ってコマンドで登録したり、curlコマンドでJSON書いたりして登録していたよりはお手軽にさわれるようになるかと思います。つぎの4x系のバージョンが出たときはこちらからデータを登録してみてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Kibana3というのもありまして</title>
      <link>https://blog.johtani.info/blog/2013/06/19/introduction-kibana3/</link>
      <pubDate>Wed, 19 Jun 2013 23:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/19/introduction-kibana3/</guid>
      <description>前回は3番煎じぐらいでしたが、今回は初記事かな？（だといいな） Kibanaには、前回の記事で書いたものとは別に開発中のKibana3というの</description>
      <content:encoded><p>前回は3番煎じぐらいでしたが、今回は初記事かな？（だといいな）</p>
<p>Kibanaには、<a href="http://blog.johtani.info/blog/2013/06/10/fluent-es-kibana/">前回の記事</a>で書いたものとは別に開発中の<a href="http://three.kibana.org/">Kibana3</a>というのが存在します。</p>
<!-- more -->
<h2 id="kibana3って">Kibana3って？</h2>
<p>Kibana2はRubyで書かれていましたが、Kibana3はHTML＋JavaScriptで構成されています。
ですので、ApacheなどのWebサーバに配置することで、利用が可能となります。
ただ、HTML＋JavaScriptのため、ブラウザ上で動作するためブラウザが動作するマシンからElasticSearch（通常だと<code>http://マシン名orIPアドレス:9200/</code>とか）にアクセスできなければいけないという制限があります。</p>
<p>この条件さえクリア出来れば、Kibana3ではKibana2よりも様々なパネルが用意されていて、色々できそうなのでお勧めです。</p>
<h2 id="インストール">インストール</h2>
<p>ElasticSearchやログについては、前回の記事の環境を利用しました。
ですので、Kibana3のインストールのみです。（ApacheもCentOSのサーバに入っていたので。）</p>
<p>ダウンロードして、Apacheの公開ディレクトリに置いただけです。（お試し環境のため、権限とかは大目に見てください。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ git clone https://github.com/elasticsearch/kibana.git kibana-javascript
$ cp -R kibana-javascript /var/www/html
</code></pre></div><p>今回はApacheとElasticSearchが同一マシン（＝同一IPアドレスでアクセス可能）で動作している＋ElasticSearchへのアクセスのポートがデフォルト（9200）のため特に設定が必要ありませんでした。</p>
<p>ElasticSeachサーバとKibana3のApacheのサーバが別のサーバの場合やElasticSearchサーバのポートが異なる場合はkibana-javascript/config.jsファイルの編集が必要になります。
cloneしてすぐのconfig.jsは、以下のとおりです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#75715e">/*
</span><span style="color:#75715e">
</span><span style="color:#75715e">elasticsearch:  URL to your elasticsearch server. You almost certainly don&#39;t
</span><span style="color:#75715e">                want &#39;http://localhost:9200&#39; here. Even if Kibana and ES are on
</span><span style="color:#75715e">                the same host
</span><span style="color:#75715e">kibana_index:   The default ES index to use for storing Kibana specific object
</span><span style="color:#75715e">                such as stored dashboards
</span><span style="color:#75715e">modules:        Panel modules to load. In the future these will be inferred
</span><span style="color:#75715e">                from your initial dashboard, though if you share dashboards you
</span><span style="color:#75715e">                will probably need to list them all here
</span><span style="color:#75715e">
</span><span style="color:#75715e">If you need to configure the default dashboard, please see dashboards/default
</span><span style="color:#75715e">
</span><span style="color:#75715e">*/</span>
<span style="color:#66d9ef">var</span> <span style="color:#a6e22e">config</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#a6e22e">Settings</span>(
{
  <span style="color:#75715e">// By default this will attempt to reach ES at the same host you have
</span><span style="color:#75715e"></span>  <span style="color:#75715e">// elasticsearch installed on. You probably want to set it to the FQDN of your
</span><span style="color:#75715e"></span>  <span style="color:#75715e">// elasticsearch host
</span><span style="color:#75715e"></span>  <span style="color:#a6e22e">elasticsearch</span><span style="color:#f92672">:</span>    <span style="color:#e6db74">&#34;http://&#34;</span><span style="color:#f92672">+</span>window.<span style="color:#a6e22e">location</span>.<span style="color:#a6e22e">hostname</span><span style="color:#f92672">+</span><span style="color:#e6db74">&#34;:9200&#34;</span>,
  <span style="color:#75715e">// elasticsearch: &#39;http://localhost:9200&#39;,
</span><span style="color:#75715e"></span>  <span style="color:#a6e22e">kibana_index</span><span style="color:#f92672">:</span>     <span style="color:#e6db74">&#34;kibana-int&#34;</span>,
  <span style="color:#a6e22e">modules</span><span style="color:#f92672">:</span>          [<span style="color:#e6db74">&#39;histogram&#39;</span>,<span style="color:#e6db74">&#39;map&#39;</span>,<span style="color:#e6db74">&#39;pie&#39;</span>,<span style="color:#e6db74">&#39;table&#39;</span>,<span style="color:#e6db74">&#39;stringquery&#39;</span>,<span style="color:#e6db74">&#39;sort&#39;</span>,
                    <span style="color:#e6db74">&#39;timepicker&#39;</span>,<span style="color:#e6db74">&#39;text&#39;</span>,<span style="color:#e6db74">&#39;fields&#39;</span>,<span style="color:#e6db74">&#39;hits&#39;</span>,<span style="color:#e6db74">&#39;dashcontrol&#39;</span>,
                    <span style="color:#e6db74">&#39;column&#39;</span>,<span style="color:#e6db74">&#39;derivequeries&#39;</span>,<span style="color:#e6db74">&#39;trends&#39;</span>,<span style="color:#e6db74">&#39;bettermap&#39;</span>],
  }
);
</code></pre></div><p>ポート番号が異なる場合は、1つ目の「elasticsearch:」で指定されている「9200」を環境に合わせて編集するだけになります。
Kibana3とElasticSearchのホストが異なる場合は、1つ目の「elasticsearch:」の行をコメントアウトし、2つ目を有効にしてから環境に合わせたURLに修正して保存すればOKです。</p>
<p>以上で、インストールは完了します。あとは、以下のURLにアクセスするだけです。</p>
<pre><code>http://hogehoge/kibana-javascript/
</code></pre><h2 id="画面構成">画面構成</h2>
<p>アクセスすると次のような画面が表示されます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3&#43;kibana2-es-index.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3&#43;kibana2-es-index.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>初期画面</h4>
      </figcaption>
  </figure>
</div>

<p>左上に赤い帯で、「 Oops! Could not match index pattern to any ElasticSearch indices」とエラーが表示されました。</p>
<p>KibanaはElasticSearchに「logstatsh-年.月.日」という日付ごとのインデックスが存在することが前提となっています。
Kibanaに初めてアクセスした場合、「logstash-当日日付」で始まるインデックスを描画しようとします。
これは、私が前回利用したElasticSearchの環境に古いデータ（試したのが19日、データは10日のみ）しか入っていないために出たエラーです。</p>
<p>日付は「Options」というエラーが出ている付近の「Absolute」というリンクをクリックすると、特定の日付をカレンダーで指定することができるようになります。データは6/10にしか入っていないので、6/10（12時くらいから20時くらいまで）のを指定します。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-selected-calendar.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-selected-calendar.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>日付指定</h4>
      </figcaption>
  </figure>
</div>

<p>選択すると無事データが見えるようになりました。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/Kibana3-sample-include-description.jpeg" />
    </div>
    <a href="/images/entries/20130619/Kibana3-sample-include-description.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>データ描画</h4>
      </figcaption>
  </figure>
</div>

<h3 id="ダッシュボードの構成初期">ダッシュボードの構成（初期）</h3>
<p>Kibana3では、この画面をダッシュボードというようです。
このダッシュボードは初期状態では、以下のパーツが表示されています。（子要素があとで説明するパネル名です）</p>
<ul>
<li>Options：描画対象の日付の指定やダッシュボードの保存などを行うRow
<ul>
<li>timepickerパネル：日付の指定</li>
<li>dashcontrolパネル：ダッシュボードの制御（保存とか）</li>
</ul>
</li>
<li>Query：ログ検索式を入れるところ
<ul>
<li>stringqueryパネル</li>
</ul>
</li>
<li>Graph：ヒストグラムの描画（X軸：時間、Y軸：ログ件数）
<ul>
<li>histogramパネル</li>
</ul>
</li>
<li>Events：検索にヒットしたログデータの描画領域
<ul>
<li>fieldsパネル：表示するフィールドの選択（左側。チェックを入れると右側のログ表示領域のカラムが増える）</li>
<li>tableパネル：ログデータ（右側。左側でチェックが入ったカラムだけが表示される。）</li>
</ul>
</li>
</ul>
<p>あくまで初期表示です。各パーツの設定アイコン（歯車のマーク）をクリックすると色々と設定が可能です。
また、「Events」など名称はクリック可能となっていて、クリックすると、そのパーツが折りたたまれた状態にすることも可能です。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-collaped.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-collaped.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>折りたたんだ状態</h4>
      </figcaption>
  </figure>
</div>

<h3 id="ダッシュボードの設定">ダッシュボードの設定</h3>
<p>ダッシュボードには独自のパネルを簡単に追加することができます。
ダッシュボードの構成はページの一番上にある「Logstash Search」の設定アイコンをクリックすると設定画面が開きます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-dashboad-setting.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-dashboad-setting.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>ダッシュボード設定</h4>
      </figcaption>
  </figure>
</div>

<p>「New row」にタイトル名を適当にいれて「Create Row」するとあたらしくパネルを追加することができるRowが追加されます。「Rows」の「Move」にある矢印でRow自体の表示場所を上下に移動することも可能です。</p>
<h3 id="rowの設定">Rowの設定</h3>
<p>追加した「Hoge」にパネルを追加する場合はHogeの上にある設定アイコンをクリックすると設定画面が開きます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-row-setting.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-row-setting.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>Rowの設定</h4>
      </figcaption>
  </figure>
</div>

<p>ここでKibana3で用意されているパネルの追加ができます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-panel-add.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-panel-add.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>Panel追加ボタン</h4>
      </figcaption>
  </figure>
</div>

<p>パネルを選んでボタンを押せばすぐに表示されます。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-sample-panels.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-sample-panels.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>パネルの羅列</h4>
      </figcaption>
  </figure>
</div>

<p>こんな感じです。とりあえず、ポコポコと追加してみました。</p>
<p>利用できるパネルの種類は以下の様なパターンです。
適当ですが、表にしてみました。</p>
<table>
<tr><th>パネル名</th><th>概要</th></tr>
<tr><td>column</td><td>Rowの中にパネルを配置するコンテナを用意するためのパネル</td></tr>
<tr><td>dashcontrol</td><td>ダッシュボードの保存、保存したダッシュボードの表示などの操作ボタン</td></tr>
<tr><td>text</td><td>markdown形式などで記述が可能な文章を表示できるパネル</td></tr>
<tr><td>stringquery</td><td>検索クエリ入力用パネル</td></tr>
<tr><td>derivequeries</td><td>フィールドと検索式がわかれた形式の検索入力用パネル</td></tr>
<tr><td>timepicker</td><td>ログ表示の期間を指定するパネル</td></tr>
<tr><td>histogram</td><td>ログの件数のヒストグラム表示用パネル</td></tr>
<tr><td>hits</td><td>ヒット件数表示用パネル</td></tr>
<tr><td>pie</td><td>パイチャート表示用パネル</td></tr>
<tr><td>trends</td><td>指定された時間でデータの増減を%表示するパネル</td></tr>
<tr><td>sort</td><td>ソート条件指定用のプルダウン表示用パネル（変更したらtableの内容がソートされる）</td></tr>
<tr><td>table</td><td>ログデータ表示用パネル</td></tr>
<tr><td>fields</td><td>tableパネルに表示するフィールドを選択するための補助パネル</td></tr>
<tr><td>bettermap</td><td>なんか地図が出てきたパネル<br/>GeoJSONデータをゴニョゴニョ（表示かな？）できるみたい</td></tr>
<tr><td>map</td><td>なんか世界地図が出てきたパネル<br/>２文字の国コード（jaとかか？）かU.S.の州コードのデータを元に地図に色をつけるのかな？</td></tr>
</table>
<p>これらのパネルは個々に色々と設定が可能です。他にもdebug、map2など有りそうでしたがまだ使えないみたいです。</p>
<p>適当に触ってて気づいた注意点です。</p>
<ul>
<li>tableは１ダッシュボードで１つだけが良さそう。
<ul>
<li>２つあると、どちらかにしか描画されない。columnに入れるとグルーピングできたりするのかなぁ？</li>
</ul>
</li>
<li>stringquery、timepickerも１ダッシュボードで１つが良さそう。
<ul>
<li>これもtableと似たような理由です。</li>
</ul>
</li>
<li>ダッシュボード保存し忘れて泣きそうになる
<ul>
<li>JSで実装されてて、自分で色々とカスタマイズできるのですが、保存するのを忘れて泣きそうになりましたｗ</li>
<li>カスタマイズしたダッシュボードについては、ローカルに保存する以外にElasticSearchにも保存ができるみたいです。チームで共有することもできそうです。</li>
</ul>
</li>
<li>derivequeriesを表示するとグラフがカラフルに
<ul>
<li>derivequeriesを追加したらグラフが急にカラフルになりました。</li>
<li>どうもderivequeriesのFieldの部分を変更すると、そのフィールドの値を元にグラフを細分化してくれるようです。色の数の上限はderivequeriesのLength属性の数値で制御出来ます。（5だと5個まで色が出る）</li>
<li>histogramのパネルで自分でクエリを記載することも可能です。ただ、derivequeriesのフィールド変更すると書き換わっちゃいます。。。</li>
</ul>
</li>
</ul>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-multi-color-histogram-type.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-multi-color-histogram-type.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>derivequeriesを追加したらカラフルに</h4>
      </figcaption>
  </figure>
</div>

<p>ヒストグラムは色々なパターンのグラフを描画できました。ラインによる描画（histo1）、総数を100%としたパーセンテージでの表示（histo2）、ライン＋点による描画（histo3）などです。</p>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/kibana3-several-histogram.jpeg" />
    </div>
    <a href="/images/entries/20130619/kibana3-several-histogram.jpeg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>ヒストグラムのいくつかのパターン</h4>
      </figcaption>
  </figure>
</div>

<h2 id="感想">感想</h2>
<p>ということで、適当にですが触ってみました。
Kibana2はApacheのアクセスログとかの表示しかできない感じがしましたが、Kibana3だといろいろなデータを描画できそうだなと。
logstash形式のインデックスを用意するのが前提になってるので、時系列データをグラフ描画するのに向いてるんでしょうか。
お手軽にグラフ化できるし、自分でダッシュボードをカスタマイズできるのは素敵です。
ただ、クエリとグラフの関係などはちょっと癖があるかもしれないので、色々と試してみないといけないかもしれないです。
（たとえば、特定のフィールドの値について「A、B、その他」みたいなグラフの描画とかをどうするかとか）</p>
<p>地図の描画は試してみたいかなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Basho Japanに遊びに行きました</title>
      <link>https://blog.johtani.info/blog/2013/06/19/visited-basho/</link>
      <pubDate>Wed, 19 Jun 2013 10:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/19/visited-basho/</guid>
      <description>ちょくちょく書こうと言いながら、前の記事が1週間以上前になってる。。。 昨日は、Basho Japanに遊びに行って来ました。 （Riak触ったこ</description>
      <content:encoded><p>ちょくちょく書こうと言いながら、前の記事が1週間以上前になってる。。。</p>
<p>昨日は、Basho Japanに遊びに行って来ました。
（Riak触ったことないのに。。。Erlangも。。。ゴメンナサイ）</p>
<!-- more -->
<p>RiakにSolrを組み合わせたYokozunaというものの名前を最近耳にしていたので、どんなものなのかなぁと興味がありまして。Solrがどんな使い方をされているのかってのが気になったので、
情報交換したいなぁと思っていたところ、Vの人が調整してくれたので色々と有意義な話ができたかなぁと。
（Yokozunaについての最新のスライドは<a href="https://speakerdeck.com/rzezeski/yokozuna-scaling-solr-with-riak">Berlin Buzzword 2013のものがここに</a>）
Twitter上で見かけたことのある方々と話ができたり面白かったです。（やっぱ英語で会話できたりスラスラと読めるの必要だよなぁと痛感したりもしました。。。）</p>
<p>ということで、遊びに行ったのに美味しいピザやこんなおみやげまでもらってしまいました。
（ピザの写真撮るの忘れてたw）</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130619/riak-goods.jpg" />
    </div>
    <a href="/images/entries/20130619/riak-goods.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>Riak＆Bashoグッズ</h4>
      </figcaption>
  </figure>
</div>

<p>ちなみに、Yokozunaですが、Riakに登録したデータを裏で起動しているSolにデータを流しこんでくれるものになります。
Solrの機能としては分散検索（Distributed Search）と呼ばれる仕組みを利用しているようです。
YokozunaのI/Fとしては、Solrのインデックスの分散構成は隠してくれていて、かつ、Solr（っぽい？）リクエストを投げれば裏の分散構成に問い合わせた結果をSolrのレスポンスの形で返してくれます。
KVSに全文検索の機能がついてくるお得感が満載な気がしますw。</p>
<p>Riak自身のデータの取り扱いがどんなものかをまだちゃんと理解していないので（ゴメンナサイ。<a href="http://littleriakbook.com">Little Riak Book</a>は開いてるんですが読んでなくて。。。）またおじゃましてもう少し情報交換したいかなぁとｗ。</p>
<p>Cloudera Searchといい、Yokozunaといい、Solrを利用したものが少しずつ増えてきて嬉しい限りです。
Solrの作りがしっかりしている？活発？、だから取り込む形が多いんですかねぇ。
Solr本を書いてから数年たちますが、やっと検索のニーズが出てきたのかもしれないなぁと思ってみたり。
（流れのつながりはあまりないですが）ElasticSearchも少しずつ人気が出てきてるし、日本語の本とかのニーズあったりするかなぁ？</p>
</content:encoded>
    </item>
    
    <item>
      <title>新しいsolr.xmlとCore探索ロジック</title>
      <link>https://blog.johtani.info/blog/2013/06/11/new-solr-xml/</link>
      <pubDate>Tue, 11 Jun 2013 19:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/11/new-solr-xml/</guid>
      <description>Lucene/Solr 4.3.1のRCのVoteが始まっていますが、そのMLできになったコトがあり、ちょっと調べたので メモを残しておきます。 マルチコアの設定ファ</description>
      <content:encoded><p>Lucene/Solr 4.3.1のRCのVoteが始まっていますが、そのMLできになったコトがあり、ちょっと調べたので
メモを残しておきます。</p>
<p>マルチコアの設定ファイルであるsolr.xmlの記述方法と、コアの探索ロジックが4.4（実装的には4.3から入っている）から変更されるようです。4.x系の最新版である、branch_4xのexampleディレクトリにあるsolr.xmlも新しい記述に変更されていました。</p>
<!-- more -->
<h3 id="参考url">参考URL</h3>
<ul>
<li><a href="http://wiki.apache.org/solr/Core%20Discovery%20%284.4%20and%20beyond%29">新しいCore探索（4.4以降）</a></li>
<li><a href="http://wiki.apache.org/solr/Solr.xml%204.4%20and%20beyond">新しいsolr.xml（4.4以降）</a></li>
<li><a href="http://wiki.apache.org/solr/Solr.xml%20%28supported%20through%204.x%29">4.3までのsolr.xml</a></li>
</ul>
<p>ちなみに、最後のold styleと呼ばれる4.3までの記述方法はつぎの5.0ではDeprecatedになるようです。（5.0がいつ出るのかはわからないですが。）</p>
<h2 id="core探索ロジック">Core探索ロジック</h2>
<p>4.4から、$SOLR_HOMEディレクトリ以下の探索ロジックは次のようになるようです。
以下では、「新スタイル」（4.4以降の書式）、「旧スタイル」（4.3以下の書式）として記述します。</p>
<ol>
<li>solr.xmlファイルの存在チェック
<ol>
<li>solr.xmlが存在しない場合→旧スタイルとして処理→3へ（旧スタイル）</li>
<li>solr.xmlが存在し<code>&lt;cores&gt;</code>タグが存在しない場合→2へ（新スタイル）</li>
<li>solr.xmlが存在し<code>&lt;cores&gt;</code>タグが存在する場合→3へ（旧スタイル）</li>
</ol>
</li>
<li>新スタイルのロジック
<ol>
<li>SOLR_HOMEディレクトリに存在するディレクトリについて以下の処理を繰り返す</li>
<li>SOLR_HOME/ディレクトリ/core.propertiesファイルが存在する→後続処理へ。存在しなければ終了</li>
<li>SOLR_HOME/ディレクトリ/conf/solrconfig.xmlを読み込み、コアを起動</li>
</ol>
</li>
<li>旧スタイルのロジック
<ol>
<li>これまで同様、solr.xmlの<code>&lt;core&gt;</code>タグの記載内容を元にコアを起動（instanceDir以下のconf/solrconfig.xmlを使って）</li>
<li>solr.xmlが存在しない場合はSOLR_HOME/collection1/conf/solrconfig.xmlが存在するものとしてコアを起動</li>
</ol>
</li>
</ol>
<p>このようなロジックになります。</p>
<p>ちなみに、以下の場合はエラーとなりSolrは起動しますがログや管理画面にエラーである表示がされます。</p>
<ul>
<li>2.3でsolrconfig.xmlが見つけられなかった場合</li>
<li>3.1で<code>&lt;core&gt;</code>タグが存在しなかった場合（この場合、ログにはエラーが出ません）</li>
</ul>
<p>propertiesに記述できる内容やsolr.xmlの記述内容については、Wikiを見てもらうということで。。。
CoreAdminHandlerでコアを生成したりした場合に、新スタイルの設定がどのように出力されるのかといった点が気になりますが、また今度にでも。</p>
</content:encoded>
    </item>
    
    <item>
      <title>apache-loggen &#43; fluentd &#43; elasticsearch &#43; kibana = ログ検索デモ</title>
      <link>https://blog.johtani.info/blog/2013/06/10/fluent-es-kibana/</link>
      <pubDate>Mon, 10 Jun 2013 23:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/10/fluent-es-kibana/</guid>
      <description>もう何番煎じだ？ってくらい書かれてますが、コリもせず書いてみました。 Elasticsearch＋Kibanaの環境を作って、タムタムさんのロ</description>
      <content:encoded><p>もう何番煎じだ？ってくらい書かれてますが、コリもせず書いてみました。
Elasticsearch＋Kibanaの環境を作って、タムタムさんのログ生成ツールからApacheのダミーログを流しこんで入れてみました。</p>
<!-- more -->
<h2 id="参考url">参考URL</h2>
<ul>
<li><a href="http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html">memorycraftさんのブログ</a></li>
<li><a href="http://kibana.org/">Kibana</a></li>
<li><a href="http://wwwelasticsearch.org">Elasticsearch</a></li>
<li><a href="http://fluentd.org">fluentd</a></li>
<li><a href="http://mt.orz.at/archives/2012/11/apacherubygems.html">apache-loggen</a></li>
</ul>
<h2 id="インストールと起動">インストールと起動</h2>
<p>今回はCentOSへのインストールです。
基本的にはmemorycraftさんのブログの流れのままです。</p>
<h3 id="elasticserchのインストールと起動">elasticserchのインストールと起動</h3>
<p>ダウンロードして、起動するだけ。
お試しということで、-fオプションにてコンソールにログ出力。</p>
<pre><code>curl -OL https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.1.tar.gz
tar zxvf elasticsearch-0.90.1.tar.gz
cd elasticsearch-0.90.1
./bin/elasticsearch -f
</code></pre><h3 id="kibanaのインストールと起動">Kibanaのインストールと起動</h3>
<p><code>git clone</code>してbundleインストール</p>
<pre><code>git clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git
cd Kibana
bundle install
ruby kibana.rb
</code></pre><p>これで、Kibana＋ESのインストール＋起動が完了。
下地が完了。</p>
<h3 id="td-agentのインストールと起動">td-agentのインストールと起動</h3>
<p>ログの流し込みはlogstashなのですが、fluentdのelasticsearchプラグインにて流しこむこともできます。
td.repoとしてtd-agentのリポジトリを登録してから以下を実行します。</p>
<pre><code>yum install td-agent -y
/usr/lib64/fluent/ruby/bin/fluent-gem install fluent-plugin-elasticsearch
vim　/etc/td-agent/td-agent.conf
/etc/init.d/td-agent start
</code></pre><p>これで、td-agentがインストール出来ました。
次は設定です。</p>
<pre><code>&lt;source&gt;
  type tail
  format apache
  path /var/log/httpd/dummy_access_log
  tag dummy.apache.access
&lt;/source&gt;

&lt;match *.apache.*&gt;
  index_name adminpack
  type_name apache
  type elasticsearch
  include_tag_key true
  tag_key @log_name
  host localhost
  port 9200
  logstash_format true
  flush_interval 10s
&lt;/match&gt;
</code></pre><p>以上が設定です。td-agentはtd-agentというユーザで起動されるので、/var/log/httpdディレクトリにアクセスできるかだけ確認が必要です。<br>
いくつかの設定値について気になったので調べました。</p>
<ul>
<li>index_name：adminpackとなってるが、elasticsearchではlogstash-xxxとなってる。
<ul>
<li>これは、logstash_formatがtrueの場合は、利用されないので、指定しなくてもいい。</li>
</ul>
</li>
<li>type_name：Elasticsearchのタイプ名
*　これはlogstash_formatを指定しても有効。ただし、Kibana側で画面からのtype指定は不可能。KibanaConfig.rbにて指定することは可能。</li>
<li>logstash_format：Kibana用にlogstashフォーマットで出力するオプション
<ul>
<li>この指定があるときは、index名が「logstash-YYYY.mm.dd」となる</li>
<li>record（ログ）に@timestampとして時刻が追加される。</li>
</ul>
</li>
<li>tag_key：include_tag_keyがtrueと指定されているため、record（ログ）にtag_keyで指定した文字列をキー、値としてtagの値（上記例だとdummy.apache.access）が付与されて登録される。</li>
</ul>
<h2 id="apache-loggenのインストールと起動">apache-loggenのインストールと起動</h2>
<p>タムタムさんが作成されたApacheのログのダミーを生成するツールです。<br>
gem化されてるので、インストールは非常に簡単です。</p>
<pre><code>gem install apache-loggen
</code></pre><p>で、ログを出力します。出力先は先程設定したdummy_access_logです。</p>
<pre><code>apache-loggen --rate=10 --progress /var/log/httpd/dummy_access_log
</code></pre><p>秒間10アクセスログを出力してくれます。
これで、Kibanaでログが見れるようになりました。
なんて簡単なんでしょう。。。
簡単なログの検索ができてしまいました。
他の形式のログがどうなるのかとかは、また時間があれば。。。</p>
<h2 id="感想とか">感想とか</h2>
<p>非常に簡単でした。素敵です。いくつかこうなるのかな？というのを試してみたのでメモを。</p>
<p>いくつか疑問点です。</p>
<ul>
<li>溜まったログの削除は手動？
<ul>
<li>おそらく。日付ごとにindexが出来上がっているので、削除は楽そう。「logstash-年月日」なので。</li>
</ul>
</li>
<li>認証とかかけれるの？
<ul>
<li>ログ検索は内部でするだろうから、まぁ、なくていいのかな。ログインすらないし。</li>
</ul>
</li>
<li>複数行のログとかってどーすんだろう？（JavaのExceptionとかが混ざるやつ）</li>
</ul>
<p>本格的に触るようになれば調べるかなぁ。。。</p>
<p>あと、ログが増えてきた時にどういった分割構成ができるだろう？って思って考えてみたのが以下になります。</p>
<h3 id="構成パターン">構成パターン</h3>
<p>ログを複数扱う場合は次のようなパターンがありそうかと。</p>
<h4 id="タグfluentdのタグで識別">タグ（fluentdのタグ）で識別</h4>
<p>「@log_name」という名前＝fluentdのタグにてログを識別することで、異なるログを検索することができそうです。
タグであれば、プラグインによってはログ出力時に制御も可能だと思うので、td-agentの設定を変更したりすることもなく対応が可能かと。
ただ、ログの種別ごとにKibanaのプロセスを別にして起動したいといった用途には向いてなさそうです。</p>
<h4 id="type_nameによる識別">type_nameによる識別</h4>
<p>ElasticSearchの機能であるtypeを利用したログの識別パターンです。<br>
fluent-plugin-elasticsearchの設定で<code>type_name</code>を指定しました。
ここを別の名前にすることで、識別することも可能です。</p>
<p><strike>ただし、この場合はKibanaの画面から指定して検索することができません。</strike>
<strong><em>→コメント頂きました。検索条件に「_type:タイプ名」と検索することでtypeを利用した検索が可能です。</em></strong><br>
タグ（@log_name）でも識別できるようにするなどの工夫が必要です。
その代わり、タグ識別ではできなかったKibanaのプロセスを別にして起動することは可能になります。<br>
KibanaConfig.rbのTypeに値を設定することで、起動したKibanaが対象とするログを絞り込むことが可能です。
こうすることで例えば、apache用のKibanaとtomcat用のKibanaは別プロセスにして、ElasticSearchのクラスタは1つという構成も可能になります。</p>
<h4 id="elasticsearchサーバを別立て">ElasticSearchサーバを別立て</h4>
<p>ElasticSearchサーバをそもそも別のプロセスor別のサーバで起動し、Kibanaも別々にすればログの識別も可能です。
可能ですが、色々と管理するものが増えてめんどくさそうですね。。。</p>
<h4 id="インデックス名変更">インデックス名変更</h4>
<p>最後は、fluent-plugin-elasticsearchの設定で「logstash_format」をfalseにすれば、好きなindex_nameを付与できるので、
ログ種別ごとに名前を変更することで識別できます。<br>
ただ、logstash形式でないインデックス名の場合、日付ローテーションができなかったり、Kibana内部で検索時に日付で検索対象を絞り込んで検索することで高速化するといった処理など、使えない機能が多々出てきてしまうのであまりおすすめじゃないかと。。。</p>
<p>ということで、流行りものは触っておこうということで、さわってブログ書いてみました。<br>
開発中に立てておいて、各サーバのログを流しこんでおくなどにも利用できるかもしれないです。
アラート通知などの機能が出てくるともっと便利かもしれないです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>AWS Summit Tokyo 2013に行って来ました。(Day2) #awssummit</title>
      <link>https://blog.johtani.info/blog/2013/06/06/aws-summit-tokyo-day2/</link>
      <pubDate>Thu, 06 Jun 2013 23:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/06/aws-summit-tokyo-day2/</guid>
      <description>昨日に引き続き、AWS Summit Tokyo 2013に行ってきました。 @ryu_kobayashiさんからパシリを仰せつかるなどあり、忙しかったのですが楽しか</description>
      <content:encoded><p>昨日に引き続き、AWS Summit Tokyo 2013に行ってきました。</p>
<p>@ryu_kobayashiさんからパシリを仰せつかるなどあり、忙しかったのですが楽しかったです。</p>
<p>今日もすごい人出で、昼時の展示ブースは人だかりで、TDの方たち忙しそうでした。
後半はちょっとつかれたなぁ。
1日目よりも空調の温度が上がってたので快適でした。
飲物忘れたので、ちょっと喉がやられ気味でしたが。</p>
<p>AWSはやっぱり勢いがあるんだなぁってのを実感しました。JAWS-UGまで残った感想です。
あとは、セルフハンズオンに参加するんだったかなぁというのもちょっとあります。あんまり触ったことないんで。</p>
<p>以下はいつものメモです。</p>
<!-- more -->
<h2 id="amazon-redshiftが切り開くクラウドデータウェアハウス">Amazon Redshiftが切り開くクラウド・データウェアハウス</h2>
<h4 id="自己紹介と流れとニュース">自己紹介と流れとニュース</h4>
<ul>
<li>6/5からTokyoリージョンでも利用可能に！</li>
</ul>
<h4 id="redshiftどんなもの">Redshiftどんなもの？</h4>
<ul>
<li>Redshift＝クラウド型データウェアハウス</li>
<li>オンプレの課題
<ul>
<li>初期投資、運用管理、費用対効果</li>
</ul>
</li>
<li>EMRと同じで、分析処理向けのサービス</li>
<li>簡単な利用例
<ul>
<li>各種データストア→S3→Redshift</li>
<li>各種データストア→EMR→Redshift</li>
<li>Data Pipelineでデータの流れの処理がかける。</li>
</ul>
</li>
<li>データロードはパラレルに実行可能</li>
<li>バックアップは管理コンソールからボタンででも可能。</li>
<li>クラスタのリサイズも管理コンソールからできるよ。</li>
</ul>
<h4 id="nriでの評価">NRIでの評価</h4>
<ul>
<li>
<p>2012年末の限定公開してすぐに先行評価に参加</p>
</li>
<li>
<p>Redshiftの性能は？</p>
<ul>
<li>500億件からの検索処理（1週間分のデータを抜き出して処理するSQL）
<ul>
<li>8XLノード2ノードで43秒、4ノードで27.8秒、8ノードで19秒</li>
<li>データロドも線形に性能が上がる</li>
</ul>
</li>
<li>1.2億件の検索処理は4ノードより8ノードのほうが性能劣化</li>
<li>EMRとRedshiftの比較。1.5TB、500億件でのJOIN＋集計処理でRedshiftのほうが早かった。</li>
</ul>
</li>
<li>
<p>Redshiftのチューニングポイント</p>
<ul>
<li>インデックスが存在得ず、Distribution Keyでノードに分散</li>
<li>Sort Keyも重要。データロード時間はSort Keyをつけると遅くなる</li>
</ul>
</li>
<li>
<p>ニガテなデータ形式もあるよ。EMRとかの組み合わせにすると安く済むこともあるよ</p>
</li>
<li>
<p>簡単につくれるので、気をつけましょう。セキュリティとか。統制されないものが乱立してくるんで。</p>
<ul>
<li>そこでNRIですよ！（宣伝）</li>
</ul>
</li>
</ul>
<p>出てきた性能に関するものだけど、「ケース１」「ケース２」とかってなってるので、その部分の詳細が書かれたレポートが公開されないとどんなのがニガテなのかとかわからなかった。</p>
<h4 id="事例紹介とか">事例紹介とか</h4>
<ul>
<li>オンプレのデータをどうやってRedshiftに持ってくの？
<ul>
<li>インフォテリアのASTERIA WARP
<ul>
<li>GUIツールでデータの変換とかして、S3に持って行って、Redshiftと連携できるよと。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="無料セミナーの紹介">無料セミナーの紹介</h4>
<ul>
<li>6/21、8/2にRedshiftの無料セミナーやりますよ。</li>
</ul>
<h2 id="インターネット上のユーザーの行動の可視化を実現したawsによるビックデータ解析基盤">インターネット上のユーザーの行動の可視化を実現したAWSによるビックデータ解析基盤</h2>
<h4 id="c-finderのサービス紹介">C-Finderのサービス紹介</h4>
<p>元オプトの方。C-Finderの紹介。
10万ユーザの行動履歴の可視化。</p>
<p>サイトに来たけど、行動せずに他サイトに移動したとか、サイトに来る前の状態ってのがわからない。これをC-Finderで可視化。</p>
<ul>
<li>例１：自動車業界
<ul>
<li>ライバル企業の動向を自社と比較したい。</li>
<li>Audi、BMW、ベンツで可視化。</li>
</ul>
</li>
<li>例２：化粧品業界
<ul>
<li>Web上での化粧品のトレンドが見たい。</li>
<li>@cosmeサイトのPVとかから。</li>
</ul>
</li>
</ul>
<p>ユーザって会員登録とかしてる人を追っかけてるってことなのかなぁ？
よくわかってない。。。</p>
<ul>
<li>レポーティングサービスしてたけど、納品まで１ヶ月かかってた。</li>
<li>リアルタイムにみたいという要望が多かったのでASPをTISと開発</li>
</ul>
<h4 id="aspサービスの開発したよ">ASPサービスの開発したよ</h4>
<ul>
<li>AWSで。</li>
</ul>
<p>可視化というタイトルだったんだけど、期待してた可視化の話ではなかった。。。</p>
<h2 id="ネット選挙クラウドオバマ大統領選挙の事例データ解析からネット募金まで">ネット選挙クラウド　～オバマ大統領選挙の事例：データ解析からネット募金まで～</h2>
<ul>
<li>ボランティアで構成ってすごいな。</li>
<li>本番まで１年</li>
<li>みんな髭の人だｗ</li>
<li>GoogleとかFacebookとかの人がボランティアで開発に参加。</li>
<li>予算は抑えながら、スケールアップダウンがすぐ出来てとか。</li>
<li>秒間10万回のI/OのDB</li>
<li>SQSとEC2のSoftware queueの比較。Softwareキュー選ぶと、さらにドレがいいのかってのを選ばないとい。</li>
<li>AWSにあるサービスなら、造らなくてもいいのではとか。
<ul>
<li>CloudSearchはつかわなかったのかなぁ？</li>
</ul>
</li>
<li>tsunami</li>
<li>S3に静的ページをおいて、最後のとりでにしたり。</li>
<li>コードとか、ノウハウってどこまで公開されてるんだろ？</li>
</ul>
<p>同時通訳もあったんですが、英語を聞いてみようと聞いてましたが、まだまだダメですねぇ。</p>
<h2 id="パネルディスカッションウェブテクノロジーをエンタープライズで活かすには">パネルディスカッション「ウェブテクノロジーをエンタープライズで活かすには？」</h2>
<h4 id="趣旨">趣旨</h4>
<ul>
<li>B2B、B2C間でのトレンドのやり取りが加速されてるよねと。</li>
<li>ICTはもはやコモディティ？</li>
</ul>
<h4 id="td太田さん">TD太田さん</h4>
<ul>
<li>Consumerization Of IT</li>
<li>色々なものがSaaSになってきてる</li>
<li>TD：Consumerization of Data Infrastructure
<ul>
<li>オンプレ＜AWS＜TD</li>
</ul>
</li>
</ul>
<h4 id="三井物産黒田さん">三井物産黒田さん</h4>
<ul>
<li>三井クラウド（プライベート＆パブリック）</li>
<li>ユビキタス</li>
<li>これまで使ってきたものがクラウドで動くのかとかが観点</li>
</ul>
<h4 id="電通平川さん">電通平川さん</h4>
<ul>
<li>マーケティングダッシュボードはsalesforceでは厳しかった？</li>
<li>ビジネスに注力したいので、柔軟で、質実剛健なICT基盤がほしい。</li>
</ul>
<h4 id="ディスカッション">ディスカッション</h4>
<ul>
<li>
<p>新しい技術に対するスタンスは？</p>
<ul>
<li>いかにダメな部分を潰していくか（太田さん）</li>
<li>R&amp;Dを予算化して評価して取り組んでいく。評価でダメならつぎ。（黒田さん）</li>
<li>消費者の方たちがネットですごい勢いで活動してる。その人達へのアプローチが重要。（平川さん）</li>
</ul>
</li>
<li>
<p>コンシューマ系技術の懸念事項は？</p>
<ul>
<li>セキュリティ面。レベルがある。自分の所だけじゃなく、クラウドベンダーと二人三脚が必要。丸投げはNG（黒田さん）</li>
<li>セキュリティ面の攻撃を受けたつぎの手をどう打つかが重要。堅牢性上げると、ユーザビリティが下がる。（平川さん）</li>
<li>法規制的に出せないとかもある。持ちきれないデータもあるよね。（太田さん）</li>
</ul>
</li>
<li>
<p>何を見分けてどうやって取り組むか？</p>
<ul>
<li>とりあえず、やってみる。そこで見える課題に対応していく。見極めるのは難しい（平川さん）</li>
</ul>
</li>
<li>
<p>ということは、すぐに出来る環境ってのは当たり前になってますよね。</p>
</li>
</ul>
<p>尖った技術＝セキュリティ面が弱い？みたいな印象があるのかなぁ？</p>
<h2 id="jaws-ug">JAWS-UG</h2>
<h4 id="awsアップデート">AWSアップデート</h4>
<ul>
<li>AmazonのCTO登場</li>
</ul>
<h4 id="game-day">Game Day</h4>
<ul>
<li>Game Day。土曜日にやるよ。</li>
<li>障害を発生させて、それを復旧させる。</li>
<li>最もひどい壊し方をしたらかちｗ</li>
</ul>
<p>最後はかるくボッチでした。。。
AWSあんまり使ってないからなぁ。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera Searchメモ（妄想版）</title>
      <link>https://blog.johtani.info/blog/2013/06/06/cloudera-search-memo2/</link>
      <pubDate>Thu, 06 Jun 2013 12:26:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/06/cloudera-search-memo2/</guid>
      <description>ざっとインストールガイドとかCloudera Searchのソース眺めて、テキトーにメモを書いてみました。 （ユーザガイドはまだ読んでないです。</description>
      <content:encoded><p>ざっとインストールガイドとかCloudera Searchのソース眺めて、テキトーにメモを書いてみました。
（ユーザガイドはまだ読んでないです。）</p>
<!-- more -->
<h2 id="ざっくりメモ">ざっくりメモ</h2>
<ul>
<li>ストリーム処理でインデックス作るときはFlume経由でSolrに
<ul>
<li>SinkとEventの両方が用意されてる？（Flumeを良く知らないので、違いがわからない）</li>
<li>FluemeからはリモートのSolrに対してインデックス登録するクラスがある。SolrServerDocumentLoaderがソレだと思う。</li>
</ul>
</li>
<li>バッチ処理でインデックス作るときはMapReduceIndexerToolsってのを使ってSolrに
<ul>
<li><a href="https://issues.apache.org/jira/browse/SOLR-1301">SOLR-1301</a>がベースになっている。色々と改良されてるようだけど、コアとなってる処理はSOLR-1301。</li>
<li>GoLiveってクラスの処理の中で、現在動作してるSolrに配布したバッチで作成されたIndexをマージする処理が書いてある。</li>
<li>HDFSへ出力されたインデックスはリモートのSolrからアクセスするとオーバヘッドとかどーなるのかなぁ？</li>
</ul>
</li>
<li>検索処理自体はHueでもできるけど、基本的にSolrCloud任せ</li>
<li>インデキシングの処理のフローについてはCloudera Mrophlinesで定義</li>
</ul>
<p>ということで、
2つの流れがありそう。</p>
<ul>
<li>HDFS→Flume→Solr</li>
<li>HDFS→MapReduce→Solr</li>
</ul>
<p>で、まだ、わかってないですが、構成要素として</p>
<ul>
<li>Hadoop（HDFS）：データソース</li>
<li>Hadoop（MapReduce）：データ変換処理、バッチインデキシング</li>
<li>Zookeeper：SolrCloudのクラスタ管理</li>
<li>Solr：インデキシング、検索エンジン</li>
<li>Flume：データをストリーミングでSolrへ</li>
<li>Coudera Morphlines：HDFSからSolrまでのETLデータ処理を定義実行する環境</li>
</ul>
<p>って感じでしょうか。
SolrCloudのクラスタとHadoopのクラスタが同一マシン上なのか、別なのかとか。組み合わせがどんなものができるのかがまだわかってないです。
ユーザガイド読んでみたらなにか出てくるかなぁ。</p>
<p>ちなみに、Cloudera SearchのgithubリポジトリにあるソースはCloudera Morphlinesのコードがメインで、SolrのHDFS対応版のソースがあるわけでは無かったです。</p>
<ul>
<li>SolrのHdfsDirectoryってのがClouderaのリポジトリにあるSolrには追加されていて、これが、HDFSのインデックスを読み込んだりする処理が出来る仕組みっぽい。</li>
<li>一応、SolrCloud以外（分散検索）も考慮された形になってるっぽい。</li>
</ul>
<p>ってとこでしょうか。</p>
<h2 id="感想">感想</h2>
<p>読んでて思ったんですが、Cloudera Searchの肝はじつは、検索じゃなくて、Morphlinesにあるんじゃないかなぁと。今はSolrが出力先ですが、
その他のデータ変換処理とかが増えてくると、処理の流れがMorphlinesで定義できてデータ変換処理が簡便になりそうな気が。</p>
<h2 id="その他に気になる観点">その他に気になる観点</h2>
<ul>
<li>CDH経由でSolrCloudのクラスタの管理するのかな？</li>
<li>SolrCloud用のクラスタとCDHのクラスタって同一マシンに載るの？別マシンにもできるの？
<ul>
<li>併存したらIOがキツそうだけど</li>
</ul>
</li>
<li>Hueで検索アプリとか組めるの？（そもそもHueがわかってないんだけど。。。）</li>
</ul>
<p>ま、とりあえず、こんなとこで。
つぎは余力があれば、ユーザガイドかなぁ。
英語力。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>AWS Summit Tokyo 2013に行って来ました。(Day1) #awssummit</title>
      <link>https://blog.johtani.info/blog/2013/06/05/aws-summit-tokyo-day1/</link>
      <pubDate>Wed, 05 Jun 2013 18:30:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/05/aws-summit-tokyo-day1/</guid>
      <description>AWS Summit Tokyo 2013に行って来ました。 @cocoatomoさんに行き掛けに出会ったので、アンデルセンの講演を一緒に聞いてました。 TDブースの@ry</description>
      <content:encoded><p><a href="http://www.awssummittokyo.com/timetable.html">AWS Summit Tokyo 2013</a>に行って来ました。</p>
<p>@cocoatomoさんに行き掛けに出会ったので、アンデルセンの講演を一緒に聞いてました。
TDブースの@ryu_kobayashiさんに挨拶に伺ったりも。</p>
<p>すごい人で、セッション間の入れ替え時には会場前のスペースが大変なことになってました。もう少し余裕のある建物のほうが良かったのかもしれないですねぇ。</p>
<p>いつものように以下はメモです。</p>
<!--  more -->
<h2 id="road-to-aws-アンデルセンサービス">Road to AWS アンデルセンサービス</h2>
<p>とてもおいしいランチボックスでした。</p>
<p>
<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130605/lunch1.jpg" />
    </div>
    <a href="/images/entries/20130605/lunch1.jpg" itemprop="contentUrl"></a>
  </figure>
</div>



<div class="box" style="max-width:300">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130605/lunch2.jpg" />
    </div>
    <a href="/images/entries/20130605/lunch2.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>広島（ここ重要！）に本社のあるパン屋さん。</p>
<ul>
<li>2003年に汎用機からOpenシステムに
<ul>
<li>内製ではなく外注に。複数のSIerにお願いしてるとデータセンタに</li>
<li>マスタ管理と生産管理はオンプレ</li>
<li>2004年に稼動時のネットワーク整備。店舗はISDNで、設定ミスとかを防ぐためにIIJのSMFサービスに</li>
</ul>
</li>
<li>PCはレンタル契約にしたので入れ替えを強制できる→レガシーシステムの対応が必要なくなる
<ul>
<li>ネットワーク広帯域化→基本システムにデータセンターに。</li>
<li>2009年にリース満了などで、OSの変更とかミドルの変更という移行SI費用だけで結構な値段が。ものによっては会社もない。</li>
<li>データセンターの月額手数料にして、機器変更などの費用を平準化。仮想化など。</li>
<li>けど、サイジングにどうしても安全係数をかけてしまうことに。結構値段がかかる</li>
</ul>
</li>
<li>そこで、今後の方向性
<ol>
<li>SIerどっぷりのOLTP受発注基幹システムはSIerのホスティングで</li>
<li>それ以外のシステムは外に出したい</li>
</ol>
</li>
<li>バッチの計算を早くすることができるとノーチラスさんから提案された
<ul>
<li>バッチを外部化するか、性能あげるか。IOネックなどもありスペック上げるとコストになるから、外部化したい</li>
<li>Hadoopを組むけど、オンプレじゃなくてAWSにした。ノウハウがないので、ノーチラスさんにお任せした。</li>
<li>AWSのEMRを利用。4時間かかっていたものが40分で。</li>
<li>EDIのサーバを6ヶ月でAWSに移行</li>
</ul>
</li>
<li>S3絡みで2度の問題が。性能劣化があった。</li>
<li>パッケージ開発環境がAWSにあるなら、AWSでも動くでしょ？（NTTイントラマートとか）</li>
<li>SIerどっぷりの部分もAWSに持っていく？</li>
</ul>
<h3 id="アンデルセンサービスの部長さんからのawsへの要望">アンデルセンサービスの部長さんからのAWSへの要望</h3>
<ul>
<li>メールサーバなどの基本サビスとか、コントロールパネルを充実して欲しい。</li>
</ul>
<h2 id="クラウド技術を活用したリアルタイム広告logicadの入札配信ログ解析">クラウド技術を活用したリアルタイム広告&quot;Logicad&quot;の入札・配信・ログ解析</h2>
<h4 id="web広告の歴史">Web広告の歴史</h4>
<ol>
<li>最初は広告主がWebサイト単位で契約</li>
<li>アドネットワークによる仲介</li>
<li>聞きそびれた。。。DSP？</li>
<li>アドネッエクスチェンジ</li>
<li>SSP(Supply Side Platform)</li>
</ol>
<h4 id="リアルタイムビッティングrtb">リアルタイムビッティング（RTB）</h4>
<p>RTPの仕組みを説明。
AWSとオンプレでシステムを構築してる。</p>
<ul>
<li>SSP事業者との取引はオンプレ</li>
<li>配信はAWS</li>
<li>オンプレとAWSはAWS Direct Conectで接続
<ul>
<li>S3とHadoopの接続が安くなったり速くなったりするらしい。</li>
</ul>
</li>
</ul>
<h5 id="オンプレミス側">オンプレミス側</h5>
<ul>
<li>Bidリクエスト：秒間数万件。。。</li>
<li>KVSのユーザ数は３億件。。。</li>
<li>AEROSPIKEというSSD向けのKVSを利用してる</li>
</ul>
<h5 id="aws側">AWS側</h5>
<ul>
<li>ログはS3に。溜まったデータは定期的にEMRで解析。DynamoDBに入れてレポート作成</li>
<li>RabbitMQを使ってる。</li>
<li>ELBで外部からのリクエストはバランシング</li>
</ul>
<h5 id="データセンタ間通信">データセンタ間通信</h5>
<ul>
<li>遅延を複数コネクションを貼る方法で回避？</li>
<li>非同期で、複数のMsgとAckをやり取りする。RabbitMQがこれに相当する機能を持ってる
<ul>
<li>QueueとConsumerで多重送信が可能。</li>
<li>配信結果をオンプレ側に送るのにRabbitMQを使ってるのかな？</li>
</ul>
</li>
</ul>
<h2 id="ハイブリッド構成を支えるawsテクノロジー">ハイブリッド構成を支えるAWSテクノロジー</h2>
<p>聞くつもりだったんだけど、すごい人だったので、諦めて充電してた。</p>
<h2 id="awsクラウドで構築するワールドクラスの分散クラウドアーキテクチャ">AWSクラウドで構築する、ワールドクラスの分散クラウドアーキテクチャ</h2>
<p>エマージングソリューション部の部長のshot6さん。</p>
<h3 id="マルチazアベイラビリティゾーンモデル">マルチAZ（アベイラビリティゾーン）モデル</h3>
<p>AWS固有のコンセプト。
ELBやDynamoDBやRDS、S3とか。</p>
<p>マルチリージョンとは違うよと。マルチリージョンは結構難しいので。
マルチリージョンがどんなに大変かをこれから説明。</p>
<h4 id="マルチリージョンアーキテクチャこれは別物">マルチリージョンアーキテクチャ（これは別物）</h4>
<ul>
<li>複数のリージョンを利用して作る。</li>
<li>AWSのビルディングブロックではカバーしない範囲をカバーしないとダメとか。</li>
<li>リージョン間の通信は基本的に非同期</li>
<li>ディザスタリカバリ
<ul>
<li>コストバランス見てから決めるよね。</li>
</ul>
</li>
<li>CAP定理とかもいろいろと出てくるよね。</li>
<li>合意プロトコル。正確性、生存性（時間が制限されても大丈夫）、理論的にパフォーマンス出る？</li>
<li>非同期レプリケーションになる（1トランザクション当たりのオーバヘッドが大きいから）けど、復旧の難しさがある。
<ul>
<li>GlusterFSとかでやってるとこもある。</li>
<li>NetflixはCassandraをクオラム＋Geo-Replicationでマルチリージョンを構成してる。</li>
</ul>
</li>
<li>マルチリージョンでのデータ一貫性は維持がむずい</li>
</ul>
<h5 id="注意点">注意点</h5>
<ul>
<li>必要になるまで分散させない。
<ul>
<li>まずは、マルチAZを考えましょう</li>
</ul>
</li>
<li>物理制限を考慮する
<ul>
<li>同期型だとタイムラグあるし。</li>
</ul>
</li>
<li>複合障害の伝搬をどう抑えるか。
<ul>
<li>NetflixはHYSTRIXというのがいて遮断できるようにしている</li>
</ul>
</li>
<li>自動化が重要。ロールバックとかロールアウトとか。</li>
</ul>
<h5 id="テストをどーするの">テストをどーするの？</h5>
<p>本番環境でアクティブ/アクティブ構成でのテスト。
ちゃんとフェイルオーバするかなどを実環境でやってる。DB落としたり。。。
障害は避けられないので、受け入れて日常に取り込むべきだよねと。</p>
<ul>
<li>Amazon.comではGameDay</li>
<li>NetflixはChaos Monkey（OSS）</li>
<li>Obama for America</li>
</ul>
<p>リカバリーオリエンテッドコンピューティングパターン</p>
<ul>
<li>ボーアバグ：再現可能なソフトウェバグ</li>
<li>ハイゼンバグ：通常ではありえないパターンで発生するバグで、調査が大変</li>
</ul>
<h4 id="ということでマルチazがいいよ">ということで、マルチAZがいいよ</h4>
<ul>
<li>同期式レプリケーションを前提にできる。</li>
<li>アプリ開発者がアプリにフォーカスできる。分散系の難しいところはAWSが隠蔽してくれるから。</li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera Searchのモジュールたち</title>
      <link>https://blog.johtani.info/blog/2013/06/05/cloudera-search-modules/</link>
      <pubDate>Wed, 05 Jun 2013 15:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/05/cloudera-search-modules/</guid>
      <description>Cloudera Searchは次のようなモジュールから構成されています。 これはCloudera Searchのモジュールで、さらにこれらがSolrとかを使っ</description>
      <content:encoded><p>Cloudera Searchは次のようなモジュールから構成されています。
これはCloudera Searchのモジュールで、さらにこれらがSolrとかを使ってるみたいですね。pom.xmlを見たら何を使ってるかがわかるかな。</p>
<ul>
<li>cdk-morphlines</li>
<li>search-contrib</li>
<li>search-core</li>
<li>search-flume</li>
<li>search-mr</li>
<li>search-solrcell</li>
</ul>
<p>てきとーに、README.mdみながらメモを残してみました。ソースとかはまだ読んでないです。
ざっと眺めたけど、インデキシング処理の話がメインで、検索側がどうやって動くかってのがわからなかったなぁ。
<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/Search/latest/PDF/Cloudera-Search-User-Guide.pdf">ユーザガイド（注：PDF）</a>ってのがあるから、これを読んでみるか。。。</p>
<p>各モジュールについては、以下。</p>
<!-- more -->
<h2 id="cdk-morphlinescloudera-morphlines">cdk-morphlines（Cloudera Morphlines）</h2>
<p>Cloudera Morphlinesという名前みたい。
検インデキシングアプリの構築、変更をラクにするためのフレームワーク。
ETLの処理チェインを簡単にCloudera Searchにデータを入れる設定（Extract/Transform/Load処理）がかけると。
バッチ処理、Near Real Timeのために使えるみたい。検索結果をさらに入れるとかもできるんかなぁ。？</p>
<p>Unixパイプラインのの進化版みたいなもので、一般的なレコードに対するStream処理から、Flueme、MapReduce、Pig、Hie、SqoopのようなHadoopコンポーネントも使えるみたい。</p>
<p>Hadoop ETLアプリケーションのプロトタイピングにも使えて、リアルタイムで複雑なStreamやイベント処理やログファイル解析とかに使えるの？</p>
<p>設定ファイルのフォーマットは<a href="https://github.com/typesafehub/config/blob/master/HOCON.md">HOCONフォーマット</a>。AkkaやPlayで使われてる。</p>
<h3 id="cdk-morphlines-core">cdk-morphlines-core</h3>
<p>Cloudera Morphlinesのコンパイラ、実行環境、コマンドのライブラリを含んでる。
ログファイル解析やsingle-lineレコード、multi-lineレコード、CSVファイル、正規表現パターンマッチ、フィールドごとの比較とか条件分岐とか、文字列変換とか色々なコマンドを含んでる。</p>
<h3 id="cdk-morphlines-avro">cdk-morphlines-avro</h3>
<p>Avroファイルやオブジェクトの抽出、変換、読み込み処理コマンド</p>
<h3 id="cdk-morphlines-tika">cdk-morphlines-tika</h3>
<p>バイナリデータからMIMEタイプを検出して、解凍するコマンド。Tikaに依存</p>
<h4 id="雑感">雑感</h4>
<p>Cloudera Searchへのデータの流し込みを設定ファイルに記述して実行するとデータの変換処理とかが記述できるって感じかな？
Morphlinesのコマンドとして独自処理や使えそうな処理を作ることで、いろんな処理ができるって感じかなぁ。</p>
<h2 id="search-core">search-core</h2>
<p>Solrに対するMorphlineコマンドの上位モジュール</p>
<h3 id="search-solrcell">search-solrcell</h3>
<p>Tikaパーサを使ったSolrCellを使うためのMorphlineコマンド。
HTML、XML、PDF、Wordなど、Tikaがサポートしてるものがサポート対象。</p>
<h3 id="search-flume">search-flume</h3>
<p>Flueme Morphline Solr Sink。
Apache Flumeのイベントから検索ドキュメントを抽出、変換し、SolrにNearRealTimeで読み込むためのコマンド</p>
<h3 id="search-mr">search-mr</h3>
<p>HDFSに保存されたファイルに含まれる大量データをMapReduceで処理してHDFS上の検索インデックスに焼きこむモジュール。</p>
<p><code>MapReduceIndexerTool</code>は入力ファイルの集合からSolrのインデックスシャードの集合を作るためのmorphlineのタスクで、MapReduceのバッチジョブドライバー。
HDFSにインデックスを書き込む。
動作してるSolrサーバに対して出力されたデータをマージするのもサポートしてる。</p>
<p>とりあえず、Near Real Time検索するにはFlueme使って、バッチ処理でインデックス焼くのはMapReduceIndexerToolみたいだなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera Searchってのが出たらしい（とりあえず、雑感？）</title>
      <link>https://blog.johtani.info/blog/2013/06/05/what-is-cloudera-search/</link>
      <pubDate>Wed, 05 Jun 2013 15:05:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/05/what-is-cloudera-search/</guid>
      <description>AWS Summitに来ていたのですが、TLでは、Cloudera Searchが賑わってました。 ということで、軽くどんなものか読んだり調べたりした</description>
      <content:encoded><p>AWS Summitに来ていたのですが、TLでは、Cloudera Searchが賑わってました。
ということで、軽くどんなものか読んだり調べたりしたメモを残しとこうかと。
英語力はあやしいので、おかしいとこがあったらツッコミを。</p>
<!-- more -->
<h2 id="cloudera-searchとは">Cloudera Searchとは？</h2>
<p>CDH4.3に対応したCDHユーザ向けの検索システム（beta版）なのかな？
CDHに統合された検索フレームワークなのかな？</p>
<p>基本はLucene/Solr 4.3でHadoopのペタバイトデータを検索することができるようになるみたいです。</p>
<h2 id="どんな仕組み">どんな仕組み？</h2>
<p>次のものを利用しているようです。（GithubのREADMEから。）</p>
<h4 id="使ってるもの">使ってるもの</h4>
<ul>
<li>Apache Solr(4.3.0＋α？)
<ul>
<li>Apache Lucene（Solrつかってるからね）</li>
<li>Apache SolrCloud（うーん、Solrに含まれるのに別に出してるのなんで？）</li>
</ul>
</li>
<li>Apache Flume</li>
<li>Apache Hadoop MapReduce &amp; HDFS</li>
<li>Apache Tika
<ul>
<li>SolrCellとしてSolrにも組み込まれてる、いろんな文書（WordとかHTMLなどなど）からメタデータと本文データとかを取り出せるライブラリラッパー。実際にはさらにpdfboxとかを使って各文書からのデータを取り出してる。</li>
</ul>
</li>
</ul>
<h4 id="何ができるの">何ができるの？</h4>
<p>HBaseやHDFSの用にZookeeperを使ってインデックスのシャーディングや高可用性ができる。（SolrCloudがZookeeperを使ってるからね。）
MapReduceのジョブの出力から自動的にSolrのインデックスにデータをマージできるらしい。
Cloudera Managerを使って、デプロイ、設定モニタリングなどが可能。</p>
<p>Flumeのフィードをつかって、ストリーミングしてインデックスを作れる。FluemeがデータをSolrに流しこむのかな？
将来的にはHiveやHBaseのテーブルをインデックスすることも可能になるらしい。Impalaクエリの結果もフィードできるのか？</p>
<p><a href="http://incubator.apache.org/blur/how_it_works.html">Apache Blur</a>ってキーワードも出てきた。HDFSのデータからLuceneのインデックス作るのかな？
NGDataのチームがSolr/HBaseの統合とかしてるみたい。</p>
<h3 id="参考url">参考URL</h3>
<ul>
<li><a href="http://blog.cloudera.com/blog/2013/06/cloudera-search-the-newest-hadoop-framework-for-cdh-users-and-developers/">Cloudera社のブログ</a></li>
<li><a href="http://cloudera.com/content/cloudera-content/cloudera-docs/Search/latest/PDF/Cloudera-Search-Frequently-Asked-Questions.pdf">Cloudera SearchのFAQ（注：PDF）</a></li>
<li><a href="https://github.com/cloudera/search">Githubのリポジトリ</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>fluent-plugin-zoomdata作りました＋悩み事とか</title>
      <link>https://blog.johtani.info/blog/2013/06/03/fluent-plugin-zoomdata-0-0-1/</link>
      <pubDate>Mon, 03 Jun 2013 13:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/03/fluent-plugin-zoomdata-0-0-1/</guid>
      <description>憧れ？のfluentを使ってみました。 こちらの記事で紹介したZoomdataを最近触っているのですが、お試しにfluentdでデータ流し込む</description>
      <content:encoded><p>憧れ？のfluentを使ってみました。
<a href="http://atl.recruit-tech.co.jp/blog/668/">こちらの記事</a>で紹介した<a href="http://zoomdata.com">Zoomdata</a>を最近触っているのですが、お試しにfluentdでデータ流し込むプラグインを作ってみようかなぁと。（今は、Javaでの接続も書いていて、主にそっちを使っています。）
ということで、作ってみました。<a href="https://github.com/johtani/fluent-plugin-zoomdata">fluent-plugin-zoomdata</a>。</p>
<p>基本的にはtagomoris先生の<a href="https://github.com/tagomoris/fluent-plugin-growthforecast">fluent-plugin-growthforecast</a>を参考（パクリ？）にさせてもらいました。
作っている最中もここわからんってツイートに反応していただき、大変助かりました。
私はベースがJavaの人間なので、手探り状態でRubyを書いています。そこおかしいんじゃないの？とかあればコメントもらえると嬉しいです。</p>
<!-- more -->
<h2 id="zoomdataのapi">ZoomdataのAPI</h2>
<p>ZoomdataのAPIはこんなかんじで、JSONをHTTPSでPOSTするものになります。</p>
<script src="https://gist.github.com/wlindner/4587444.js"></script>
<p>指定する必要があるものは、以下の項目です。</p>
<ul>
<li>source：Zoomdataのデータ保存先（Zoomdataでのデータを保存する単位。）</li>
<li>user：Zoomdataのユーザ名</li>
<li>password：Zoomdataのユーザのパスワード</li>
<li>JSONデータ：Zoomdataでグラフ化するデータ</li>
</ul>
<p>sourceはデータ保存先の名称で、この単位でZoomdataはデータを保存、描画します。fluentdのタグをこれにするとわかりやすいかなぁ？と考えていったん、実装してみています。</p>
<p>で、作成したプログラムを使いつつ、Zoomdataの検証をしたかったので、つぎのような簡単なプログラムを作って動かしてみました。</p>
<h2 id="サンプルプログラムの構成">サンプルプログラムの構成</h2>
<p>基本的にJavaの人なので、クライアントはJavaで書いてます。
CLIプログラムで適当なJSONを作って、fluent-loggerを使って、fluentdに投げ込みます。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" style="max-width:500">
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/sample_pg_zoomdata.jpg" />
    </div>
    <a href="/images/entries/sample_pg_zoomdata.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

<p>fluentdにfluent-plugin-zoomdataを設定して、localのZoomdataサーバに対してHTTPSでJSONをPostする仕組みです。(初keynote)<br>
利用しているライブラリなどのバージョンは次の通り</p>
<ul>
<li>td-agent.x86_64：1.1.12-0</li>
<li>fluent-logger：0.2.8</li>
<li>Zoomdata：1.0.3</li>
</ul>
<h2 id="バグ">バグ？</h2>
<p>で、Zoomdataにいろんなデータを流し込んでみたのですが、つぎのようなエラーが出て、エラーが出力されたあとはZoomdataにデータが流れ込まなくなってしまいました。</p>
<pre><code>2013-06-03 14:42:33 +0900 [warn]: emit transaction failed  error=&quot;SSL_connect returned=1 errno=0 state=SSLv3 read finished A: sslv3 alert handshake failure&quot;
  2013-06-03 14:42:33 +0900 [warn]: /usr/lib64/fluent/ruby/lib/ruby/1.9.1/net/http.rb:799:in `connect'
  2013-06-03 14:42:33 +0900 [warn]: /usr/lib64/fluent/ruby/lib/ruby/1.9.1/net/http.rb:799:in `block in connect'
  2013-06-03 14:42:33 +0900 [warn]: /usr/lib64/fluent/ruby/lib/ruby/1.9.1/timeout.rb:54:in `timeout'

</code></pre><p><a href="https://gist.github.com/johtani/5696295">全ログはこちら</a>。</p>
<p>まだきちんと問題を調査しないでブログを書いています、すみません。</p>
<h3 id="現象">現象</h3>
<p>ログが発生した時の症状です。</p>
<ul>
<li>クライアントプログラムは送信が続いており、エラーは出ない</li>
<li>td-agent.logに先ほどのエラーが出力</li>
<li>別途<code>type file</code>にて出力しているログも停止</li>
</ul>
<h3 id="想像">想像</h3>
<p>とりあえず、ログを見た想像、所感です。</p>
<ul>
<li>問題の箇所はfluent-plugin-zoomdataからZoomdataサーバへのデータ送信部分</li>
<li>emit処理内部で、HTTPSでデータをPOSTする処理でエラーが起きて</li>
<li>リトライ処理とか書いてないので、emitがコケて、その後データが送信されなくなる</li>
<li>emitで例外をつかみそこねてるのがあるから止まってる？</li>
</ul>
<p>とまぁ、ちゃんと仕組みを理解しないでRubyとか書くからこうなるんですねぇ。
あとでちゃんと調べて考えて、改良してブログ書きます。</p>
<h2 id="悩んでいる点今後手を入れたい点">悩んでいる点、今後手を入れたい点</h2>
<p>上記バグとは別に作りの点でいくつか悩んでる点も書いてみます。</p>
<h3 id="bufferedoutputにしてみたい">BufferedOutputにしてみたい</h3>
<p>fluentdのバッファリングを使って、Zoomdataが落ちていても使えるようにしたいと思っているのでBufferedOutputで書くのがいいのかなぁとか。
ちょうど<a href="http://www.slideshare.net/harukayon/fluentd-22317236">いいスライド</a>があったので、読みながらまずは中身を理解してみよう。</p>
<h3 id="zoomdataのsourceuserなどの扱い">Zoomdataのsource、userなどの扱い</h3>
<p>基本的には設定ファイルで切り替えるのが妥当かなぁと思っています。<br>
ただ、Zoomdataのsourceやuserが増えるたびにfluentdの設定を書き換えて再起動するのかなぁと。userはしょうが無いにしても、sourceは設定じゃない所で切り替えたいなぁと。</p>
<p>で、切り替えるのにつぎの案があるかなぁと。</p>
<ol>
<li>タグで指定（今実装してるもの）</li>
<li>メッセージにメタ情報とボディ構造を設ける</li>
<li>設定をどんどん増やす（やりたくない）</li>
</ol>
<p>1と3はまぁ、いいかと。2.のパターンはどうなのかなぁと。
毎回のメッセージでヘッダ部分が送信されるのはなんだか無駄だなぁというのが否めないので悩ましいところです。1、2の両方対応できるように作るのもありか。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
	<span style="color:#f92672">&#34;header&#34;</span>: {
		<span style="color:#f92672">&#34;source&#34;</span>: <span style="color:#e6db74">&#34;source_name&#34;</span>, 
		<span style="color:#f92672">&#34;user&#34;</span>: <span style="color:#e6db74">&#34;userid&#34;</span>,
		<span style="color:#f92672">&#34;password&#34;</span>: <span style="color:#e6db74">&#34;userid&#34;</span>,
	},
	<span style="color:#f92672">&#34;body&#34;</span>: {
		<span style="color:#f92672">&#34;label&#34;</span>: <span style="color:#e6db74">&#34;label1&#34;</span>,
		<span style="color:#f92672">&#34;count&#34;</span>: <span style="color:#ae81ff">1</span>
	}
}
</code></pre></div><p>ということで、fluent触って遊ぶの楽しいですね。Rubyの勉強にもなりそうだし。
ちょっとずつ頑張ってみようかなぁと。
まぁ、まだ私以外にニーズは無さそうなプラグインですが。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Octopressを試し中(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/06/03/octopress%E3%82%92%E8%A9%A6%E3%81%97%E4%B8%AD/</link>
      <pubDate>Mon, 03 Jun 2013 11:53:58 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/03/octopress%E3%82%92%E8%A9%A6%E3%81%97%E4%B8%AD/</guid>
      <description>最近、Markdownで文章を書くのに慣れてきました。 ということで、ブログをMarkdownで書けないかなぁと思い、TLの人たちのブログを見</description>
      <content:encoded><p>最近、Markdownで文章を書くのに慣れてきました。
ということで、ブログをMarkdownで書けないかなぁと思い、TLの人たちのブログを見たりしていると
「<a href="http://octopress.org">Octopress</a>」というキーワードがあるじゃないですか。
ということで、Github Pages作って、Octopressを使ってブログを書いてみました。
<a href="http://blog.johtani.info">こちら</a>です。</p>
<p>独自ドメインは昨年から使っているものがあったので、ついでに独自ドメインの設定もしてみました。
ケチなので、.comなどではなく、.infoですが。。。</p>
<p>ということで、徐々にOctopressに移行していこうと考えているところです。
こちらのブログも当面は残しておく予定です。（有料会員は解約して、広告が出る形になると思いますが。。。）</p>
<p>Octopressでこんなの便利だよとかアレば教えてもらえると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Octopress始めました</title>
      <link>https://blog.johtani.info/blog/2013/06/01/first-octopress/</link>
      <pubDate>Sat, 01 Jun 2013 23:57:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/01/first-octopress/</guid>
      <description>昨年から、ブログをどうしようかって話をしていて、そのままになっていたのですが、Octopress＋Github Pagesというのがあるらしい</description>
      <content:encoded><p>昨年から、ブログをどうしようかって話をしていて、そのままになっていたのですが、Octopress＋Github Pagesというのがあるらしいと聞きつけて（Twitterで見かけたのかなぁ？）ちょっとやってみました。</p>
<p>最近は、ドキュメントをMarkdownで記述しようとして癖をつけているのもあり、
ちょうどいい練習になるかなぁと。</p>
<p>ということで、まずは手始めのエントリーでした。
いくつか書きたいこともあるので、調べながら書いていこうかと。
まだ、イメージできてないこと</p>
<ul>
<li>画像をどうやって貼るの？</li>
<li>About meみたいなのも貼り付けたい</li>
<li>bitbucketやTwitterのリンクとかも</li>
<li>アフィリエイトも貼る（とりあえずSolr本を）</li>
<li>検索とかは？</li>
<li>デザインは？</li>
</ul>
<p>などなど。ちょっとずつ調べていこうかなぁと。
あ、調べたことも書いてくのもありか。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen4.3.0をリリースしました。（Lucene/Solr4.3.0以上での利用が可能です）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/05/06/lucene-gosen4-3-0%E3%82%92%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9Flucene-solr4-3-0%E4%BB%A5%E4%B8%8A%E3%81%A7%E3%81%AE%E5%88%A9%E7%94%A8%E3%81%8C%E5%8F%AF%E8%83%BD%E3%81%A7%E3%81%99/</link>
      <pubDate>Mon, 06 May 2013 23:25:24 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/05/06/lucene-gosen4-3-0%E3%82%92%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9Flucene-solr4-3-0%E4%BB%A5%E4%B8%8A%E3%81%A7%E3%81%AE%E5%88%A9%E7%94%A8%E3%81%8C%E5%8F%AF%E8%83%BD%E3%81%A7%E3%81%99/</guid>
      <description>Lucene/Solr 4.3.0がリリースされた（LuceneのChanges、SolrのChanges）ので、lucene-gosen 4.3.0をリリースしま</description>
      <content:encoded><p>Lucene/Solr 4.3.0がリリースされた（<a href="http://lucene.apache.org/core/4_3_0/changes/Changes.html">LuceneのChanges</a>、<a href="http://lucene.apache.org/solr/4_3_0/changes/Changes.html">SolrのChanges</a>）ので、lucene-gosen 4.3.0をリリースしました。（<a href="http://code.google.com/p/lucene-gosen/downloads/list?PHPSESSID=ab5edaac2154a82b90c0d9865454c0c9">ダウンロードはこちら</a>）
なお、lucene-gosen 4.3.0ですが、<span style="color:#FF0000">Lucene/Solr 4.2.1以下</span>のバージョンのLucene/Solrでは利用<span style="color:#FF0000">できません。</span>
注意してください。
また、lucene-gosen 4.2.1もLucene/Solr 4.3.0では動作しませんので注意が必要です。</p>
<p>現時点（2013/05/06）では、lucene-gosen 4.3.0はLucene/Solr 4.3.0でのみ利用できます。
これは、<a href="http://johtani.jugem.jp/?eid=129">先日のエントリ</a>にも書きましたが、LuceneにてAPIの変更が行われたためとなります。
いくつかのクラスおよびメソッドが廃止されたため、下位互換が保てない変更が入っているためです。</p>
<p>独自のTokenizerやTokenizerFactory、TokenFilterFactory、CharFilterFactoryを作成されている方は、Lucene/Solrのバージョンアップを行う際は注意が必要です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ポモドーロ回してます。（ポモドーロテクニック入門読みました）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/05/03/%E3%83%9D%E3%83%A2%E3%83%89%E3%83%BC%E3%83%AD%E5%9B%9E%E3%81%97%E3%81%A6%E3%81%BE%E3%81%99%E3%83%9D%E3%83%A2%E3%83%89%E3%83%BC%E3%83%AD%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF%E5%85%A5%E9%96%80%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Fri, 03 May 2013 23:42:19 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/05/03/%E3%83%9D%E3%83%A2%E3%83%89%E3%83%BC%E3%83%AD%E5%9B%9E%E3%81%97%E3%81%A6%E3%81%BE%E3%81%99%E3%83%9D%E3%83%A2%E3%83%89%E3%83%BC%E3%83%AD%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF%E5%85%A5%E9%96%80%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>アジャイルな時間管理術 ポモドーロテクニック入門 ポモドーロテクニック入門という本を読みました。 きっかけは、Twitter上で何度か「ポモドーロ</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4048689525/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4048689525&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4048689525/?tag=johtani-22">
      アジャイルな時間管理術 ポモドーロテクニック入門
      </a>
    </p>
  </div>
</div>
ポモドーロテクニック入門という本を読みました。
きっかけは、Twitter上で何度か「ポモドーロ」という単語を何度か見ていたためです。
最初は、なんだろう？というのが発端です。
「ポモドーロ＝トマト」なのですが、実際にはトマト型のキッチンタイマーが元になっているらしいです。
このタイマーを使った時間（タスク）管理術がポモドーロ・テクニックです。</p>
<p>私は、ここ数年、複数の仕事がパラで走ることが時々ありました。
このような場合に、日によって異なる複数のタスクが存在します。
このとき、異なるタスクにスイッチするのに結構な時間を取られます。。。
また、急な割り込みが入った時も同様に、以前のタスクに戻るのになにしてたっけ？となることが多々あります。
普通に自己管理ができている方なら問題ないのでしょうが、私は結構ニガテでした。
そのようなときに、Twitter上で「ポモドーロ」という単語を見かけて、軽くググってみたところ、
タスク管理、時間管理によさそうな本だったので、その点を矯正するのも兼ねて、読んだ次第です。
また、タスクに集中できるという利点もあるそうです。</p>
<p>本については、少し読みにくいところがありました。
ポモドーロテクニックとはどんなものかという全体像や単語に関する説明がないままに、話が進んでいくので。。。
1度読み終わったあとに実践しながらパラパラめくっているような状況です。
実際には、個々人のやり方などを考慮しながら、改善していくべきなのもあり、型を説明してないのかもしれないですが、もうすこし概観がわかる感じのほうが良かったです。</p>
<p>で、4月初旬くらいから実践してみています。
効果が実際にあるかというと、まだわからないです。</p>
<p>私がポモドーロテクニックに利用しているのはキッチンタイマーではなく、<a href="http://code.google.com/p/pomodairo/">pomodairo</a>というAdobeAIR上で動くアプリになります。
ほかにも<a href="http://www.pomodoroapp.com">PomodoroApp</a>というのもあるのですが、Free版だと登録できるタスクの上限があったので、pomodairoを選びました。（今見たら、3.0にバージョンが上がって、Limitがなくなってるかも）
AIRだと、WinでもMacでも動作するのというも決定した要因です。</p>
<p>まだ、1ヶ月経ってませんが、私が実践してきて良かった点、できてない点、うまくいってない点はつぎのような感じです。</p>
<h3 id="良かった点">良かった点</h3>
<hr>
<ul>
<li>目の前のタスクに集中できる。（25分スパンなので、Twitterを意識的に見なくできる。。。）</li>
<li>適度な休憩が挟める。25分に5分の休憩が入るので、適度な没頭になる（没頭し過ぎない）</li>
<li>自宅で作業するときにかなり有効。（5分の休憩時にTwitterやFB以外に漫画をパラパラ読んだりもできるので）</li>
</ul>
<p>ということで、自宅で作業するときには結構いいです。
自宅ですと、pomodairoを使っていてタイマーの音を気兼ねなく出せるので、きちんとポモドーロが回せます。</p>
<h3 id="できてない点">できてない点</h3>
<hr>
<ul>
<li>アクティビティ在庫管理。個人的にJIRAを使っていて、そこで管理しようと思っているのですが、うまくできてないです。pomodairoのアプリにもタスクを登録しているのもあり2重登録などを手間に思ってしまって。。。</li>
<li>レコーディングと今日のTodo作成。</li>
<li>インタラプトの記録</li>
</ul>
<p>アクティビティ在庫管理ができてないのは、レコーディングがきちんと出来てないためでしょう。。。
二重管理になっている＋pomodairoで統計情報が出るが、当日分の統計情報がレコーディングできてないというのが痛いです。
また、このレコーディングが出来てないので、効果が出ているかがわからないという問題かと。。。
きりが悪かったりして、どうしても、仕事時間ギリギリまでタスクをこなしてしまい、レコーディング＋アクティビティ在庫の管理の時間が取れていません。
ここは意識してちゃんとやらないと意味がないよなぁと。今後の大きな課題です。</p>
<h3 id="うまくいってない点">うまくいってない点</h3>
<hr>
<p>できてない（やろうとしてできてない）点とは別に、どうもしっくり来ていないのがつぎのような点です。</p>
<ul>
<li>プログラミングしていると、25分のタイマーで区切りがすごく悪い時がおおい</li>
<li>自宅以外でのタイマー音が出せない</li>
<li>自宅以外での休憩の取り方</li>
</ul>
<p>プログラミングをやっていて、乗ってきたタイミングでタイマーが鳴ってしまったり、
ちょっと頭のなかで整理していたあとの今まさに、頭のなかにある処理の流れをコードに落としている途中でタイマーがなってしまったり。
このような状況だと、休憩に入れなくて、ずるずるとコーディングを続けてしまうということが多々あります。
メモ（ソース上のコメントや手元）を残して休憩すればいいのでしょうが、どうしても今までの癖もありズルズルとやってしまい、すごく時間が経ってることが何度もあります。
ポモドーロテクニック的にはやはりNGなんでしょうが、なかなか治らない＋治したくない気もしています。
また、自宅以外の場合、基本的には自社ではなく客先に出ていることが多いのでどうしても音を出すことができません。
これもまた、切り替えができない要因になっています。
タイマーだけ携帯のアプリを使用しするという手もあるのでしょうが、この場合さらにレコーディングが出来ない状況に陥りそうで。。。
また、スマホだと電池が持たないのも問題点です。（Twitterを見るのに利用してるから電池が持たないという話もあるのですが。。。）
レコーディングに関しては、手描きのメモを使うのがいいのかなぁと。本では＋や◎などの印を付けるだけにしておけば良いとありますが、アプリのタイマーだと自動でそれができるので、悩みどころです。
最後の休憩の取り方も、ネットやTwitterを見るのもありなのですが、画面から離れる休憩を取りたいなぁと思うところもあり。。。
職場だと技術書やWEB+DBのような雑誌はあるのですが、休憩にはあまり向いていないなぁと。</p>
<p>つらつらと書いて来ましたが、本を読んで、1ヶ月実践してきた（できてないとこも多いが）現状をメモしておきます。
こうやってるよ、こうしたら良かったよ、こうしてみれば？などありましたら、コメントいただけると助かります。</p>
<p>来週以降はとりあえず、JIRAできちんとアクティビティ在庫管理をしながら、1日の結果をレコーディングしていくのを意識していこうと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Solr4.3.0のChangesを訳してみた。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/25/solr4-3-0%E3%81%AEchanges%E3%82%92%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</link>
      <pubDate>Thu, 25 Apr 2013 11:14:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/25/solr4-3-0%E3%81%AEchanges%E3%82%92%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</guid>
      <description>まだ、Vote公開されていない、Solr 4.3（2013/04/25 11:00現在）ですが、 ひさびさに訳してみた。詳細まで追っていないので、</description>
      <content:encoded><p><strong><span style="color:#FF0000">まだ、Vote公開されていない、Solr 4.3（2013/04/25 11:00現在）ですが</span>、</strong>
ひさびさに訳してみた。詳細まで追っていないので、誤訳があるかもしれないですが。
おかしいとこあったらツッコミを。</p>
<hr>
<p>○Solr 4.3.0のChanges
　・Upgrading from Solr 4.2.0
　　1.schema REST APIのcopyFields、dynamicFieldsの出力パスをCamelCaseに。他も同様。（<a href="http://issues.apache.org/jira/browse/SOLR-4623">SOLR-4623</a>）
　　2.Slf4j/logging jarをSolrのwarに含めないことに。すべてのlogging jarはexample/lib/extに。（<a href="http://issues.apache.org/jira/browse/SOLR-3706">SOLR-3706</a>、<a href="http://issues.apache.org/jira/browse/SOLR-4651">SOLR-4651</a>）
　　3.SolrCloudでハードコードされてたhostContextとhostPortをdeprecatedに。Solr5.0で削除する。（<a href="http://issues.apache.org/jira/browse/SOLR-4622">SOLR-4622</a>）</p>
<p>　・New Features
　　1.<a href="http://issues.apache.org/jira/browse/SOLR-4648">SOLR-4648</a>　PreAnalyzedUpdateProcessorFactoryでPreAnalyzedFieldの機能をほかのフィールドタイプでも使えるようにした。詳しくはJavadocとexampleを見て。
　　2.<a href="http://issues.apache.org/jira/browse/SOLR-4623">SOLR-4623</a>　REST APIで現在のschemaのエレメントをすべて読めるように。REST APIの返却の形式として、XMLとJSONとschema.xmlの形式を追加REST APIのパッケージを変更。
　　　クラス名も変更しschemaにフォーカスした機能も除去。今後のschema以外のREST APIのために。
　　　copyFieldsとdynamicFieldsの出力パスをすべてlowercaseのものからCamelCaseに変更。他のREST APIも同様。
　　3.<a href="http://issues.apache.org/jira/browse/SOLR-4658">SOLR-4658</a>　REST APIリクエストでschemaを変更できるようにするために、「managed schema」を導入。solrconfig.xmlに「&lt;schemaFactory class=&quot;ManagedSchemaFactory&rdquo; mutable=&quot;true&rdquo;/&gt;」を追加。REST APIリクエストでスキーマ変更が可能にするために。
　　4.<a href="http://issues.apache.org/jira/browse/SOLR-4656">SOLR-4656</a>　2つのハイライトパラメータ（hl.maxMultiValuedToMatch、hl.maxMultiValuedToExamine）を追加。
　　　hl.maxMultiValuedToMatchは指定された数のsnippetが見つかったらそれ以降の探索を停止する設定。multiValuedフィールドがどんなに離れてても探索する。
　　　hl.maxMultiValuedToExamineは指定された数のmultiValuedのエントリ数を調査したら探索を停止する設定。
　　　両方を指定した場合、最初のlimitに達したら停止する。ドキュメント全体をハイライトするためにコピーされるのを削減する。これらの最適化はmultiValuedフィールドに大量のエントリが存在する時に効く。。。
　　5.<a href="http://issues.apache.org/jira/browse/SOLR-4675">SOLR-4675</a>　PostingsSolrHighlighterでper-field/クエリ次のパラメータ指定のサポート
　　6.<a href="http://issues.apache.org/jira/browse/SOLR-3755">SOLR-3755</a>　既存のshardを動的にsplitしてshardを追加するための新コレクションAPI（shard splitting）
　　7.<a href="http://issues.apache.org/jira/browse/SOLR-4530">SOLR-4530</a>　DIH：TikaのIdentityHtmlMapperを使う設定の提供
　　8.<a href="http://issues.apache.org/jira/browse/SOLR-4662">SOLR-4662</a>　solr.xmlにあるSolrCoreの定義よりもディレクトリ構造で見つける。また、solr.xmlのフォーマットを変えて、solrconfig.xmlに近くする。Solrのこのバージョンは旧スタイルの例で提供するが、新しいスタイルも試すことができる。Solr 4.4では、新しいスタイルで提供し、Solr 5.0では旧スタイルは廃止する予定。
　　　<a href="http://issues.apache.org/jira/browse/SOLR-4347">SOLR-4347</a>　Adminハンドラで新しく生成されたコアがsolr.xmlに永続化される
　　　<a href="http://issues.apache.org/jira/browse/SOLR-1905">SOLR-1905</a>　Adminリクエストハンドラで生成されたコアもsolr.xmlに永続化される。また、solr.solr.datadirのようなプロパティの用にsolr.xmlに永続化される問題のfix。
　　9.<a href="http://issues.apache.org/jira/browse/SOLR-4717">SOLR-4717</a>/SOLR-1351　SimpleFacetで同じフィールドに異なるファセットを適用出来るlocalParamsを追加
　　10.<a href="http://issues.apache.org/jira/browse/SOLR-4671">SOLR-4671</a>　CSVResponseWriterのpseudoフィールドのサポート
　　11.<a href="http://issues.apache.org/jira/browse/SOLR-4358">SOLR-4358</a>　HttpSolrServerでuseMultiPartPostでstream名を送信できる
　・Bug Fixes
　　1.<a href="http://issues.apache.org/jira/browse/SOLR-4543">SOLR-4543</a>：solr.xml/solr.propertiesでshardHandlerFactoryの設定が動作しない
　　2.<a href="http://issues.apache.org/jira/browse/SOLR-4634">SOLR-4634</a>：Java 8&quot;Nashorn&quot;JavaScript実装の動作に関するscripting engineのテストのfix
　　3.<a href="http://issues.apache.org/jira/browse/SOLR-4636">SOLR-4636</a>：SolrIndexSearcherをオープンする時に何かの理由でreaderがオープンできない時に、ディレクトリがリリースされない
　　4.<a href="http://issues.apache.org/jira/browse/SOLR-4405">SOLR-4405</a>：Admin UIのadmin-extraファイルでcore-menuが表示されない
　　5.<a href="http://issues.apache.org/jira/browse/SOLR-3956">SOLR-3956</a>：group.facet=trueでfacet.limitがマイナスの時の動作
　　6.<a href="http://issues.apache.org/jira/browse/SOLR-4650">SOLR-4650</a>：copyFieldでダイナミックフィールドや暗黙的なフィールドがsourceでマッチしない。4.2で入ったバグ
　　7.<a href="http://issues.apache.org/jira/browse/SOLR-4641">SOLR-4641</a>：Schemaで、illegalなフィールドパラメータで例外が発生するようにする。
　　8.<a href="http://issues.apache.org/jira/browse/SOLR-3758">SOLR-3758</a>：SpellCheckComponentが分散groupingで動作しない。
　　9.<a href="http://issues.apache.org/jira/browse/SOLR-4652">SOLR-4652</a>：solr.xmlプラグインのresource loaderで共有ライブラリの挙動がおかしい
　　10.<a href="http://issues.apache.org/jira/browse/SOLR-4664">SOLR-4664</a>：ZkStateReaderがaliasを更新しても見えない
　　11.<a href="http://issues.apache.org/jira/browse/SOLR-4682">SOLR-4682</a>：CoreAdminRequest.mergeIndexが複数コアやindexDirが複数の場合にマージできない
　　12.<a href="http://issues.apache.org/jira/browse/SOLR-4581">SOLR-4581</a>：Solr4.2で数値フィールドのファセットでマイナスの値があるとソートがおかしい
　　13.<a href="http://issues.apache.org/jira/browse/SOLR-4699">SOLR-4699</a>：Admin Handlerでデータディレクトリの場所がファイルシステムだと思い込んでる。（RAMの場合もある）
　　14.<a href="http://issues.apache.org/jira/browse/SOLR-4695">SOLR-4695</a>：non-cloudセットアップでもコア管理のSPLITが使えるように
　　15.<a href="http://issues.apache.org/jira/browse/SOLR-4680">SOLR-4680</a>：exampleのspellcheck設定のqueryAnalyzerFieldTypeの修正
　　16.<a href="http://issues.apache.org/jira/browse/SOLR-4702">SOLR-4702</a>：exampleの/browseの「Did you mean?」のサジェストをFix
　　17.<a href="http://issues.apache.org/jira/browse/SOLR-4710">SOLR-4710</a>：Zookeeperから全ノードをアップせずにコレクションを削除できないのを修正
　　18.<a href="http://issues.apache.org/jira/browse/SOLR-4487">SOLR-4487</a>：HttpSolrServerからのSolrExceptionがリモートのサーバから戻るHTTPステータスコードを含んでない
　　19.<a href="http://issues.apache.org/jira/browse/SOLR-4661">SOLR-4661</a>：Admin UIのレプリケーションで現在のレプリカ可能なマスタのバージョンを正確に表示
　　20.<a href="http://issues.apache.org/jira/browse/SOLR-4716">SOLR-4716</a>,<a href="http://issues.apache.org/jira/browse/SOLR-4584">SOLR-4584</a>：SolrCloudリクエストプロキシがTomcatなどJetty出ないコンテナで動作していない
　　21.<a href="http://issues.apache.org/jira/browse/SOLR-4746">SOLR-4746</a>：Distributed groupingのトップレベルグループコマンドでSimpleOrderedMapの代わりにNamedListを使う。non-distributed groupingと出力形式が異なるため。
　　</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene 4.3.0のChangesにあるChanges in backwards compatibility policyが気になったので訳してみた。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/24/lucene-4-3-0%E3%81%AEchanges%E3%81%AB%E3%81%82%E3%82%8Bchanges-in-backwards-compatibility-policy%E3%81%8C%E6%B0%97%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%AE%E3%81%A7%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</link>
      <pubDate>Wed, 24 Apr 2013 16:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/24/lucene-4-3-0%E3%81%AEchanges%E3%81%AB%E3%81%82%E3%82%8Bchanges-in-backwards-compatibility-policy%E3%81%8C%E6%B0%97%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%AE%E3%81%A7%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</guid>
      <description>現在、RC3のVoteをやっている最中（2013/04/24 16:00時点）で、まだリリースされていない、4.3.0についてです。 開発者ML</description>
      <content:encoded><p><span style="color:#FF0000">現在、RC3のVoteをやっている最中（2013/04/24　16:00時点）で、まだリリースされていない、4.3.0についてです。</span>
開発者MLでChangesの書き方を考えないとね、みたいなエントリーが流れてて気になっていたので、訳してみた。
lucene-gosenの実装を変更しないといけないっぽいなぁ。Lucene/Solr 4.2.1以前と4.3.0でI/Fとかが変わることになりそうです。（3.とか8.とか）
（ここで力尽きて、それより下はまだ読んでないです。。。）</p>
<hr>
<p>○Changes in backwards compatibility policy
　　1.<a href="http://issues.apache.org/jira/browse/LUCENE-4810">LUCENE-4810</a>：EdgeNGramTokenFilterが同じ入力tokenから複数のngramを生成した時にpositionを増加させていないのを修正
　　2.<a href="http://issues.apache.org/jira/browse/LUCENE-4822">LUCENE-4822</a>：KeywordMarkerFilterがabstractクラスで、サブクラスがisKeyword()メソッドを実装する必要がある。新しく、SetKeywordTokenFilterというクラスにすでにある機能を分解した。
　　3.<a href="http://issues.apache.org/jira/browse/LUCENE-4642">LUCENE-4642</a>：TokenizerとサブクラスのAttributeSourceのコンストラクタを削除。代わりにAttributeFactoryをもつコンストラクタを追加。
　　4.<a href="http://issues.apache.org/jira/browse/LUCENE-4833">LUCENE-4833</a>：IndexWriterConfigがsetMergePolicy(null)の時にLogByteSizeMergePolicyを使っているのをデフォルトmerge policyをTieredMergePolicyに。また、nullが引数に渡されたらExceptionを返す。
　　5.<a href="http://issues.apache.org/jira/browse/LUCENE-4849">LUCENE-4849</a>：ParallelTaxonomyArraysをDirectoryTaxonomyWriter/Readerのためのabstractとして作成。あと、o.a.l.facet.taxonomyに移動。
　　6.<a href="http://issues.apache.org/jira/browse/LUCENE-4876">LUCENE-4876</a>：IndexDeletionPolicyをInterfaceではなく、abstractクラスに。IndexDeletionPolicy、MergeScheduler、InfoStreamでCloneableをimplement。
　　7.<a href="http://issues.apache.org/jira/browse/LUCENE-4874">LUCENE-4874</a>：FilterAtomicReaderと関連するクラス（FilterTerms、FilterDocsEnumなど）でフィルタされたインスタンスをforwardしないように。メソッドが他のabstractメソッドを実装している場合に。（？）
　　8.<a href="http://issues.apache.org/jira/browse/LUCENE-4642">LUCENE-4642</a>, <a href="http://issues.apache.org/jira/browse/LUCENE-4877">LUCENE-4877</a>：TokenizerFactory、TokenFilterFactory、CharFilterFactoryの実装者は、少なくともMap&lt;String,String&gt;（SPIフレームワーク（Solrとか）によってロードされる）を引数にするコンストラクタを提供する必要がある。さらに、TokenizerFactoryはcreate(AttributeFactory,Reader)メソッドを実装する必要もある。</p>
</content:encoded>
    </item>
    
    <item>
      <title>エンジニアのためのスキルアップ勉強会『Tech Compass』 #tecomp ― Vol.2 「人気Webサービスの作り方教えます！」に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/23/%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%82%B9%E3%82%AD%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E5%8B%89%E5%BC%B7%E4%BC%9Atech-compass-tecomp-vol-2-%E4%BA%BA%E6%B0%97web%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%AE%E4%BD%9C%E3%82%8A%E6%96%B9%E6%95%99%E3%81%88%E3%81%BE%E3%81%99%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 23 Apr 2013 20:45:42 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/23/%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%82%B9%E3%82%AD%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E5%8B%89%E5%BC%B7%E4%BC%9Atech-compass-tecomp-vol-2-%E4%BA%BA%E6%B0%97web%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%AE%E4%BD%9C%E3%82%8A%E6%96%B9%E6%95%99%E3%81%88%E3%81%BE%E3%81%99%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>第2回目も参加しました。 とりあえずメモ。 自分には欠けてる視点の話しなので面白かった。 ちょっと寒かったなぁ。 エンジニアのためのスキルアップ勉強</description>
      <content:encoded><p>第2回目も参加しました。
とりあえずメモ。
自分には欠けてる視点の話しなので面白かった。
ちょっと寒かったなぁ。</p>
<hr>
<pre><code>
エンジニアのためのスキルアップ勉強会『Tech Compass』 #tecomp ― Vol.2 「人気Webサービスの作り方教えます！」 ―
日時：2013/04/23 19:00
場所：パレスサイドビル9F マイナビルーム

◎自己紹介
　●株式会社Zaim／閑歳孝子
　　小学校3年からFMVで草の根チャットとかにつないでた。（すげー）
　　これがいい記事ですよ。http://www.1101.com/umeda_iwata/
　　テレビとかで紹介されてるZaimやってます。
　　3つの基準　　
　　・日常的に使うもの
　　・普通の人が使うもの
　　・少なくとも自分は使うもの
　　サービスの良さの基準
　　　縦軸：影響の深さ
　　　横軸：影響する人数
　　　この面積が大きいのがいいんでは。

　●株式会社ワディット／和田裕介
　　いろいろ作ってます「カウントダウンチューブ」とか「君のラジオ」とか30～40くらい作ってます。
　　・「ボケて」ってのやってます。
　　　600万ボケ。アプリ120万DL　　　
　　僕らがつくるための５Wについて
　　・なぜ？
　　　・サービスの根本となる「哲学」をみんなで共有できるかが重要
　　　・内向けのビジョンも大事
　　・何を？
　　　・ユースケースで整理する
　　・いつ？
　　　・つくろう！すぐ作れるようにしようねって感じ
　　・誰と？
　　　・最強のチーム。意思決定がはやい。
　　自分主体でサービス設計して、作りなおすのをためらわない

◎ディスカッション
　Q：どうしてサービス作ったの？
　A：エンジニアへのあこがれから、サービスを作った（閑歳さん）
　　　ものづくりという意味では、Webサービス以外もあるけど？（馬場さん）
　　　学内のSNSのようなものを作ってて、アクセスが伸びるのが面白かった。（閑歳さん）
　　　大学でlastfmみたいなものを研究してて。。。（和田さん）
　　　社会に出てサービスを作るまでの話は？作って稼ごうって思ったのは？（馬場さん）
　　　あんまりなかった。（閑歳さん）
　　　すでに起業してた。サービス作ると実績として認められて仕事が入ってきてた。（和田さん）
　　　直接あった時に反応がもらえるのが楽しい。（和田さん）
　
　Q：どうやって、チームを組み立てたりして開発とかしてきたのか？（馬場さん）
　A：「ボケて」まとめサイトでブレイクしたけど、アクセス数が落ちてない。
　　　同年代の知人で色々とチームが組めてて楽しい（和田さん）
　　　はじめは一人でやってて、しかも構想とか。あとロゴだけ最初に作ってた。
　　　ノマド的に作業してました。（閑歳さん）

　Q：チームがバラバラですが、困らないですか？
　A：今のところ困ってないです。もっと人が増えると困るかもしれないですけど。（閑歳さん）
　　　それぞれが他に職を持ってるので特に困ってないです（和田さん）
　　　向き不向きはあるんじゃないかなぁ。10人とかになるとどうなるか不明（閑歳さん）
　　　GoogleのMLでレスが早ければ問題ないかな。（和田さん）
　　　あとは、チームが大きくならないように上手く分割してる（？）（ふたりとも）
　Q：ユーザの声の吸い上げ方、サービス改善の判断材料は？
　A：ユーザの声は聞くけど、全部取り込むものではない。
　　　細かな点はユーザの声を取り込むけど、軸はブレないようにしてる（閑歳さん）
　　　古参ユーザよりも新しいユーザを取り込むのが大事。（和田さん）
　　　nanapiのけんすうさん
　Q：品質はどうしてる？
　A：セキュリティは絶対。（閑歳さん）
　　　品質を気にするってのは難しい。品質を担保できる仕組みを作るとこまでいけるようにしたい。
　　　投稿される画像はチェックしている。（和田さん）
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>JIRAのデータをS3へバックアップ(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/22/jira%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92s3%E3%81%B8%E3%83%90%E3%83%83%E3%82%AF%E3%82%A2%E3%83%83%E3%83%97/</link>
      <pubDate>Mon, 22 Apr 2013 23:07:28 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/22/jira%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92s3%E3%81%B8%E3%83%90%E3%83%83%E3%82%AF%E3%82%A2%E3%83%83%E3%83%97/</guid>
      <description>だいぶ、ブログを書くペースが落ちてきてて、危機感を感じている今日このごろです。。。 今回は、個人的に利用しているJIRAのバックアップについて</description>
      <content:encoded><p>だいぶ、ブログを書くペースが落ちてきてて、危機感を感じている今日このごろです。。。
今回は、個人的に利用しているJIRAのバックアップについて、ブログに残しておこうかと。</p>
<p>さくらVPSのサーバを借りてJIRAを運用しています。
運用といっても、自分の備忘録のためというのが大半の理由です。
タスクを忘れないように管理するのと合わせて、その作業を行ったときのメモも残したいなと。</p>
<p>バックアップといっても、同じサーバ内に保存しても意味がない＋AWSを触ると言いつつ1年以上経ってしまったので、
このへんで本腰を入れるという意味も込めてAWSのS3（ゆくゆくはGlacier）にバックアップを取る仕組みを作りました。
「作りました」というと凄そうですが、Cronとシェルだけで終わりました、なんて便利な世の中。
ということで、以下は作業の備忘録です。</p>
<h2 id="1awsのアカウント作成">1.AWSのアカウント作成</h2>
<hr>
<p>アカウントを作ってください。ここは特に説明しなくてもいいかと。。。クレジットカードの登録が必要なのが注意点でしょうか。
アカウント作成後の1年間は<a href="http://aws.amazon.com/jp/free/">無料枠</a>と呼ばれる仕組みが用意されており、いろんなことが無料で行えます。
（まだ、S3しか使ってないので、他にももっと使わないと。。。）</p>
<h2 id="2s3のバケット作成">2.S3のバケット作成</h2>
<hr>
<p>AWSのアカウントが作成できたら、<a href="http://aws.amazon.com/jp/s3/">Amazon S3（Simple Storage Service）</a>のバケットと呼ばれる、データの保存先を作成します。
作成手順はこちらの<a href="http://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html?r=8046">公式入門ドキュメント（英語）</a>をそのまま行いました。
簡単な手順はこちら</p>
<ol>
<li>AWS Management Consoleと呼ばれるところから、S3のコンソールにアクセス</li>
<li>「Create Bucket」ボタンをクリック</li>
<li>「Bucket Name」を入力し、リージョンを選択したら「Create」ボタンをクリック</li>
</ol>
<p>以上でバケットが作成されました。これだけです。WebのConsoleからファイルをアップロードすることも可能です。
ただ、今回はさくらVPSから定期的にバックアップしたいのでシェルでアクセスできるようにします。</p>
<h2 id="3awsのアクセスキーとシークレットアクセスキーの取得">3.AWSのアクセスキーとシークレットアクセスキーの取得</h2>
<hr>
<p>次に紹介するs3cmdというツールを利用するのに、AWSのアクセスキーとシークレットキーが必要になります。
取得方法は<a href="http://www.atmarkit.co.jp/fwin2k/operation/aec2s3_1/aec2s3_1_04.html">こちら</a>を参考にしましたが、今は日本語のページが用意されています。
セキュリティ証明書（Security Credentials）のページで、「アクセス証明書」というタブで、「新しいアクセスキーを作成する」リンクをクリックしてください。
アクセスキーIDが新しく表示されます。このアクセスキーIDとシークレットアクセスキー（表示リンククリック後に表示される）をメモしておきます。</p>
<h2 id="4s3cmdのインストール">4.s3cmdのインストール</h2>
<hr>
<p>世の中には便利なものを作ってくれる方がいるもので。
<a href="http://s3tools.org/s3cmd">s3cmd</a>と呼ばれるS3へアクセスできるコマンドラインツールが存在します。
ということで、こちらをインストール。これまた、便利なもので<a href="http://understeer.hatenablog.com/entry/2012/07/23/124402">インストール手順を書いてくれれてるブログ</a>もありました。（あれ、なんか、見たことある名前がURLにはいってるなぁ）
インストールはこちら。</p>
<pre><code>
# yum -y --enablerepo epel install s3cmd
</code></pre><p>インストール後に、先ほど取得したアクセスキーIDを設定します。</p>
<pre><code>
$ s3cmd --configure
Access Key: xxxx
Secret Key: xxxx
</code></pre><p>以上で、s3cmdが使えるようになりました。</p>
<h2 id="5バックアップスクリプトの作成">5.バックアップスクリプトの作成</h2>
<hr>
<p>つぎは、本題のJIRAのバックアップです。
バックアップ方法は<a href="http://doc.go2group.jp/pages/viewpage.action?pageId=34865916">こちら</a>を参考にしました。ちょっと古いみたいですが。
おもなデータは「JIRAのXMLバックアップユーティリティ」にてバックアップするか、「データベースのバックアップツール」を利用する方法があります。
推奨は「データベースのバックアップツール」のようなので、私の場合は「pg_dump」にてJIRAのデータベースをまるごとバックアップすることにしています。
また、データベースには添付ファイルが保存されていないようなので、別途「/var/atlassian/application-data/jira/data」というディレクトリをtarコマンドで圧縮して保存すようにしました。
あとは、pg_dumpの結果と添付ファイルの保存先の圧縮したデータをまとめてディレクトリに保存して圧縮します。
さいごは、S3にアップロードすればおしまいです。
一応、ファイルが連綿と残り続けるのは嫌だなぁということで、ファイル数で世代管理することにもしました。
これまた、ググって見つけてきた<a href="http://d.hatena.ne.jp/nigogonigo/20121003/1349272523">サイトを参考</a>にしただけですが。。。
ということで、こんなかんじのスクリプトを日時でcronに設定して動かしてます。</p>
<pre><code>
#!/bin/sh
TODAY=`date +%Y%m%d`
BACKUP_HOME=&quot;/var/atlassian/backups&quot;
S3SYNC_DIR=&quot;/var/atlassian/backups/s3sync&quot;
AGE=7

mkdir -p ${BACKUP_HOME}/${TODAY}/
/usr/bin/pg_dump -U postgres -Fc jiradb &gt; ${BACKUP_HOME}/${TODAY}/jiradb_backup.dump
tar zcvf ${BACKUP_HOME}/${TODAY}/attachment.tgz /var/atlassian/application-data/jira/data
tar zcvf ${S3SYNC_DIR}/jira_backup_${TODAY}.tgz ${BACKUP_HOME}/${TODAY}
rm -rf ${BACKUP_HOME}/${TODAY}
NUM=`ls ${S3SYNC_DIR} | wc -l`
while [ ${NUM} -ge ${AGE} ]
do
  DEL_FILE=`ls -lt ${S3SYNC_DIR} | tail -1 | awk '{print $9}'`
  rm -r ${S3SYNC_DIR}/${DEL_FILE}
  NUM=`ls ${S3SYNC_DIR} | wc -l`
done

s3cmd sync --delete-removed ${S3SYNC_DIR}/* s3://my_jira_backup/s3sync/
</code></pre><p>そこへんだよとかあれば、ツッコミを。
今のところ、S3へのsyncだけなので、このあとは、月1くらいでGlacierに落とすとかの仕組みも考えるかなぁ。これまた何かあるんだろうけど。
あとは、この応用で家のNASに溜まっている写真をS3経由もしくは直接Glacierにバックアップする仕組みを考える予定です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Heroku Meetup #8 TreasureData &#43; Waza Report!! に参加しました。#herokujp(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/05/heroku-meetup-8-treasuredata-waza-report-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-herokujp/</link>
      <pubDate>Fri, 05 Apr 2013 11:20:43 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/05/heroku-meetup-8-treasuredata-waza-report-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-herokujp/</guid>
      <description>HerokuではじめるRailsプログラミング入門 heroku気になってるのに使ってなくて、TDのアカウント作ってデータアップしてない軟弱者</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4797371838/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4797371838&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4797371838/?tag=johtani-22">
      HerokuではじめるRailsプログラミング入門
      </a>
    </p>
  </div>
</div>
heroku気になってるのに使ってなくて、TDのアカウント作ってデータアップしてない軟弱者ですが、参加して来ました。。。
とりあえず、大事なことです。まずは、上記の書籍を買ってください（って中の人が言ってました。）
<a href="http://herokujp.doorkeeper.jp/events/3405">イベントページはこちら</a>。</p>
<p>総じて、herokuの中のエンジニアの方たちがすごく情熱があって、ユーザと会話をしたがっているという感想だったようで、まだまだ、よくなりそうだなぁと。
私は、ベースがJavaなので、JavaやScala、Playで使ってる方の感想とか聞きたいかなぁ。
あと、herokuとS3の組み合わせだと思うんですが、料金とかはAmazonとheroku両方に別々に払うのかな？とかはちょっと気になりました。
AWSのアカウントも作ってS3にバックアップあげるの作ろうと思って手をつけてない軟弱者ですが。。。
今月は余裕がありそうなので、TDとか触ってみようと思います。。。</p>
<p>懇親会ではTDのmuga-sanとお話できて、いくつか気になってた話ができたのでスッキリしました。
あと、株式会社サムライズムの名刺頂きました。写真載せろって言われたけど、また今度ｗ</p>
<p>最後に、大事なことです。まずは、上記の書籍を買ってください（って中の人が言ってました。）</p>
<p>以下は、いつものようなメモです。最後の方はちょっとくたばってたのでメモがおざなりになってます、すみません。</p>
<hr>
<pre><code>
日時：2013年04月04日(木) 18時30分 - 21時00分
場所：日本創生ビレッジ 事業開発支援オフィス 東京21cクラブ コラボレーションスペース


◯Ayumu Aizawa（Heroku, Inc.）
　・■
　　PostgreSQLが9.2になりました。
　・◆
　　メモリが2倍。βテスタ向けにスケールアップ。
　　JavaとかJavaとかJavaとかデプロイしてもいいよね。
　　けど、メモリ2xは価格も2x！
　・●
　　Heroku OAuthを提供。Experimentalだけど。
　　heroku-bouncer使うとOAuthが楽になる。

◯Treasure Data and heroku
　Masahiro Nakagawa（TreasureData）
　・会社紹介
　・フロントエンド部分の担当（fluentd）
　・1500億レコード！？
　・投資家の中にHerokuの方がいる。
　・ターゲットは？
　　Cloud + Big Dataが対象
　　Hadoopは立ち上げるのはいいんだけどメンテナンスコストが。
　　Hadoopの処理基板を提供
　・Hadoop生ではなく、Plazmaを使っていたりする。
　・Viki
　　herokuにtd-agent入れて、TDにデータ送って、Postgresに書き出して使ってる。
　・TDはどうやってheroku使ってるの？
　　Webコンソール。
　　　http://console.treasure-data.com
　　Webサイトは大体heroku
　　　fluentdとかも
　・herokuのaddonとしてtd-agentを提供してる。
　・STDOUTからTDにデータ送るのもできるよと。


◯Waza Report
　◯吉田雄哉さん（co-meeting）
　　・まずは、co-meetingの紹介。
　　　1文字ずつ送信してるよと
　　・Chief Talk Officerらしいｗ
　　・MongoDB使ってるって言ったら、herokuのPostgreSQLの人と話して、鼻で笑われたｗ
　　・すごく熱意のある人達がエンジニアとして働いてる
　　・ユーザの声をきちんと聞いてくれる体制ができてるミーティングだった。
　　・「クラウド」って単語を聞いてない。勉強会のレベルもすごい。

　◯山本裕介さん（株式会社サムライズム）
　　・ニッチなブログ書いてます。
　　・Java屋が見るWaza
　　　Tシャツプレゼント！
　　・OSS好きが多い。
　　　Java/Scala系の話が少なかった。Scala界隈では人気みたい

　◯岡村純一さん（株式会社シャノン）
　　・スライド1枚も作らずに喋る人とかいました。
　　　（すごい。。。）
　　・Django

　　　Playに似てて面白いかもと
　　・Ruby2.0
　　　Matzが喋ってたとか
　　・クロージングはビールが出てきてた。
　　　交流パーティみたいになってた。CROSSがそれに似てますね。日本でやってるイベント

　◯小西俊司さん（株式会社フレクト）
　　・バックエンドの性能とかを収集して見ることができるツールがあるらしい。
　　・クエリを登録しておくと監視ができるツールとか。（TDとかぶってる？）
　　・やっぱり、情熱的だし、OSS好きでオープンな感じのエンジニアが多い。


◯Heroku LT
　無慈悲なLTです。3分たったらケーブル引っこ抜きます。
　（最後はくたばっててあまり聞けてない。。。）

　◯山本 裕介（株式会社サムライズム）
　　・herokuでJava7！
　　　Java6終わってますからherokuも移行してね！
　◯竹野 淳（Grow!）
　　・BoxTo？
　　・コラボレーター募集！
　◯小西 俊司（株式会社フレクト）
　　・ExcelのテンプレートをアップロードしてGETでパラメータわたせばいいよみたいなの作ってる。
　◯大久保英樹（Job-Hub）
　　・CarrierWaveとかの注意点
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>第10回Solr勉強会を主催しました。#SolrJP(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/03/26/%E7%AC%AC10%E5%9B%9Esolr%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%82%92%E4%B8%BB%E5%82%AC%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-solrjp/</link>
      <pubDate>Tue, 26 Mar 2013 21:16:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/03/26/%E7%AC%AC10%E5%9B%9Esolr%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%82%92%E4%B8%BB%E5%82%AC%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-solrjp/</guid>
      <description>記念すべき！？第10回のSolr勉強会です。 発表者が無事あつまり（本当にありがとうございました！）、今回も盛況な感じでほぼ満員でした。 ツイー</description>
      <content:encoded><p>記念すべき！？<a href="http://atnd.org/events/37170/">第10回のSolr勉強会</a>です。</p>
<p>発表者が無事あつまり（本当にありがとうございました！）、今回も盛況な感じでほぼ満員でした。
ツイートのおかげか、キャンセル処理もちゃんと行なってもらえて助かりました。
開場直後にドタバタしてしまい、すみませんでした。。。</p>
<p><del>とりあえず、第一報の記事をアップしておきます。</del>
<del>懇親会での話とか感想はまたあとで。</del></p>
<p>関口さんの資料は実は、前もって見たことがある資料でした。
最初の発表にしては、少しむずかしいと思った方もいるかなぁと。
ただ、類義語の辞書は結構作るのが大変だし、探しても出てこないものなので、面白い話だったんではないかなぁと。
「ミスチル」はできないけど、「マツケン」ができるのは読みがあるからとかなんですかねぇ？って質問するの忘れてた。</p>
<p>尾形さんの話は結構、みんな通ってきた道かもなぁと思いました。他の方も同じ経験してるんじゃないかなぁと。
ただ、一人でやるのはすごいですよね。検索って結構人数が割かれてない場合が多いのかなぁ。
あんまり使われていないというのが少し悲しい話でしたが。。。
サーバを要求すれば結構なスペックが用意してもらえるのはうらやましい限りですねぇ。
スキーマ変更については、レプリケーション機能を使うと追加などならうまくいくんじゃないでしょうか。（そんなツイートもありましたよ。togetter読み返すと出てきます。）
フィールド名を変更しないで型を変更するなどしたらおかしくなると思いますが。</p>
<p>野口さんの話はなかなかチャレンジングでいいなぁと思いました。よく挫けずに頑張られているなぁとｗ
試行錯誤した仮定も発表してもらえると同じ轍を踏んだ人が助かると思います。
大きな企業で本格的に横断的な社内検索が出来る仕組みが出来上がっているって話はなかなかきかないかなと。
どうしても、社内検索とかお金が出なくて手を付けられないといいう悲しい話が多いので、こういう話は共有したい情シスの方がいっぱいいるんじゃないかなぁと。
ManifoldCFが結構地雷を多く含んでいるのは大変そうですね。。。
SolrにもTikaが入っていたりしますが、個人的にはTikaがやるべき処理は前処理と思っているので、Solrとは別の場所でやりたいとか考えていたりします。
ManifoldCFがその辺りまでやってくれるかまではちゃんと調べてないんですが。
Solrは検索だけに注力させることで、役割を分割できるので、性能の対処とかを行うのが楽になるんじゃないかなぁとか。
ManifoldCFで困ってる人は他にもいるようなので、ジャンジャン使って、どんどんチケット上げて貢献してもらうといいかと。
また、定期的に発表してもらうと面白そうだなぁと。</p>
<p>弘瀬さんの話は結構興味ある方がいたんじゃないかなと。
SolrCloudは壮大だなぁと思いつつ、手を出しづらいと思ってる方が多いと思います。
実際のサービスに投入して試行錯誤された話を細かな数値も上げて発表してもらえるのは検証をやる方の助けになります。
残念ながら、私もSolrCloudは興味有りつつちゃんと追っかけてないので、途中でnodeとshardとcoreの関係がわからなくなってしまいましたので。。。
もう一度勉強して、スライドを見たいと思います。。。
分散検索（1つのインデックスが複数のcoreやshardに分割された状態）が絡んでくると、複数台の検索の性能のうち、一番遅い性能が最終的な検索性能に響いてくるので、検索リクエストの偏りとかも影響が出たりするかもしれないなぁと。
そういった意味でも試行回数を3以上で計測した結果で再度発表してもらうと面白そう！（なんか、下心見え見えですがｗ）</p>
<p>前回、今回も感じたのですが、もう少し質問をしてもらえると発表された方も励みになるかなぁと思いました。
質問しにくい雰囲気になってるのでしょうか？参加者が結構いるから質問しにくく感じたりするのでしょうか？
そのあたりをもう少しうまくやれるようになにかコメントもらえると嬉しいかなぁと。
運営で気になった点などもコメントやツイートをいただけると今後の改善にも役立てますので気兼ねなく連絡いただけると助かります。
開場がドタバタしすぎとか、ハッシュタグがわからなかったとか。</p>
<p>今回は思ったよりも懇親会に残る方が少なめでした。
コミッターの方（LuceneやManifoldCFとかlucene-gosenとか）が複数いたり、Solrを結構触ってる方がいたりと面白い話が聞けそうだったのですが。。。
Kuromojiのユーザ辞書の改良点をチケットにあげるって約束したのでやらないとなぁ。
早く帰るつもりだったのに気づいたら23時でしたwやっぱり色々と話ができるのが楽しくて。。。
駅前の機動隊とかびっくりしながら帰りました。
今回、お話ができなかった方もいらっしゃるかと思いますが、気兼ねなく、ツイートしてもらったり、声をかけていただければと。</p>
<p>あと、常に発表していただける方は歓迎しておりますので、連絡いただけると非常に助かります！
こんな話が聞きたいなどでもいいかと思いますので、連絡いただければと。</p>
<p>#SolrJPも<a href="http://togetter.com/li/478117">togetterにまとめてもらいました</a>。ありがとうございます。
以下は、いつものメモになります。</p>
<hr>
<pre><code>
日時：2013/03/26 19:00 to 21:00
場所：VOYAGE GROUP 会議室

1. 株式会社 ロンウイット　関口　宏司さん
　　タイトル：Wikipediaからの類義語知識の自動獲得について
</code></pre><p>　<a href="http://www.slideshare.net/KojiSekiguchi/wikipediasolr">発表資料はこちら</a></p>
<pre><code>
　・「辞書型コーパス」という単語は造語かもしれません。
　・なんでこんなことを？
　　→類義語辞書を自動生成したいから。
　・自賠責保険、自動車賠償責任保険を例にSynonymFilterの説明。
　・Wikipediaを入力として、類義語辞書を作成するときにLuceneのインデックスを活用してる。
　・類義語候補の見つけ方
　　いくつかヒューリスティクスな処理とかも入れてます。
　　日本語Wikipedia固有なもの。
　・実際に導出された単語も載ってる。
　　FTPなども導出で来てる。
　　丸ビルとかも。
　・導出できなかったものもあります。
　　「十六進法」が「十進法」になってる
　　「ミスチル」も無理。
　・ミスもあるけど、類義語が存在しない場合になんとなく、使う辞書としては採用できるのでは？
　　類義語検索対象のブーストを小さくするなどをすれば役に立つ

　Q：類似度にしきい値を用意したりしてますか？
　A：min.scoreという値を用意し、足切りをしている。
　Q：ベクトルを作るという話があったが、品詞でフィルタリングとかしてる？
　A：名詞に限って処理してます。名詞に限らなくてもいいかも。。。（若干聴き逃しました）
　　　重みの高いn件をベクトルの対象にしてます。

2. グリー株式会社　尾形　暢俊さん
　　タイトル：GREEにおける全文検索の歴史
</code></pre><p>　<a href="http://www.slideshare.net/NobutoshiOgata/solr10">発表資料はこちら</a></p>
<pre><code>
　・GREEさん、検索はないがしろにされてる。。。
　　一人でつくって、一人でメンテナンスしてる。
　・GREEの検索は右上の検索ボックスが
　・2007年はSennaつかってました。
　　Tritonnに移行。2009年くらいまで。
　　やっぱり安定しない＋MySQLのバージョンアップしたいけど、追従できない
　・2012年初頭までLuceneでやってた。（結構古い）
　　フラグメントが発生してOptimizeすると、検索サーバが使えなくなる。。。
　　検索しにくるサーバが1000台いるので、Optimizeかけるときに、1000台のサーバの設定を書き換えるとかしてた。。。
　・現在まではSolrをつかってる。
　　Luceneのバージョンも古かったので100倍くらい速くなった。
　・Solr本が必須ですよ！！！
　・Lucene+Tomcatから移行。
　　移行に気をつける点として、I/FをそのままSolrに置き換えると。
　　Solr返却のXMLをカスタマイズしたり、クエリをSolr向けに書き換えたり。
　　あと、メンテナンスが楽になるように。
　　40数台のインデックスサーバがあると。
　　一人でメンテナンスしてるから、楽になる方法が重要
　・レプリケーションで、Optimizeの影響が出ないように。
　　キューをつかって、マスタに登録して、スレーブにレプリカを配布
　・スキーマが7つ
　　あんまり使われてなくてかなしい。。。
　・負荷のグラフもありました。
　・RangeQueryを結構使う。
　・作りこんだ部分
　　インデックスのMasterへ分散させる処理とか
　　クエリの変換とか人力監視処理とかNGワードとか
　　サーバ監視のための仕組み
　・今でも大変なこと
　　スキーマ変更が大変
　　スレーブをマスタに昇格とかが手動
　・今後改善したいこと
　　精度を上げたい
　　辞書を使ってみたいけど、各国語対応
　　あと、自動化とか


3. ソフトバンクBB株式会社　野口　勝義さん
　　タイトル：企業内の大規模ファイルサーバ検索事例
　・情シスの企画版？という立場のかた。
　・売上に貢献したいのでクラウドサービスとして検索をオプションとして立ち上げてみた！
　・Solr＋ManifoldCFで作ってみたよと
　・なんでSolr？
　　OSSだし、機能が充実
　・なんでManifoldCF？
　　Active Directory連携が使いたかったと。
　　社内検索ってやっぱりアクセス権がうるさいので。
　・ManifoldCFの説明はロンウイットさんの画像を使わせてもらいましたｗ
　・ファイルサーバが、70TB。。。

　・困ったこと。
　　・その１
　　　・クローラージョブの構成の最適化どうする？
　　　・マルチコアで、クローラーとファイルサーバを1対1の構成にしてみた。
　　・その２
　　　・ファイル数が増えるとまた問題が。。。
　　　・ファイルサイズが大きい→Heapが足りないエラーとか、MCFのタイムアウトとか。。。
　　　　ファイルサイズのリミットを設けてみた。
　　　　mp4とかでエラーがでるとか。既知のエラーでしたとか。
　　　　ulimitがたりないとか。
　　　　MCFの稀に出るバグとか。。。
　　　　ファイルサーバの不良ブロックとか。。。
　　・その３
　　　・クロールに時間がかかる
　　　・MySQLでスロークエリとか
　　　　MySQLよく知らないとか言われながらコミッターに対応してもらうなど。
　　　　SSDつかうとか考え中
　　　・フルクロールで1週間
　　　　差分でも1日強かかる。
　　　　ManifoldCFだけで対応できないから、ファイルの特徴を元に
　　　　→ManifoldCFを経由しないリアルタイムインデックス更新のAPIを経由してMasterじゃなくて、更新かけると。（特定のクライアントからの方法）
　　・その４
　　　・本文データをstored=falseに
　　　　けど、ハイライトできないから、どうにかしたい
　　・ユーザの要望
　　　・もしかして検索
　　　　類義語？じゃないよねぇ。テザリング、tezaringuとか
　　　　フロント側で頑張った。（Solr諦めました）
　　　・検索スコアも弄りたいとか
　　　　外部データでブーストとかもしたい。External Fieldとか使うといいのでは？とか。

4. 株式会社サイバーエージェント　弘瀬　健さん
　　タイトル：SolrCloudの導入事例
</code></pre><p>　<a href="http://www.slideshare.net/kenhirose547/10solr-solr-cloud">発表資料はこちら</a></p>
<pre><code>
　　・Webエンジニアだったのに、検索エンジニアに！
　　・SolrCloudもサービスインしてると。
　　・SolrCloud概要
　　　4.0以降の機能とか。
　　・SolrCloudの構成要素
　　　概念的なもの。Collection、Shard
　　　物理的なもの。Node、Core
　　・ Simplogってサービスに導入済み。
　　　ZooKeeperが3台、6Shard、3nodeという形式
　　・性能
　　　平均レスポンスタイム50msec
　　　　思ったより出てないので、調べてみた。node数とかshard数を変えてみて調べてみた（まだ、模索中。）
　　・色々テストケース試したけど、試行回数が1回だけです。
　　　詳細なデータが出てるのありがたい（全部はまだ理解できてないですｗ）
　　・検証まとめ
　　　ノード当たりのcore数が少ないほうが検索、更新性能がいい
　　　1コレクション当たりのshard数が少ないほうが検索性能がいい
　　・まとめ
　　　・SolrCloudの利点
　　　　クライアントが色々意識しなくていいのがうれしい。
　　　・SolrCloudの注意点
　　　　shardの分割機能がまだないので、大変。
　　　　コレクション情報が壊れると検索更新できないとか。4.0だとバグが有った
　　　・性能的には素のSolrのほうがいいよと。


</code></pre></content:encoded>
    </item>
    
    <item>
      <title>elasticsearch-analysis-kuromojiでユーザ辞書の利用方法(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/03/20/elasticsearch-analysis-kuromoji%E3%81%A7%E3%83%A6%E3%83%BC%E3%82%B6%E8%BE%9E%E6%9B%B8%E3%81%AE%E5%88%A9%E7%94%A8%E6%96%B9%E6%B3%95/</link>
      <pubDate>Wed, 20 Mar 2013 16:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/03/20/elasticsearch-analysis-kuromoji%E3%81%A7%E3%83%A6%E3%83%BC%E3%82%B6%E8%BE%9E%E6%9B%B8%E3%81%AE%E5%88%A9%E7%94%A8%E6%96%B9%E6%B3%95/</guid>
      <description>なんか、とても久しぶりにイベント参加メモ以外の投稿です。 elastic searchのMLを見てたら、KuromojiのAnalyzerを使うときにユーザ</description>
      <content:encoded><p>なんか、とても久しぶりにイベント参加メモ以外の投稿です。
elastic searchのMLを見てたら、KuromojiのAnalyzerを使うときにユーザ辞書使うのどうするの？という<a href="https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/7oGNCM7QH4s">投稿</a>を見かけました。</p>
<p>Kuromojiのユーザ辞書にもちょうど興味があったり、elasticsearchもちょっとずつ触りたかったのでちょっと試してみました。（返信もしてみましたが、テキトーな英語です。。。）</p>
<p>elasticsearch-kuromoji-pluginのインストールなどは<a href="http://qiita.com/items/134b049a59fe396c9475">ElasticSearch で kuromoji を使う (ES 0.90.Beta1 + kuromoji 1.2.0篇)</a>を参考にしてください。
私もこちらに記述のある組み合わせ（elasticsearch-0.90.0Beta1 + elasticsearch-analysis-kuromoji/1.2.0）を利用しました。
KuromojiのAnalyzerはデフォルトで「kuromoji」として登録済みですが、こちらはユーザ辞書の指定がありません。
ということで、「kuromoji_user_dict」というユーザ辞書指定をしたtokenizerと、それと使う「my_analyzer」というanalyzerを登録したIndexを作成します。
定義する前に、「userdict_ja.txt」を用意して、elasticsearch-0.90.0Beta1/config/ディレクトリに配置しておきます。
（以下のサンプルでは、SOLE_HOME/example/solr/collection1/conf/lang/userdict_ja.txtをコピーして使いました）</p>
<pre><code>
$ curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
{
    &quot;index&quot;:{
        &quot;analysis&quot;:{
            &quot;tokenizer&quot; : {
                &quot;kuromoji_user_dict&quot; : {
                   &quot;type&quot;:&quot;kuromoji_tokenizer&quot;,
                   &quot;user_dictionary&quot;:&quot;userdict_ja.txt&quot;
                }
            },
            &quot;analyzer&quot; : {
                &quot;my_analyzer&quot; : {
                    &quot;type&quot; : &quot;custom&quot;,
                    &quot;tokenizer&quot; : &quot;kuromoji_user_dict&quot;
                }
            }
            
        }
    }
}
'
</code></pre><p>「user_dictionary」というのがユーザ辞書の定義ファイルになります。
注意点としては、6行目で指定した名前「kuromoji_user_dict」を14行目の「tokenizer」に指定しないとちゃんと動かないという点でしょうか。</p>
<p>上記で指定したAnalyzerを利用して「朝青龍」という単語をを解析してみます。</p>
<pre><code>
$ curl -XGET 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=my_analyzer&amp;pretty' -d '朝青龍'
{
 &quot;tokens&quot; : [ {
   &quot;token&quot; : &quot;朝青龍&quot;,
   &quot;start_offset&quot; : 0,
   &quot;end_offset&quot; : 3,
   &quot;type&quot; : &quot;word&quot;,
   &quot;position&quot; : 1
 } ]
</code></pre><p>「朝青龍」という単語がユーザ辞書に登録されているので、1単語として出力されます。
ちなみに、デフォルトの「kuromoji」のanalyzerを指定すると以下の様な出力です。</p>
<pre><code>
$ curl -XGET 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=kuromoji&amp;pretty' -d '朝青龍'
{
 &quot;tokens&quot; : [ {
   &quot;token&quot; : &quot;朝&quot;,
   &quot;start_offset&quot; : 0,
   &quot;end_offset&quot; : 1,
   &quot;type&quot; : &quot;word&quot;,
   &quot;position&quot; : 1
 }, {
   &quot;token&quot; : &quot;青龍&quot;,
   &quot;start_offset&quot; : 1,
   &quot;end_offset&quot; : 3,
   &quot;type&quot; : &quot;word&quot;,
   &quot;position&quot; : 2
 } ]
</code></pre><p>とまぁ、こんなかんじです。
ユーザ辞書を書き換えたあとは「close/open」しないと読み込めないのかなぁ？そのへんはまたあとで調べようかな。</p>
<hr>
<p>ちなみ、以下のページを参考にさせてもらいました。
<a href="http://www.hirotakaster.com/archives/2012/11/elasticsearch-kuromoji-plugin.php">elasticsearch kuromoji plugin - natural days</a>
<a href="http://qiita.com/items/134b049a59fe396c9475">ElasticSearch で kuromoji を使う (ES 0.90.Beta1 + kuromoji 1.2.0篇)</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>エンジニアのためのスキルアップ勉強会『Tech Compass』＠竹橋 ― Vol1 「スマホ時代到来、この先生きのこるエンジニアとは！？」に参加しました。#tecomp(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/03/19/%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%82%B9%E3%82%AD%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E5%8B%89%E5%BC%B7%E4%BC%9Atech-compass%E7%AB%B9%E6%A9%8B-vol1-%E3%82%B9%E3%83%9E%E3%83%9B%E6%99%82%E4%BB%A3%E5%88%B0%E6%9D%A5%E3%81%93%E3%81%AE%E5%85%88%E7%94%9F%E3%81%8D%E3%81%AE%E3%81%93%E3%82%8B%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%A8%E3%81%AF%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-tecomp/</link>
      <pubDate>Tue, 19 Mar 2013 20:28:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/03/19/%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%82%B9%E3%82%AD%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97%E5%8B%89%E5%BC%B7%E4%BC%9Atech-compass%E7%AB%B9%E6%A9%8B-vol1-%E3%82%B9%E3%83%9E%E3%83%9B%E6%99%82%E4%BB%A3%E5%88%B0%E6%9D%A5%E3%81%93%E3%81%AE%E5%85%88%E7%94%9F%E3%81%8D%E3%81%AE%E3%81%93%E3%82%8B%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%A8%E3%81%AF%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-tecomp/</guid>
      <description>久々にブログ＋イベント参加です。 知り合いが開催＋出演したので参加してみました。 「スマホ」ってキーワードはあまりなかったけど、面白かったです。</description>
      <content:encoded><p>久々にブログ＋イベント参加です。
知り合いが開催＋出演したので参加してみました。
「スマホ」ってキーワードはあまりなかったけど、面白かったです。</p>
<p>とりあえず、メモ。
余力があれば感想追加します。</p>
<hr>
<pre><code>

日時：2013/03/19 19:00
場所：パレスサイドビル9F マイナビルーム （東京都千代田区一ツ橋1-1-1）
URL：http://atnd.org/events/36966

登壇者：@naoya_ito @yusuke

◯@naoya_itoさんの発表
　最近、感心してること。
　・RubyMotionやってます。
　　今年5月に
　・AWSもやってるよ。
　・「継続的デリバリー」の話
　　GREEでは毎日デリバリーしてるよと。
　・Chefの話
　　DevOpsとか。
　・Obama for Americaの話
　　GitHubとか使ってた。
　身に付けるべきものは？
　　「変化に適応する力」が必要だよね。
　最近は
　　Ruby、クラウド、AWS、iOS開発とかやってる、どうしてこうなったｗ
　　先は予測してないけど、変化に適応するには5分でいいからやり続けること。

◯@yusukeさんの発表
　自己紹介が難しい
　・今やってること
　　ブログ執筆
　　刺さるツイート研究
　　ソフトウェア販売代理店
　　　思わぬ収穫。InDesign/Illustratorのスキル、ビジネスの仕方ｗ
　　　会社設立中。
　　Webサイト構築・運用
　・ハッタリが大事
　・プログラム好き
　　プログラミング以外を仕事にしてる。
　・人と違うことがしたい。市場がない場所での差別化はしない＝トレンドに逆行し過ぎない。バランス。
　・今は種まき中。

◯ディスカッション
　・2,3年後とか何やってると思いますか？
　　@naoya_itoさん
　　　ドラクエやってると思ってるかもしれませんが、サービス作るコード書いてます。
　　　将来的な見通しでやってるのか？って言われるとそうでもない。
　　　ツールとかよりは、コミュニティとかのサービスを作るほうが好き。
　　@yusuke
　　　一定の収入がありつつ、プログラミングしてたい。あわよくばサービス作ったのが当たるといいなぁ
　・そもそもフリーランスをどう捉えているか？
　　@naoya_ito
　　　いいところ。自由に自分の時間をコントロールできる。
　　　悪いところ。自分でコントロールしないといけない。
　　　自分をコントロールしないと厳しい。
　　　フリーになってわかったのは、会社は人間を働かせるための仕組みをよく考えて出来てる。
　　　フリーでいいのは、選択した結果に自分が責任を持つというのがいい。
　　@yusuke
　　　セフルマネージメントできたほうがいい。
　　　しがらみを捨てるためにフリーになるのはおすすめしない。
　・情報発信を継続してるけど、必要なことか？
　　@naoya_ito
　　　必要とは思わないけど、みんな書いたらいいと思う。
　　　検索すれば自分が助かるからｗ
　　　セルフブランディングのために重要？かなぁ。あまりそれが唯一の方法ではないと思う。
　　@yusuke
　　　発信することで、何に興味を持ってるかをわかってもらえるから、便利かな。
　　　ブログに残したほうがググって引っかかるよと。
　・技術の方向性？見つけ方？は？
　　@naoya_ito
　　　直近は、クラウド流行るしiOSはまだ伸びてるしと。
　　　そっから先はどうするの？
　　　わからないよね。じゃあ、どうするの？って変化に対応してボトムアップで見つけてくのがいいんじゃないかなぁ。
　　　人がやってないところを先にやらないと武器にならないけど、どうやって見つけるのか？
　　　プログラミングやってる時に、その先に必要なこととか、気付きがあるので、手を動かさないとだめだよねと。
　　@yusuke
　　　トレンドを追いかけつつ、知ったかぶりするのは重要
　　　ある程度ハッタリ＋手を動かす

</code></pre></content:encoded>
    </item>
    
    <item>
      <title>Xperia Z買いました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/02/13/xperia-z%E8%B2%B7%E3%81%84%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 13 Feb 2013 01:53:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/02/13/xperia-z%E8%B2%B7%E3%81%84%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>技術ブログから離れていってますが。。。 Xperia Zを購入しました。 前に使っていたのはXperia arcになります。 もうすぐ２年だったのですが、容量不</description>
      <content:encoded><p>技術ブログから離れていってますが。。。
Xperia Zを購入しました。</p>
<p>前に使っていたのはXperia arcになります。
もうすぐ２年だったのですが、容量不足に悩まされていたため＋芸の肥やし（仕事の足し）という名目で
Xperia Zの購入に踏み切りました。
予約しての購入です。</p>
<p>いくつか戸惑いなどを感じているので備忘録としてブログに残しておきます。
<strong>サイズが大きい</strong>___
まぁ、わかっていたことですが、5インチと大きいです。
ステータスバーを表示するのがちょっときついくらいです。
私は手が大きい方なのですがそれでもキツイので、普通の人は両手じゃないと厳しいんじゃないかと。。。
一応、ステータスバーを表示するためのアプリなんかもあるようなので、その辺を検討するのもいいかなぁと（まだインストールもしてませんが。）
また、Angry Birdを演ってびっくりしたのですが、解像度が大きいため、startボタンなどが小さくて押しづらいですｗ</p>
<p><strong>移行がめんどくさい</strong>___
iPadも持っていて、ソレと比較するとひどくめんどくさかったです。
実は方法を知らないだけかもしれないんですが。。。
iPadだと、初期化したりしてもiTunesにインストールしたアプリなども覚えていてくれます。
ですので、すぐに同じ環境が出来上がります。
Xperiaに関しては、arcでインストールしてたアプリをインストール履歴を元に一つずつインストールして、
それぞれのアプリやアカウントを設定して行きました。
これが結構面倒で、まだ完全にはアプリがインストールできてないかも（他にも新しいものを入れたいなぁと思いつつ、探す手間を惜しんでます。。。おすすめアプリお待ちしてます。）</p>
<p><strong>Zite、Angry Bird star warsがインストールできない</strong>___
Ziteは英語の勉強も兼ねて入れていたのですが、対応していないバージョンですと言われます。
AngryBirdもいっしょですね。
starwarsじゃない奴はインストールできたのですが、肝心のやってみたいstar warsがインストールできなくて。。。</p>
<p><strong>ドコモアプリがいっぱい</strong>___
ドコモのアプリやプリインストールアプリがいっぱいあったので、どうしようか悩んでいます。
いくつかはアンインストールしました。HotpepperBeautyとか間違いなく使わないし
SFA-QRというよくわからないアプリも入っていたのでこれまたアンインストールしました。</p>
<p><strong>通知に出さないアプリとか選択できない？</strong>___
容量不足だったのも有り、TwitterクライアントにはTwiccaのみをインストールしていたのですが、今回はTwitter純正のクライアントもインストールしました。
お気に入り登録されたりといった状況も見れるので重宝するのですが、基本的にはtwiccaを使うつもりです。
ですが、通知バー（ステータスバー）に@ツイートが届いたなどといった通知は見なくてもいいと思っているんですが、なにか方法あるんでしょうか。</p>
<p><strong>dビデオが危険</strong>___
いわゆる携帯ショップで機種変更したため、いくつかのサービスに登録しないといけませんでした。
で、dビデオ（月額525円で映画とか見放題）に登録したので、せっかくだからと映画をダウンロードしてみたのですが、危険です。。。
ただでさえ、本読むのが遅かったり、コーディングしてないのにビデオ見ると更に時間ががが。。。
けど、面白くて。。。危険です。（スターシップ・トゥルーパーズ見ちゃいました。。）</p>
<p><strong>クレードルがもう一つほしい</strong>___
防水なので、あまりキャップを開けたくないんです。
そして、dビデオとか見てるとやっぱり電池の減りがはやいんです。（ディスプレイが大きいのもあるのかなぁ。要らないサービス切るとかしないと）
結局、出先でも充電をしないといけなくて。
ただ、純正のクレードルが2100円と結構な値段でして。。。
あと、イヤホンを指すところもキャップがあるので、Bluetoothレシーバーも気になってたりします。
こちらも結構な値段でして。。。</p>
<p>とまぁ、新しいものを手に入れて、色々悩んでますが、楽しいですね。購入したての色々と悩む時間がｗ
おすすめアプリとかあったらコメントください</p>
</content:encoded>
    </item>
    
    <item>
      <title>Hadoop Conference Japan 2013 Winterに参加しました。#hcj13w(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/01/22/hadoop-conference-japan-2013-winter%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-hcj13w/</link>
      <pubDate>Tue, 22 Jan 2013 00:50:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/01/22/hadoop-conference-japan-2013-winter%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-hcj13w/</guid>
      <description>金曜日に引き続き、イベントに参加して来ました。（仕事。。。これも仕事だよ。） 「Hadoop Conference Japan 2013 Winter」です。 普段、Hadoopを触る</description>
      <content:encoded><p>金曜日に引き続き、イベントに参加して来ました。（仕事。。。これも仕事だよ。）</p>
<p><a href="http://hcj2013w.eventbrite.com">「Hadoop Conference Japan 2013 Winter」</a>です。
普段、Hadoopを触るのも少ないのですが情報を仕入れときたいなぁということで。

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130122/20130122_44963.jpg" alt="ビッグサイト"/>
    </div>
    <a href="/images/entries/20130122/20130122_44963.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

今年はビッグサイトですよ。そろそろ無料のカンファレンスもキツイのではないでしょうか。。。
こちらの写真はセッション会場。今回もすごいです。。。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130122/20130122_44958.jpg" alt="壇上。相変わらずすごい演出"/>
    </div>
    <a href="/images/entries/20130122/20130122_44958.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>簡単ですが、聞いていた感想です。
全体的に、Hadoop本体の話ではなく、エコシステムと呼ばれる周辺のプロダクトの話や実際に運用している実例が増えていました。
だいぶ普及期に来たのかなぁと。そして、自分の不勉強を実感できたなぁと。
（あと、TreasureDataに絡む話が多いなぁと言うのも印象です。そういうセッションを選んで出ていたのかもしれませんが）
懇親会まで残っていたのですが、結構、すごい方たちの顔ぶれだなぁというのを今更ながらに実感するとか。
（ハチ象Tシャツを着ているすごい集団でもありましたが。。。）</p>
<p>以下はいつものメモになります。だいぶ金曜日のイベント後で腑抜けてるのでメモが雑ですが。</p>
<pre><code>
◯ご挨拶
　Doug Cuttingさんのビデオ（あんまり聞けてない）　さすがのhamakenさんのトークの安定感。
　後援はリクルートテクノロジーズさんが大半。
　リクルートテクノロジーズの米谷さんが軽く発表

◯LINEのHBaseを利用した大規模なメッセージストレージ：NHN Japan 中村 俊介
　まずは、LINEの紹介が続く。
　New Yearのメッセージは3倍位だったけど、何とかなっったよと。
　データロスがない。
　サーバ故障からのデータリカバリも自動でやってくれる
　書き込み1ms、読み出し1-10msでできてる
　・IDC onine migration
　　クライアントベースで2つに書き込み
　　Incremental replication（新データ）、BulkMigration（旧データ）
　　元のHBaseのレプリケータは利用してない（pushだったから。スループットコントロールしたかったから）

　・NN failover
　　2012.10にNameNode障害発生
　　HA構成にしてたから問題が発生。
　　VIPはHDFSにつかうとリスキー
　　現在：
　　　少ないダウンタイムを許容する

　・Stabilizing LINE message cluster
　　※あとでHBase触るときに資料見直すくらいで。。。
　　Case1：Too many HLogs
　　Case2：Hotspot問題
　　まぁ、けど1億ユーザのインフラとして使えるってすごいよな。。。　　

◯Hadoop meets Cloud with Multi-tenancy： Treasure Data 太田 一樹
　TDもFluentdも有名だなぁ
　Hadoopのメンテナンスとか、つらいよねというのを見てきたのでTD作った
　・なぜ、BigData+Cloud？
　　Hadoopだけみてもバージョンが混在してる＋ディストリビューターも多くなってる
　　十徳ナイフみたいになってきてるけど、必要なのはナタだよね。というのを提示するためにTD立ち上げた。
　・なぜCloud？
　　・IaaSベンダーの対象としてるのはSCM。
　　　オンプレだとHWが陳腐化してくよね→HWはクラウドが主流に
　　・PaaS、SaaSの対象は時は金なり
　　　バージョンアップとか大変だよねと
　・TDのご紹介
　　唯一の解析用DBを目指して
　・哲学とアーキテクチャ
　　解析とか運用をいかに楽にしていくかというのを主体においたサービスを提供したい？
　　いかに速くレポーティングシステムを簡単に構築していくかとかの話
　　簡単なインタフェースと目的に集中することで、価値を提供
　・AWSとの違いは？
　・アーキテクチャとか技術的な話
　　データを集める処理がデータ解析に実際には重要なフェーズ
　・カラム指向でデータ保存
　　実装がどーなってるのかとか、TD内部のHadoopクラスタの運用、バージョンアップとかがどうなってるのかもきになる。（企業秘密だろうけど）

◯Amazon Elasti MapReduceとHadoopコミュニティの関わり：Amazon Web Services Peter Sirota
　・３つのV
　　Volume、Velocity、Variety
　・yelpのAuto-suggestの例
　　カスタマーのレコメンデーションにElasticMapReduce使ってるよと。
　・razorfishの広告インプレッションの解析に使ってる
　・Amazon.comでの話
　　AWS Public Datasetsの話
　　http://aws.amazon.com/jp/publicdatasets/
　　IonFluxのDNA解析の話
　・いろんな分野でのBig Data
　　事例がちらほら
　※わかりやすい英語なんだけど寝不足が。。。

◯ランチ！
　サンドイッチと豚汁とあとなんとかライス。
　TDブースにて、fluentdのTシャツもらったよ！

◯Hadoop's Power to Transform Business：MapR　Ted Dunning
　・Mahaut＋Solrの単語が。8時間のレコメンデーションデータの作成が3分に？？
　・Stormにてリアルタイム処理と連携。バッチ処理はMRで。
　・バッチ処理とリアルタイム処理の間としてのDrill
　・Drillの概観とできるところの話。
　　（あとでスライドで）データ解析のために機会学習の処理とかも投げられそう。
　　Q：ImplaraとDrillの違いは？
　　　コミュニティベースかなどが違うよね（あまり聞き取れず）
　　Q：Drillの開発はどのくらいすすんでるの？
　　A：。。。
　　Q.クエリ言語としてSQLがいいの？
　　A.No。というのも、単独の言語ですべてにベストというのはないから。SQLはわかりやすくて良いが、実行が非効率な場面も。

◯Introduction to Impala～Hadoop用のSQLエンジン～：Cloudera　川崎 達夫
　・Impalaとは
　　分析に特化した低レイテンシクエリ実行基盤
　　Apacheライセンス
　　バッチ処理要ではなく、データサイエンティストが試行錯誤するときに利用するのを想定
　　パフォーマンスが良い
　　　実行エンジンはC＋＋とかで実装されてる
　　　MapReduceに依存してない
　・MapReduceの問題点
　　MR直接は難しい→Hiveとか、M/Rの実装を隠して使いやすくするものが増えてきた
　　Hiveを例に説明。MRがベースなので高レイテンシ
    ・Impalaのアーキテクチャ
       機能制限やGA版について言及されてる資料なのが良い
    ・GA以降の話もあり
        ピンチヒッターなのに落ち着いて発表とか凄すぎです。
     jdbcサポートも入ると。
     プランナーはjava実装
     Hiveとの互換性は？→完全互換を目指す。
     開発のプロセスが見えにくいのでは？開発主体がcloudera

 ◯Flumeを活用したAmebaにおける大規模ログ収集システム：CyberAgent　飯島 賢志
　立ち見。
　Flumeのコミッターの人がCAにいたんだ。

◯Log analysis system with Hadoop in livedoor 2013 Winter：NHN Japan　田籠さん@tagomorisさん
　・NHNJapanのお話
　・Webサービスのログ解析について話していく
　　400+Webサービス
　・2011年8月にLTしました。
　・1.5TB/day。。。
　・batchとstream
　　Batch集計も重要だし、Stream処理も重要なので、ハイブリッドシステム
　・システムーバービュー
　　FluentdClusterを中心にして各種ツール、保存先に転送してる。
　　これが結構重要。だけど、今日はFluentdの話ではない
　・Fluentd周りを一人でやってるのか。。。
　・処理の流れ
　　・ログの収集と、生ログ保存
　　・パース（主にHive用に）、変換、フラグ追加（分類しておくとあとで集計したいとか、保存すべきかを処理可能に）
　　・Hiveにロード
　・第1世代のはなし
　　すべてbatch処理、Scribe
　　遅延が1時間ちょっとあるため、Hiveクエリで結果が見れないとか。
　・第2世代の話
　　Fluentdのstream処理にHadoop Streamingを呼び出せるようなプラグインを書いた
　　Fluentd＋HoopServerの構成
　・第3世代
　　HoopをWebHDFS
　　Fluentdでのオンライン集計
　　GrowthForcast、HRForcast
　・第4世代（ここ1週間でやったこと）
　　CDH4でQJMベースのNameNode（Failoverは手動）
　　Hiveのスキーマを変更（これはブログに今度書くよ）
　　とりあえず、現時点で改善点が思いつかない
　・総括
　　HTTPベースでRPCベースにしたのでコンポーネント切り替えが結構楽
　　OSSで公開されてるからいいよね
　　公開することにより色々とドライブするよ！

◯いかにしてHadoopにデータを集めるか：Treasure Data　古橋 貞之
　・自己紹介
　・ビッグデータ収集の問題点
　　・壊れたデータが入ってる
　　・読み書きの時間がかかる
　　　ログはサブケースである。メインはサービスとかだから。
　・トライ＆エラー処理が時間かかる
　・データを分割するのが基本的なアイデア
　　失敗した時のリトライが楽になる。さらに、それをStream処理すれば良いよねと。
    ・flumeのお話
    ・fluentdのお話
       バッファリングは性能アップも兼ねてる
       設定のクラスタへの伝搬とかインストールはpuppetとか使おうねと。
       プラグインのインストールが楽
    ・いくつかのプラグインの紹介
    ・TDへのデータ投入のお話
    ・最後にmuddydixonさんのプラグインの宣伝がw

◯Hadoopの次に考える分散システム技術：Microsoft　萩原 正義
    ・CAP定理の克服をどうしていくか
    ・CAPのおさらい
    ・Lease
       クライアント主導、サーバ手動とか
　　（理論重視の話で最後のセッションには辛かった。。。頭が疲れててついていけませんでした）
</code></pre><hr>
<p>おまけ
今回頂いたノベルティなどを写真に撮ってみました。（ランチはお腹すいてて写真取らずに食べちゃいました。。。）


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130122/20130122_44962.jpg" alt="ノベルティ色々"/>
    </div>
    <a href="/images/entries/20130122/20130122_44962.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

Hiveロゴへの愛を語ってHive Tシャツをゲット。
FluentdのTシャツももらいました！
Hiveステッカーなどもゲット


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130122/20130122_44964.jpg" alt="メッセージボード"/>
    </div>
    <a href="/images/entries/20130122/20130122_44964.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

あと、メッセージボードなるものがあったので書いてみました。一応、Hadoopに絡んだことですよね！？</p>
</content:encoded>
    </item>
    
    <item>
      <title>エンジニアサポートCROSS 2013に参加（＋お手伝い＋モデレータ）しました #cross2013(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/01/19/%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88cross-2013%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%8A%E6%89%8B%E4%BC%9D%E3%81%84%E3%83%A2%E3%83%87%E3%83%AC%E3%83%BC%E3%82%BF%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-cross2013/</link>
      <pubDate>Sat, 19 Jan 2013 22:09:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/01/19/%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88cross-2013%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%8A%E6%89%8B%E4%BC%9D%E3%81%84%E3%83%A2%E3%83%87%E3%83%AC%E3%83%BC%E3%82%BF%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-cross2013/</guid>
      <description>1/18（金）に開催された、エンジニアサポートCROSS 2013で「検索CROSS」セッションのセッションオーナー（モデレータ）をやって来ま</description>
      <content:encoded><p>1/18（金）に開催された、エンジニアサポートCROSS 2013で「<a href="http://www.cross-party.com/programs/?p=366">検索CROSS</a>」セッションのセッションオーナー（モデレータ）をやって来ました。</p>
<p><a href="http://twitter.com/muddydixon/">@muddydixon</a>さんに昨年の夏くらいに声をかけていただいたのがきっかけです。
こういったイベントの運営のお手伝い（ほとんど何もしてない）も初めてでしたし、セッションオーナー（モデレータ）も初めての経験で色々といい勉強をさせて頂きました（自分の足りない所とか、考慮すべき点がどういったところにあるとか）。本当にありがとうございます。</p>
<p>スピーカーとして登壇していただいた久保田さん、佐藤さん、安田さんほんとうにありがとうございました。
頼りないオーナーで、モヤッとした内容を提示したにもかかわらず、意図を汲み取って話の内容をふくらませていただいてすごく助かりました。
また、会場にお越しいただいたみなさん、ありがとうございました。すこしでも検索に興味をもっていただければ、セッションは成功だったと思っています。
朝一だったのと、私の認知度の低さもあったのとで残念ながら満席とはいきませんでしたが。。。</p>
<p>後日、録画されたセッションの内容を見て色々と反省しようかなぁと。。。
緊張していたので、うまくモデレートできてたのか、話ができていたのか不安ですが。
忌憚ない感想をお待ちしています。</p>
<p>ということで、反省終わり！ここからは、イベントの感想と聞いたセッションに関する感想です。</p>
<p>###聞いたセッション</p>
<p>####<a href="http://www.cross-party.com/programs/?p=369">企業CROSS DeNA x グリー x Aiming： 大企業・ベンチャーが語る「スクラム」開発 (仮)</a></p>
<hr>
<p>Googleの及川さん見たさで参加しました。
スクラム自体は名前くらいは聞いたことある程度だったのですが、その程度の私でもわかりやすいディスカッションで楽しかったです。
聞いていて感じたのは、次のようなこと。</p>
<ul>
<li>「スクラム」という開発手法の話しなんだけど目的はそこじゃなくて、どうやってうまくプロジェクトを回して（ドライブして）、目的（サービスを作るとか顧客の要望を叶えるとか）を達成するかというのが重要だなぁと。</li>
<li>「スクラムマスター」の仕事は子育てに似てる</li>
<li>「スクラムマスター」になりたい人はなかなかいないらしい</li>
<li>「スクラムマスター」の人は、俯瞰してプロジェクト全体を見渡して落穂拾いをしていく人なのかもなぁと</li>
</ul>
<p>個人的には「スクラムマスター」の仕事がちょっとおもしろそうと思いました。（いつもいろんなことを面白そうと思う自分なのでなんともいえないですが。。。）</p>
<p>####<a href="http://www.cross-party.com/programs/?p=134">データ理解を助けるビジュアライゼーション</a></p>
<hr>
<p>昨年からビジュアライゼーションには興味だけ（実践とかできてない）は持っていたので、参加しました。
スピーカーの方々の対立軸がはっきりしていたのが面白かったかと。
聞いていて感じたのは、次のようなこと</p>
<ul>
<li>ビジネスの視点からのビジュアライゼーションはコスパがどうなのかというのがまだ不明</li>
<li>可視化できたらおしまいじゃなくて、お金につながる可視化に心がけるのが重要そう</li>
<li>可視化して楽しむのも有りだとは思うけど、それは趣味だったり芸術の話かもなぁ</li>
<li>異なるコンテキストの人にでも誤解を与えない可視化というのが重要</li>
<li>何をどう可視化するかというのが実は統計学知ってると必要ないこともある</li>
</ul>
<p>基本的にはCAの方の内容が一番しっくり来ました。やっぱりビジネス的な視点と学術的な視点は違うのかもなぁと。</p>
<p>####<a href="http://www.cross-party.com/programs/?p=140">継続的システム運用のゲンバのハナシ</a></p>
<hr>
<p>面白かったです。のっけから燃料がテーブルにならんでるしｗ
運用とかインフラはそこまで詳しくないのですが、少しは作業したりするので気になってましたが、ぶっ飛んだ話が聞けて楽しめました。
聞いていて感じたのは、次のようなこと</p>
<ul>
<li>サービス内容によっては夜中にアクセス数が低いものもある（cookpadは夜中はアクセス数が低い）</li>
<li>アプリを修正することでインフラを楽にするという考えもある</li>
<li>インフラエンジニアは実はリア充</li>
<li>cookpad面白い人多いw</li>
</ul>
<p>このあとは、おまちかねのプレモルタイムだったので、プレモル片手（失礼だろｗ）にTech10の池本さんに挨拶してきました。（Solr本のときはお世話になりました！）
あとは、RedBull片手に知り合いの方々（Twitter上のみやいつもお世話になってる人）に挨拶をして回ってました。（プレモルも美味しかったのですが、プレモルおつまみも美味しかったです！）
また、ちょっとつかれて座るために入った「<a href="http://www.cross-party.com/programs/?p=131">体系的に学ぶ安全な利用規約の作り方</a>」でも、面白い話を聞けました。
（利用規約をきちんと考えて変更している企業のほうが炎上をしてしまうとか無駄だしおかしいよねとか）</p>
<p>イベント終了まで、会場にいて（いただけで大した手伝いしてないです、すみません。）その後打ち上げに向かって楽しみました。</p>
<p>####さいごは、イベント通じて感じたことと気になったこと。（若干、意図的なものが入り込んでますが。。。）</p>
<hr>
<ul>
<li>題材にもなってる「CROSS」ですが、ここ数年特に、エンジニアとして複数の技術や考えなどを身につけないといけないなぁと</li>
<li>有名人に会える！（ミーハーです。。。）</li>
<li>プレモル美味しい！（いつも飲んでるビールが「え？」って思った）</li>
<li>ケンタッキー美味しい！</li>
<li>RedBull美味しい！</li>
<li>いろんな技術分野に触れられる機会ができそう</li>
<li>Wi-fiは用意してもらえると助かる。（参加者の方々が一斉にLTEとかでつなごうとするのでみんなが繋がらなくなってツイートされないとかちょっとつらいかも）</li>
<li>各会場が覗けるようになってたのは良かった（ただ、会場の後ろの人はざわつきを感じるためスピーカーの声が聴こえないことも）</li>
<li>スクリーンはもう少し高い位置にあると良かった（後ろに座ると下のほうが見えないことも）</li>
<li>飲食の配布は会場内でできたほうが良かったのかも（NGなのかな？）。プレモルやRedBullは配ってくれたので良かったのだが、食べ物を取りに行くために入口付近が混雑してた気がした。</li>
<li>（これで、メッキ剥がれたかなぁ）</li>
<li>会場入り口の写真とか撮ってくるんだった。。。</li>
</ul>
<p>ということで、総じて楽しかったので来年もあるならまた、手伝いたいです！</p>
<hr>
<p>おまけ</p>
<hr>
<p>
<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130119/20130120_42001.jpg" alt="プレモル！"/>
    </div>
    <a href="/images/entries/20130119/20130120_42001.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

プレモル！


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130119/20130120_42002.jpg" alt="Amebaウォーター"/>
    </div>
    <a href="/images/entries/20130119/20130120_42002.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

Amebaウォーター


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130119/20130120_42000.jpg" alt="プレモル＋RedBull＋プレモルおつまみ"/>
    </div>
    <a href="/images/entries/20130119/20130120_42000.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

プレモル＋プレモルおつまみ＋RedBull


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20130119/20130120_42021.jpg" alt="もらったスタッフTシャツとプレモル！"/>
    </div>
    <a href="/images/entries/20130119/20130120_42021.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

いただいたスタッフTシャツとプレモル！</p>
</content:encoded>
    </item>
    
    <item>
      <title>いまさらですが、CROSS 2013で検索CROSSというセッションを担当します(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/01/16/%E3%81%84%E3%81%BE%E3%81%95%E3%82%89%E3%81%A7%E3%81%99%E3%81%8Ccross-2013%E3%81%A7%E6%A4%9C%E7%B4%A2cross%E3%81%A8%E3%81%84%E3%81%86%E3%82%BB%E3%83%83%E3%82%B7%E3%83%A7%E3%83%B3%E3%82%92%E6%8B%85%E5%BD%93%E3%81%97%E3%81%BE%E3%81%99/</link>
      <pubDate>Wed, 16 Jan 2013 01:29:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/01/16/%E3%81%84%E3%81%BE%E3%81%95%E3%82%89%E3%81%A7%E3%81%99%E3%81%8Ccross-2013%E3%81%A7%E6%A4%9C%E7%B4%A2cross%E3%81%A8%E3%81%84%E3%81%86%E3%82%BB%E3%83%83%E3%82%B7%E3%83%A7%E3%83%B3%E3%82%92%E6%8B%85%E5%BD%93%E3%81%97%E3%81%BE%E3%81%99/</guid>
      <description>いまさら感満載ですが、今週の金曜日(1/18)に開催されるCROSS 2013というイベントで「検索CROSS」のセッションを担当することにな</description>
      <content:encoded><p>いまさら感満載ですが、今週の金曜日(1/18)に開催される<a href="http://www.cross-party.com">CROSS 2013</a>というイベントで「<a href="http://www.cross-party.com/programs/?p=366">検索CROSS</a>」のセッションを担当することになってます。</p>
<p>ここ数年、検索に携わっていますが、検索は色々な技術の組み合わせ（検索用インデックス、自然言語処理、機械学習などなど）でなりったていて奥が深いなぁと感じる毎日です。
そこで、実際のサービスで検索をやられている方、検索プラットフォーム製品を開発している方、自然言語処理や検索に長けた方を招いてつぎのような話をしてもらおうかと。</p>
<p>・現在の事例（検索とその周辺技術）
・今後の検索にクロスすると面白い技術</p>
<p>がっつり検索をやられている方々に登壇していただきますので、検索に興味のある方はぜひ、ご参加を！
登壇者の方々については<a href="http://www.cross-party.com/programs/?p=366">こちら</a>を御覧ください。</p>
<p>検索CROSS以外にも楽しみなセッションが多数ありますので、参加をご検討いただければと。
あと、実はこっちがメインなのですが、夕方からのプレモルタイムも期待できるのでこちらへの参加だけでもぜひ！！</p>
</content:encoded>
    </item>
    
    <item>
      <title>「コード・シンプリシティ」を読みました。（Kindle paperwhiteの使い勝手の確認も兼ねて）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/01/09/%E3%82%B3%E3%83%BC%E3%83%89%E3%82%B7%E3%83%B3%E3%83%97%E3%83%AA%E3%82%B7%E3%83%86%E3%82%A3%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9Fkindle-paperwhite%E3%81%AE%E4%BD%BF%E3%81%84%E5%8B%9D%E6%89%8B%E3%81%AE%E7%A2%BA%E8%AA%8D%E3%82%82%E5%85%BC%E3%81%AD%E3%81%A6/</link>
      <pubDate>Wed, 09 Jan 2013 00:28:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/01/09/%E3%82%B3%E3%83%BC%E3%83%89%E3%82%B7%E3%83%B3%E3%83%97%E3%83%AA%E3%82%B7%E3%83%86%E3%82%A3%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9Fkindle-paperwhite%E3%81%AE%E4%BD%BF%E3%81%84%E5%8B%9D%E6%89%8B%E3%81%AE%E7%A2%BA%E8%AA%8D%E3%82%82%E5%85%BC%E3%81%AD%E3%81%A6/</guid>
      <description>年末から年始にかけて、「コード・シンプリシティ」を読みました。 先日の記事にも書きましたが、Kindle paperwhiteを思いかけず入手で</description>
      <content:encoded><p>年末から年始にかけて、「<a href="http://booklog.jp/item/11/9784873115757">コード・シンプリシティ</a>」を読みました。</p>
<p>先日の記事にも書きましたが、Kindle paperwhiteを思いかけず入手できたので、eBookを読みたいなぁと思いまして。
Kindleのお試しも兼ねて読んでみました。</p>
<p><strong>まずは、本の感想。</strong>
短めの書籍＋読み物なので、サクっと読めました。
題名に「コード」と書かれていますが、コードの実例が出てくる書籍ではありません。
「ソフトウェア」を以下にシンプルに保って、管理しやすくするか、機能追加、テストをやりやすくするかといった指針について書かれています。
機能追加に関して検討しないといけないバランス（先読みしすぎずと実装しすぎないとか）の話や定期的なデザインの見直しなどについても書かれています。
ソフトウェアに関しては動くこともだが、デザインが重要であること。（確かに。）
すでにあるソフトウェアについて、どのように簡潔にしていくかという話もありました。
ただ、読んでいて一番重要だと思ったのは、「悪いプログラマーと良いプログラマーの違いは理解力だ」という一文です。
何をしているかを「理解」していないと、根本的な問題点の解決もできないですし、どうすればコードが簡潔になるかといったこともわからないです。
また、やっている作業がどんなもので、何のためにやっているのか、それを行うことで今後にどのような影響があるのかといったことも理解しておく必要があるかと。
ま、ある程度考えながら作業とかしましょうってことですかね。（強引なまとめかも。。。）</p>
<p><strong>で本題です。</strong>
Kindle paperwhiteとiPadの使い分けを検討するのも兼ねて本を読んでました。
iPadを持っているのもあり、Kindleはそもそも眼中になかったのですが、めっけ物でした。
やはり実際に使ってみないとわからないことありますねぇと。
paperwihteはつぎのような利点があるかと。</p>
<ul>
<li>読書に没頭できる。→本を読むというシンプルな目的を達成するものだけが実装されている</li>
<li>目が疲れない→Twitterでも話に上がったのですが、液晶とは異なり字が読みやすいです</li>
<li>軽いし小さい→電車で立っていても楽に持てるし、カバンからの出し入れも楽です（iPadに比べて）</li>
<li>複数の端末のKindleアプリで同期できる。→あまり異なる端末では読まないですが、paperwhiteで読んだところが、iPadでも同期されるのですんなり続きが読めます</li>
<li>電池長持ち</li>
</ul>
<p>ということで、本を読むのに没頭できるし目が疲れないと。
ただ、つぎの点では不満もあります。</p>
<ul>
<li>PDFは読みにくい。→画面のサイズが小さいので大きめの書籍のPDF版は読みにくいです。</li>
<li>PDFは読みにくい。→文字のサイズを変更できないのもキツイです。Kindle専用のmobi形式の書籍が読みやすいです</li>
<li>ページめくりに一定時間がかかる→紙の書籍やiPadで書籍を読むのとは異なり、ページをめくるのに時間がかかります。e-inkの再描画の問題かと。パラパラと書籍を読みたい場合は厳しいです</li>
<li>白黒。カラーの電子書籍は読めないですねぇ</li>
<li>ソースコードなどもキツイ→長めのコードが書いてある本だとコードが頭に入ってこないです。。。</li>
</ul>
<p>という感想でした。
今後は基本Kindle paperwhiteを持ち歩きmobi形式の本を読むようにすると思います。
iPadは自炊本やカラーの本を読むとき、映像、ネットを見るような場合に持ち歩くことになりそうです。
私は基本的にMBAを持ち歩いているというのもあるので、paperwhiteのほうが主流になりそうです。
ネットを見るときなどは最悪、MBAを開くでしょうし。
小説とかをメインに読むのであれば断然、paperwhiteだと思います（最近読んでないなぁ）</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負（2012）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/12/31/%E4%BB%8A%E5%B9%B4%E3%81%AE%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A%E3%81%A8%E6%9D%A5%E5%B9%B4%E3%81%AE%E6%8A%B1%E8%B2%A02012/</link>
      <pubDate>Mon, 31 Dec 2012 08:01:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/12/31/%E4%BB%8A%E5%B9%B4%E3%81%AE%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A%E3%81%A8%E6%9D%A5%E5%B9%B4%E3%81%AE%E6%8A%B1%E8%B2%A02012/</guid>
      <description>書こうと思いながら、気づいたら大晦日。 よくないですね、思いついた時に書かないと。 昨年も書いたので、昨年書いた抱負から振り返りをと。 Mongo</description>
      <content:encoded><p>書こうと思いながら、気づいたら大晦日。
よくないですね、思いついた時に書かないと。</p>
<p>昨年も書いたので、昨年書いた抱負から振り返りをと。</p>
<ul>
<li><del>MongoDB触ってみる</del></li>
<li>Scala触ってみる</li>
<li><del>なんかWebサービス作ってみる（Amazonとかで）</del></li>
<li>elasticsearch、IndexTankを調べてみる</li>
<li><del>NewSolrCloudDesignを読み込んで自分なりにまとめる</del></li>
<li><del>Junsaiに参加する</del></li>
<li><del>「7つの言語7つの世界」の続き</del></li>
</ul>
<p>えーと。。。できてないことばかりですねｗ
MongoDBはインストールすらしてないですし、Webサービスも作ってないです。（さくらVPSは借りたんですが）
Junsaiは参加したけど何もできてないし。7つの言語もそのままですね。。。
SolrCloudDesignはまとめられてないですしね。
Scalaは少しだけ触りました。結局Javaみたいな記述をしてしまうという良くないくせが出てしまうので、
見送ってしまいましたが。。。
elasticsearchは少しだけやりました。#pyfesで話をさせて貰える機会があったので。</p>
<p>興味がコロコロ変わってしまうのが問題点かもしれないです。少しは地に足をつけてなんかやったほうがいいのになぁ</p>
<p>###今年の振り返り
今年はこんなかんじでしょうか。</p>
<ul>
<li>いろんな勉強会に出没</li>
<li>#pyfesでelasticsearchのさわりを紹介</li>
<li>個人的にJIRA導入</li>
<li>MIR輪読会開始</li>
<li>Solr勉強会の主催を引き継ぐ</li>
<li>Kindle paperwhite当たった</li>
</ul>
<p>相変わらずいろんな勉強会に出てました。最近は勉強しに行くのもありますが、人に会いに行ってるような気がします。
勉強会から帰ってきて復習したりしないとすぐ忘れちゃうんですよね。。。</p>
<p>#pyfesではelasticsearchの入門中という話をしてきました。
人の多さに緊張しまくりでよくわかりにくい発表になってしまったのが反省点かと。。。
もっと話をする機会を増やすとなれるのでしょうか。</p>
<p>JIRAの導入はパラで仕事をしているとタスク管理がおろそかになってくるので導入しました。
フリーのredmineを考えていたのですが、インストールが簡単なJIRAを導入しました。
有料ですが、1-10人までなら、ダウンロード版が$10ですし。いろんな時間を買ったと考えると安いかと</p>
<p>検索に関してもまだまだ初心者に近いということでModern Information Retrievalという英語の本の輪読を主催しました。
Twitterでの呼びかけに答えてくれた方々に感謝です。
まだ、半分も行ってないですが、何周かしたいと思っています。</p>
<p>Solr勉強会の主催者も引き継ぎました。Atnd立てるの手伝おうか？って言ったら主催者になってましたｗ
来年も開催するので、喋りたい方、こんな話を聞きたいなどTwitterやブログでコンタクトいただければと。</p>
<p>あと、会社の忘年会で思いがけずKindle paperwhiteが当たりました！（やったー！）
iPadを持っていることもあり、完全に購入する気も無かったのですが。。。
使ってみると結構いいです。画面のギラツキがないのと、他のソフトがないのがいいです。
読書に没頭できます。通常はKindleを持ち歩くことになるかと。（没頭できるけど、読むスピードが遅い＋すぐ忘れるのが難点。。。）</p>
<p>###来年の抱負
来年の抱負も書いておきます。
昨年の前科があるのでどこまでできるかわかりませんが。
すぐに興味が変わってしまうのもありますし</p>
<ul>
<li>IntelliJ IDEAをメインに使う</li>
<li>JIRAを継続して活用</li>
<li>ブログの継続</li>
<li>elasticsearch</li>
<li>Luceneのソースコードリーディング</li>
<li>何かOSSのソースを読む</li>
<li>Macでもっと開発</li>
<li>読書と英語を継続</li>
</ul>
<p>IDEA（アイデア）は11をインストールしてたのですが、使えていませんでした。
イメケンが主催したJetBrainsユーザー会で洗脳されて＋タイミング良い75%オフセールにやられて購入しました。
普段はEclipseを使っているのですが、これを機にチャレンジしようかと。
少し変化をしてかないとすぐ取り残されちゃうので。。。</p>
<p>JIRAはまだタスクを登録してクローズするという初歩段階の使い方しかできてないので、少しずつ研究してフィールド追加したり、ワークフローを変更してみたりと研究したいです。</p>
<p>ブログは、今年後半が勉強会参加ブログしかないのを反省して来年は、もう少しSolrやLucene、lucene-gosenについて書いて行きたいです。
（すくなくとも、4.0で動かす話を書かないと。。。）</p>
<p>elasticsearchはまぁ触れる環境を維持しようという話です。
どこかで導入できればいいんでしょうが、なかなか。
（勉強会開いてみるのも面白いかな？）</p>
<p>Luceneソースコードリーディングは誘われてるのに、ちゃんと返事をしていなかったので。
来年2月のどこかの土日を候補日にする予定です。</p>
<p>何かOSSのソースも読もうと思ってます。
「何か」はまだ決めてないですが、人のソースを実はあまり読んでないなぁと。
Vさんと話をした時に人のソースを読むのがいいですよという話になったので。
真似をするためにもなんか読みたいなぁと。Javaになるのかなぁ。</p>
<p>とある事情で週4はWindowsで開発してます。
Macはちょっとしか触ってないのでMacの環境が成長していないのが難点です。
できれば、Macで開発を継続できる環境を作りたいなぁと。Kobitoとか入れてるけど触ってないし。。。
IntelliJも入れたのでMacで開発したいものです。</p>
<p>最後は毎年のことですが、本を読むスピードが買うスピードに追いついてないと。。。
英語の本もかってるんですが、これまた読むスピードが。。。
ということで、ざっくり目を通す読書をもっとやりたいなぁと。
買うのはいいのですが、必要にならないと呼んだ内容を忘れてしまうので、読んだあとに実践する機会も設けたいと思ってます（思ってるだけかもしれないですが）</p>
<p>あと、来年もいろんな人に会うためにいろんな勉強会に出没すると思いますので、声をかけていただければと
また、不抜けているのを見かけたらカツを入れてください。（技術内容のブログ書けとか、本読んだ感想をブログにかけとか）
来年もTwitterなどで絡みまくると思いますが、よろしくです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>JIRAのDB移行（HSQLDBからPostgreSQLへ）#augj(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/12/20/jira%E3%81%AEdb%E7%A7%BB%E8%A1%8Chsqldb%E3%81%8B%E3%82%89postgresql%E3%81%B8-augj/</link>
      <pubDate>Thu, 20 Dec 2012 20:18:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/12/20/jira%E3%81%AEdb%E7%A7%BB%E8%A1%8Chsqldb%E3%81%8B%E3%82%89postgresql%E3%81%B8-augj/</guid>
      <description>Atlassian Advent Calendar 2012の20日目。（急遽参戦） （某イケメンにMT（改変リツイート）されたので書いてみました） 久々に、勉強会以外のエントリです。（So</description>
      <content:encoded><p>Atlassian Advent Calendar 2012の20日目。（急遽参戦）
（某イケメンにMT（改変リツイート）されたので書いてみました）
久々に、勉強会以外のエントリです。（Solrとかlucene-gosenじゃないんだが。。。）</p>
<p>私は、最近忘れっぽくなってきてしまったので、さくらVPSを借りてJIRAを立てて、タスク管理とかに使い始めました。
有料ツールなので、まずは、評価版から入れましたと。
評価版をインストールするときに、楽なのでHSQLDBを選択していました。が、
インストール時に「HSQLDBは評価版だけで使用してください。正式版ではサポートしてないです」みたいなことを言われていたので、PostgreSQLに切り替えようかと。
ちなみに、MySQLも選択肢としてはあったのですが、PostgreSQLのほうが触ってるし（つかいこなせてるわけではない）、補完機能になれてるというのもありPostgreSQLを選んでみました。</p>
<p>作業のログとして、ブログ書いとこうと思いたったのでメモ的に残しておきます。</p>
<p>作業の流れはこんなかんじでした。
（基本的にはサポートサイトの<a href="https://confluence.atlassian.com/display/JIRA052/Switching+Databases#SwitchingDatabases-differenttype">ここ</a>に載ってます。サポートいいね。）
この手順通りでいいみたい。</p>
<p>###1.  JIRAのデータのバックアップ（JIRAの管理画面からバックアップ）</p>
<hr>
<p>まずは管理画面へ。ページ右上に管理へのリンクがあるのでクリック。

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548699.jpg" alt="管理画面へ"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548699.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>開いた管理画面の右下あたりにエクスポート（バックアップ）へのリンクがあります。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548700.jpg" alt="管理画面右下のこのへん"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548700.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>クリックしたあとに、エクスポートのファイル指定画面が現れるので、ファイル名を指定してバックアップボタンを押します。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548714.jpg" alt="ファイル指定画面"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548714.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>バックアップが完了するとバックアップファイルの場所が表示されます。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548701.jpg" alt="完了画面"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548701.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>###2. JIRAの停止</p>
<hr>
<p>バックアップが終わったのでJIRAを停止します。</p>
<p>###3.  PostgreSQLのインストール（yum）
PostgreSQLが入ってないので、yum installします。（軟弱者なので）</p>
<pre><code>
yum install postgresql-server
</code></pre><p>####initdbで四苦八苦
PostgreSQLはインストールしただけでは起動できません。なので、初期化をします。
yumでインストールしたPostgreSQLだとつぎのコマンドでinitdbできるようです。（これが罠でした）</p>
<pre><code>
service postgresql initdb
</code></pre><p>このコマンドでinitdbすると、encodingが指定できませんでした。（私がしらないだけという話も。。。）
encodingを指定できていないと、このあと4.を実行するときにエラーが出ました。（これも回避方法あるのかも。）
ということで、結局、昔とった杵柄なやり方でinitdbコマンドで&ndash;encoding=UTF8 &ndash;no-localeを指定しました。</p>
<pre><code>
initdb --encoding=UTF8 --no-locale -D/var/lib/pgsql/data
service postgresql start
</code></pre><p>これで初期化が完了するので、PostgreSQLを起動します。</p>
<p>###4.  JIRA用DBとユーザの作成</p>
<hr>
<p>まずは、ユーザの作成。そして、DB作成</p>
<pre><code>
createuser jirauser
createdb -E UNICODE jiradb
</code></pre><p>テーブルなど作成しません。あとの処理がやってくれます。</p>
<p>###5.  JIRAにJDBCドライバを入れる（必要なかった。）</p>
<hr>
<p>warじゃないJIRAをインストールした場合はすでに入ってるみたいです。
ここを見るとわかります。JIRA Installation Directoryの下のlibですかね。</p>
<pre><code>
/opt/atlassian/jira/lib
</code></pre><p>###<del>6.  マニュアルでPostgreSQLの接続設定（あれ、これいらないのか。）</del></p>
<hr>
<p>~~　コンソールオンリーなので、JIRAが提供してくれているツールが使えないため、手で設定。
　/var/atlassian/application-data/jira
　にあるdbconfig.xmlを修正~~
ちゃんと英語読めってことですね。。。必要なかったです。。。</p>
<p>###7.  JIRAのdbconfig.xmlを削除</p>
<hr>
<p>DB接続の設定を初期セットアップウィザードで再設定するため、dbconfig.xmlを消します。
（一応、インストールディレクトリとホームディレクトリとかバックアップしたほうがいいみたい）
場所はここ。
中身はdbcpとかのデータソースの設定に似てました。
今は、HSQLDBのJDBCドライバが記載されてました。</p>
<pre><code>
rm /var/atlassian/application-data/jira/dbconfig.xml
</code></pre><p>###8.  JIRAの起動</p>
<hr>
<p>7.のファイルを消したあとにJIRAを起動します。</p>
<pre><code>
service jira start
</code></pre><p>###9.  JIRAにアクセスとDB接続設定</p>
<hr>
<p>アクセスすると、インストールした時と同じDB接続の指定を行うセットアップウィザードの画面が現れます。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548715.jpg" alt="最初のセットアップ画面"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548715.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>今回はJIRAサーバの外にあるPostgreSQLなので、「外部」を選んで四角で囲んだ部分を入力します。
入力したら、「接続テスト」を押して接続できることを確認したら「次へ」を押します。
マシンのスペックにもよると思いますが、ここでJIRAサーバがDBに接続してテーブルのCREATEなどDBのセットアップを実行してるのでちょっと時間がかかります。</p>
<p>###10. バックアップしたデータのインポート</p>
<hr>
<p>DBのセットアップが終わったら、アプリケーションのセットアップ画面が出てきます。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548716.jpg" alt="インポートファイル指定画面へ"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548716.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

新規インストールではないので、「ここ」にある「既存のデータをインポート」リンクをクリックします。
すると、インポートファイルの指定画面が現れるので、1.でバックアップしたファイル名を指定してインポートします。
すると、インポート中画面が出てきます。


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121220/20121220_2548817.jpg" alt="インポート中画面"/>
    </div>
    <a href="/images/entries/20121220/20121220_2548817.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

で、インポートが完了すると無事JIRAにログインできますと。</p>
<hr>
<p>とこんなかんじです。
私の場合は、まだJIRAをインストールしてから間もないため、チケットの数が少なかったり、添付ファイルが無かったりなので、すぐにインポートが完了しました。
添付ファイルがある場合は、別途添付ファイルが保存されているディレクトリのバックアップと復旧などもあるみたいです。
最初にも書きましたが、ドキュメントのサイトに「Switching Databases」という項目があり、そのページの手順で問題なく切り替えできました。
次は、定期的にデータをバックアップするのを考えないとですかね。</p>
<p>いやぁブログ書くのが、思いの外、手こずりました。
手こずった原因はスクリーンショットだったんですけどね。。。
safariでページ全体のスクリーンショットがとれなかったので、結局Chromeで<a href="http://awesomescreenshot.com/">Awesome Screenshotプラグイン</a>使いました。
safariの拡張もあるんですが、ページ全体のスクリーンショットがうまく動かないみたいです。。。
ということで、備忘録でした。（次はちゃんとSolrとかの記事書かないとなー。その前に忘年ブログかな）</p>
</content:encoded>
    </item>
    
    <item>
      <title>第一回 JetBrainsユーザーグループ #jbugj に参加してきました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/12/11/%E7%AC%AC%E4%B8%80%E5%9B%9E-jetbrains%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%82%B0%E3%83%AB%E3%83%BC%E3%83%97-jbugj-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 11 Dec 2012 21:22:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/12/11/%E7%AC%AC%E4%B8%80%E5%9B%9E-jetbrains%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%82%B0%E3%83%AB%E3%83%BC%E3%83%97-jbugj-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>イケメンが主催した「第一回 JetBrainsユーザーグループ #jbugj」に参加してきました。 いろんな製品があるんだなぁと。入り口には良いイ</description>
      <content:encoded><p>イケメンが主催した「<a href="http://www.zusaar.com/event/450003">第一回 JetBrainsユーザーグループ #jbugj</a>」に参加してきました。</p>
<p>いろんな製品があるんだなぁと。入り口には良いイベントだったと思います。
個人的に、WebStorm、IntelliJが気になっていたので、参加しました。（あとは、イケメンがやってるイベントだからというのもあるかも）
ちなみに、MBAにしてからインストールはしていたのですが、ほぼ触っていなかったのでこれを機会にちょっと触ってみようかなぁと。</p>
<p>発表は、全般的な製品紹介、ライセンスの紹介から、ライブコーディングあり、ここがいいよ！という話ありという感じでした。</p>
<p>MTLさんのおしゃれな空間で発表を聞いたので、いつもとは少し違った感じでしたでしょうか。</p>
<p>私自身は、発表を聞きつつ、IDEAのCEで色々やってました。
一応、Lucene/SolrのSVNからチェックアウトしたり、lucene-gosenのtrunkをチェックアウトしたり、個人のbitbucketからlucene-gosen-wikiparseとかcloneしてました。
Eclipseで作ったプロジェクトなので、cloneとかチェックアウトしたあとに、Eclipseと並行開発するとどうなるのかなぁ？というのがちょっと気になります。（普通は移行しておしまいなんだろうけど。。。）
ちなみに、lucene-gosenのプロジェクトもSVNからチェックアウトしたらすんなり使えそうでした。
こんな感じ。（lucene-gosenのプロジェクト）

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20121211/20121213_2542798.jpg" alt="IntelliJ IDEAでlucene-gosenのプロジェクトを取り込んでみた"/>
    </div>
    <a href="/images/entries/20121211/20121213_2542798.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

とりあえず、見た目がカッコイイｗ</p>
<p>いくつか触ったり、聞いていて気になったのはつぎのような点です。ちょっとずつ触る機会を作った時に調べてみようかなぁと</p>
<ul>
<li>Eclipseのワーキングセット相当の機能はどうするのか？→<a href="http://d.hatena.ne.jp/masanobuimai/touch/20121024">モジュールとかでやればいいらしい。</a></li>
<li>プラグインってどんなものがあるの？探す方法とインストールの仕方</li>
<li>JDKの複数の切り替え方とか</li>
<li>Eclipseでやってる人とIntelliJでやってる人の混在チームでの開発とか</li>
</ul>
<p>で、いつものごとく、懇親会に参加しました。
@mike_neckさんや@sue445さんとお話できたのですが、VCSにEclipseの設定ファイルとか挙げないほうがいいですよねとか、テストが無いプロジェクトってないわーなどの話を聞いて、やっぱり自分はまだまだなんだなぁと認識できるとか面白かったです。
lucene-gosenのSVNにはEclipseの設定ファイルとかアップされてるからなぁ。ホントはAntとかMavenのターゲットでこれらの設定ファイルができるようにしとくのがいいんでしょうねぇ。
まだまだ勉強することだらけですが、コツコツ教えてもらいます</p>
<p>ちなみに、イベント参加者にはショートカットキーが印字されたキートップシールや2ヶ月有効な評価ライセンスがあとからもらえるなどの特典もありました。
普段はEclipseなのですが、運良くライセンスが当たったら本気で乗り換えようかなぁｗ</p>
<p>ちなみに、Eclipseでもそうですが、私自身はショートカットキーをほとんど覚えてない軟弱者ですので、メニューの場所などを覚えれば乗り換え自体はこんなんじゃないかと。
Mac自体にKeyRemap４MacBookを入れていて、Emacsっぽいショートカットを使うので、覚えられないというのもありますが。
ライセンス当たらないかなぁー</p>
<p>以下はいつもの通り、聴きながら自分用にメモしたものになります。</p>
<hr>
<pre><code>
日時：2012/12/11（火）19:00-21:00
場所：メディアテクノロジーラボ 
</code></pre><p>イベントサイト：<a href="http://www.zusaar.com/event/450003">http://www.zusaar.com/event/450003</a></p>
<pre><code>
◎JetBrains製品群、ライセンス形態などの紹介 - @yusuke
　・日本でヤル気あるの！？→あるある！→じゃあ、ユーザグループ立ち上げるぜー
　・あくまで情報交換の場を設けるだけ。
　・アンケート結果
　　IntelliJが1番人気
　　WebStromが2番
　・ステッカー（ちょっとかっこ悪いかも。。。）
　・2名限定でIDEAのライセンスプレゼント！
　　ブログ書いて、応募用フォームに登録してランダム抽選
</code></pre><pre><code>
◎IntelliJの基本(インストール～プロジェクト作成、テスト、実行までのウォークスルー) @yusuke
　・JetBRAINSはチェコの会社
　・IntelliJはなんでもできる（AppCodeは違うけど）
　・.Net系もある（書きそびれた）
　・YouTrack：課題追跡
　・TeamCity：継続インテグレーション
　・読み方は「イデア」じゃなくて「アイデア」だよ！
　・Community Editionはあんまり機能がない。
　　OSSライセンスだと、Ultimateの機能が使えるよと。
　・仙台の人が一人アドベント・カレンダーやってます。
</code></pre><p>　　<a href="http://d.hatena.ne.jp/masanobuimai/20121201">http://d.hatena.ne.jp/masanobuimai/20121201</a></p>
<pre><code>
　・ライブデモ
　　黒いインタフェースにするのどーすんだろう？
　　→Preferences→AppearancesでThemeで「Darcula」を選択すると黒くなる。
　・やべ、時間過ぎてたｗ終わり！

◎IntelliJのここが気持ちいい！→普通IntelliJでしょ？ @mike_neck
　・DQXやってます。
　・僕とeclipse
　　・新規クラス作成：3ステップかかる
　　・補完の素早さ：
　　・SprintFramework：Ultimateなら対応してるよ。
関係ないけど、TLで質問したので。
　Eclipseのワーキング・セットに相当するもの。
</code></pre><p>　<a href="http://d.hatena.ne.jp/masanobuimai/touch/20121024">http://d.hatena.ne.jp/masanobuimai/touch/20121024</a></p>
<pre><code>
◎LT
　◯WebStormとRubyMineについて @sue445
　　・JavaScriptのIDEとして最強
　　　jsの補完が素晴らしい。（外部のJSはプロジェクトに入れないと難しい。）
　　　jsTestDriver pluginがデフォルトで入ってる！
　　・RubyMine
　　　RubyのIDE
　　　ソースを追うのが楽。
　　　erbのインポートしたものも追っかけられるの便利。
　　　viewからhelperクラスにも飛べる。
　　QA
　　　UMLとか出せるの？→出せるよ。
　　　DBクライアントもあるよ。
</code></pre><pre><code>
◎JetBrains発のJVM言語Kotlinの紹介 - @ngsw_taro
</code></pre><p>　<a href="http://atnd.org/events/34627">アドベントカレンダー（一人）http://atnd.org/events/34627</a></p>
<pre><code>
　・社畜してます
　・Androidアプリとか作ってる。
　・Kotlinな活動
　　Pull Requestしてる
　・マイルストーンなど。
　　M4が3時間前に出た！
　・どんな言語？
　　静的型付け、オブジェクト指向、関数型プログラミング的、JSへコンパイル可
　　産業利用目的（初めて聞いたかも、この言葉ｗ）
　・Java大変だよね。
　　互換性問題とか
　・ライブコーディング
　　大変そうだｗKUnitってのもあるらしい。
</code></pre><p>　（ゴメンナサイ、IDEA触ってて、流してました。。。Eclipseで作ってたプロジェクトをBitbucketから落としてきて四苦八苦してた）
　（参考にしたサイト（ググった）：<a href="http://d.hatena.ne.jp/waman/20100506/1273166533">http://d.hatena.ne.jp/waman/20100506/1273166533</a>）</p>
<pre><code>
◎AppCodeについて
　・IDEAには含まれないので、別途ラインセンス購入が必要。
　・XcodeとAppCodeの違いをライブコーディングで説明。
　・XCodeよりも補完が良くできてるっぽい。
　GUIはXCodeで作って、コーディングはAppCodeでやるってのがいいですよ。
　※私が、iOS系のアプリの開発ってやったこと無いからよくわかってないです。。。
　　XCodeはコードフォーマットがないのが辛い by @yusuke

</code></pre></content:encoded>
    </item>
    
    <item>
      <title>#DSIRNLP 3.5に参加しました。＆「Emacs実践入門」を頂いちゃいました！(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/11/29/dsirnlp-3-5%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9Femacs%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80%E3%82%92%E9%A0%82%E3%81%84%E3%81%A1%E3%82%83%E3%81%84%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Thu, 29 Nov 2012 02:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/11/29/dsirnlp-3-5%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9Femacs%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80%E3%82%92%E9%A0%82%E3%81%84%E3%81%A1%E3%82%83%E3%81%84%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>ヒカリエに行ってみたいという不純な理由で参加してきました。 新しいということもあり、おしゃれで綺麗なカフェでした。 入り口にはおっきな人形も立っ</description>
      <content:encoded><p>ヒカリエに行ってみたいという不純な理由で参加してきました。
新しいということもあり、おしゃれで綺麗なカフェでした。
入り口にはおっきな人形も立ってたし。
渋谷の夜景も見下ろせて素敵な場所でした。</p>
<p>で、内容です。残念ながら、本編の3回めには参加していなかったのですが、
今回も濃い話が聞けたので楽しかったです。
論文読まなかったり、基礎を勉強したのに忘れてたりと抜けてる部分が多いので、
こういう機会が与えてもらえるというだけで目からうろこです。</p>
<p>@kumagiさんの<a href="http://www.slideboom.com/presentations/655201/%E3%81%82%E3%81%AA%E3%81%9F%E3%81%AE%E7%9F%A5%E3%82%89%E3%81%AA%E3%81%84%E3%83%8F%E3%83%83%E3%82%B7%E3%83%A5%E3%83%86%E3%83%BC%E3%83%96%E3%83%AB%E3%81%AE%E4%B8%96%E7%95%8C">「あなたの知らないハッシュテーブルの世界」</a>はハッシュテーブルの基本的な話から、最近の論文で発表されてる内容までをカバーする幅広いお話で面白かったです。
（大学でやってると思うんだけどすっかり抜けてる自分がなんとも。。。）
こういうコアな中身も知ってると、色々とプログラム書いたりするときの見方や考え方も変わってきますよね。
（そんなプログラム書いてないけど。。。）
で、随分おとなしい内容だなぁ？と思いきや、途中からちゃんとLock-Freeの話も出てきてさすがと感心させられましたｗ
最後はJubatusの宣伝まで入ってたし。（某氏のすごい写真入りで。。。）</p>
<p>@hitoshi_niさんの文書要約の話は、NLPに興味があるので、楽しみにしていた内容でした。
今回もなめらかによどみなく喋られる発表にただただ感心させられるばかりでした。
内容は中級編ということで、文書要約のキモになる処理の文章の短縮の話です。
係り受け木を元にする手法をわかりやすく説明されて、もうなんか、すぐに実装できちゃうんじゃないかと錯覚してしまう始末でした。
係り受け解析というと、CaboChaを思い浮かべてしまうんですが、きっと違う実装なんだろうなぁ。
入門編と次回の重要文抽出の話も聞きたいなぁと。</p>
<p>最後に、技術評論社さんから「Emacs実践入門」など3冊の書籍のプレゼントまでありました。
その他の2冊は購入済みだったのですが、Emacs本は購入したいリストに入れたままでした。
ということで、欲しいですというアピールをしてゲットしてきました！
Emacsはなんだかんだで、もう10年以上使っていますが、そこまで深入りしないような使い方をしていました。
これを機に、再入門してもっと使いこなせるようになろうかと。
また、読了したタイミングでブログに感想かきます。</p>
<p>ということで、以下はいつもの自分用のメモになります。おかしいところ、それ書いちゃダメでしょ的なところのツッコミをいただければ。</p>
<hr>
<pre><code>
日時：2012年11月28日(水) 19:00
場所：渋谷ヒカリエ27F NHN Japan カフェ


◎開会、諸注意など　@overlast
　人材募集、会場説明など。
　前回、本をもらった人はブログ書いてね。オライリー様より
　今回も本のプレゼントあり。技術評論社様より

◎あなたの知らないハッシュテーブルの世界(30分 + 質疑応答10分)　@kumagi さん
　・まずは前提。
　　データの集合を扱いたいよね
　　配列でもできるね。けど、データ増えるとキツイね。
　・ハッシュ関数の話から。
　　リハッシュとかの話
　・ClosedAddressingとOpenAddressingの話
　・ClosedAddressingの場合、ポインタ使ってるからキャッシュミスあり。
　　メモリとかの話
　・OpenAddressingメモリに乗るのでキャッシュミスは少ないけど、削除データの扱いがちょっと大変
　　→削除がいっぱい有ると処理が面倒
　・RubyはClosedAddressing、PythonはOpenAddressing
　　memcachedはClosedAddressing
　・Cuckoo Hashing（2001）
　　密度50%以上になると急にコストが高くなる。
　　挿入がすごく遅くなる。追い出し操作が増えるから？
　・そこで、Hopscotch
　　ググった参考ページ：http://shnya.jp/blog/?p=639
　　http://en.wikipedia.org/wiki/Hopscotch_hashing
　　密度が上がっても性能劣化がない。
　・C++でHashtableが欲しくなったら、google_sparse_hashとdense使うよと。
　・ConcurrentHashmapのお話
　　テーブル部分がvolatile、Chain部分はfinal
　　insertはChainの先頭に。
　　削除は遅い。ReadCopyUpdate。
　　空でも1.7M（K？）持ってく
　・ここからはLock-free系
　　・Lock-Free Hash Table
　　　http://www.azulsystems.com/events/javaone_2007/2007_LockFreeHash.pdf
　　　HotSpot VMの人のもの？こんなのやってる。http://www.0xdata.com/faq.html
　　・（聞きそびれた）
　　・日立謹製Lock-free hashtable
　　　日立のDBで使ってる部品？
　　　ベンチマークが胡散臭い
　　・最後はJubatusのCM
　
◎文書要約入門 中級編(40分 + 質疑応答10分)　@hitoshi_ni
　・画数が少ないです。
　・ヒカリエ綺麗ですね。
　・文書要約とは？
　　「機械に」要約させる。
　・なんで要約？
　　長い文章読みたくない。人件費の削減
　・どうやって要約？
　　1.文分割：文書を文に分割
　　2.文短縮：就職説を削除するなどして、原文より短い文の亜種を出す。
　　3.重要文抽出：要約にふさわしい文を選び出す。
　・今回は文短縮について
　・動機
　　長い文は文抽出で扱いにくい
　　文の中にも重要なところとそうでないとこがある
　・係り受け木の剪定すると短くできると。
　　剪定のルール
　　　中間ノードは落としちゃダメ
　　除去の時に考えること
　　　重要度
　　　言語
　・重要度？
　　文節に点数を付ける
　　文書集合中の出現頻度とかを採用。訓練データからでもいいよ（ロジスティック回帰とか）。
　・言語尤度
　　言語としての尤もらしさ
　　典型的にn-gram言語モデルを使う
　・そして探索
　　基本的には2値ラベリング
　　ビタビアルゴリズムではだめ。係り受け制約が考慮できない
　　ナイーブいはビームサーチをする。
　・文短縮の評価
　　・人間が書いた短縮文と比較
　　・ROUGE-Lという尺度などで評価（これしらないなぁ。）
　・幾つかの論点
　　係り受け解析しない
　　文節じゃなくて、単語単位でもいいよねとか。
　Q：硬い文章以外の要約ってやってるの？
　A：あります。
　　　技術的な話だと、係り受け解析がうまく出来ればできる。
　　　係り受け解析がうまくいけば、そこまで大変じゃない。
　Q：短さが短くなるほど難易度があがるけどどこまでやってます？
　A：短くすればするほど難しい。これは情報の欠落が激しくなるから。
　　　文法性を担保するのも難しい。
　　　10文字くらいならできそう。
　Q：実例としてどのくらいの長さをどのくらい短くしてる？
　A：ある程度の長さを20文字にしてくれとか。Twitterに入るくらいにしてくれとか。
</code></pre><hr>
<p>頂いちゃいました！


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4774150029/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4774150029&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4774150029/?tag=johtani-22">
      Emacs実践入門　～思考を直感的にコード化し、開発を加速する (WEB&#43;DB PRESS plus)
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>第9回Solr勉強会を主催しました。#SolrJP(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/11/26/%E7%AC%AC9%E5%9B%9Esolr%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%82%92%E4%B8%BB%E5%82%AC%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-solrjp/</link>
      <pubDate>Mon, 26 Nov 2012 18:44:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/11/26/%E7%AC%AC9%E5%9B%9Esolr%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%82%92%E4%B8%BB%E5%82%AC%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-solrjp/</guid>
      <description>第9回Solr勉強会に参加しました。 皆勤賞です！というか、主催者になってしまいました。 まだ不定期の開催になると思いますが、話をされたい方など</description>
      <content:encoded><p><a href="http://atnd.org/events/33718">第9回Solr勉強会</a>に参加しました。
皆勤賞です！というか、主催者になってしまいました。
まだ不定期の開催になると思いますが、話をされたい方などいらっしゃいましたら連絡いただければ助かります。</p>
<p>今回も面白い話が聞けました。</p>
<p>最初はKuromojiの開発者でSolrのコミッターでもあるChristianの発表です。
Solr勉強会では初の英語の発表だったんじゃないでしょうか。
Atilikaでやってることの紹介から、Kuromojiの紹介、今後の改良に関する話とひと通り話してもらいました。
途中でKuromoji使ってる人？などの質問があったのですが、残念ながら反応が薄かったです。
漢数字をアラビア数字でも検索できるようにするチケットなど、今後のKuromojiの発展も楽しみです。
（コメントとかパッチを送れというプレッシャーもあったので、パッチ頑張って書きます。。。）</p>
<p>つぎはニコ生でのSolrのお話。結構、赤裸々（前任者がいない状態で引き取ったとか）に語っていただき、ハラハラしながら聞いてました。
やはり、新語や略語で苦労されてるんだなぁと。
複数のサービスや開発者に対してSolrの環境を提供するという話はなかなか興味深かったです。
いろんな人がSolrを触るような状況になってきてるんだなぁと。
基盤となるラッパーのようなフレームワークとかも作ってるのかなぁ？
今後は台湾語や英語への展開も考えられているようなので、<a href="http://wiki.apache.org/solr/LanguageDetection">Language Detection</a>などを利用してみた感想とその内容を今度発表してもらいたいですねｗ</p>
<p>つぎはFacetPivotの話です。
昔から要望が出ていたのですが、4.0系でやっと使えるようになりました。
ファセットはSolrの売りの一つだと思います。
最初はこの考え方がしっくりこない人もいるかもなぁと。特にデータをどのように作れば、いいのかって悩むこともあります。
その悩んだ内容について発表してもらいました。
実際にどうやって使うかを悩んだ内容を発表してもらうのもいいなと思いました。</p>
<p>最後はSolr勉強会なのに、elasticsearchの洗脳会になってましたｗ
elasticsearchはSolrと同じ、Luceneをコアに採用した検索エンジンサーバーになります。
Solrとは別のアプローチでLuceneをラップし、REST APIでアクセスしやすくしたプロダクトです。
Luceneのコミッターの方もelasticsearchの開発に参加しています。
分散インデックスを念頭においた設計や、インストールが簡単なプラグイン構造といったSolrとは違ったアプローチがなされており、面白いものになっています。
残念ながら日本で利用されているという話はまだ聞いた事ないですが、だからこそ、触ってみて事例を紹介してみるのも面白いのではないでしょうか。
今回紹介したKuromojiも使えるようになっていたりしますので、日本語でもある程度使えると思います。</p>
<p>以上が簡単ですが感想です。主催者だったのに、@hirotakasterさんや@ajiyoshiさんに受付などをやっていただいたので、いつもの様にしっかり話を聞いてしまいました。
発表者の方、会場提供いただいたVOYAGE GROUP、お手伝いいただいた皆さんに感謝です！</p>
<p>今回初の主催でしたが、本当に助かりました。たどたどしい説明や紹介など至らない点も多々有ったかと思いますが、今後もよろしくお願いいたします。</p>
<p>主催者的な立場として感じたことも書いておこうかと。
無料の勉強会で、ATNDという参加しやすい環境というのもあるかもしれないのですが、キャンセルをきちんとしていただくほうがいいなと思っています。
幸いにもSolr勉強会はここ数回は盛況で、キャンセル待ちの方が結構いらっしゃいます。
ギリギリまで業務との兼ね合いを見つつ、参加しようと思っていらっしゃる方もいると思うのですが、キャンセル待ちで行けるかな？どうかな？と思っている方もいらっしゃいます。
ドタキャンは問題ないのですが、キャンセルせずに欠席は出来れば避けていただけると助かるなぁと。
（残念ながら、きちんと集計をとれなかったので、次回からは集計取ってみようかなぁと）</p>
<p>次回の開催は今のところ未定です。発表してみたい方、こんな話を聞いてみたいなど、気兼ねなく連絡いただければと思います。
このブログにコメントを頂いてもいいですし、ツイートしていただいてもいいので、反応をいただけると嬉しいです。
また、今回至らない点があったなどのツッコミ、批判も気兼ねなく言っていただければと思います。
今後の反省点にもしたいので、ぜひ反応をいただければと！</p>
<p>懇親会でも色々な方とお話できました。（もう少し、Christianと英語で話す努力とかしないとなぁ。。。）</p>
<p><del>とりあえず、メモをアップしときます。
リンクとか感想とかはまた（飲んだ）後で。。。</del></p>
<p><a href="http://togetter.com/li/413428">togetterでまとめてももらったみたいです</a>。ありがたいです。</p>
<hr>
<pre><code>
第9回Solr勉強会
場所：VOYAGE GROUP 会議室
日時：11/26(月) 19:00～21:00

1. Atilika Inc.　Christian Moenさん
　　タイトル：Who we are, what we do, and a little bit about Kuromoji
　◎Atilikaの紹介。
　　会社の目指すもの
　　・BigData、検索、NLP
　◎プロダクト
　　Kuromoji：形態素解析エンジン
　　Akahai：日本語クエリサジェストエンジン
　　Keywords：日本語キーフレーズ抽出
　◎Kuromojiの紹介
　　3.6からデフォルトで使える。

　◎将来の改良の話。
　　・踊り字対応（コミット済み）
　　・漢数字に関するチケット＆パッチのお話。
　　・ユーザ辞書の重複エントリ改良とか（すみません、パッチ書きます。。。）

2. 株式会社ドワンゴ　吉村総一郎さん(@sifue)
　　タイトル：Solr@ニコニコ生放送
　◎ニコニコ生放送の紹介
　　・1日に10万番組。。。
　　・10/17にバージョンQをリリースしたら、トップはひどい叩かれようでした。。。
　◎これまで。
　　Jackrabbit→Lucene→Solr→ニコ生のSolr
　◎退職者と入れ替わりでSolr担当。。。
　　今回は資料と環境を調べて発掘した機能のお話。。。
　◎機能
　　キーワード検索。論理クエリ、などなど。
　◎利用してる環境
　　3.4ベース＋Jetty
　　マスタスレーブ構成（スレーブ2台）
　　途中は分散インデックスを自分で実装？
　　ボトルネック自体がDBからのデータ収集だった
　◎インデックス対象
　　・見れるのは過去1週間と過去の公式番組すべて。
　　　この部分だけ検索可能。
　　・更新頻度の高い情報に「来場者数」「コメント数」
　◎インデックス作成
　　・バッチにて更新
　◎アナライザ
　　CJKTokenizerFactoryを利用
　　HTMLStripCharFilterFactory
　　Bi-gramなので、「FF」とか「DQ」に弱い（FF1でFF13とかヒットしちゃう）
　　検索精度は悪いと言われてるみたい。
　◎1日のリクエスト
　　ピーク時40QPS程度
　　5分おきにスパイクがある。（ユーザが作ったツールによる検索とか。。。）
　◎UPDATEリクエスト
　　ピーク時は80QPS
　◎開発用のJettyのマルチテナント機能を利用したSolr環境の提供
　◎台湾語とか英語もやりたいなぁ。　　

3. 株式会社マーズフラッグ　柳吾朗さん(@hitode7456)
　　タイトル：ドリルダウン色々
　◎Facetの紹介から
　◎楽天でのドリルダウン例（これはFacetの紹介での例であり、実際にSolrが利用されているかはわからないです。）
　◎多段ドリルダウン（ファセット）のお話。
　　アプリを実装するときの考え方とか。
　◎実直形、工夫形、PivotFacet
　Q&amp;A
　　Q：3つの性能系のコストは？
　　A：まだ調べてないです。残り2つは工夫形がいいですよ。と
　　次回、調べた結果の発表もやってほしいなぁ。

4. 兼山元太さん (@penguinana_) https://speakerdeck.com/penguinco/solrtoelasticsearchfalsebi-jiao
　　タイトル：SolrとElasticsearchの比較
　◎クックパッド！
　◎elasticsearchの紹介
　◎比較サイトもあるよ！
　　http://solr-vs-elasticsearch.com
　◎サンプルデータ・セット（ライブドアグルメ）でサンプル実装。
　　https://github.com/penguinco/ld_gourmet_search
　◎APIの紹介
　　REST APIがちゃんと造られてますよと。
　　設計時点でコレクションなどがURLに含まれてるのがいいよねと。
　◎_analyzeによりアナライザーもAPIとして公開されてるよと。
　◎Kuromojiも対応してるよ！
　◎DynamicFieldよりも便利だよ。
　◎クエリのDSLが違うのでちょっとアレ。
　◎スコアリングも色々できるよ。
　◎感想
　　・機能面の不足なし
　　・APIがいい
　　　コア追加とか、curlだけでできるのがいい。
　　・習得が容易（Solrやってると機能とか似てる）
　　・大規模じゃなくても使えそう

　◎分散検索がデザイン時に組み込まれてるのがいいよね。
　　write consistencyなどがインデックスごと（コレクションごと？）に設定可能なので便利。

　◎multi-tenant
　　open/closeなどができる（時系列データとか）
　　shard allocationなどの細かな制御も可能ですよと。

　◎plugin
　　色々プラグインがあるよ。管理画面もプラグインであります。
　　プラグインもコマンド一発で追加可能。
　◎クエリキャッシュがないので、自前でnginx、varnishなどでキャッシュが必要。


</code></pre></content:encoded>
    </item>
    
    <item>
      <title>AWS ビッグデータ活用事例セミナーに参加しました。#jawsug(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/11/10/aws-%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E6%B4%BB%E7%94%A8%E4%BA%8B%E4%BE%8B%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-jawsug/</link>
      <pubDate>Sat, 10 Nov 2012 01:07:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/11/10/aws-%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E6%B4%BB%E7%94%A8%E4%BA%8B%E4%BE%8B%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-jawsug/</guid>
      <description>初目黒（たぶん）で、初Amazonな感じで、流行りの「ビッグデータ」のセミナーに参加してきました。 とりあえず、いつもの自分用のメモを残してお</description>
      <content:encoded><p>初目黒（たぶん）で、初Amazonな感じで、流行りの<a href="http://peatix.com/event/7497">「ビッグデータ」のセミナー</a>に参加してきました。</p>
<p>とりあえず、いつもの自分用のメモを残しておきます。
感想はまた後日。。。（たぶん、頑張れ私。。。）</p>
<hr>
<pre><code>
AWS ビッグデータ活用事例セミナー
日時：2012/11/09 [ 金 ] 9:30 - 12:00
場所：アマゾンデータサービスジャパン目黒オフィス

★AWSビッグデータ概要 @understeer　
　○Amazonの紹介
　○Amazon Web Serviceの紹介
　　・オンプレミスと比べ、初期費投資が不要
　　・コストダウンを促進
　　　過去6年で21回の値下げを実施
　　・IaaSだけじゃなく、PaaSもやってますよ。
　　・OSより上の層は好きに選べるよ。
　○AWSのサービスだけで20ある。
　○3つのV
　　Volume　　2012年で1.2ZB。95%が非構造データ。今後も非構造データが増加
　　Velocity　　デバイスの増加、パーソナライゼーション系の増加
　　Variaty
　　　素粒子のデータの解析とか、地質学、気象予報とかいろいろやってるみたい。
　○BIG DATAの4つのプロセス。
　　収集、保存、分析、共有を繰り返しやりましょう。
　そこで！AWS使いましょう。

　○収集
　　AWSへのデータアップロード
　　オンプレにあるデータをAWS（S3）にアップロード
　　・インターネット経由
　　・専用線サービス（AWS Direct Connect、1/10Gbps）も可能
　　・AWS Import/Export（HDDを送りつけてS3にアップロードしてくれる。Tokyoリージョンにはだない）
　　・インターネットVPN経由も可能
　　・EC2上のデータももちろんできる。
　　※WAN高速化ソリューションも利用可能。
　○保存
　　・AmazonS3
　　　99.999999999%の耐久性。
　　　同一リージョン内の3箇所以上のDCに自動複製
　　　容量無制限で低コスト（1G 約10円/月）
　　　
　　・Amazon Glacier

　　　データの利用準備に3.5～4.5時間かかる。
　　　S3の約1/10という低コスト
　
　○分析　
　　オンプレミスだと運用が大変。
　　・Amazon EC2
　　　スケールアップ/ダウン、アウト/インが即座に可能。ライセンス持ち込みや従量課金に対応
　　　スペックの種類が豊富（SSDとかもあるよと）
　　・Elastic MapReduce
　　　Hadoopをサポートしたの仮想サーバが簡単に用意可能。
　　　S3、Dynamoとの連携も可能
　　　ディストリビューションも選択可能（Apache、MapR）
　　　追加のアプリ（Hive、Pig、HBase）なども利用可能。
　　　ジョブの大きさに合わせてクラスタサイズを適切にすることでコスパがよくなる。
　　　→HadoopのMRのデータのローカリティとかはどうなってるんだろう？
　○共有
　　・AWS RDS
　　　MySQL、Oracle、SQLServerをサポート（PostgreSQLはないのかな？）
　　　自動バックアップ、フェイルオーバー、パッチ適用機能があるよ
　　・Amazon DynamoDB
　　　AmazonオリジナルなNoSQLデータベース。論文有るよ。
　　　運用管理は気にしなくていい。
　　　性能については客指定の性能が出せるようになる。しかも変えられるよと。
　　※データサイズに応じて以下を選択しましょう
　　　RDS、HBase on EMR、DynamoDB、S3
　　　他にもEC2上で、CassandraやmongoDB、GlusterFSを使ってる事例もありますよ。
　○以上を繰り返すのが意味があります。そこで！
　　・Simple Queue Service
　　　マネージド分散キューサービス
　　　最低1度のメッセージ到達の保証。複数DC間で複製保存
　　・Simple Workflow Service
　　　進捗などの管理も可能
　　　NASAのCURIOSITYの制御に利用？10m前進するのに10時間のバッチ処理がある。。。
　データがなくて試せない！そんなあなたに！
　○AWS Public Data Sets
　　すぐ使えるPublic Dataが用意されてるよ。
　　http://aws.amazon.com/jp/publicdatasets/

★AWSのビッグデータ事例紹介　@shot6
　○NETFLIX
　　・どんな会社？
　　　2500万人以上のストリーミング会員
　　　500億以上のイベント
　　・AWSの利用は？（保存）
　　　8TB/日のイベントデータを収集しS3に。
　　　Cassandra上の顧客データもS3に。

　　　1PB以上のデータがS3に保存。
　　・AWSの利用は？（解析）
　　　EMRでレコメンデーション、アドホック分析、パーソナライゼーションなど。
　　　本番クラスタ（ずっと動かしている）
　　　アドホック分析用クラスタ（必要に応じて構築）
　　　解析用のアルゴリズムをAPIで叩けて、ジョブとして定義されてるらしい
　　・Cassandraを使ってるよ。
　　　Cassandraのクラスタをマルチリージョンで対応したりもしてる。
　　　CassandraのBackupもやってる。OSSで公開されてる？
　　　※HBaseのものもあるよ。
　　　※AWS上のCassandra事例もいくつか広告系で出てるみたい。
　　・High I/O Instances for EC2（SSDのインスタンスまだ東京に無いらしい。）
　　・AWSスケールアウトだけでなく、スケールアップも徐々に増えています。
　○yelp

　　・どんな会社？
　　　口コミサイト。
　　　スペルミスの自動修正、検索ワード自動補完、
　　・AWSの利用は？
　　　WebサイトのログをS3に保存。
　　　EMRを利用してHadoopClusterで解析してS3に保存している。
　　　EMRは処理終了後にシャットダウンしてる。
　　・データ解析が日常になった
　○SHAZAM
　　・どんな会社？
　　　広告配信、モバイル系の配信をやってる会社。
　　　Super Bowlの広告配信でAWS、DynamoDBを利用。
　　※マイネット・ジャパンでもDynamoDBを利用している
　○CLIMATE Corporation
　　・どんな会社？
　　　天候保険の会社
　　・AWSの利用は？
　　　200TBの地質、天候データを解析して
　○THOMSON REUTERS
　　・どんな会社？
　　　情報提供の会社。データの提供が元だけど、解析した結果の情報も提供してるみたい。
　　・AWSの利用は？
　　　MarketScanという18年分の個人の医療データ（個人情報自体はないみたい）を販売する、販促ールとして利用。
　　　1500万人分の患者データを提供。

　　　マーケットの分析に使えるデータの提供。
　　　KARMASPHEREとEMRの組み合わせで、ソリューションを提供していますよと。
　　・事例
　　　1.MarketScanをS3にアップロード。
　　　2.分析官が、KARMASPHERE経由でEMRにアクセス。
　　　3.EMRにS3からデータロードされ、結果がRDS（Oracle）に保存
　　　4.RDSに他の人達もアクセスして使ってるよと。
　○RANGESPAN
　　・どんな会社？
　　　ECサイト向けのPaaSサービスを提供してるロンドンの会社。
　　・AWSの利用は？
　　　NLP、機械学習にEMRを利用？
　　　mongoDBのクラスタを構築してる。
　　※mongoDBの日本の事例としてCAがあるらしい。

★Huahin Framework活用事例 on AWS @ryu_kobayashi
　○Cassandra本とかもやられてるみたい。
　○EMRとは
　　PaaSなんだけど、中身を自分でいじれるらしい。
　　Management Consoleでは3つのバージョンが使えるが、コマンドラインからだと他にも使えるみい。
　　・HBaseはAmazonのDistributionのものだけ。
　　・EMRのメリットは？
　　　インフラの面倒を見なくていい。
　　　クラスタの立ち上げも複数あげられるので簡単。本番実行と同じ環境
　　・EMRのデメリット
　　　オンプレミスにすでにデータがあるとアップロードが大変。実際に物理HDDを送ったこともある（2,3日でAWS上にアップロードされた）
　　　外に出せないデータがあると。。。
　○EMRのTips（EMRの連載に載ってますよ！http://gihyo.jp/dev/serial/01/emr）
　　・Bootstrapを設定
　　　EMRのクラスタの起動前にメモリ設定とかHadoopの設定が可能にできる。
　　・ファイルサイズを適切に
　　　Map数=splitを決めるコード
　　　　を指定することで、処理が早くできるのかな？
　○EMRの起動には
　　いろいろあるけど、HuahinEManagerを使うと使いやすいよ？

　○Huahin Frameworkの構成
　　・Huahinの名前の由来は？
　　　社内のコードはワインの産地にするという決まり。
　　　タイの観光地Hua Hinがワインの産地。
　　・他のHadoop関連のマスコットより可愛いでしょ！？
　　・Huahin Core
　　　MRのプログラムを簡易化
　　　WritableとかSecondary Sortとか書かなくていい
　　　考え方がSQL寄り
　　　素のMRも書ける。Pig、Hiveだとパフォーマンスが難しい
　　　Huahin UnitというMRUnitをラップしたものもある
　　・Huahin Tools
　　　汎用的な処理を集めたツール群だけど、Apache Logの成形のみ。
　　　オンプレミスHadoop、スタンドアロン環境でも動かせるようになってる。
　　・Huahin Manager
　　　Jobを管理するマネージャ
　　　　Jobの実行

　　　　　キューを持ってるので、複数の実行が可能だよ。
　　　EMR対応してる。bootstrapに設定するとできるらしい。
　　・Huahin EManager
　　　　EMRのいろいろが管理できるみたい。
　　　　初期設定20までしかインスタンスが挙げられない上限があるらしいので気をつけましょうと。
　　　　キュー登録のPOST機能は便利そうだ。（EMR触る機会まだまだないけど。。。）
　　　EMRにはJobのkillがない→Job FlowをターミネートすればOK→実際にはEMRのマスタノードにSSHすればできるよ。→めんどくさいよね。。。
　　　EManagerなら可能ですよと。これは必須だよなぁ。
</code></pre><p>　　　</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 4.0.0リリース＆lucene-gosenの4.0対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/10/12/lucene-solr-4-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9lucene-gosen%E3%81%AE4-0%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Fri, 12 Oct 2012 17:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/10/12/lucene-solr-4-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9lucene-gosen%E3%81%AE4-0%E5%AF%BE%E5%BF%9C/</guid>
      <description>##Lucene/Solr 4.0.0リリース ついに、Lucene/Solr4.0.0がリリースされました。 MLで流れていましたが、3年越しのリリースだったようです。</description>
      <content:encoded><p>##Lucene/Solr 4.0.0リリース
ついに、Lucene/Solr4.0.0がリリースされました。
MLで流れていましたが、3年越しのリリースだったようです。
コミッターの方々、JIRAにバグ報告をした方々、お疲れ様でした。</p>
<p>ということで、ちょっと忙しくなりそうです。。。
4.0の機能を調べたりもしたいですし、すこしずつ紹介もしたいです。</p>
<p>本家サイトのニュースは<a href="http://lucene.apache.org/solr/solrnews.html">こちら</a></p>
<hr>
<p>##lucene-gosenの4.0対応版について
lucene-gosenも4.0正式版のjarを利用したバージョンを公開する予定です。
branches/4xでは、すでに作業を行なっており、jarファイルの差し替えは終了しています。
お急ぎの方は、branches/4xをエクスポートして、ビルドしていただくと利用可能となっています。
なお、Lucene/Solrのバージョンが上がっているため、lucene-gosenのメジャーバージョンも変更し、lucene-gosen-4.0.0としてリリース<del>する予定です</del>しました。。（順当に行けば、3.0.0ですが、Luceneのメジャーバージョンに合わせたほうがわかりやすいかと思いまして。）
また、現在、trunkが3.6.x対応のソースになっていますが、このあと、現在のbranches/4xのソースをtrunkにする予定です。
3.6.x対応のlucene-gosenについては、branches/lucene-gosen-rel2.0にて作業を行うこととなります。
今後は注意してチェックアウトするようにお願いいたします。</p>
<p>ということで、<a href="http://code.google.com/p/lucene-gosen/downloads/list">ダウンロードできるようにしました。</a>
lcuene-gosen-4.0.0*.jarとついているものがLucene/Solr 4.0.0に対応したライブラリになります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>LucidのGrant Ingersollさんの講演を聞いてきました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/09/26/lucid%E3%81%AEgrant-ingersoll%E3%81%95%E3%82%93%E3%81%AE%E8%AC%9B%E6%BC%94%E3%82%92%E8%81%9E%E3%81%84%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 26 Sep 2012 18:08:12 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/09/26/lucid%E3%81%AEgrant-ingersoll%E3%81%95%E3%82%93%E3%81%AE%E8%AC%9B%E6%BC%94%E3%82%92%E8%81%9E%E3%81%84%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>このイベントにウチダスペクトラムの枠でMahoutのコミッターである、Grant Ingersollさんが講演されるということで、興味があった</description>
      <content:encoded><p><a href="https://itmedia.smartseminar.jp/public/seminar/view/434">このイベント</a>にウチダスペクトラムの枠でMahoutのコミッターである、Grant Ingersollさんが講演されるということで、興味があったので聞いて来ました。
（この枠だけ）</p>
<p><a href="http://www.lucidworks.com">LucidWorks社</a>が現在展開している、LucidWorks BigDataの概要とコンセプトといった話の内容でしょうか。
LucidWorks社（元Lucid Imagination）はLucene/Solrのコミッターの方々が多く在籍している会社です。
検索システムに関するノウハウを元に、発見や解析といった部分にニーズが広がってきているという話の
ざっくりした概要のはなしでした。
検索システムを中核にして、ログや検索で提供しているデータの解析などの重要そうなポイントが散りばめられて
いるお話でした。</p>
<p>もっと詳しい話を聞きたいなぁ。</p>
<p>講演では日本語の資料でしたが、サイトに英語の資料がアップされているとのことでした。
原文が読めるのは非常に助かります。他のイベントなどでもこのように英語の資料も見れるようになると嬉しいです。</p>
<p>以下は、いつものメモになります。</p>
<hr>
<pre><code>
場所：富士ソフト　アキバプラザ5Fホール
日時：16:10-

◯サーチ技術による情報の可視化
　通常、検索と言うとWebサーチだけど、ウチダスペクトラムのやっている部分はエンタープライズ向け
　ナイスガイ＝Grant S. Ingersollという紹介
◯サーチからSDAへ
　LuceneやSolrのお陰で、検索自体は簡単になってきている。
◯サーチの進化
　ユーザとデータを結びつける意味での検索の進化が必要
　ユーザインタラクションや、アクセスの方法とか
◯SDA
　Search, Discovery and Analytics
　・ユーザからのニーズ
　　検索、優先順序付け、新たな気づき、フィードバックによる学習
　・ビジネスからのニーズ
　　ナレッジの有効活用
◯ユーザ事例
　保険会社での請求に関する不正利用分析を含んだクレーム処理と分析
◯事例：個人に最適化された医薬品
　DNAをベースに検索やファセットで医薬品を検索したり。
◯事例：通信会社における通話記録処理
　ログを元に検索して、不正通話などを解析
◯SDA基盤に必要な要素
　高速で拡張性のある、検索
　大規模でのコスト効率が高いストレージと処理
　NLPとMLにより解析などが向上
◯SDAのアーキテクチャ
　基盤
　　LucidWorks Search、Hadoop、HBase、ApachePig、Mahout、
　NLP、
　管理
　　Zookeeper
　インフラ
　　ZABBIX、AWS、Chef
　データの流し込み
　　Twitterからのデータとか
◯検索部分にフォーカス
　・LucidWorks Search
　　SolrCloudによる簡単なshard処理
　・Hadoop
　　ログ、生データ、中間ファイルの保存
　　WebHDFS
　　小さなファイルには向いていない
　・HBase
　　メトリック、ユーザ履歴などのストレージ
　課題
　　どこに正式に保存する？
　　リアルタイム処理 vs バッチ処理
　　分析はどこで行われるべきか？
◯検索の実装に関連すること
　3つのポイント
　　性能と拡張性
　　関連性
　　オペレーション（モニタリング、フェイルオーバーなど）
　ビジネス側では検索結果の適合性を重要視する
　開発側は性能を重視する傾向がある。
◯適合性に関して
　テストが重要。
　クエリ、クリック、表示したドキュメントなど、すべて保存すべき！
◯Discoveryにフォーカス
◯MahoutによるDiscovery
　3つのC
　・協調フィルタリング
　・クラシフィケーション
　・クラスタリング
　追加事項
　課題
　　収束を伴う計算コストの高い機械学習アルゴリズム
　　Mahout
◯余談：Experiment Management
◯Analyticsにフォーカス
　Rとか、うまく活用
　検索エンジン自体でもできることがある。ファセット、TF、DF/IDF
　SearchとDiscoveryの定量化
　　ログ、ナビゲーション分析
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>PFIオープンセミナー2012に参加してきました。 #pfiopen2012(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/09/21/pfi%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC2012%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F-pfiopen2012/</link>
      <pubDate>Fri, 21 Sep 2012 17:05:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/09/21/pfi%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC2012%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F-pfiopen2012/</guid>
      <description>PFIオープンセミナー2012に参加してきました。 対象から微妙に外れてたり、話の内容についていけるか自信がありませんでしたが、参加してきまし</description>
      <content:encoded><p>PFIオープンセミナー2012に参加してきました。
対象から微妙に外れてたり、話の内容についていけるか自信がありませんでしたが、参加してきました。
PFIさんは前から面白そうなことやってる会社だなぁと思っていたので。</p>
<p>面白い話がいっぱい聞けました。
<del>電池が切れそうなので、とりあえず、まずはメモをアップしときます。</del>
かろうじてついていけたという感じですが。
丸山先生の話はアーキテクチャの話に入る前のビッグデータの光と影の話がよかったです。
ビッグデータと言っても、まずは、サンプリングなどで小さなデータで処理できるかもしれないと考えるのも必要なのでは？という話や、相関があるからといって、因果が有るわけではないとか、おそらく、統計やってる人や、数学やってる人にしてみれば、当たり前の事なんでしょうが、その部分に警鐘を鳴らす話が聞けたのは良かったです。
もちろん、ビッグデータでなければ意味が無い解析などもありますという話もきちんと出ていました。</p>
<p>伊藤さんの話は、Screwと呼ばれる、多言語解析基盤のお話です。その前にSedueの紹介で、SolrとSedueの比較の話も出ていました。（若干、強引な感じもしましたが。。。）
多言語解析基盤は、Solrでも少し入ってきています。ただ、それよりも汎用的な作りになるようなので、今後Solrと組み合わせて使うといったことも可能になるかもしれません。
まだ、対応言語などが少ないので今後に期待という感じでしょうか。
複数の言語が混ざった時の挙動がどうなるのかや、身近な文章での言語判定の正確さは少し気になります。</p>
<p>サイバーセキュリティの話も面白かったのですが、話が多岐にわたるのと、スライドの情報量の多さに少しついていけませんでした。
資料が公開されたら、もう一度見直したいかなぁと。</p>
<p>比戸さんの話は、Jubatusと関連のある話でした。機械学習の実際の利用の話が特に興味深かったです。
最近になって、ようやく、実際のデータを利用した話が出てきているみたいで、もっと事例が出てくると機械学習も身近になりそうだなぁと。</p>
<p>最後は、日経BPの中田さんの話でした。これが、一番想像していたものと内容が違って、驚きつつ、楽しめた話でした。
ビッグデータというバズワードがいかにして生まれたのかがよく分かりました。
私は、バズワードだなぁと思う程度だったのですが、出てきた背景にある程度意味があるという考察に感心して聞き入ってしまいました。</p>
<p>ということで、思っていたよりも話の内容についていけたので、講演された方々の話しのされ方が良かっただと思います。</p>
<p>少し無理をして参加してよかったと。</p>
<p>残念だったのは、会場が地下だったため、携帯が入らなかったことでしょうか。
私はe-mobileで接続していたので大丈夫でしたが、docomoの携帯は圏外でした。
ツイートがもう少し盛り上がれば、もっと質問も出たのかもしれないです。</p>
<p><a href="http://preferred.jp/news/seminar/">http://preferred.jp/news/seminar/</a></p>
<p>資料が公開されたので、リンクを貼っておきます。
PFIの方たちの資料へのリンク：
<a href="http://preferred.jp/news/?id=1139">http://preferred.jp/news/?id=1139</a></p>
<p>ゲスト講師の資料へのリンク：
<a href="http://preferred.jp/news/?id=1159">http://preferred.jp/news/?id=1159</a></p>
<hr>
<pre><code>
◯「多様化する情報を支える技術」　講師：西川徹（株式会社プリファードインフラストラクチャー　代表取締役）
　・PFIの説明
　　VCに頼らない。製品につながるビジネスにこだわる（受託開発しない）、技術の多様性を重視
　・PFIの技術領域、ビジネス
　　製品開発（Sedue/Bazil/Jubatus）、自然言語処理、機械学習、分散システムなど
　・”人”が生み出すデータと&quot;機械&quot;が生み出すデータ
　　ビッグデータの発端はGoogleが元じゃないか？→最後の公演で解説があるよ
　　人：質が高いけど、量が少ない
　　機械：質は低いけど、量が多い
　・検索システムについてのお話
　　社内の資料とか情報が、人によって、まちまちなデータの保存（形式、場所など）が実施されてしまう。
　　情報検索技術と大規模データ
　・人のデータへ必要なアプローチ
　　より検索システムを活用してもらうために、楽に整理できる仕組みなどをどう提供するか
　　質の高いデータなのに、形式的な共有しかできていないのはもったいない
　・機械のデータへ必要なアプローチ

　　大量データと高度な解析が重要（CEPとか）
　　デバイスが性能向上→流れてくるデータが大量に→蓄積するだけでも問題になってくる
　　　→蓄積したデータを扱うだけでも処理コストが高くなる
　　分析をオンライン化、ストリーム化すること→Jubatusで貯めずに高度な解析をしましょう。
　　Edge-Heavyになりつつある。　　

◯「ITアーキテクチャはどこへ向かうのか」
　講師：丸山宏氏（統計数理研究所　副所長　モデリング研究系教授　工学博士）
　・ビッグデータの光と影
　　「その数学が戦略を決める」という本がオススメ
　　・大量データでも、ランダムサンプリングでとければ、ビッグデータじゃなくてもいいよね。
　　　もちろん、ランダムサンプリングだけじゃダメな場合もある。
　　・Hadoopが解ける問題領域って少ないのでは。
　　・TVを見る時間が長い人ほど、方言の使用率が高い
　　　因果関係と相関関係の違いをきちんと理解しましょう。
　　・データをきちんと理解して意思決定などをしたほうがいいよと。
　・つぎのアーキテクチャは何か？
　　・コンピュータ・アーキテクチャの歴史
　　　ConnetionMachine CM-1（1985）
　　　SPARC
　　　Transputer（CSPによる並列性、Occam）
　　　SymbolicsLispMachine
　　　Intelアーキテクチャの台頭により、アーキテクチャの研究が廃れてくる
　　・クラサバ、スマホ・クラウドなどのアーキテクチャの話
　　・じゃあつぎは？
　　　Edge-Heay Data＝スマホなどデータが保存される場所がEdgeになりつつある
　　　ビッグデータのほとんどが廃棄されるデータ
　　・Edge-Heavy Dataに特化したアーキテクチャとは？
　　　分散マッチング・プロトコル→サマリ情報を交換することで、絞り込みが可能
　　　X=3とした場合、センサーとかなら、ピンポイントな値ではなく、範囲では。
　　　分布表現を1stクラスオブジェクトとするプログラミング言語が必要では？
　・アーキテクチャの変節点を見極めよう
　QA：
　　Q：スパースネス問題がランダムサンプリングやフィルタリングじゃ解けないんでは？
　　A：はい。ただ、その前にやることがあるはずですよねという注意喚起の意味での発表です。

　　価値に応じて、EdgeにあるデータをCenterに持ってくるという考え方が必要。
　　今は価値が見いだせないのなら、Centerにまで持ってこなくてもいいのでは。

◯「グローバル化する情報処理」
　講師：伊藤敬彦（株式会社プリファードインフラストラクチャー 研究開発部門　リサーチャー）
　・Sedueの説明
　　NHKニュースなどで
　・提供する機能
　　・検索補助
　　　レコメンド、サジェストなど
　・レコメンド機能の紹介
　・Sedue/Solrの比較
　　サポート体制：開発チームがサポートしてくれる
　　安定性：GCがないのがいい
　　付加機能：
　　検索の完全性：接尾辞配列による検索

　・多言語処理の話
　　・翻訳ではなく、任意の自然言語言語で動作・精度を向上させる処理の話。
　　・背景
　　　サービスのグローバル化、会社組織のグローバル化
　　・複数言語を扱う場合の難しさ
　　　多言語解析基盤Screwの開発。
　　　1.必要な処理を順番に適用する
　　　　処理の順序は設定で。出力はJSONで。
　　　　例：言語同定、単語分割、単語正規化
　　　　　→言語同定処理で
　　　2.言語ごとに必要な処理を適用
　・疑問
　　ScrewはSolrとの組み合わせもできる？
　　複数言語が混ざった文章の場合にどういう形で動作する？
　　言語判定は独自実装？
　
◯「BigData処理技術とサイバーセキュリティ」→題名変更されてた
　講師：桑名栄二氏（NTTセキュアプラットフォーム研究所　所長）
　・経歴
　　Jubatusプロジェクト立ち上げに参画
　・攻撃に関する話
　　原因のわかっていないケースが多い。
　・端末の初期設定のパスワードとかが狙われるケースも多い
　・変化する攻撃、変化するシステム・サービス、変化するデータ
　・マルウェアの分類にJubatus
　・不正IPアドレスを機械学習して
　・ABC
　　「あたりまえ」のことを「ばかみたいに」「ちゃんとやる」

◯「先進ビッグデータ応用を支える機械学習に求められる新技術」
　講師：比戸将平（株式会社プリファードインフラストラクチャー　研究開発部門　リサーチャー）
　・ビッグデータ分析はより深い地検を得られるビッグデータ「解析」へ
　　・ビッグデータ分析プロセス
　　　　Volume、Variety、Velocity
　　　　蓄積（NoSQL系）、分析（CEP）、両方やるのがHadoop
　　・分析から深い解析へ
　　　予測、カテゴリ分類、レコメンド、異常検知　
　　　これを機械学習で解決する方向で
　　・機械学習を応用している例
　　　クレジットカードの不正利用検知：FICO
　　　ネットワーク攻撃/侵入検出
　　　Jeopardy!でクイズ王に勝利
　　　医療診断支援

　・データ解析技術への過度な期待と現実とのギャップ
　　いろいろできるみたいだけど、何が必要？
　　・ビッグデータ処理系を使える人
　　・データサイエンティスト
　　・機械学習ツール

　・ビッグデータ処理系での機械学習への対応状況
　　Hadoop本体（YARN）
　　MapReduce系（Mahout、AllReduce or Vowpal Wabbit、SystemML）
　　非MapReduce系（Spark）
　　・機械学習からビッグデータへの歩み寄り
　　　ベンチマーク性能への固執とか、応用との乖離を批判する論文もあるらしい。
　　・機械学習の応用例
　　　Machine Lerning for the New York City Power Grid[Rudin et al., TPAMI, 2012]
　　　電力配電設備の障害予測・検知
　　　実データを用いた例が今後増えていくのでは。
　・今後重要になる技術とPFIの取り組み
　　・データ解析の敷居を下げるためのトレーサビリティ
　　　機械学習向けスクリプト言語は敷居が高い
　　　WekaやSPSSのようなアイコンベースのデータ処理プロセスの記述は前処理には強力だけど、機械学習とは相性が良くない
　　　結果が見える化部分との統合が不十分。
　　・Bazil Farm学習結果分析例
　　　Tweet年齢推定、Tweet性別推定

◯「“ビッグデータ”が話題になった理由」
　講師：中田敦氏（株式会社日経BP社　記者）
　・自己紹介
　・バズワードができるまで
　　まずは、「クラウド」のバズワードの歴史
　　「バズワードはIT企業やThe Economist誌の煽りでなく一般企業の経営陣が納得すると生まれる」
　・なぜ経営者がビッグデータに興味を？
　　「ザ・クオンツ」という書籍に金融業界のルールの変化が書かれてる。面白いよ。
　　Google/Amazonに対する警戒心から。
　　破壊的な新規産業者へ対抗して行かないといけない思うところからビッグデータが流行ってるのでは。
　　「買ってきたIT」は差別化要因にならないのでは？→自分で作ったITなら差別化できる。
　・競争力は自分で作るしか無い
　　日本のとある特殊事情
　　ITエンジニアの所属先が日米で割合がぜんぜん違う。米国はユーザ企業が75%、日本は25%くらい
　・ビッグデータの次はなに？
　　3次元プリンタがあれば、好きなモノが作れちゃう。＝消費地の近くで作成しちゃえば良くなるのでは。
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>メインMBAをMountain Lionにアップデート（いろいろ確認中）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/09/03/%E3%83%A1%E3%82%A4%E3%83%B3mba%E3%82%92mountain-lion%E3%81%AB%E3%82%A2%E3%83%83%E3%83%97%E3%83%87%E3%83%BC%E3%83%88%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E7%A2%BA%E8%AA%8D%E4%B8%AD/</link>
      <pubDate>Mon, 03 Sep 2012 14:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/09/03/%E3%83%A1%E3%82%A4%E3%83%B3mba%E3%82%92mountain-lion%E3%81%AB%E3%82%A2%E3%83%83%E3%83%97%E3%83%87%E3%83%BC%E3%83%88%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E7%A2%BA%E8%AA%8D%E4%B8%AD/</guid>
      <description>友人にお値打価格で購入したMac miniはMountain Lionにアップデートしてたのですが、手元のAirはまだアップデートしていませんで</description>
      <content:encoded><p>友人にお値打価格で購入したMac miniはMountain Lionにアップデートしてたのですが、手元のAirはまだアップデートしていませんでした。</p>
<p>夏休み中にバックアップ＋アップデートをしてしまおうと思いMountain Lionにアップデートはしていたのですが、
本格的に触り始めると色々と動かないものがあったので、備忘録としてブログを書いておきます。
（まだ途中ですが。）</p>
<div>
###1.Eclipseが起動しない
Java6が未インストール状態になったみたいで、Eclipseを起動すると、Java6をインストールしなさいと言われました。
で、私の記憶が確かなら、前はJavaコマンドを実行するとインストールされたんですが、ターミナルでjavaコマンドを実行するとjavaはすでにインストールされてる模様。。。
あれ？と思い、-versionを実行すると、JDK7u4と出るじゃないですか。
どうやら、Java7u4は別途入れてたものが残ってる模様。
さて、どーする？ということで、ツイートしたりぐぐってみて、「Java Preferences」なる設定用のアプリ？が有ることに気づき、とりあえず開いてみたらJava6ないからインストールしたら？と言われました。
結果オーライということで、インストール開始。
無事、インストールも終了。Eclipseも起動しました。（ただし、今度はMercurialのプラグインがPythonのライブラリが無いとエラーを出すことに）
</div>
<div>
###2.Xcodeが起動しない。
使ってはいないんですが、Javaの件を調べるのに、起動したらLionじゃないと駄目だよと。
ということで、Xcodeもインストールしなおし。
</div>
<div>
###3. homebrewが動かない
1.で書いた方法で、Eclipse自体は起動したんですが、Mercurialのプラグインがエラーを吐き始めました。
で、見てみると、hgコマンドを実行しようとしてなんか、壊れていると。
で、hgコマンドってどうやってインストールしたか思い出さないまま、とりあえず、homebrewだっけ？と勘違いしたまま、「brew update」を実行したらエラーがでて、「brew install git」でしょ？って言われてしまいました。
コメントにもいただいていたように、Xcodeをインストールしたのだが、コマンドラインツールが入ってないというのが影響しているみたいです。。。
で、ググってみると、こんな情報が。[http://labs.torques.jp/2012/07/04/2830/](http://labs.torques.jp/2012/07/04/2830/)
ただ、そこに有るようにPreferenceのダウンロードを開いてもなーんにも表示されません。
で、更にググってこちらの情報に。[http://qiita.com/items/9dd797f42e7bea674705](http://qiita.com/items/9dd797f42e7bea674705)（なんだか、TLで見かける人のお名前が！）
AppleのDeveloperサイトからダウンロードかぁと思いつつダウンロード用のリンクを踏んだら、アカウント登録しろとのページが。。。
なかなか遠い道のりですね。。。
Appleのアカウント自体は持ってるので、Developerへの追加登録なんですが、日本語が文字化けしまくりの確認ページが出る始末。
もう、めんどくさいので、そのまま登録しちゃいました。
で、ダウンロードする前にもう一回XcodeのPreference見てみたらなんか、ダウンロードする候補が出てきてるじゃないですか！
結局、XcodeのPereference画面からダウンロードすることにしました。
AppleのDeveloperに登録したからなのか、Xcodeのプロジェクトを作ろうとしてXcodeをきちんと起動したからなのかは結局わかっていません。。。
良くないよなぁ。（ご存知の方いたらコメントもらえると嬉しいです。）
で、やっと「homebrew」を入れようかなぁと。
一応その前に「sudo brew update」ってやってみたらどうなる？と挑戦したら今度は動きました。
homebrewが何を判断してinstallでしょ？って言ってたのかもわかっていませんが、updateで良かったみたいです。
<p>ついでに、upgradeもしときました。（こっちは後からインストールしたもののバージョンが上がってるものがあったら更新してくれるコマンドかなぁ？）</p>
</div>
<div>
###4. pipの更新
ただ、brewをupdateしても相変わらず、EclipseのMercurialのPluginはエラーを出してました。
で、ターミナルからhgコマンドを実行してみたら同じエラーが。まぁ、そうですよね。
エラーメッセージを頼りにこれまたググってみたら、同じ状況のひとがいました。[https://groups.google.com/forum/#!msg/mercurial-ja/MxY6lejLXxo/OgJA_knXV6cJ](https://groups.google.com/forum/#!msg/mercurial-ja/MxY6lejLXxo/OgJA_knXV6cJ)（ここにもTLでお世話になってる方のお名前が！！）
で、書いてあるように「とりあえず pip install mercurial で解決しました. 」ということで、コマンドを実行して見ることに。
これが、また失敗します。。。（日頃の行いが悪い？？てか、日頃MBAをメンテできてないのが悪いのか。。。）
そういえば、過去のMBAセットアップのメモを残してたなぁと思い出して「johtani mercurial」でググってみるとちゃんと[書いてある](http://johtani.jugem.jp/?eid=34)じゃないですかー（偉いぞ、自分）
ということで、「sudo easy_install pip」でpipを更新してから「sudo pip install Mercurial」で無事、hgコマンドもインストールし直せました。
Eclipseを起動してもエラーが出ない。（まだbitbucketに接続確認まではできてないですが。。。眠さに勝てず寝てしまいました。）
</div>
</content:encoded>
    </item>
    
    <item>
      <title>Fluentd meetup in Japan #2 #fluentd に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/08/23/fluentd-meetup-in-japan-2-fluentd-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Thu, 23 Aug 2012 02:40:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/08/23/fluentd-meetup-in-japan-2-fluentd-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>興味をもちつつ、触っていない軟弱者ですが、興味があるので今回も話を聞きに行って来ました。 まずは、作者古橋さんによるFluentdの魅力や次期</description>
      <content:encoded><p>興味をもちつつ、触っていない軟弱者ですが、興味があるので今回も話を聞きに行って来ました。</p>
<p>まずは、作者古橋さんによるFluentdの魅力や次期バージョンのお話。
あいかわらずわかりやすいスライドで話もわかりやすくてよかったです。
どうしても実績という点を懸念事項として上げる人が多いというアンケートを元に、各社が使ってるし導入もしやすいですというお話。
ここまでしてもらってるのに触ってないなんてほんと申し訳ないです。。。</p>
<p>次は楽天の方によるCloud Foundryのログの問題点解消のためのFluentd導入のお話。
EC2でもそうですが、ファイルが永続化されない？のでログが消えてしまうという問題があるので、
集約しましょうと。
いままでとは少し違う問題点からの話でした。</p>
<p>次はドリコムの方（浴衣？甚平？でかっこ良く発表）のIDCをまたいだFluentdの活用と、Fluentd自体の監視などについてのお話。
実際にログが増殖して苦労された点を解決するために考えられた監視項目など、あとで見返したくなる資料でした。
実際に試行錯誤されたあとの話はやはりありがたいです。
あと、お子さんが可愛かったｗ</p>
<p>つぎのCROOZの方のPCがWindowsだったため（？）プロジェクターに繋がらず、急遽QAタイム。
このあたりの@doryokujinの話のつなぎの旨さとかほんとすごいなぁと感心します。</p>
<p>で、Macに乗り換えてCROOZの方の発表。
Fluentd＋TreasureDataのお話。少人数（というか一人？）でも簡単にログ収集の仕組みが作れて、しかも保存先のサーバを用意せずに簡単な解析もできるというお話。
これは、ちょっとやってみようと思う人（少なくとも、私はやろうと思った）が増えたんじゃないかなぁという発表でした。
スライドがなぜか最後のほうが見えなくなってしまったので、第３回でも発表されるということになってましたｗ
最後は新大阪から文字通り駆けつけた玉川さんのHBase本＋そのた今後の翻訳本の紹介。
つぎはHiveの本も出てくるみたいでした。
日本語の資料ってホント助かります。購入しないと翻訳本が出る機会もないみたいなので皆さん買ってくださいとのこと。
HBaseはまだ触りそうにないから９月に出るAWSの本でも買おうかなぁ。
あ、日本語で解説してある<a href="http://www.amazon.co.jp/exec/obidos/ASIN/4774141755/johtani-22/ref=nosim/">Solrの本</a>もあるので是非買ってください！</p>
<p>その後は懇親会でした。今回もTL上でズケズケと私が勝手に絡んでいる方たちにリアルにお会いできたので楽しかったです。</p>
<p>自分もフロントよりも、バックエンドに興味があるし、実際に運用されてる人の話が多く聞けるので次回も参加したいです。
それまでにどこかで触るかplugin作るかしないとなぁ。</p>
<p>ということで、以下はいつもの適当メモです。</p>
<hr>
<pre><code>
開催日時：2012/08/22 18:00  ～  22:00
場所：グリー株式会社 14F セミナールームYosemite

◎「Fluentdの現在と未来」　Treasure Data, Inc.　古橋　貞之 (@frsyuki)
　◯アンケートの内訳
　◯ドキュメント欲しい？
　　※思ったより日本語のドキュメントじゃなくてもよさそうだった。
　◯loggingってなんでいるの？
　　いろいろな解析ってあるよね。
　◯ログの集約、保存、などの問題点について
　　フォーマットが混在
　　集約するのもいろんなスクリプトが混在
　◯メリット
　　・プラグインアーキテクチャ
　　　in/outに合わせてプラグインが用意/開発可能
　　・フォーマットがJSON
　　　アプリでの解析が楽
　　・HA構成が可能
　◯実績がない？→
　　誰が使ってる？
　　　COOKPADとか、NHNとか
　◯次期バージョンの構想
　　・設定ファイルで色々とらくできるよ。
　　・MessagePackのv5に対応
　　・td-agent-lite
　　などなど

　◯QA
　　Q：時刻にミリ秒を持つことは可能？
　　A：互換性も気になりますが、検討します。
　　Q：JSONで構造化が売りだが、Flumeとかはテキストだけど、テキスト
　　A：ログのパース時にやるというスタンス。
　　Q：日本語ドキュメントがやっぱり欲しい。手伝います！
　　A：別ブランチで翻訳しながら公開して欲しいし、バラバラにやるよりいいので。
　　Q：Windowsでも動かしたいけど、cool.ioの移植とか考えてないですか。
　　A：次期で、fluentdのコアからはcool.ioを外す予定です。

◎「Logging Infrastructure in PaaS by Fluentd」　Rakuten, Inc.　Yohei Sasaki (@yssk22), Waldemar Quevedo (@wallyqs)

　◯Cloud Foundryの説明
　◯Cloud Foundryの問題点
　　解析しようにもログが消えてしまう。。。
　　なので、Fluentdでログを集める仕組みを作ったよと。
　これかな？ https://github.com/rakutentech/dea/
◎「Fluentdを優しく見守る監視事例」　株式会社ドリコム　外道父 ( @GedowFather )
　◯概要：
　　Fluentdをより穏やかに安定稼働させるための監視項目と自動処理について。また，その実運用における障害例なども紹介したいと思います。
　◯目次
　◯動作環境
　　・IDCもバラバラな環境のログを一箇所に集約。
　　　グローバルなネットで、圧縮、暗号化し、VPN使ってない
　　・tailのプラグインを改良して利用
　　　copy、flow counterを利用
　　　forwardも改良
　　Flume OGとは比較にならないし、FlumeNGはOGと全然違うから論外だった。
　◯ローカル監視
　　・monit使って監視してる。
　　ログを記録してるか、内容が正しいか
　　td-agentが正しく起動してるか、Collectorに送っているか
　　　重複起動してないかとか、起動してるかとか。
　　　※重複起動でログが増えてた（@mazgi濡れ衣事件）
　　HDFSに送ってるか、保存されてるか
　　
　◯リモート監視
　　アラート/グラフ作成の集約
　　状態の可視化
　　Collectorのキャパシティ管理
　　Agentにキャパシティの心配はほぼないが、Collectorは足りなくなる可能性がある。
　◯野望
　　CollectorでAgentを把握したい
　
　◯QA
　　Q：圧縮はどうやって？
　　A：forwardを改造してやっている。

◎QAタイム
　Q：秒間どのくらい出るの？
　A；秒間8000メッセージくらいらしい。
　Q：ハートビートの取りこぼしは？
　A：案1：UDPじゃなくて、TCPにする。案２：TCP接続してたらハートビートのカウントとしてしまう。
　Q：CollectorのCPUに影響があるのってなに？
　A：ロックがCPUを食う＝ロックが影響→リクエスト量を減らす
　Q：Windows対応はいつ？（発生源がWindows）
　A：td-agent-liteをWindows対応にしたいと思ってる。
　Q：F#の実装とかテストは？
　A：性能値の測定までは行ってない。メッセージが送れたなぁくらい。
　Q：設定のDSL化はv11ではなくなったの？
　A：ホスト名は入れたい。設定はやっぱり設定だけにしたい（プログラムは入れたくない）
　　　プラグイン側がDSL対応してればDSLできるようなものは入れようかと思ってるが、
　　　DSLは延期したい。
　A（tagomoris）：DSL化したいパターンが幾つかに絞れるなぁと思ってて、それに合わせたプラグインをいくつか作ってるよー。

◎「Fluentd &amp; Treasure Data でこっそり始めるログ集計」　CROOZ 株式会社　池田 朋大（ @mikeda ）
　◯概要：
　　FluentdとTreasureDataプラットフォームを使って、1インフラエンジニアが勢いでログ集計システムを作ってみたお話です
　◯アクセスログ、エラーログ、メールログ（試験中）を集めてる。
　◯TreasureData
　　500Gまで無料なのかー。
　◯ダマで入れてもばれないぞ！
　◯最後は心の目で見えるスライドでした。

◎祝・O'Reilly HBase 訳本発売。訳者本人によるPR。　Sky株式会社 玉川 竜司　※ O'Reillyの新刊「HBase 」　http://www.oreilly.co.jp/books/9784873115665/


◎懇親会
</code></pre><p>そうそう、ステッカーもらったのでアンケート書きましたｗ

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20120823/20120823_2440232.jpg" alt="Fluentdステッカー"/>
    </div>
    <a href="/images/entries/20120823/20120823_2440232.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
</content:encoded>
    </item>
    
    <item>
      <title>Twitter 勉強会 #twtr_hack に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/08/02/twitter-%E5%8B%89%E5%BC%B7%E4%BC%9A-twtr_hack-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Thu, 02 Aug 2012 11:50:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/08/02/twitter-%E5%8B%89%E5%BC%B7%E4%BC%9A-twtr_hack-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>またまた飲みに行って参加してきました。 今回は、Rails、iOSでのTwitter連携の話から、ツイート分析、クライアントアプリの開発の苦労</description>
      <content:encoded><p>またまた<del>飲みに行って</del>参加してきました。
今回は、Rails、iOSでのTwitter連携の話から、ツイート分析、クライアントアプリの開発の苦労？楽しい話と、
幅広い話題でこれまた面白かったです。</p>
<p>Railsはあとで、もう一回資料＋ビデオがみたいかも。あと、発表者の方が言ってたけど他の言語の似たようなサンプルがあると面白いかも。（<a href="http://www.amazon.co.jp/exec/obidos/ASIN/4774141755/johtani-22/ref=nosim/?PHPSESSID=4c20b4d061334d9a7c44ab7afec36007">Solr入門</a>みたいに同じ題材で違う言語のサンプルとか）</p>
<p>ツイート分析は、私の使い方とは異なる分析結果がちょっと意外でした。土日はあんまりツイートしないからなぁ。
利用時間帯とかは、他のSNS（Facebookとかmixiとか）の分析と比較してみると面白いのかも。
まぁ、深夜帯はそれほど利用は無いだろうけど。</p>
<p>Attaccaは自社や自宅でコーディングするときに利用させてもらってます。
どうしても自分のお気に入りのリストを作ってそれを聞くので満足しちゃうんで、
他の人のお気に入りも一緒にシャッフルして再生とかできると面白いかもなぁ。
もう少し、他にも曲を発見したいんだけど、その導線がもう少しうまく行くと嬉しいかも。</p>
<p>チャーハン諸島の話は開発者の原点みたいな話で面白かった。やっぱり、自分で作るの大事だよなぁと。
作りたいと思うものがあるのはいいことだし、実際作ってみないとわからないこともいっぱいありますよねぇ。
ただ、何か作ろうかなぁと思うものがあるのはちょっとうらやましいとも思いました。
なかなかサービスとか、ほしいものを作ろうと思うところまで行かないからなぁ。年取ったのかなぁ。</p>
<p>懇親会では、いつものように@twtrfkさんと喋って、あと <a href="http://www.lytro.com">Lytro</a>を触らせてもらいました！
思ったよりも大きいのが第一印象。
ぱっと見で、何の変哲もないところがズームするところだったりと、インタフェースがちょっとおもしろかったです。
ピントが後から合わせられるということで、どうしても同じ構図になっちゃうのがなぁという話も聞けましたｗ
けど、ちょっと欲しいかもなぁ。動くものを撮るとどんな感じなのかも聞くんだった。</p>
<p>次回は9月中旬！らしいので、余力がありそうだったらまた遊びに行きます。</p>
<hr>
<p>日時：2012/08/01 19:00  ～  21:00
場所：デジタルハリウッド東京本校 1Fセミナールーム</p>
<p>いつもの自己紹介タイム</p>
<p>@i7a16k(@_gifteeの中の人)　<a href="http://www.slideshare.net/i7a/rails-and-twitter-twtrhack">スライドはこちら</a></p>
<pre><code>
 「RailsでTwitter連携アプリをサクっと作る」
　・まずは、Railsの紹介
　　MVC+routes.rbの紹介
　・Dev Twitterの登録する必要なとことか。
　・Railsのインストールから起動まで。
　・実際にログイン画面を作成するまでの紹介
　　コーディングするコマンドの紹介。動画付き
　 　omniauth_twitter
　　ってのを使うみたい。
　・サインイン、サインアウトまで。
　　ツイートは次回！
　　録画がよくできてて、それに合わせてしゃべるのもうまいなぁ。
</code></pre><p>@teapipin(<a href="http://teapipin.blog10.fc2.com/blog-entry-298.html">ツイッター分析シリーズ </a>の方)　<a href="http://www.slideshare.net/teapipin/173twitter-twtrhack-13837615">スライドはこちら</a></p>
<pre><code>
 「約173万ツイートを調査して分かったTwitterの利用動向」
　・ハンドル名は午後の紅茶からきてる？＋ピピン＠
　・ブログで色々公開してます。
　・サービス作るのに、下調べをしてみましたというお話
　　情報が無かったから、自分で調べてみたよと。（すばらしい）
　・Streaming APIで取得
　　タイムゾーンとか言語設定の取得でもうまく取れない。。。
　　ということで、UnicodeBlockで判定してみたけど、、、
　　最後は手作業で不要データを除去（すごい！）
　・4日間で172万ツイート
　　（金環日食とかスカイツリーのイベントがあったので、4日間で我慢）
　・上位5個で50%を占めるクライアントみたい
　・日曜日が多いらしい
　・携帯が60%くらい
　・位置情報（Geoタグつき）
　　日本が多い。4sqが40%占めてる。
　　店舗情報や天気情報などもあるらしい。
　　人口と関係した相関が散布図でわかった。
　　そこで、ツイート内容との関係を分析
　　　あとで資料みたいなー
</code></pre><p>@i2key(#attacca の関係者)　<a href="http://www.slideshare.net/i2key/iostwitterframework">スライドはこちら</a></p>
<pre><code>
 「iOSのTwitterFrameworkを使ってみたら・・・・」
　・Twitter4Jのほうが楽だったよー
　　デモがいいね！
　・アーキテクチャ
　　play!をバックエンド。Amazonとか。
　　iOS Twitter framework
　・Reverse Authの使い方とか。
　　申請してから、20日間かかった。
</code></pre><p>@Mocel(<a href="http://archive.guma.jp/rice-islands.html">チャーハン諸島</a> 開発者)　<a href="http://www.slideshare.net/Mocel77/twitter-twtrhack">スライドはこちら</a></p>
<pre><code>
 「(仮)Twitter クライアントの開発とかについて」

　・趣味プログラマー
　・「ラーメン大陸」のクローン：「チャーハン諸島」を開発
　　Excel溶けこむGUI
　　Javaで実装
　　コマンドライン風のTL画面もある（自分では使ってないけど）
　　「電力会社の電力使用量モニター」もクライアント初搭載！
　　ラーメン大陸のバージョンチェックも可能ｗ
　・開発したことで
　　自分のニーズにジャストフィット
　　優しい気持ちになれる（苦労がわかる）
　　Twitter APIのテストとかもすぐ試せる
　・GUIアプリ開発のノウハウも手に入るからオススメ
　・API利用規約は読んどこうね
　・自動アップデート機能がいるよ。→バージョンごとのサポートがなくなるよ。
　・通信エラー前提で作りましょう
　・鍵付きの非公式RTはやめなさい。
　・Twitterクライアントの作成はおもしろいよ！
　　反応がプレッシャーになることもあるけど。
　　おもしろ機能をつけるのがいいよーと
　話が上手で聞きやすかった。
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>Python Developers Festa 2012.07に参加してしゃべってきました #pyfes(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/28/python-developers-festa-2012-07%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%97%E3%82%83%E3%81%B9%E3%81%A3%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F-pyfes/</link>
      <pubDate>Sat, 28 Jul 2012 18:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/28/python-developers-festa-2012-07%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%97%E3%82%83%E3%81%B9%E3%81%A3%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F-pyfes/</guid>
      <description>ということで、ステッカー欲しさ？に勉強中の話を恥ずかしげもなく偉そうにしゃべってきました。 #pyfesは以前から、気になっていたんですが、タ</description>
      <content:encoded><p>ということで、<a href="http://via.me/-3guon2i">ステッカー</a>欲しさ？に勉強中の話を恥ずかしげもなく偉そうにしゃべってきました。
#pyfesは以前から、気になっていたんですが、タイミングがあわず初の参加になりました。
TwitterのプロフィールにSenseiDBに興味あると書いていたら、@voluntusさんに声をかけていただけて、
さらになぜかelasticsearchの話をすることにして話をしてきました。
まだまだ、いろんな意味（プレゼン的にも内容的にも）で至らない所だらけだったので反省しまくりですが、
これでまた経験値が稼げたかなと。次回に活かしたいと思いますです。
やっぱり、しっかり勉強して、シナリオを練ってから発表しないとダメですね。。。</p>
<p>発表のスライドは一番最後にリンクを用意しておきましたので、興味があれば見てもらえればと思います。</p>
<hr>
<p>ということで、いつものメモを残しておきます。</p>
<pre><code>
日時：日本オラクル青山センター
場所：2012/07/28 10:00 - 20:00
</code></pre><p>概要：<a href="https://github.com/pyspa/pyfes/blob/develop/201207.rst">こちらにページあり</a></p>
<p>前半（10時から15時）はハンズオンなどをやられてました。参加せずにスライドを微調整して、他の勉強会のスライドをいじったりしてました。
以下は、15時から行われたスライドのメモになります。</p>
<pre><code>
　◯PyConJP の宣伝　@shomah4a（LT）
　　9/15-17
　　PythonカンファレンスJapan
　　App Engine、Django、Sphinxなどのカンファレンスも併設
　　遠方参加者支援制度があるらしい。
</code></pre><pre><code>
　◯elasticsearch 入門　@johtani
　　わかりにくい話でしたかねぇ。。。
</code></pre><pre><code>
　◯たのしいうぇっぶくろーら　@tokoroten（LT）
　　index.htmlをクロールしまくってる社畜2.0の人らしい。
</code></pre><pre><code>
　◯Sphinxを使って翻訳してたら本が出てた話　@ymotongpoo（LT）
　　OSSでもドキュメント翻訳でお手伝いできるよ。
　　そしたら、いつのまにか書籍も出せたよ。
</code></pre><p><a href="http://www.slideshare.net/ymotongpoo/sphinx-13784014">スライド</a></p>
<pre><code>
　◯iOS関連のお話　@Seasons
　　バイナリ解析をしてゴニョゴニョする話。
　　解析するのに何を使ったとか思考の遷移を説明してくれるのでわかりやすい。
　　スライドが大きなマインドマップを切り出した形。
</code></pre><pre><code>
　◯HBaseのお話　@shiumachi
　　HBase
　　　分散DB
　　　列ファミリ思考
　　HBaseなんで？
　　　RDB→シャーディング→だるい。。。
　　　シャーディング→スケールできねー
　　nandeHBase？
　　　書き込みスケールできるよ。
　　　KVS
　　HBaseのデータ構造
　　　キーがいろいろな情報を含んでる
　　　キーがソートされてる
　　HBaseのテーブル構造
　　　リージョンがシャーディングの情報もと？
　　リージョン見つけなど
</code></pre><p><a href="http://www.slideshare.net/shiumachi/20hbase">スライド</a></p>
<pre><code>
　◯PythonではじめるGit　@mkouhei
　　GitPython
　　LXCホスト？
　　GitもPythonも初心者だわー
</code></pre><pre><code>
　◯勉強会を成長させる参加者になろう　@sawonya
　　イラストレーター（スタートアップRubyのイラスト書いた人。サインもらいましたｗ）。
　　参加者が増えるとなにがいいの？など。
　　勉強会参加に向けた勉強会の講師とかやられてるらしい。
</code></pre><p><a href="http://www.sawonya.com/ss.htm">スライド</a></p>
<pre><code>
　◯IT 系勉強会ネタ(仮)　@tmmkr 
　　アジャイルサムライを読んだ情報を共有したくなって読書会を開催してみた！
　　ビアバッシュのケータリングとかは楽天デリバリーとか、カクヤスがいいよ。
　　かなり、いいスライドなので、あとで見返す。
　　今、読書会やったりしてるし、Solr勉強会の役にも立てそうだし。
</code></pre><p><a href="http://slidesha.re/Qt1Rpq">スライド</a></p>
<pre><code>
　◯Do not invent your RNG...　@kenji_rikitake
　　Androidの乱数のコードがすごいらしい（ひどい）
　　Pythonの乱数ではos.urandomを使うのが安全です。
　　オレオレ乱数は作っちゃ駄目！
</code></pre><pre><code>
　◯分散ファイルシステム（LeoFS）　@yosukehara
　　LeoFSの開発者の方。
　　Erlangで98%書いてある。
　　Masterノードは存在しない。SPOFになるから。
　　分散システムとして元にした概念とか論文ってあるんだろうか？
</code></pre><pre><code>
　◯継続的デリバリー　@troter
　　CIとデリバリーの話。
　　いいこと書いてあるんだけど、実際のツールの話しがないのが辛いこともある
　　ということで、Python周りのツールをこうして見たよというお話。
　　Rubyの方がものがいろいろ揃ってるらしい
</code></pre><pre><code>
　◯クライアントサイドのみで作ったダッシュボード　@takufukushima 
　　RESTアクセス用のUIのフロントエンドの話？
　　JSのお話の？node.jsとかの話。
　　MVCにしたり、CSSフレームワーク使ったり。
　　backbone.jsつかってるらしい。
　　実際の画面がみたいなぁ。
　　現状の話なので、
</code></pre><pre><code>
　◯Meinheld　@mopemope
　　Python3対応とかLoggerとかやってから秋くらいに出るみたい。
　　このあたりは未知の領域です。。。
</code></pre><pre><code>
　◯3分間で開発環境構築 @tk0miya
　　Vagrant＋Chefみたい。
　　VeeWeeってのでIOSイメージからVMイメージを作ってくれる。
　　（githubから持ってこないといろいろ古いらしい）
　　これ、重要だと思う。
　　実践するようにしよう。
　　手順書がわりにChefのレシピを書こうよと。
　　環境マニア募集中！
　　継続的デリバリー座談会やってます
</code></pre><pre><code>
　◯筋トレ講座　@hiroki_niinuma
　　ジムに通い続けるのはキツイ。
　　成功率5%
　　以下の条件に
　　・10時間以下の仕事時間
　　・ジムが近い
　　・ジムという環境が好き
　　ベンチマークｗ先入観を捨てましょうとｗ
　　ジムで筋トレとかよりも歩くのが全然いいよと。
</code></pre><p>togetterがあったのでリンク。
<a href="http://togetter.com/li/346242">http://togetter.com/li/346242</a>
<a href="http://togetter.com/li/346270">http://togetter.com/li/346270</a></p>
<hr>
<p>スライドはこちら。</p>
<iframe src="http://www.slideshare.net/slideshow/embed_code/13783936" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen> </iframe> <div style="margin-bottom:5px"> ** [Elasticsearch入門 pyfes 201207](http://www.slideshare.net/JunOhtani/elasticsearch-pyfes-201207) ** from **[Jun Ohtani](http://www.slideshare.net/JunOhtani)** </div>
<hr>
<p>それにしても発表するといういい機会を与えてもらえて良かったです！。
継続的にelasticsearchも調べていきたいので、興味ある人は声をかけてくださいー</p>
</content:encoded>
    </item>
    
    <item>
      <title>Partial UpdateとcopyFieldのバグ【Solr 4.0 ALPHA】(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/13/partial-update%E3%81%A8copyfield%E3%81%AE%E3%83%90%E3%82%B0solr-4-0-alpha/</link>
      <pubDate>Fri, 13 Jul 2012 20:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/13/partial-update%E3%81%A8copyfield%E3%81%AE%E3%83%90%E3%82%B0solr-4-0-alpha/</guid>
      <description>今日はSolr 4.0 ALPHAの興味深い機能があったので紹介です。 数日前に「Solr 4.0: Partial documents update」という記事を見つけました。 Solrには、</description>
      <content:encoded><p>今日はSolr 4.0 ALPHAの興味深い機能があったので紹介です。
数日前に<a href="http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/">「Solr 4.0: Partial documents update」</a>という記事を見つけました。</p>
<p>Solrには、ドキュメント（RDBで言うレコード）のデータを更新したい場合には、特定のフィールドだけを更新するという機能がありませんでした。
ですので、特定の項目（例えば、priceなど）を更新したい場合、ドキュメントの全データをSolrに再度上書き登録するという処理をしなければなりませんでした。
RDBを触っていた方が、Solrを始めた場合に必ず使いづらいと思われる点だと思います。</p>
<p>で、4.0でその機能がありますという、<a href="http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/">「Solr 4.0: Partial documents update」</a>の記事を見つけました。
ただ、SolrのWikiや4.0 ALPHAの紹介のページには「partial update」という記述が見当たりません。
（あれ、これかな？<a href="http://wiki.apache.org/solr/Per%20Steffensen/Update%20semantics">Update semantics</a>）
あと、まだ完成していないので、載っていないのかもしれないです。（この<a href="https://issues.apache.org/jira/browse/SOLR-139">チケットSOLR-139</a>が部分更新に関するもののはず。チケット番号をみても古くから望まれている機能だということがわかります。）</p>
<p>ということで、調べてみました。</p>
<p>###機能概要</p>
<hr>
<p>Solrの機能として、特定のフィールドのみを更新するという機能です。
あくまでも、Solrレベルでの機能となり、Luceneの機能を利用したものではありません。
つぎのような流れになっています。</p>
<ol>
<li>Solrに対して特定フィールドを更新したいという形のドキュメントを投げる</li>
<li>Solrはドキュメントを受け取ると、内部のインデックスに保存してあるデータを取り出す</li>
<li>取り出したドキュメントオブジェクトに対して、更新対象フィールドの値だけデータを更新する</li>
<li>ドキュメントオブジェクトをインデックスに保存する</li>
</ol>
<p>このような流れです。
まぁ、言われてみれば当たり前な処理です。
ただし、この機能を使う場合はいくつかの前提条件があります。</p>
<p>###前提条件</p>
<hr>
<p>前提条件はつぎのとおりです。</p>
<ul>
<li>すべてのフィールドをstored=&quot;true&quot;にする</li>
<li>「<em>version</em>」という特殊なフィールドを用意する</li>
</ul>
<p>1点目は、データの保存方法についてです。
先ほど流れに書きましたが、Solrが内部に保存してあるデータを取り出して、更新対象以外のデータを保存しなおしてくれます。
このため、stored=&quot;true&quot;にしておかないと、元のデータがSolr内部で取得できません。</p>
<p>2点目の「<em>version</em>」というフィールドは4.0から導入されたフィールドです。
SolrCloudに必要な機能としてドキュメントのバージョン管理を行うために導入されたフィールドだと思います。（あまり詳しく調べていない。。。）
SolrCloud内でレプリカの更新などに使ってるのかなぁと（そのうち調べます。）
以上の2点が前提条件です。すべてのデータをstored=&quot;true&quot;としなければならない点は、インデックスのサイズや性能に関わってくるので考えて利用するほうがいいかと思います。</p>
<p>###利用方法</p>
<hr>
<p>Solrのサンプルデータ（exampledocs/mem.xml）を例として利用します。
部分更新を行うにはつぎのような形のデータを投げると部分更新が可能です。
（JSONでの更新のサンプルについては、<a href="http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/">こちらの記事</a>を参考にしてください。）
####XMLのサンプル（partial_update.xmlというファイルで保存する）</p>
<pre><code>&lt;add&amp;gt;
&lt;doc&amp;gt;
  &lt;field name=&quot;id&quot;&amp;gt;VS1GB400C3&lt;/field&amp;gt;
  &lt;field name=&quot;_version_&quot;&amp;gt;バージョン番号&lt;/field&amp;gt;
  &lt;field name=&quot;cat&quot; update=&quot;add&quot;&amp;gt;cats_and_dogs&lt;/field&amp;gt;
  &lt;field name=&quot;popularity&quot; update=&quot;inc&quot;&amp;gt;10&lt;/field&amp;gt;
  &lt;!-- set empty for SOLR-3502 bug --&amp;gt;
  &lt;field name=&quot;price_c&quot; update=&quot;set&quot;&amp;gt;0.0,USD&lt;/field&amp;gt;
&lt;/doc&amp;gt;
&lt;/add&amp;gt;
</code></pre><p>上記サンプルのうち、<b>バージョン番号</b>の部分は、現在Solrに登録してある値を指定します。（Solrの管理画面で検索すれば表示されます。）
上記ファイルを「SOLR_HOME/example/exampledocs」に保存し、同フォルダにてつぎのコマンドを実行すると、部分更新されるのがわかります。
Solrに更新であるというフィールドがわかるように、fieldタグにupdateという属性を指定してあります。</p>
<pre><code>
java -Durl=http://localhost:8983/solr/update?versions=on -Dout=yes -jar post.jar partial_update.xml
</code></pre><p>ちなみに、上記post.jarのオプションで、「-Durl」「-Dout」を追加してあります。
「-Durl」はverions=onというパラメータを追加したいためです。
「-Dout」はPOSTした結果をターミナルに表示するために追加しています。
これらのオプションを指定すると、データ更新後のバージョンが取得できるようになります。</p>
<p>####更新に利用できるコマンド？
部分更新にはつぎの3つのコマンド？（正式名は不明）が用意されています。fieldタグのupdate属性に指定します。</p>
<table class="list_view">
  <thead>
    <tr>
      <th>コマンド？</th>
      <th>説明</th>
    </tr> 
  </thead>
<tbody>
<tr>
  <td>add</td>
  <td>値を追加します。multiValuedのフィールドでない場合はエラーが出ます。</td>
</tr>
<tr>
  <td>set</td>
  <td>値を新規に登録しなおします。現在入っているデータは無くなります</td>
</tr>
<tr>
<td>inc</td>
<td>指定された数値を加算（数値形式のみ）</td>
</tr>
</tbody>
</table>
以上が、部分更新の機能になります。
ちなみに、登録されているバージョンと更新データに入っているバージョンが異なる場合はエラーが発生する仕組みになっているようです。
<p>それとは別に、この機能を調べていて、copyFieldのバグにぶつかってしまいました。。。
multiValuedでない、copyFieldを利用しているしている場合には注意が必要です。</p>
<p>###copyFieldのバグ（SOLR-3502）</p>
<hr>
<p>4.0-ALPHA（3.6.0でも再現しました。）のexampleのデータで部分更新の機能を確認できると言いました。
ただし、「price_c」というフィールドのせいで、2回部分更新を行うと2回目にエラーが発生します。
根本的な問題は、部分更新ではなく<a href="https://issues.apache.org/jira/browse/SOLR-3502">copyFieldのバグ</a>のようです。（部分更新の処理にも問題は有るような気がしますが。。。）</p>
<p>バグの内容はつぎのとおりです。</p>
<ul>
<li>multiValued=&quot;false&quot;のフィールドをdestに指定</li>
<li>srcに指定されたフィールドに値を設定（exampleのpriceフィールドに「1」を指定）</li>
<li>destに指定されたフィールドに値を設定（exampleのprice_cフィールドに「2,USD」を指定）</li>
</ul>
<p>上記のように設定した場合、「price_c」フィールドに、指定された値＋「price」の値がcopyにより追加されます。
通常は「price_c」フィールドはmultiValued=&quot;false&quot;なのでエラーが出るはずなのですが、エラーが発生せず2つの値が登録されてしまいます。</p>
<p>このバグのため、exampleのデータを利用して部分更新を行うとつぎのような状態が発生します。
更新を行う対象のデータはprice、price_cフィールド以外のフィールドとします。</p>
<ul>
<li>1回目の登録後：priceフィールド「&ldquo;185.0&rdquo;」、price_cフィールド「&ldquo;185.0,USD&rdquo;」</li>
<li>2回目の登録後：priceフィールド「&ldquo;185.0&rdquo;」、price_cフィールド「[&ldquo;185.0,USD&rdquo;,&ldquo;185.0,USD&rdquo;]」</li>
<li>3回目の登録：エラーが発生</li>
</ul>
<p>部分更新の処理で、すでに登録済みのデータをSolrが自動で取り出すため、2回目の登録処理にて「price_c」の登録済みの値がSolrから取り出され、さらにcopyField設定により、「price」の値が追加されます。
本当は2回目の登録でエラーが発生すべきなのですが、バグのためエラーが発生せずに登録できてしまいます。
部分更新の処理としては、copyフィールドのdestに指定されているフィールドの値を取り出さないほうがいいような気もしますが、きちんと考えてないのでなんとも言えないです。（制約事項とする形のほうがいいかもしれません）</p>
</content:encoded>
    </item>
    
    <item>
      <title>MIR輪読会始めました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/13/mir%E8%BC%AA%E8%AA%AD%E4%BC%9A%E5%A7%8B%E3%82%81%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Fri, 13 Jul 2012 01:41:38 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/13/mir%E8%BC%AA%E8%AA%AD%E4%BC%9A%E5%A7%8B%E3%82%81%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>Modern Information Retrieval: The Concepts and Technology behind Search (2nd Edition) (ACM Press Books) いやぁ、蒸し暑くてなかなか寝れない日がはじまりましたね。（あんまり関係ないですね。。。） Modern Information Retrieval 2nd Editionを</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/0321416910/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=0321416910&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/0321416910/?tag=johtani-22">
      Modern Information Retrieval: The Concepts and Technology behind Search (2nd Edition) (ACM Press Books)
      </a>
    </p>
  </div>
</div>
いやぁ、蒸し暑くてなかなか寝れない日がはじまりましたね。（あんまり関係ないですね。。。）</p>
<p>Modern Information Retrieval 2nd Editionを輪読会という形で読み始めました。
Solrに関わって数年ですが、昔から検索をやっていたわけではありません。
なので、そろそろ基礎的、理論的なところも勉強して行かないとなと思い、この本を買いました。
ただ、約1000ページある英語の本でして。。。
一人で読むと間違いなく挫折するし、理解不能になりそうだなと。。。</p>
<p>ということで、Twitterで呟いたら賛同してくれる方が現れ、輪読会を開催することにしました。
イベントの開催とか初めてなので、手さぐりしながらです。（それにしても、ほんと、Twitterは素晴らしい。賛同してもらえる人が見つかったのもTwitterのおかげだし。）</p>
<p>さすがに細かく読んでいくと終わらなそうなので、１周目（できれば、２周目もやりたいなぁと思ってる。１週目が１年でも終わりそうにない感じだけど）は公開されているスライドを元に進めようと思ってます。
それにしても検索周りはいろんな技術が必要なのだなぁと分厚い書籍を見て、途方に暮れつつ、楽しみでもあるなと思いながら、輪読会後の飲みを楽しんでましたｗ</p>
<p>ということで、各分野の専門家もいそうなので、特別ゲストとして読んできて話に混ざってもらうのも面白いかもと夢想しつつブログを書いています。
だれかいないかなーｗ</p>
<hr>
<p>参考URL：
書籍のHPで公開されているスライドのページです。
<a href="http://grupoweb.upf.es/WRG/mir2ed/contents.php">http://grupoweb.upf.es/WRG/mir2ed/contents.php</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのLucene/Solr4.0-ALPHA対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/05/lucene-gosen%E3%81%AElucene-solr4-0-alpha%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Thu, 05 Jul 2012 12:11:49 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/05/lucene-gosen%E3%81%AElucene-solr4-0-alpha%E5%AF%BE%E5%BF%9C/</guid>
      <description>Lucene/Solrの4.0.0-ALPHAが7/3にリリースされました。 これに伴い、lucene-goenの4xブランチのjarファイル</description>
      <content:encoded><p>Lucene/Solrの4.0.0-ALPHAが7/3にリリースされました。</p>
<p>これに伴い、lucene-goenの4xブランチのjarファイルも4.0-ALPHAのものに置き換え、現在のtrunkの修正もマージしました。
<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2F4x">こちらに</a>あります。チェックアウトしてビルドしてから利用してください。</p>
<p>※さすがに、jarをダウンロードできるようにすべきかもなぁ。
あと、Maven登録も。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Solr勉強会第8回に参加しました。 #SolrJP(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/04/solr%E5%8B%89%E5%BC%B7%E4%BC%9A%E7%AC%AC8%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-solrjp/</link>
      <pubDate>Wed, 04 Jul 2012 20:40:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/04/solr%E5%8B%89%E5%BC%B7%E4%BC%9A%E7%AC%AC8%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-solrjp/</guid>
      <description>またまた参加しました。いまだ皆勤賞です。 感想などはあとで。とりあえず、メモとったので第一弾です。 ということで、感想です。 まずは、参加人数。 今</description>
      <content:encoded><p>またまた参加しました。いまだ皆勤賞です。
<del>感想などはあとで。とりあえず、メモとったので第一弾です。</del></p>
<p>ということで、感想です。
まずは、参加人数。
今回は今までで一番、ATND登録した人が多かったんじゃないかなぁと。
埋まるのも早かったですし。やっとSolrというキーワードが多くの方に触れられるようになってきたんですかねぇ。</p>
<p>mixiの事例はやはり、SSDを使った11億文書のインデックスが圧巻です。
実際にマイニングに利用していて、ネガポジ分析なども行われているようで楽しそう。
TLにもありましたが、「ヤバイ」はネガ？ポジ？など、そのへんの分析方法をもう少し詳しく聞いてみたい感じもしました。
あとは、Luceneソースコードリーディングの開催が楽しみです！（候補日知らせないと。。。）</p>
<p>Lucene Revolution 2012の参加レポートは、自己紹介がおもしろかったですｗ
ずっと検索をやらてているのもあり、色々と理論ではなく、実践的なノウハウを持っていそうで、つぎはそのあたりの話を聞いてみるのも面白そうです（発表してくれないかなーｗ）
残念ながら、私はまだスライドを見ていないので、事例を中心にピックアップして見てみようかなぁと（時間がトレない。。。）</p>
<p>最後は阿部さんの4.0の紹介です。タイムリーに、前日に4.0-ALPHAがリリースされたので、
資料がすごく参考になりそうです。
SolrCloudについても詳しく書かれてたし。（ちゃんと動くのかなぁ？）</p>
<p>最後は懇親会です。最近知り合った方から、発表者、昔からの勉強会の参加者といろいろな方と今回も話ができて楽しかったです。
TL上で知り合った方にもお会いできたし。
次回もしゃべってもらえそうな人を捕まえつつあるので、また企画してもらうようにつついてみようかな。</p>
<p>※そういえば、毎度のことながら4.0ベースで、書籍は出さないのかって言われましたｗ</p>
<p>※ちなみに、4.0-ALPHAが出たので、lucene-gosenも4xブランチの更新作業をしています。
終わったらまたブログに書くと思います。</p>
<hr>
<pre><code>
第8回Solr勉強会
場所：VOYAGE GROUP 会議室
日時：7/4(水)　19:00 ～
1. ＠haruyamaさん
　　mixi での Solr の利用
　・mixiの全文検索
　　2011年以前：Hyper Etraier、Tokyo Dystopia、Senna
　　2011年以降：Solrを利用して新規案件の検索システムの構築、入れ替えを行なっている。
　・Anuenueの論理構成など。
　・物理構成
　　1マスター、2スレーブ
　　インデックスが小さい、QPSが100以下
　　インデックスサイズが大きいものは今後構築予定
　・今後やりたいこと
　　・ログ分析
　　・パーソナライズ
　　・外部ストレージ参照のカスタム関数
　・外部ストレージをファンクションカスタム関数クエリ
　　FunctionQueryを活用したい。
　・上記のデモ（検討中のもの？）
　　現在はjar内部のファイルを読んでるよと。
　　速度的な面がどうなるかがきになるところ。
　・テキストマイニング
　　mixiボイス
　　　haruyamaさん入社前：ダンプして解析してた
　　　haruyamaさん入社後：Solrに載せちゃえば

　　600GのSSD　　　
　　　約11億文書
　　　約450GB
　　利用してるもの：Solr 4.0（2012/01）
　　lucene-gosen 1.2.1
　　自作フィルタ
　　　haruyama/solr-filter - GitHub
　
　・利用統計の説明。
　　女性が多い。
　　「AKB」だと20代前半が多い。男性はおっさんも頑張ってる。
　・mergeindex機能を利用して、過去データとマージしてる。
　　1日分だけ集計したいこともあるかもしれないから。
　　updateじゃなくて、mergeindexなのは、ソッチのほうが早かったから。
　・拡張してる分析
　　・ポジネガ分析
　　　形容詞＞絵文字＞顔文字でスコアが効く
　　　機械学習して辞書を調整してる
　・Luceneソースコードリーディングまたやりますよ！

2. 楽天株式会社 大須賀 稔さん
　　Lucene Revolution 2012 in Boston参加レポート(仮)
　・まずは自己紹介。
　　infoseekに転職→楽天→Ask.com→楽天（そして英語）
　・Lucene Revolutionってなに？
　・トレーニング
　　Scaling Search with Big Data &amp; Solr
　　　Hadoopの紹介
　　　SolrとHadoopのMapReduceを利用したインデキシングのハンズオン
　　　Solrのスケーリング（Sharding、Replication）、マルチテナント
　　　※http://www.lucidimagination.com/services/training/big-data-training-scaling-solr
　　　日本ではやってない、残念。

　・カンファレンス
　　スライドとかはlucidimaginationのサイトで見れるよと。
　　　http://www.lucidimagination.com/devzone/events/conferences/lucene-revolution-2012
　　・Lucidworks Big Dataの紹介
　　　Hadoopとかいろいろ組み合わせて使えるよと
　　・Microsoftの人がAzureでSolrの紹介
　　　IEとかWindows8の話ばっかり。
　　・Kuromojiの紹介
　　　やはり、マイノリティ。
　　　内容は日本語勉強会ｗ
　　　中国語とかは対応するの？日本語しか知らないです。。。
　　・ErickさんのSolrCloudの話
　　　4.0は2012年にリリースする予定
　　　スコアリングをプラガブルに。
　　　管理系画面がリッチだよと。
　・一番重要だなぁと思ったのは。。。
　　「英語」！（会社的な感想ではありません。。。）
　Q：これはみとけ的なスライドは？
　A：Hadoop上でインデキシングして、ビットトレントとかで連携してるという例が面白かった。
　Q：FASTとかと比べてSolrってどーなの？
　A：ESPは洗練されてる。クローラーとか、ベイシスのトークナイザーを内包してるとか。
　　　Solrは言語処理系が弱かったとかあるけど、そろってきてるのでは。
　　　4.0は互角になるんじゃないかなぁ。
　　　ESPがWindowsオンリーになるので、LinuxユーザがSolrに行きつつある。

3. 株式会社 ロンウイット　阿部さん
　　Solr 4.0の紹介
　・Solr 4.0の主な機能の紹介
　　　3.xは3.6が最後4.0-ALPHAが7/3に出た
　　・プラガブルなスコアリング
　　　BM25、Language Models、Divergence from Randomness、Information-based Models
　　　関口さんがスライド作ってる
　　・FST対応
　　　Finite State Automata/Transducer
　　　オートマトン理論を活用したもの。
　　　TokenStreamはFSAで実装
　　　SynonymFilterがFSTになると、オフセットが変わってくるらしいと。
　　・Codecプラグイン
　　　Luceneレベルのお話。
　　　ドキュメントをファイルに保存するときの形式をプラガブルに変更可能。
　　　SimpleTextなどもあるらしい。テストに利用できそう。
　　　APIレベルで、マイグレーションの必要があるかも。
　　・NRT
　　　Near Real Time Search
　　　　softCommitのお話
　　　　Realtime-get：IDを入れたらGETできるよと。
　　　　KVSとしても活用できるぞ～と。
　　・PivotFacet
　　　Facetが階層的（？）な感じで取れる
　　・JOIN、pseudo-join
　　　ローカルパラメータでできるよーと。
　　・SolrCloud
　　　インデックスの分散配置をやってくれる（3.6まではやってくれない）
　　　shardがダウンしたらフェイルオーバーしてくれそう
　　　Master/Slave環境
　　　リアルタイムインデクシングとリアルタイム検索とか
　　　・ZooKeeperIntegration実装
　　　　リーダー選出、コンフィグの管理などなど

　・ManifoldCFの近況
　　5月にトップレベルに昇格！
　　http://manifoldcf.apache.org/ja_JP/index.html
　　0.6は7月に出そう。日本語にもなってる。すげー
　　Alfresco Connector、ElasticSearch Connectorなども
　　Solr Plugin for Enterprise Searchとか
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>Hadoopソースコードリーディング第10回に参加しました。#hadoopreading(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/26/hadoop%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E7%AC%AC10%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-hadoopreading/</link>
      <pubDate>Tue, 26 Jun 2012 01:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/26/hadoop%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E7%AC%AC10%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-hadoopreading/</guid>
      <description>Hadoopからはちょっと離れているのに、面白そうなネタなので参加しました。 Data Science Summit、HBaseCon、Hadoop Summitのイ</description>
      <content:encoded><p>Hadoopからはちょっと離れているのに、面白そうなネタなので参加しました。
Data Science Summit、HBaseCon、Hadoop Summitのイベント参加レポートです。</p>
<p>最初の草薙さんの発表が実は一番興味を惹かれていたので、参加しました。
データの解析に関するサミットというのはなかなか聞いたことが無いのでどんな内容なのかなぁと。
ちょうどVisualizeなどに興味を持っていたり、データ解析、今後重要ですよねという話が出ていたりしていたので。
実際にデータ解析が今後重要で、どんなことに使えるのかなど、製品に偏らない内容のようで色々とためになりました。
この内容をずっと聞くのは私には無理ですw英語も数学もイマイチなので、ついていけない自信がありますｗ。
「データ分析の結果をビジネスに結び付けられる人とかが今後重要になります」という話が一番気になったキーワードでした。</p>
<p>HBaseConもかなり濃い内容だったようです。
私は残念ながら、HBaseの概要の概要くらいしか知らないので、内容にはついていけてないですが。。。
Facebookがかなり活用しているようですが、残念ながらスライドが上がっていないようです。
Solrに関連する話もあったようです。HBaseとSolrを組み合わせた<a href="http://www.lilyproject.org/lily/index.html">Lilyプロジェクト</a>に関連する話のようでした。
<a href="http://www.hbasecon.com/sessions/lightning-session-getting-real-about-interactive-big-data-management-with-lily-hbase/">スライド</a>は登録しないと見れないみたいです。</p>
<p>最後はHadoop Summitの参加レポートです。
まずは、ユーザ寄りの内容を@muddydixonさんから。個人的に、Twitterの話が多いのかなぁと。
ここでも、Visualizeの話が出ていたとか。
Lucidの話もあったようです。<a href="http://www.lucidimagination.com/products/lucidworks-search-platform/lucidworks-big-data">LucidWorks BigDataの話かな？</a></p>
<p>最後は@shiumachiさんのHadoopのプロダクト寄りのお話。
YARN（Map/Reduce2.0？）やHBaseの今後の展望など。YARNはキーワードだけ知っていたので、わかりやすい解説で、やっと理解できました。
全体を通して、HBaseが今後もっといろんな局面で使われそうだなぁと。日本語の本も7/24に出るし。（まだページが無いみたいなので、<a href="http://shop.oreilly.com/product/0636920014348.do">英語のほうを。</a>）</p>
<p>いつものごとく、途中でビールが入ったので後半はメモが適当ですが、楽しかったです。皆さんお疲れ様でした。</p>
<p>帰り着いたら、さっそくスライドが上がってました。すごい！
スライドはこちら（2012/06/26 0時現在）</p>
<ul>
<li><a href="http://www.slideshare.net/nagix/data-science-summit-2012-13444399">Data Science Summit / EMC Worldレポート</a></li>
<li><a href="http://www.slideshare.net/hadoopxnttdata/hbasecon-2012">HBaseCon 2012 参加レポート』の発表スライドをアップしました。（NTTデータ 猿田 @raspberry1123 ／岩崎）</a></li>
<li><a href="https://docs.google.com/presentation/d/1I4UTI-ylJc9iLWa0fHkPbIT4icjC4xnabBMX2c-6sHE/present#slide=id.p">@muddydixonさんのHadoopSummit2012参加レポート</a></li>
<li><a href="http://www.slideshare.net/shiumachi/hadoop-summit-2012-report">@shiumachiさんのHadoopSummit2012参加レポート</a></li>
<li><a href="http://d.hatena.ne.jp/muddydixon/20120617/1339919125">@muddydixonさんのブログ</a></li>
</ul>
<p>いろんなキーワード、特に、データサイエンス寄りの話が面白かったです。
次回はJenkins関連のようで、7/30開催みたいです。</p>
<p>以下はいつものメモです。</p>
<hr>
<pre><code>
日　時： 2012年6月25日（月） 19:00～21:00 （受付開始 18:40）
場　所： 豊洲センタービルアネックス（NTTデータ、豊洲駅直通）

◯ Data Science Summit / EMC World レポート （EMC Japan 草薙）
　・Data Science Summitのレポート
　　揃ってるようで揃ってないスロットの写真ｗ
　　EMC World2012と併設
　　今回2回目。
　・OpeningKeynote: What We Can Predict About Prediction by Nate Silver
　　研究者は不確実性やリスクを包含した、現実的な予測モデルを開発すべき
　　　いろいろ
　・Roundtable: Economic, Political, &amp; Societal Roles of Social Data
　　ユーザの「query-like intent」を自然言語解析と機械学習で捉える
　　属性だけじゃなく、活動を
　　コンテンツからコンテキストへ、コンバージョンからカンバセーション
　・Big Data Transformation - HealthMap
　　ウィルスのアウトブレイクの検知（160日→20日）に。
　　ウィルス＝コンピュータじゃない方
　・Big Data Transformation - Intuit
　　int.com？
　　データがあるから、こういうことが知りたいなどの新しいプロセスが
　　「https://www.mint.com/」
　・Big Data Transformation - InfoMotion Sports Technologies
　　バスケのボールにセンサーつけて、試合や選手の解析に利用して、
　　チームが強くなったりしてるらしい。
　・Big Data Transformation - Decide.com
　　アメリカ？の価格比較サイト
　　ソーシャルデータを元に、買い時、売り時を予測して教えてくれる。
　　もうすぐ新しいモデル出るかもよ？などの噂を利用してる。
　・Analytics Maturity: Master or Novice?
　　2010年のアメリカの教育事情のレポート
　　データサイエンス系の統計とかがもっと必要なんじゃないか。
　・Keynote: Navigating the Road from Business Intelligence to Data science
　　Piyanka Jain
　　BIの限界とか、データサイエンスの恩恵を受け入れるのに必要なもの？など
　　システムじゃなくて、人やスキルに投資しましょう。
　　データ分析の結果をビジネスに結び付けられる人とか。
　Panel: From Raw Data とValue Data
　　プライバシー問題
　　データ品質の問題
　　　異常値を除外するなと。最も興味深いデータになることもある。
　　&quot;Data exhaust&quot;の問題
　　　個人が日々インターネット上で行う様々なインタラクションに関するデータの集合
　　　相関と因果関係の区別が大変難しい。
　Panel: Tapping Into the Pulse of the Data Science movement
　　ストーリを持ってデータを語れるのが重要。
　Keynote: Data Visualizeation at the Point of Influence by Adam Bly
　Closing Keynote: The Promise and Peril in the Human / Technology Relationship by Jonathan Harris
　　TEDで有名な人
　まとめ
　　アメリカはこの分野での投資は回り始めている.
　　http://www.greenplum.com/datasciencesummit/


◯ HBaseCon レポート （NTTデータ 猿田／岩崎）
　・GeneralSession
　　HBaseの開発に参加してね。
　　・レプリケーション、セキュリティ、セカンダリインデックス、スナップショット、バックアップなど特に貢献して欲しいと。
　　・HBaseによりカバーできるアプリケーション領域
　　　メッセージング、位置ベースアプリケーション、リアルタイムレコメンデーション、広告最適化
　　・開発者への要望
　　　メッセージをちゃんとして
　　　解析用メトリクス
　　　管理ツール
　　・M/Rジョブとの連携の改善
　　・自動チューニングなど
　・Applications
　　・GAPの事例
　　　色々試してHBaseにした。
　　　クラスタ構成、16Slave、3Master、NN Failover via NFS
　　　※ZKはスレーブに置くと、アウト。
　　・Tumblr
　　　最初は失敗した。
　　　OpenTSDBを経験して、Motherboy V1に。
　　　　テストフェーズまでが目的
　　　　→幾つかの知見が得られたよーと。
　　　Motherboy V2を構築中。
　　・Facebook
　　　ひと月250TBペースで増加中。。。
　　　なんで、HBaseにしたの？
　　　　低レイテンシ、水平スケール、一貫性重視の設計、M/Rとの親和性とか
　　　Schema V1＝Snapshot Schema
　　　Schema V2＝Split Snapshot Schema
　　　Schema V3＝Hybrid Schema
　　　　HBaseのデータにさらにインデックス作ったりとか。
　　　Schema Vertion Current＝Finer grained schema and Indexer
　　　　読み書きの単位を分析して、スキーマを分割して細かくして行っていた。
　・Operations
　　・HBaseで困ったよ at Facebook
　　　・リージョンサーバに特定データ入れると連鎖的に死亡
　　　・サーバが死に切らないとか。
　　　・HBaseは落ちるときは落ちるので、しぶとく生き残れるアプリを作ってねと。
　　　※Facebook関連のスライドなどがあがってませんと。
　　・HBaseバックアップについて Clouderaの人とFacebookの人
　　　・選択肢
　　　　DistCP
　　　　　/hbaseディレクトリまるごとコピー。一貫性が保証されない
　　　　exportツール
　　　　　MRジョブ。1度に1つのテーブルのみ
　　　　copytableツール
　　　　　データを別クラスタに保存
　　　　　exportツールに似てる。
　　　　レプリケーション
　　　　　。。。
　　　　アプリケーションから複数クラスタへの書き込み
　　　　　。。。
　　　・ユースケース
　　　　・HBASE-5509を利用したバックアップ
　　　　　・開発中バックアップ
　　　　　　HBASE-6055スナップショット、HBASE-4618など

　・Development
　　・HBase Schema Design　Salesforce.comの人
　　　非正規化でやりなさいと。Joinはおすすめしないと
　　　最後にデザインパターンが載ってると。
　　　0.行キーの設計がすべて
　　　1.Design for the questions, not the answers.
　　　2.データサイズは2種類しかない。大きすぎるか、そうじゃないか。
　　　3.コンパクトに詰め込め
　　　4.行単位の原子性を活用する。
　　　5.属性は行キー内に移動することができる。（Lars Georgeがfoldingと読んでいる手法）
　　　6.エンティティをネストさせると、データを事前に集計できる。

　　・HBase Performance Tuning And Organizations　Facebookの中の人
　　　テーブルの事前スプリット
　　　オフピーク時間を設定する。（0.94で入った）
　　　コンパクション設定が効いてくるよ
　　・SolrのバックエンドにHBase？？？


◯ Hadoop Summit レポート (@muddydixonさん)
　・オープニングビデオは必見
　・ショーケース
　　・Datameerを注目した。Visualizationの専任チームがいる。
　　　UIじゃなくて、解析部隊だけで60名
　　　お値打価格。$2900/年
　　・Azure Big Data
　　　JavaScriptで
　　・ショーケースLucid
　　　Solr、HBaseとかいろいろ組み合わせて。
　・Session Hadoop...
　　2015までにApache Hadoopに世界のデータの半分が乗るらしいと。 
　・Session Realtime analytics with Storm and Hadoop
　　フォロワのフォロワをunique処理したり。
　・Session Scalding Twitter's new DSL for Hadoop
　　※Zipkinにも出てきたな、名前。
　・Hadoop Plugin for MongoDB
　・Hadoop and Vertica The Data Analytics Platform at Twitter
　　Twitter社でのHadoop周りのデータフローとか。
　　Verticaは速度がいるものの処理に利用
　　80-100TB/dayとか。。。
　・Keynotes
　　・Big data analyzing system is censor of company.
　　・It is difficult in blind and deaf.
　・Session Large Scale Search, Discovery and Analytics with Hadoop, Mahout and Solr
　※@muddydixonさんのブログにセッションに対応するスライドのリンクを公開中。http://d.hatena.ne.jp/muddydixon/20120617/1339919125

◯Hadoop Summit レポート (@shiumachiさん)
　・Hadoop1.x MapReduce
　　非常に安定
　　・課題。。。
　・YARN（Yet Another Resource Negociator）
　　ターゲット：6000～10000ノード
　　　　　　　　100,000以上のタスクの同時実行
　　　　　　　　10,000ジョブの同時実行
　　性能は倍以上
　　　Q：これは同じプログラムを動かして？
　　　A：。。。倍以上です！（わかんないっす。）
　　今後の予定
　　　幾つかのJIRAの紹介
　　　MAPREDUCE-4327とかHBASE-4329とか
　まとめ
　　YARNは「汎用」分散処理基盤に向けて一歩踏み出したもの！
　　更なる進化に注目と
　・HBaseの可用性とリペア
　　ダウンタイムを短くしよう。
　　　障害停止7割くらい。
　　　　設定ミスが44%
　　不安定な機能は使うな、推奨構成を推奨！
　　HBase 0.92＋Hadoop 2.0
　　　HDFS HAによる高可用性とか
　　将来：HBase 0.96＋Hadoop 2.x
　　　計画停止時間削減：オンラインスキーマ変更（HBASE-1730）
　　　ローリングアップデート：バージョン間互換性が必須
　　HBase本読みなさい。
　　金があれば、サポート買ってください。
　　日本語HBaseトレーニング開催予定（来月）
　・HBase NameNode HighAvailability
```　　
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのUniDic対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/18/lucene-gosen%E3%81%AEunidic%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Mon, 18 Jun 2012 23:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/18/lucene-gosen%E3%81%AEunidic%E5%AF%BE%E5%BF%9C/</guid>
      <description>Issue 32で上がってきたlucee-gosenのUniDic対応の最初のパッチを書いたので、ブログに残しておきます。 ###UniDicとは___</description>
      <content:encoded><p>Issue 32で上がってきたlucee-gosenのUniDic対応の最初のパッチを書いたので、ブログに残しておきます。</p>
<p>###UniDicとは___
<a href="http://www.tokuteicorpus.jp/dist/">UniDic</a>とは、日本語テキストを単語に分割し，形態論情報を付与するための電子化辞書です。
<a href="http://www.tokuteicorpus.jp/dist/">UniDicの詳細や特長についてはHP</a>を御覧ください。</p>
<p>残念ながら、UniDicは利用者登録をして、利用規約に従うと利用が可能となります。
ですので、lucene-gosenでは、Ipadicやnaist-chasenの辞書とは異なり自動で辞書をダウンロードする機能はありません。</p>
<p>###利用手順___
以下が、Unidicの辞書を利用したjarファイルの作成方法となります。</p>
<p><b>1. lucene-gosenをダウンロードし、パッチを当てる</b>
lucene-gosenのリポジトリからソースをエクスポートし、パッチをダウンロードし、パッチを適用します。
コマンドは以下のとおりです。</p>
<pre><code>
svn co . lucene-gosen-trunk-readonly
cd lucene-gosen-trunk-readonly
patch -p0 &amp;gt; ...patch
</code></pre><p>（パッチに関しては今後正式版をリリースされたら手順からは必要なくなります。）</p>
<p><b>2. Unidic辞書生成のためのディレクトリを作成「$GOSEN_HOME/dictionary/unidic」</b></p>
<pre><code>
mkdir dictionary/unidic
</code></pre><p><b>3. 対象となるUnidicの辞書のソースファイルをダウンロード</b>
利用者登録をし、利用規約に同意の上、以下のファイルをダウンロードします。
「/」に添ってダウンロードページから遷移してダウンロードしてください。</p>
<pre><code>
1.3.12個別ファイル/unidic-chasen/unidic-chasen1312src.tar.gz
</code></pre><p><b>4. ダウンロードしたtar.gzファイルを「dictionary/unidic/」にコピー</b></p>
<pre><code>
cp .. lucene-gosen-trunk-readonly/dictionary/unidic/
</code></pre><p><b>5. Antを実行してjarファイルの作成</b></p>
<pre><code>
ant -Ddictype=unidic
</code></pre><p>成功すれば、lucene-gosen-trunk-readonly/dist/lucene-gosen-2.1-dev-unidic.jarファイルが生成されます。
あとは、通常通り、SolrやLuceneで利用することが可能です。</p>
<p>以上がjarファイルの作成手順となります。</p>
<p>###制約事項（2012/06/18現在）___
現在（2012/06/18）時点で公開しているパッチは、以下の制約が存在します。</p>
<ul>
<li>COMPOUNDエントリー未対応</li>
<li>品詞情報（発音）の内容の制限</li>
</ul>
<p><b>COMPOUNDエントリー未対応</b>
Unidicの辞書のエントリーの中に1件だけ、COMPOUNDと呼ばれるエントリーが1件だけ存在します。
別々の単語を組み合わせて1単語として扱うことができるようになっているようです。
lucene-gosenでは、残念ながら、このような辞書の形式には対応していません。
1件しか存在しないデータでもあることを鑑みて、今回の辞書構築処理では、スキップするようにしました。</p>
<p><b>品詞情報（発音）の内容の制限</b>
lucene-gosenの実装上、単語の読みのバリエーション数と発音のバリエーション数には以下の制限が存在します。</p>
<pre><code>
「読み」バリエーション数  ＜ 「発音」バリエーション数
</code></pre><p>「読み」に対応する形で、「発音」がlucene-gosenでは品詞情報としてデータ登録されています。
UniDicのデータには上記制約を満たさないデータが5件ほど存在します。
現在、これら5件のデータについて、「読み」に対応した「発音」データには空文字が表示されるようになっています。</p>
<p>まだ、簡単に動作確認をしただけです。UniDicを利用していて問題など有りましたら連絡、Issueへのアップをしていただけると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>ZipkinのReadme読んでる（その２、残り）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/16/zipkin%E3%81%AEreadme%E8%AA%AD%E3%82%93%E3%81%A7%E3%82%8B%E3%81%9D%E3%81%AE%EF%BC%92%E6%AE%8B%E3%82%8A/</link>
      <pubDate>Sat, 16 Jun 2012 00:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/16/zipkin%E3%81%AEreadme%E8%AA%AD%E3%82%93%E3%81%A7%E3%82%8B%E3%81%9D%E3%81%AE%EF%BC%92%E6%AE%8B%E3%82%8A/</guid>
      <description>「鉄は熱いうちに打て」ということで、残りも勢いでメモ。 まだ、見直しとかしてない状態なのでおかしいところとかありますが。。。 図とか入れるのはま</description>
      <content:encoded><p>「鉄は熱いうちに打て」ということで、残りも勢いでメモ。
まだ、見直しとかしてない状態なのでおかしいところとかありますが。。。
図とか入れるのはまた今度。</p>
<p><b>　Transport（転送）</b></p>
<pre><code>
　ZipkinとHadoopに異なるサービスからのトレースを送信するのにScribeを利用します。
　ScribeはFacebookにより開発されました。
　システムの各サーバで実行できるデーモンとして作成されています。
　ログメッセージをListenし、カテゴリごとのcorrectレシーバーに配送します。

</code></pre><p><b>　Zipkin collector daemon</b></p>
<pre><code>
　トレースデータがZipkinコレクターデーモンに配送されたらvalidかどうかをチェックしてから保管し、インデックスを作成します。

</code></pre><p><b>　Storage</b></p>
<pre><code>
　ストレージにはCassandraを選びました。
　スケーラブルで、柔軟なスキーマをもっており、Twitter内部で大変使われています。
　このコンポーネントをプラガブルにしようと試みましたが、困難なため、ここでは公開しません。

</code></pre><p><b>　Zipkin query daemon</b></p>
<pre><code>
　保存、インデックスされたデータを探す方法が必要です。
</code></pre><p>　クエリーデーモンはユーザに対して簡単なThriftAPIを公開しており、トレースを探すことが可能です。<a href="https://github.com/twitter/zipkin/blob/master/zipkin-thrift/src/main/thrift/zipkinQuery.thrift">Thrift file</a>を見て下さい。</p>
<p><b>　UI</b></p>
<pre><code>
　多くのユーザはUI経由でデータにアクセスします。
</code></pre><p>　Visualizeに<a href="http://d3js.org/">D3</a>を利用したRailsアプリケーションです。
　UIの認証は実装していないことに注意してください。</p>
<p><b>モジュール</b>
　<a href="https://github.com/twitter/zipkin/raw/master/doc/modules.png">Zipkin内部のモジュール関連図</a></p>
<p>##インストール___
<b>　<a href="http://cassandra.apache.org/">Cassandra</a></b></p>
<pre><code>
　ZipkinはCassandraをストレージにしています。Cassandraクラスタが必要になります。
　1. Cassandraサイトを参考にしてクラスタを構築してください。
　2. Zipkin Cassandraスキーマを利用します。つぎのコマンドでスキーマが作成できます。
</code></pre><pre><code>
bin/cassandra-cli -host localhost -port 9160 -f zipkin-server/src/schema/cassandra-schema.txt
</code></pre><p><b>　<a href="http://zookeeper.apache.org/">Zookeeper</a></b></p>
<pre><code>
　Zipkinは協調のためにZooKeeperを利用します。
　これは、保存すべきサーバをサンプルレートで決定し、サーバを登録します。？
　1. ZooKeeperのサイトを参考にインストールしてください。
</code></pre><p><b>　<a href="https://github.com/facebook/scribe">Scribe</a></b></p>
<pre><code>
　Scribeはトレースデータを配送するのに利用するロギングフレームワークです。
　ネットワーク保存先としてZipkinコレクターデーモンを指定する必要があります。

　Scribeの設定は次のようにします。
</code></pre><pre><code>
&lt;store&amp;gt;
  category=zipkin
  type=network
  remote_host=zk://zookeeper-hostname:2181/scribe/zipkin
  remote_port=9410
  use_conn_pool=yes
  default_max_msg_before_reconnect=50000
  allowable_delta_before_reconnect=12500
  must_succeed=no
&lt;/store&amp;gt;
</code></pre><pre><code>
　注意：上記設定は、カテゴリーにより送信ホストを見つけるためにZooKeeperを利用するサポートのScribeのTwitterバージョンを使用する方法です。
　コレクターのためのDNSエントリーを利用したりもできます。
</code></pre><p><b>　Zipkinサーバ</b>
　Zipkinサーバは<a href="http://www.scala-lang.org/downloads">Scala 2.9.1</a>、<a href="http://www.scala-sbt.org/download.html">SBT 0.11.2</a>そしてJDK6で開発しました。</p>
<pre><code>
 1. git clone https://github.com/twitter/zipkin.git
 2. cd zipkin
 3. cp zipkin-scribe/config/collector-dev.scala zipkin-scribe/config/collector-prod.scala
 4. cp zipkin-server/config/query-dev.scala zipkin-server/config/query-prod.scala
 5. Modify the configs above as needed. Pay particular attention to ZooKeeper and Cassandra server entries.
 6. bin/sbt update package-dist (This downloads SBT 0.11.2 if it doesn't already exist)
 7. scp dist/zipkin*.zip [server]
 8. ssh [server]
 9. unzip zipkin*.zip
10. mkdir -p /var/log/zipkin
11. zipkin-scribe/scripts/collector.sh -f zipkin-scribe/config/collector-prod.scala
12. zipkin-server/scripts/query.sh -f zipkin-server/config/query-prod.scala
</code></pre><pre><code>
　SBTでコレクターとクエリサービスを起動します。
　Scribeコレクターサービスの起動方法は次の通り。
</code></pre><pre><code>
bin/sbt 'project zipkin-scribe' 'run -f zipkin-scribe/config/collector-dev.scala'
</code></pre><pre><code>
　クエリサービスは次の通り
</code></pre><pre><code>
bin/sbt 'project zipkin-server' 'run -f zipkin-server/config/query-dev.scala'
</code></pre><p><b>　Zipkin UI</b></p>
<pre><code>
　UIはRails3アプリです。
　1. 設定をZooKeeperサーバでアップデートします。これはクエリデーモンを見つけるのに利用します。
　2. Rails3アプリケーションサーバにデプロイします。テストの場合は次のようにすることもできます。
</code></pre><pre><code>
bundle install &amp;amp;&amp;amp; bundle exec rails server.
</code></pre><p><b>　zipkin-tracer gem</b></p>
<pre><code>
　zipkin-tracer gemをトレースしたいRailsアプリケーションにRack Handlerで追加します。
　config.ruにつぎの記載をします。
</code></pre><pre><code>
  use ZipkinTracer::RackHandler
  run &lt;YOUR_APPLICATION&gt;
</code></pre><pre><code>
　もし、アプリケーションのstatic assetsがRailsで提供されれば、リクエストがトレースされます。

</code></pre><p>##　Running a Hadoop job<hr></p>
<pre><code>
　ScribeからHadoopにログを保存する設定をすることも可能です。
　これをすると、Zipkin自身でオンザフライで簡単に作れないデータから様々なレポートが作成可能です。
</code></pre><p>　ScalaでHadoopのジョブをかける<a href="https://github.com/twitter/scalding">Scalding</a>というライブラリを利用します。</p>
<div style=" margin-left: 2em;">
```
<p>　1. Hadoopジョブを実行するためのfatなjarを作成します。
　2. scald.rbをjarをコピーしたいホスト名とjobを実行するホスト名に書き換えます。
　3. 必要なら、scald.rbのjarファイルのバージョンを更新します。
　4. scald.rbスクリプトを利用してジョブを実行できます。</p>
<pre><code class="language-</div>" data-lang="</div>"></code></pre><p>./scald.rb &ndash;hdfs com.twitter.zipkin.hadoop.[classname] &ndash;date yyyy-mm-ddThh:mm yyyy-mm-ddThh:mm &ndash;output [dir]</p>
<pre><code>
##計測ライブラリの利用方法&lt;hr&gt;
</code></pre><p>　計測のためのライブラリとプロトコルがちょっとだけあります。
　しかし、もっと計測するための役に立つものを望んでいます。
　開始する前にトレースデータがどんな構造なのかを知る必要があります。</p>
<p>　　・Annotation - 値、タイムスタンプ、ホストを含みます。
　　・Span - 特定のRPCに相当するAnnotationの集合
　　・Trace - あるルートSpanにぶら下がるSpanの集合</p>
<p>　Zipkinに送信するトレースデータです。</p>
<pre><code>　これらの詳細な記述は[こちら](https://github.com/twitter/zipkin/blob/master/zipkin-thrift/src/main/thrift/zipkinCore.thrift)を見て下さい。
</code></pre><p>　その他にトレースデータの重要なものは、トレースされたサービス間でやり取りされる情報である、軽量なヘッダーです。
　トレースヘッダは次のものから構成されます。</p>
<p>　・Trace Id - トレース全体のID
　・Span Id - 個々のリクエストのID
　・Optional Parent Span Id - このリクエストが他のリクエストの一部として生成されたら付与される
　・Sampled boolean - トレースすべきかどうかを表すフラグ</p>
<p>　データタイプについて、理解したので、どのように計測が行われるかを順をおってみてみましょう
　例はFinagleのHTTPがどのようにトレースされるかを示しています。
　他のライブラリやプロトコルはもちろん、異なりますが、基本的な部分は一緒です。</p>
<pre><code>&lt;b&gt;　サーバサイド&lt;/b&gt;
　1. 到達したリクエストのトレースヘッダー存在するかどうかを調べます。存在すれば、なら、このリクエストに対して関連するIDとします。トレースヘッダーがなければ、サンプリング対象かどうかを決めて、新しいTrace Id、Span Idを生成します。参考には[HttpSererTracingFilter](https://github.com/twitter/finagle/blob/master/finagle-http/src/main/scala/com/twitter/finagle/http/Codec.scala)を見て下さい。

　2. もし、現在のリクエストがサンプリングされる場合、サービス名、ホスト名、スパン名（例えば、http get/put）、その他のAnnotationのような情報を集めます。
　　リクエスト受信時には「server received」というAnnotationを生成し、処理が終了して結果を返すときに「server send」というAnnotationを生成します。
　　参考には[HttpServerTracingFilter](https://github.com/twitter/finagle/blob/master/finagle-http/src/main/scala/com/twitter/finagle/http/Codec.scala)を見てください。

　3. 生成されたトレースデータはServerBuilderにより設定されたトレーサーに渡されます。
　　デバッグのためのConsoleTracerが例としてありますが、ZipkinTracerになります。
　　トレースデータを[ZipkinTracer](https://github.com/twitter/finagle/tree/master/finagle-zipkin)が受け取った時、Span Idにより集約されます。

　4. ZipkinTracerが&quot;end of span&quot;イベント（&quot;server received&quot;アノテーションやタイムアウトのような）を受け取ると、Thrift構造としてデータを集約してScribeに送ります。もし、そのようなイベントが発生しない場合、ZipkinTracerはいつかそのデータを送信します？？？おかしい？。データ送信のための他の方法も追加します。
　ThriftやScribeのようなものですが、JSONやHttpかもしれません？

&lt;b&gt;　クライアントサイド&lt;/b&gt;
　1. リクエストを送る前にそれがトレースの一部かどうかをチェックします。
　　サーバで利用されているとします。　
　　サーバは、リクエストを処理してすでに付与されているトレースIDを割り当てます。
　　トレースIDを再利用しますが、この新しいリクエストには新しいスパンIDを生成します。
　　また、親のスパンIDが存在すれば、前のスパンIDに設定します。
　　[ここ(TracingFilter)](https://github.com/twitter/finagle/blob/master/finagle-core/src/main/scala/com/twitter/finagle/tracing/TracingFilter.scala)や[ここ(Trace)](https://github.com/twitter/finagle/blob/master/finagle-core/src/main/scala/com/twitter/finagle/tracing/Trace.scala)が参考になります。

　2. サーバサイドと同様に、送信されるHttpリクエストにトレースヘッダーを追加するための[HttpClientTracingFilter](https://github.com/twitter/finagle/blob/master/finagle-http/src/main/scala/com/twitter/finagle/http/Codec.scala)があります。

　3. リクエスト送信前の「client send」やサーバからの返信を受信した「client receive」のようなアノテーションを生成します。

　4. サーバサイドと同様に、データがZipkinにデータを送るZipkinTracerに到達します。

</code></pre></content:encoded>
    </item>
    
    <item>
      <title>ZipkinのReadmeを読んでる（クライアント周りについて）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/15/zipkin%E3%81%AEreadme%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E3%82%8B%E3%82%AF%E3%83%A9%E3%82%A4%E3%82%A2%E3%83%B3%E3%83%88%E5%91%A8%E3%82%8A%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Fri, 15 Jun 2012 09:57:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/15/zipkin%E3%81%AEreadme%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E3%82%8B%E3%82%AF%E3%83%A9%E3%82%A4%E3%82%A2%E3%83%B3%E3%83%88%E5%91%A8%E3%82%8A%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>ZipkinのGithubにあるReadmeを読んでます。 せっかくというか、頭が悪いので読みながら内容をメモ。 まずは、アーキテクチャとトレー</description>
      <content:encoded><p>ZipkinのGithubにあるReadmeを読んでます。
せっかくというか、頭が悪いので読みながら内容をメモ。
まずは、アーキテクチャとトレースデータ送信のためのクライアント側あたりです。
（誤訳とかおかしいだろというツッコミ大歓迎です。）
あとで、リンク貼ったり絵を入れたりするかもしれませんが、とりあえず。</p>
<p><b>◯アーキテクチャ（<a href="https://github.com/twitter/zipkin/raw/master/doc/architecture-0.png">図はこちら</a>）</b></p>
<pre><code>
　・対象とするサービスからScribeでデータを収集し、ZipkinのCollectorに投げる
　・CollectorはCassandraにデータを格納
　・WebUIからはQueryでCassandraに問い合わせを行なってデータ取得
　・Scripe、CollectorはZookeeperと連携している（妄想）

</code></pre><p><b>◯計測用ライブラリ（<a href="https://github.com/twitter/zipkin/raw/master/doc/architecture-1.png">図はこちら。</a>図のSと書かれた青い箱）</b></p>
<pre><code>
　・各ホストの計測用ライブラリがトレースデータを集めてZipkinに送信する。
　・ホストは他のサービスへリクエストを飛ばすときに、リクエストにトレース用のIDを付与してます
　　こうすることで、データをあとで、束ねることが可能となります。

</code></pre><p><b>◯計測ライブラリの利用方法</b>
<b>　・<a href="https://github.com/twitter/finagle">Finagle</a></b></p>
<pre><code>
　　JVMのための非同期ネットワークスタックである。
　　それは、JVMベース言語（JavaやScalaなど）で非同期RPCのクライアント・サーバを構築するのに利用できる。
　
　FinagleはTwitterの内部ですごく利用されていて、トレースサポートを実現するのに当然のポイントです。
　現時点で（Finagleは）ThriftやHTTP、Memcache、Redisのクライアント・サーバサポートも持っています。

　ScalaでFinagleサーバをセットアップするのはつぎのようなコードになります。
</code></pre><p>　トレースを追加するには、<a href="https://github.com/twitter/finagle/tree/master/finagle-zipkin">finagle-zipkin</a>を追加して、ServerBuilderのtraceFactory関数を呼ぶだけです。</p>
<pre><code>
ServerBuilder()
  .codec(ThriftServerFramedCodec())
  .bindTo(serverAddr)
  .name(&quot;servicename&quot;)
  .tracerFactory(ZipkinTracer())
  .build(new SomeService.FinagledService(queryService, new TBinaryProtocol.Factory()))
</code></pre><pre><code>
　クライアント側も同様です。
　上記のようにサンプリングしたリクエストにZipkinトレーサーを指定することで
　自動的にトレースできるようになります。
　サービスやホストでのリクエストの開始と終了を記録できます。

　さらに付加的な情報を記録したい場合は、つぎのようなコードを追加します。
</code></pre><pre><code>
Trace.record(&quot;starting that extremely expensive computation&quot;)
</code></pre><pre><code>

　上記コードは、上記コードが実行された時間に文字列を付加できます。
　キーバリュー情報を付加したい場合は次のようになります。
</code></pre><pre><code>
Trace.recordBinary(&quot;http.response.code&quot;, &quot;500&quot;)
</code></pre><p><b>　Ruby Thrift</b></p>
<pre><code>
　リクエストのトレースに利用できるgemもあります。
　リクエストに対してトレースIDを生成し、リクエストに付与し、トレーサーにプッシュするのにRackHandlerのgemを利用できます。
</code></pre><p>　トレーサをトレースするサンプルは<a href="https://github.com/twitter/zipkin/blob/master/zipkin-web/config/application.rb">zipkin-web</a>を参照。</p>
<p>　Rubyからトレースクライアントをコールするのに、<a href="https://github.com/twitter/thrift_client">Twitter Ruby Thrift client</a>を使います。
　つぎのようなコードを書きます。</p>
<pre><code>
client = ThriftClient.new(SomeService::Client, '127.0.0.1:1234')
client_id = FinagleThrift::ClientId.new(:name =&gt; &quot;service_example.sample_environment&quot;)
FinagleThrift.enable_tracing!(client, client_id), &quot;service_name&quot;)
</code></pre><p><b>　<a href="https://github.com/twitter/querulous?PHPSESSID=2fb5ece17b7c5b72b57b7aed4a21ae1f">Querulous</a></b></p>
<pre><code>
　QuerulousはScala用のSQLデータベースのインタフェースライブラリです。
　SQLクエリの実行のタイミング情報をトレースに追加できます。
</code></pre><p><b>　<a href="https://github.com/twitter/cassie?PHPSESSID=2fb5ece17b7c5b72b57b7aed4a21ae1f">Cassie</a></b></p>
<pre><code>
　CassieはFinagleベースのCassandraクライアントライブラリです。
　CassieのトレーサーはFinagleの例とほぼ一緒ですが、
　CassieではKeyspaceBuilderに設定します。
</code></pre><pre><code>
cluster.keyspace(keyspace).tracerFactory(ZipkinTracer())
</code></pre><p>とりあえず、ここまで。___
<span style="color:#FF0000">2012/06/22 リンクを貼って体裁を修正</span></p>
</content:encoded>
    </item>
    
    <item>
      <title>autoGeneratePhraseQueriesのデフォルト値について(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/14/autogeneratephrasequeries%E3%81%AE%E3%83%87%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E5%80%A4%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Thu, 14 Jun 2012 01:09:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/14/autogeneratephrasequeries%E3%81%AE%E3%83%87%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E5%80%A4%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>久々にSolrの話です。 といっても、結構前からの話でして。。。 schema.xmlのfieldTypeの設定に「autoGeneratePh</description>
      <content:encoded><p>久々にSolrの話です。
といっても、結構前からの話でして。。。</p>
<p>schema.xmlのfieldTypeの設定に「autoGeneratePhraseQueries」という属性があります。
Solr3.1で導入されました。動作に関しては<a href="http://lucene.jugem.jp/?eid=403">関口さんのブログ</a>で説明されています。
Solr 1.4までは、Analyzerがトークンを複数返してくる場合（例：lucene-gosenで「Solr入門」という文字列を入れた場合など）にフレーズクエリとして処理していました。
Lucene 3.1.0から、この処理がデフォルトfalse（つまり、フレーズクエリにならない）という挙動になりました。（詳しくは<a href="http://lucene.jugem.jp/?eid=403">関口さんのブログ</a>で。）
ただ、Solr 3.1.0では、下位互換性を考慮して、autoGeneratePhraseQueriesの設定値はデフォルトが「true」でした。</p>
<p>このデフォルト値がSolr 3.3以降で提供されているschemaのバージョン（1.4以上）からデフォルト値が「false」に変更されています。
schemaのバージョンを1.3以前のものから1.4以上に移行する場合は注意が必要です。</p>
<p>とまぁ、偉そうに書きましたが、私もちゃんと追えてませんでした。
Solr勉強会第６回で、<a href="http://www.slideshare.net/KojiSekiguchi/lu-solr32-3420110912">関口さんの発表</a>できちんと説明されていて、参加してたのに聞けてなかったですし。（<a href="http://johtani.jugem.jp/?eid=26">メモ取ってるのに、書いてない。</a>）</p>
<p>ということで、Solr入門のサンプルschemaも少し修正しました。
<a href="http://johtani.jugem.jp/?eid=76">こちら</a>と<a href="http://johtani.jugem.jp/?eid=76">こちら</a>の記事に追記してありますので、参考にしてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Twitterが公開した分散トレーシング（追跡？）システム、Zipkin(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/13/twitter%E3%81%8C%E5%85%AC%E9%96%8B%E3%81%97%E3%81%9F%E5%88%86%E6%95%A3%E3%83%88%E3%83%AC%E3%83%BC%E3%82%B7%E3%83%B3%E3%82%B0%E8%BF%BD%E8%B7%A1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0zipkin/</link>
      <pubDate>Wed, 13 Jun 2012 00:40:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/13/twitter%E3%81%8C%E5%85%AC%E9%96%8B%E3%81%97%E3%81%9F%E5%88%86%E6%95%A3%E3%83%88%E3%83%AC%E3%83%BC%E3%82%B7%E3%83%B3%E3%82%B0%E8%BF%BD%E8%B7%A1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0zipkin/</guid>
      <description>すでに読まれた方もいるかも知れませんが、気になったのでメモを書いてみようかと。 先週の木曜日にTwitterのエンジニアブログでZipkinと</description>
      <content:encoded><p>すでに読まれた方もいるかも知れませんが、気になったのでメモを書いてみようかと。</p>
<p>先週の木曜日に<a href="http://engineering.twitter.com/2012/06/distributed-systems-tracing-with-zipkin.html">TwitterのエンジニアブログでZipkinというOSSを公開したという記事</a>がでました。
非常に興味深いシステムだったので、ちょっとずつ読み解いていきたいなという宣言（というか、ハッパをかけてもらうため）も兼ねて、まずはブログの内容をメモ程度に残しておきます。</p>
<hr>
<p><a href="https://github.com/twitter/zipkin">Zipkin</a>は分散トレースシステム（distributed tracing system）です。Twitter APIの１リクエストを構成する様々なサービスのタイミングデータ（計時データ）を集めるために作りました。
Firebugのような性能プロファイラのよなもので、しかもバックエンドのサービスもプロファイル可能です。
ZipkinはAPLv2ライセンスでOSSとしてGithubで公開しています。</p>
<p>ZipkinはWebのユーザインタフェースを持っています。（元記事参照）
各サービス（縦軸）でどのくらい時間がかかっているか（横軸）がわかり、クリックすることでより詳細な情報が得られます。
Zipkinはパフォーマンス改善の余地のある部分（遅いMySQLのSELECTなど）を見つけるのに役立ちます。</p>
<p><b>Zipkinはどのように動くの？</b>
Twitterに届いたリクエストからサンプリングしたリクエストに対してトレース可能なIDを付与して、すべてのサービスに渡していきます。
全リクエストの一部をサンプリングすることで、トレースのオーバヘッドを減らし、常に本番環境で利用できるようにしています。
Zipkinコレクタ（collector）が、Scribe経由でタイミングデータを受け取り、Cassandraに保存してインデックスを作成します。
Zipkinクエリデーモンがインデックスを利用して、WebUIにトレースデータを見つけます。</p>
<p>ZipkinはHack Weekで開始されました。
最初はThriftに対する<a href="http://research.google.com/pubs/pub36356.html">Google Dapper</a>の論文の基本的な部分の実装から始まり、現在ではHttp、Thrift、Memcache、SQL、Redisリクエストをサポートしています。
これらはFinagleライブラリで経由で動作します。Rubyのgemも用意してあります。</p>
<hr>
<p>ということで、ほぼ直訳ですが、何かの役に立てればと。
ちょっとずつですが、Githubのページやソースを読みながら記事を書いていこうと思っています。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 2.0.2リリース（リソース周りの改善など）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/07/lucene-gosen-2-0-2%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E5%91%A8%E3%82%8A%E3%81%AE%E6%94%B9%E5%96%84%E3%81%AA%E3%81%A9/</link>
      <pubDate>Thu, 07 Jun 2012 17:53:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/07/lucene-gosen-2-0-2%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E5%91%A8%E3%82%8A%E3%81%AE%E6%94%B9%E5%96%84%E3%81%AA%E3%81%A9/</guid>
      <description>lucene-gosenの最新版（2.0.1）をリリースしました。 プロジェクトページよりダウンロードが可能です。 今回の修正では、@haruy</description>
      <content:encoded><p>lucene-gosenの最新版（2.0.1）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>今回の修正では、@haruyama さんからいただいていたパッチの取り込み（<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=29">リソース周りの改善など</a>）が主な対応となっています。
また、コンパイルに利用するjarファイルがLucene/Solr3.6.0に変更になっています。（<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=31">Issueはこちら</a>）
3.6.0から追加されたテストケースにて、<a href="http://johtani.jugem.jp/?eid=88">発生する問題への対処</a>も施したものとなっています。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Issue32について（4096の壁）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/06/issue32%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A64096%E3%81%AE%E5%A3%81/</link>
      <pubDate>Wed, 06 Jun 2012 01:21:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/06/issue32%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A64096%E3%81%AE%E5%A3%81/</guid>
      <description>昨晩に引き続き、情けない内容のブログになってしまいますが。。。 昨晩、書いた記事の調査をしていた時に気づいた、問題になるケースがあったので調査</description>
      <content:encoded><p>昨晩に引き続き、情けない内容のブログになってしまいますが。。。</p>
<p>昨晩、<a href="http://johtani.jugem.jp/?eid=88">書いた記事</a>の調査をしていた時に気づいた、問題になるケースがあったので調査をしていました。
<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=32">Issue32</a>に登録した内容になります。
拙い英語を振り絞って書いた英語なので、伝わらないかもしれないのでブログに残しておきます。
昨晩の問題点となったクラス（StreamTagger2.java）の内部処理についてです。
lucene-gosenのLucene向けのTokenizerの内部処理では入力文字列の処理を行うのに、「char buffer[]」を用いて
入力文字列をReaderから読み込むときにバッファリングしています。
このバッファリングにて、特定のケースにて、想定していない場所を単語の切れ目と認識してしまう問題が実装上存在しました。
<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=32">Issue32</a>に記載した内容は次のようになります。</p>
<p>上記バッファは4096文字というサイズで固定になっています。
StreamTagger2クラスでは、この4096文字をオーバーするような文字列の場合に、つぎの処理により、4096文字以下の場所に文章の区切りを探そうとします。</p>
<ol>
<li>4096文字以上の文字列が入力される</li>
<li>バッファに入れられるだけの文字（4096文字）をバッファにコピーする。</li>
<li>バッファの後ろから、つぎの5種類の文字を探索し、見つかった場合（場所をkとし、ｋ＞０の場合）は、その場所を文章の切れ目と判定して、0からk+1文字目までの文字をStringTaggerを利用して形態素解析する。</li>
<li>あとは、繰り返し</li>
<li>上記条件に合致しない場合は、4096文字を文章とみなして形態素解析を行います。</li>
</ol>
<p>ここで、5種類の文字は以下のとおり。</p>
<pre><code>
0x000D:CARRIAGE RETURN(CR)
0x000A:LINE FEED(LF)
0x0085:NEXT LINE (NEL)
0x2028:LINE SEPARATOR
0x2029:PARAGRAPH SEPARATOR
</code></pre><p>見ての通り、制御文字ばかりです。
この5文字以外は切れ目と判断してくれません。
ということで、4097文字の文字列が存在し、上記5種類の文字が一度も出てこない場合、バッファのサイズで文字列が途切れてしまい、想定しない区切り位置で区切られた文章に対して形態素解析が実行されてしまいます.
HTMLから文字列を抜き出して解析したり、長い文書を解析する場合に、改行文字を削除して処理するといったことも考えられます。
上記5種類の文字列のみで文章の区切り位置を判断してしまうのは問題ではないかと。
まずは、4096文字内に「。」句点が存在した場合に、その部分を区切り位置として認識するようなパッチを記載して、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=32">Issue32</a>に添付しました。</p>
<p>「。」句点を採用した理由はつぎのとおりです。
StreamTagger2では、上記バッファリングした文字列を更に、BreakIterator.getSentenceIterator(Locale.JAPANESE)にて取得したBreakIteratorにて文節単位に区切ってから、各文節ごとに形態素解析を実施しています。
ということは、BraekIteratorにて分節の区切りとして判断される文字については、上記の文字種に追加しても問題無いという判断からです。
ただし、この修正でも、純粋に4096文字以上、句点が出てこない場合には区切り位置がおかしなことになってしまいますが。。。
もう少し、BreakIteratorの挙動を調べて、他にも利用可能な区切り文字が存在しないかを調査していく予定です。。。</p>
<p>1年以上コミッターをやっているのに、こんなことも理解していないのかよいうツッコミを受けてしまいそうでなんとも情けない話です。。。
まずは、現状報告でした。</p>
</content:encoded>
    </item>
    
    <item>
      <title>trunkのライブラリ差し替え（Lucene/Solr3.6.0）とランダムテストの失敗について(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/05/trunk%E3%81%AE%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E5%B7%AE%E3%81%97%E6%9B%BF%E3%81%88lucene-solr3-6-0%E3%81%A8%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%83%86%E3%82%B9%E3%83%88%E3%81%AE%E5%A4%B1%E6%95%97%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Tue, 05 Jun 2012 01:29:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/05/trunk%E3%81%AE%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E5%B7%AE%E3%81%97%E6%9B%BF%E3%81%88lucene-solr3-6-0%E3%81%A8%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%83%86%E3%82%B9%E3%83%88%E3%81%AE%E5%A4%B1%E6%95%97%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>久々にlucene-gosenを触っています。 trunkのlibにある、jarファイルが3.5ベースだったので、3.6ベースにしてテストをし</description>
      <content:encoded><p>久々にlucene-gosenを触っています。
trunkのlibにある、jarファイルが3.5ベースだったので、3.6ベースにしてテストをしたところ、
いくつかある、ランダムテストで結果の不整合が検出されたので、調査していました。
先程、trunkに対応版をコミットしました。もう少しテストケースを追加してからリリースします。
おそらく、通常の使い方では問題無いと思います。</p>
<p>Luceneでは、ランダムな文字列を利用したテストが実装されています。
lucene-gosenでもこのテストを利用してランダムなテストをしています。
実際にはtest/以下のorg.apache.luceneパッケージにテストケースがあります。
今回、jarファイルを差し替えた時に、このランダムなテスト実施にて、assertが失敗するケースが発生しました。</p>
<p>原因究明までに、いくつかフェーズがあったので、忘れないように書いておきます。</p>
<p>1.ランダムテストのテストケースにてエラーがassertが失敗するケースが発生
※ただし、成功する場合もあり。
2.該当のテストを再現しつつデバッグ＋該当のテストがどんなものかを解読（勉強不足。。。）
※テストは、失敗した場合につぎのようなメッセージが表示され、同じテストが再現可能です。
以下、エラーの出力例。「NOTE: reproduce with: 」のあとにあるantコマンドを実行すれば、同じテストが再現可能です。</p>
<pre><code>
    [junit] ------------- Standard Error -----------------
    [junit] TEST FAIL: useCharFilter=false text='wgxznwk'
    [junit] 
    [junit] ===&gt;
    [junit] Uncaught exception by thread: Thread[Thread-3,5,main]
    [junit] org.junit.ComparisonFailure: term 0 expected:&lt;w[gxznwk]&gt; but was:&lt;w[]&gt;
    [junit] 	at org.junit.Assert.assertEquals(Assert.java:125)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:146)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:565)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:396)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.access$000(BaseTokenStreamTestCase.java:51)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:337)
    [junit] &lt;===
    [junit] 
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGosenAnalyzer -Dtestmethod=testReliability -Dtests.seed=4ad9618caecb9fb2:d5476c03b8172df:-9c569f70013ffbb -Dargs=&quot;-Dfile.encoding=SJIS&quot;
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testReliability(org.apache.lucene.analysis.gosen.TestGosenAnalyzer):	Caused an ERROR

</code></pre><p>3.デバッグの結果、Luceneのtest-frameworkにある「MockReaderWrapper」というクラスの影響を確認
LuceneのJIRAの<a href="https://issues.apache.org/jira/browse/LUCENE-3894">LUCENE-3894</a>にて追加されたクラスであるとわかる。
このクラス、Readerのreadメソッド内部で、ランダムな値を元に長さを途中で返すという実装のReaderになっている。
（全部で18文字の文字列なのだが、ランダムな値を元に、12文字として結果を一旦返すという仕組みが実装されている）
4.lucene-gosenの問題箇所を特定。
Readerが途切れた部分を分節として扱ってしまう実装になっていた。
<a href="http://code.google.com/p/lucene-gosen/source/detail?r=204">該当の部分に修正をいれてコミット。</a>
という具合です。
一応、テストを数回走らせてランダムテストが問題なく終了するのを確認はしてあります。
今回の問題に対する、個別のテストケースを追加してから近日中リリースする予定です。
対応が遅くなって申し訳ないです。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>オープンソースへの貢献のススメ(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/05/31/%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%B8%E3%81%AE%E8%B2%A2%E7%8C%AE%E3%81%AE%E3%82%B9%E3%82%B9%E3%83%A1/</link>
      <pubDate>Thu, 31 May 2012 01:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/05/31/%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%B8%E3%81%AE%E8%B2%A2%E7%8C%AE%E3%81%AE%E3%82%B9%E3%82%B9%E3%83%A1/</guid>
      <description>録画してたEテレのスーパープレゼンテーションを見ててふと書きたくなったので、書いてます。 あとから読んだら恥ずかしくなりそうだけど。。。 「&amp;l</description>
      <content:encoded><p>録画してたEテレの<a href="http://www.nhk.or.jp/superpresentation/backnumber/120528.html">スーパープレゼンテーション</a>を見ててふと書きたくなったので、書いてます。
あとから読んだら恥ずかしくなりそうだけど。。。
「&ldquo;知力の余剰&quot;が世界を変える」というClay Shirkyさんの話を見て思ったことです。
社会的に貢献できる仕組みが最近増えているという話の内容でした。そこで、OSSについても同じことが言えるよなぁと思った次第でして。</p>
<p>昔からいろんなかたが言ってると思うし、今さら何をとおもわれるかもしれないですが、なんとなく書きたくなってしまったので。</p>
<p>私はシステム開発を生業にしています。特にOSSにはお世話になっています。
実際に、OSSを利用している方はかなりいると思います。
幸いにも、私はOSSのコミッターもやっています。
OSSに色々と関わってきて常々思っていることがありまして。</p>
<p>OSSは基本は有志の方々が開発やメンテナンスを行なっています。（最近では会社で行なっているところもありますが。）
そんなOSSに貢献する方法はいくつかあるのかなぁと。</p>
<p>バグを発見したら報告したり、こんな機能を作ってみたけどどうですか？とパッチを送ってみたりするのは素晴らしい貢献だと思います。
ただ、なかなかアクションを起こすのって勇気がいるかなぁと。
実際、私もパッチ書いたりバグ報告するのは気後れすることがありしました。
ただ、貢献する方法ってこれだけじゃないよなぁと、コミッターをやり始めて思いました。</p>
<p>もちろん、バグ報告してもらえるともっとありがたいですし、新機能とかリクエストを送ってもらえるともっと嬉しいです。
MLに質問するのもありです。ただ、ちょっとハードルが高いと思うこともあります。</p>
<p>そんな時は、ただ使っているよと、ツイートしたりブログを書いてくれるだけでもありがたいし、ヤル気が出るものです。
使い勝手が悪いなぁというツイートでも、なんでこんなに使いにくいんだというブログでもいいんです。
気になるなぁという形でもいいと思っています。
githubでフォローするだけでもいいかと。
反応があれば、使ったり触ってもらえていることがわかります。
なので、使っているよとかアピールしてもらえると嬉しいよなぁと。</p>
<p>なんか、締りのない感じになっちゃいましたが、反応があるとうれしいので、お気楽に反応しましょうよというお話です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>JJUG CCC 2012 Springに参加してきました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/05/29/jjug-ccc-2012-spring%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 29 May 2012 11:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/05/29/jjug-ccc-2012-spring%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>JJUG CCC 2012 Springに参加してきました。 昨年のFallに続き、2回目です。 概要や、タイムテーブルはこちらを御覧ください。 今回は、午後一から参加</description>
      <content:encoded><p>JJUG CCC 2012 Springに参加してきました。
昨年のFallに続き、2回目です。
<a href="http://www.java-users.jp/contents/events/ccc2012spring/">概要や、タイムテーブルはこちらを御覧ください。</a>
今回は、午後一から参加しました。
色々と迷いましたが、つぎのを聞いて来ました。</p>
<ul>
<li>HotSpot vs JRockit ～ HotRockit到来の前に予習しよう！</li>
<li>Play! Framework - モダンで高速なWeb開発</li>
<li>Grails/Groovyの開発活用術</li>
<li>Scala 最新状況報告</li>
<li>From Swing to JavaFX SwingからJavaFXへのマイグレーションガイド</li>
</ul>
<p>JRockitは使ったことがなく（たぶん）、新鮮でした。いろいろなコマンドが用意されているなぁと。
あと、jvisualvmの使い方も知らないこともあったのでいい話が聞けました。
プレゼンに慣れているようで、プレゼン中に、コマンドラインの拡大＋コマンドのオプションに赤い下線を入れるなど、どうやってやっているのか、プレゼンの内容よりも気になってしまったのが本音です</p>
<p>Playは今回の目玉の一つでした。まだ、手元でPlayを触り始めたばかりだったので、概観がつかめたのが良かったです。
あとで<a href="http://www.slideshare.net/ikeike443/play-jjug2012spring-13100524">スライドを</a>見なおさないと。
頭がどうしてもServlet、JSPのため、未だにPlayに切り替えができてないので、もっと触ってみないとなぁと。
ちなみに、紆余曲折してまして、Play 1.2.4→Play 2.0（Javaアプリ）→Play 2.0（Scalaアプリ）と心変わりしまくりで、まだ完全に作ってないのに、ふらふらしてます。。。</p>
<p>GrailsはGrailsを現場にどのように投入していくのがいいかという、技術よりは、政治的な話でしたが面白かったです。
実際に、あまり有名でないプロダクトを現場に導入するのに、Javaだからと無理やり説得してみたりｗ、開発コストがこれくらい下がりますという話をしてみたりと、いろいろやられているようでした。
あと、PlayのあとにGrailsということもあり、すこしPlayを意識した話もされていました。</p>
<p>ScalaはScalaDaysの話＋Scalaの最近の動向ということで、ある程度Scalaを知っている人がターゲットのようでした。
少し触っただけなので、何となく分かる部分もありましたが、最後の方はついていけてないです、すみません。。。
一番感じたのは、発表者の水島さんのScalaに対する愛情でしょうか。</p>
<p>最後は桜庭さんのJavaFXのお話です。
一応、昔、JFreeChart＋Swingでちょっとしたものを作ったことがあるので面白かったです。
プレゼン自体もJavaFXで作成されていたり、オープニングがStarWars風だったりと凝ったプレゼンでした。
JavaFXとしては、JavaOneでも聞いたのですが、グラフやHTMLがリッチに書けるようになったことがびっくりです。</p>
<p>実際のSwingアプリからJavaFXへの移行に関して、注意する点などがわかりやすく聞けました。
FXMLは確かに便利ですよねぇ。レイアウトをJavaで組むのがすごくめんどくさかったもんなぁ。</p>
<p>さて、簡単ですが、感想でした。
今回一番残念だったのは、長丁場の割に電源の確保が難しかったことでしょうか。
ツイートしたり、メモ取ったりしたかったのですが、電源が乏しいので、Wi-fiオフにしてメモ取るのが限度でした。。。</p>
<hr>
<pre><code>
場所：オリンピック記念青少年総合センター
日時：2012/05/28 10:00 - 19:00

◎13:10 - 14:00 C-1 HotSpot vs JRockit ～ HotRockit到来の前に予習しよう！ 谷本 心 @cero_t
　◯HotSpot from Sun
　◯JRockit from BEA
　　今は、どちらもOracle
　◯違いは？
　1.歴史
　2.プラットフォーム
　　JRockitはMacではNG。Solarisは一部。
　　2.1Oracleさん曰く
　　　Solaris/Mac → HotSpot
　　　Windows/Linuxのサーバ → JRockit
　　　Windows/Linuxのクライアント → HotSpot
　　2.2谷本さんは？
　　　WebLogic → JRockit
　　　1.4、5の時の開発環境はJRockit
　　　当時はJRockitの解析ツールがカッコ良かった
　3.解析ツール
　　3.1コマンドラインツール
　　　プロセス
　　　HotSpot ： jps
　　　JRockit ： jrcmd
　　　スレッドダンプ
　　　HotSpot ：jstack 
　　　JRockit ： jjrcmd &lt;pid&gt; print_threads
　　　ヒープ解析
　　　HotSpot：jmap -histo &lt;pid&gt;
　　　JRocket：jrcmd &lt;pid&gt; heap_diagnostics
　　　HotSpot：
　　　JRockit：jrcmd &lt;pid&gt; 
　　他にもJRockitは色いろある。
　　　print_utf8poolとか（内部の文字列が出てくる）
　　3.2GUIツール
　　　HotSpot：jvisualvm　NetBeansベース
　　　JRockit：Mission Control　Eclipseベース
　　メモリリークの解消をツールを使ってみてみましょうデモ。
　　・hprofファイルを吐き出して、jvisualvmで読みこむのが楽な方法
　　・jrmcはヒープダンプファイルを読み込む機能がない。
　　　memleakというツールがある。アプリを起動してから、プロセスを右クリックして選択可能。
　　　　タイプグラフや割当てトレースみたいなものが使えるよ。
　　　　フライトレコーダーというのもあるよ。
　4.HotRockitの紹介
　　まだいつ出るのかなぁという状態だけど、HotSpotにJRockitのツールも使えるようになるVMが出る模様。（2013？）

　※デモ中に画面拡大した時に、赤線でラインを引いているのがすごく気になった。（便利なツールなのかな？そこだけ？）

◎14:15 - 15:05 C-2 Play! Framework - モダンで高速なWeb開発 池田尚史 @ikeike443
　◯自己紹介
　　Play!Frameworkコミッター
　　日本Playframeworkユーザ会
　◯アンケート
　　メイン言語は？Java多数
　　触ったことある？半分くらい？
　　Play1？Play2？半々くらい
　　プロダクションで使ってる人？3人
　◯Playframeworkって？
　　JEEではないよ。
　　Webだよ。
　　ServletとかXML使ってないよ。
　◯JEEは難しいよね。RailsとかDjangoから流れてくると。
　◯Webフレームワーク
　　なので、Webアプリが作れればいいよね。
　　開発すべきものに注力して、抽象化とかを頑張らないようにと。
　◯ライブコーディング！
　　Play2.0のScalaアプリみたい。
　　プロジェクト作成～編集して起動まで。
　　エラーを起こして、エラーがどのように表示されるか。
　　エラーのリンクをクリックして、エディタを起動するということも可能みたい。
　　TODOとかで、まだ終わってないのも記述可能。
　　パラメータとControllerの関数の引数が勝手にひもづけられますよと。
　◯歴史
　　Servletとかもあった。
　　1.2からNetty、Websocket、Scalaサポート
　　2.0.1：Scalaで書き直し。Netty+Akkaで非同期
　◯１と２のちがいは？
　　・Play1
　　　Javaで書かれたJavaのフレームワーク。Scalaはプラグインサポート
　　・Play2
　　　Scalaで書かれたScala/Javaのフレームワーク
　◯Playの特徴
　　ステートレスとかノンブロッキングとかリアクティブとか
　　・高生産性
　　　XMLがないし、unzipするとすぐ使えるよ。
　　　ホットスワップできるよ。
　　　CoffeeScript、LESSサポートも。assetsに入れとくとコンパイルしてくれて静的コンテンツにしてくれる。（Railsにも似たようなのあったっけか？）
　　・ステートレス
　　　HttpSessionがない→必要ならMemcachedとかで管理してね。
　　　「デプロイ→ニーズ・状況に応じて即時スケールアウトという時代じゃないか？」という主張
　　　Playはステートレス養成ギブスであり、時代の要請にマッチ
　　・広範囲な型安全
　　　コンパイルしてエラー検知
　　・ノンブロッキングI/O
　　　非同期処理が手軽に書けるように考えられている。
　　　→リアルタイムWebの時代
　　　　NettyやAkkaにより実現されてるのがいい
　　　Akkaを使ったアプリを書くと、長い処理のActorを別サーバにするなども設定で変更が可能。
　◯テスタビリティ
　　BDDフレームワーク（Specs2？）
　　Viewもテストできるぞと。
　◯事例
　　Klout：ソーシャルスコアリング
　　イギリスのガーディアン：コンテンツAPIの実装がPlay2
　　MinecraftのWebサイト

◎15:20 - 16:10 C-3 Grails/Groovyの開発活用術 ～Java EE資産を活かして開発を加速する～(仮) 上原潤二 山本剛
　◯充電中のためお休み

◎16:25 - 17:15 C-4 Scala 最新状況報告 ～或いはScala Days 2012リポート～ 水島宏太
　◯自己紹介
　　言語を作るのが夢みたい。
　◯Scala最新状況報告
　　ScalaDaysの雰囲気を伝えるよと。（どっちかというと、旅行記かも）
　◯Scala？
　　・オブジェクト指向関数型言語
　　　ハイブリッドじゃなくて、統合したもの
　　・強力な静的型付け
　　　NullPointerExceptionなども起きにくい
　　・超強力なコレクションフレームワーク
　　・Javaと同等の実行速度
　　・コードが簡潔（1/4くらい）
　◯Scala採用企業
　　Twitter、Amazon.com（どこに使ってるかは不明）、Foursquare、LinkedIn、VMWare、Klout、Tumblrなど
　◯Scalaのバージョンは？
　　2.10が開発版。2.9.2がステーブル版。
　◯開発体制
　　Typesafe＋世界のContributor
　　Typesafeメンバの議決でいろいろ決定
　　githubでオープンに開発
　◯ScalaDays2012の目玉
　　豪華ゲスト（私は、わからなかった）
　　Scala2.10の新機能紹介
　　今後のScala、多数の応用例
　◯ScalaDays2012を見ての方向性
　　・All-in-oneパッケージの提供
　　　Typesafe Stackの提供
　　　重要なツール
　　　　sbt（Simple Build Tool）
　　　　gitter8（プロジェクトテンプレート生成ツール。githubを元に色々取ってくる？）
　　　　Akka
　　　　Play 2.0 Framework
　　・学習コストの削減
　　　言語機能のモジュール化
　　　高度な開発者が使う昨日はデフォルトOff
　　・バイナリ互換性問題への対処
　　　・Minor Release間での互換性を維持
　　　　MIMAでジドウテキに非互換性を検出
　　　・Major Release間では互換性は保証しない。
　　　　No more java.util.Date
　　　　ソース互換性は「概ね」保証される
　　　　deprecatedは次期メジャーバージョン時に削除される。
　　・Scala IDEへの注力
　　　インクリメンタルにコンパイルしてくれるから、遅いのも気にならなくなるかも。
　　　デバッガとか、できるよと。
　　・さらなるパフォーマンス改善
　　　Value classes
　　　AnyValを継承したクラスが作成可能
　　　　該クラスのオブジェクトがインライン化
　　　　Pimp my libraryによるヒープ使用料が0に！
　◯2.10最新機能紹介
　　&quot;1+2=#{1+2}&quot;ができない
　　s&quot;1+2=${1+2}&quot;ができるように String = 1 + 2 = 3
　　f&quot;1=${1}%03d&quot;もできるようにSring = 1 = 001
　　自分でStringコンテキストにメソッド追加できるらしい（聞き取った日本語が合ってるか？）
　　とか。（かなり不安。。。まだまだわかってない。。。）

◎17:30 - 18:30 BOF-B-1 From Swing to JavaFX SwingからJavaFXへのマイグレーションガイド 櫻庭 祐一
　◯JavaFX
　　次世代のJava GUI Library
　　Swing+Java2D+α
　　JavaSE8から標準（JavaFX3.0）
　◯サンプル
　　クラス名がいろいろ変わってる。
　◯はまりそうなところ
　　コンテナへの追加がちょっと違う
　　イベントリスナは1種類のみになった。（Genericsを使うようになったよと。）
　◯Bind
　　値が変わるとModelが勝手に検知して変わるみたい。
　　双方向もあり。これだとEventを書かなくても良くなりつつ有るよと。
　◯シナリオベースでマイグレーション考えましょう
　　1.JavaFX in Swing
　　　JavaFXにSwingを埋め込むことはできないぞと。
　　　SwingでできないことをJavaFXでやりますよと。
　　　おー、グラフが動く。JavaDocのHTMLも綺麗に出てる。
　　　使い方：JFXPanelを使う
　　　　シーングラフを記述可能
　　　　データのやり取りが大変。Threadが違うから。
　　　　パフォーマンスが落ちます。Java2Dで画像を書くので遅いですよと。
　　　　新規のものはJavaFXで書きましょうと。
　　2.Swing to JavaFX w/o FXML
　　　SwingをJavaFXに置き換える。
　　　使い方が違うものはTableViewなど、◯Viewとついてるもの。
　　　ちょっと考えるのはLayout
　　　　Swing：コンテナ＋レイアウトマネージャー
　　　　JavaFX：コンテナがレイアウトを含む
　　　　　BorderPaneクラスとか。
　　　問題はTableとか
　　　　Swing：TableModel
　　　　JavaFX：BeanをColumnにバインド
　　3.Swing to JavaFX w/ FXML
　　　・FXML
　　　　GUIの構造をXMLで表す。
　　　　シーングラフを表現。
　　　　スキーマレス
　　　　　クラス：要素
　　　　　プロパティ：属性 or 要素
　　　アノテーションバリバリです。これで、FXMLとJavaのバインディングができるよと。
　　ツール
　　　Java ：NetBeans
　　　　e(fx)clipseってのがあるかも。
　　　FXML：Scene Builder
　　
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>PlayFramework 2.0(Javaの方)を触ってみてる(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/05/16/playframework-2-0-java%E3%81%AE%E6%96%B9-%E3%82%92%E8%A7%A6%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%A6%E3%82%8B/</link>
      <pubDate>Wed, 16 May 2012 15:55:14 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/05/16/playframework-2-0-java%E3%81%AE%E6%96%B9-%E3%82%92%E8%A7%A6%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%A6%E3%82%8B/</guid>
      <description>PlayFramework2.0を触ってみてます。 ちょっとコーディングしたくなったのと、最近のアプリの作成の調査も兼ねて。 まぁ、せっかくなの</description>
      <content:encoded><p>PlayFramework2.0を触ってみてます。
ちょっとコーディングしたくなったのと、最近のアプリの作成の調査も兼ねて。
まぁ、せっかくなので、Solr検索のアプリでも作ってみようかと言うことで触ってます。
ただ、Solr検索アプリでしかなく、今のところDBを使わないので、実はPlay Frameworkじゃなくてもいいのではないかという疑問も。。。</p>
<p>まだ、触り始めたばかりなので、なんともですが。感想を。
たぶん、Ruby on Railsに似ているのかなと。
RoRは仕事で少し関わったので、なんとなく知っていますが、アプリの作成手順や、ディレクトリ構成などが似ている気がします。</p>
<p>コントローラーの生成のタイミングとか、内部でオブジェクトをSingletonで保持する方法とかのイメージがまだ良くつかめていない状態で、まずは、Solr検索部分（Palyにあんまり関係ないところ）を実装しているところです。</p>
<p>現状で一番の疑問点は、Eclipseプロジェクト化した時の参照ライブラリのパスに関するところでしょうか。
Play Frameworkは「play new ほげほげ」コマンドを実行するとアプリの<a href="http://www.playframework.org/documentation/2.0.1/Anatomy">ディレクトリ構成が作成</a>されます。
このあとに、eclipsfyというplayのコマンドを実行すると、Ecilpseのプロジェクトファイルが作成されます。
この時、PlayFrameworkが利用するjarファイルたちが参照ライブラリとしてクラスパスに設定されます。</p>
<p>このパスには、PlayFrameworkのインストールディレクトリが含まれた絶対パスが記述されるのですが、
複数人で開発するときにはどうしているのかなぁと。
あと、BitbucketやGitHubにアップするときもどうするのかなぁと。
以前、この疑問ツイートして、頂いた回答としては、環境変数として定義して、各自で設定してもらうというものでした。
私も同じ事を考えていたので、腑に落ちたのですが、他にもっといい方法があったりするんでしょうか？
クラスパスやEclipseプロジェクトのファイルをアップしないというのもあると思いますが、それもちょっとなぁと。
なにか、オススメとかあれば、教えていただけると嬉しいです。</p>
<p>最近、ブログ更新してなかったのもあったので、まずは、導入編でした。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 2.0.1リリース（Java7対応）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/05/09/lucene-gosen-2-0-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9java7%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Wed, 09 May 2012 01:35:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/05/09/lucene-gosen-2-0-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9java7%E5%AF%BE%E5%BF%9C/</guid>
      <description>lucene-gosenの最新版（2.0.1）をリリースしました。 プロジェクトページよりダウンロードが可能です。 今回の修正では、Java7で</description>
      <content:encoded><p>lucene-gosenの最新版（2.0.1）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>今回の修正では、Java7でUnicodeのバージョン変更に伴う対応（<a href="http://johtani.jugem.jp/?eid=73">詳細はこちらを参照</a>）を行なっています。</p>
<p>リソース周りの対応はまた後日。。。すみません。2012/05/16
遅くなりましたが、昨晩、JavaDocをアップしました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>「IDの秘密」を読みました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/30/id%E3%81%AE%E7%A7%98%E5%AF%86%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Mon, 30 Apr 2012 23:03:55 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/30/id%E3%81%AE%E7%A7%98%E5%AF%86%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>IDの秘密 (丸善ライブラリー―情報研シリーズ) 非常に面白く読めました。 バーコードの話に始まり、最後はシステムで付与するIDに関する考慮点まで</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4621053809/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4621053809&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4621053809/?tag=johtani-22">
      IDの秘密 (丸善ライブラリー―情報研シリーズ)
      </a>
    </p>
  </div>
</div>
非常に面白く読めました。
バーコードの話に始まり、最後はシステムで付与するIDに関する考慮点まで幅広くIDについて語られています。</p>
<p>適度に配置されたコラムがまた面白く、ここまで書いてもいいのかな？と思いながらも楽しく読ませて頂きました。
2次元バーコードが汚れに強いのも知らなかったし、チロルチョコの話は知らなかったし、指コレクションとか面白すぎです。
また、JRのSuicaの導入に7年もかけている点などは、やはりすごい技術なのだなぁというため息混じりの感想です。
それほど長い期間のテストや設計は想像がつかないです。</p>
<p>最後の2章（７，８章）については、エンジニアの以外のシステムに関わる方やエンジニアになられたばかりの方たちにぜひ読んで欲しいと思いました。
もちろん、エンジニアの方にも読んでほしい内容です。</p>
<p>いくつか疑問点や気になる点もあったので。</p>
<ul>
<li>「静脈や指紋が人によって異なるって確率はどうやって決めたんだろう？」</li>
<li>Twitterの説明文のあとにFBの画面の図番号が書いてある。</li>
<li>日本語がデコードされたけど、QRコードの文字コードって、この中に含まれてるのかな？それとも規格で決まってるのかな？</li>
</ul>
<p>最後に、<a href="http://yfrog.com/nuepmlrj">書影がNIIに合ったので撮影しました。写真へのリンク</a>です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Heroku JP Meetup ＃4に参加しました。#herokujp(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/21/heroku-jp-meetup-4%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-herokujp/</link>
      <pubDate>Sat, 21 Apr 2012 00:23:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/21/heroku-jp-meetup-4%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-herokujp/</guid>
      <description>WebSolrの話があるらしいというのを嗅ぎつけて、初めてHeroku JP Meetupに参加しました。 herokuもWebSolrも知りつつ、</description>
      <content:encoded><p>WebSolrの話があるらしいというのを嗅ぎつけて、初めて<a href="http://atnd.org/events/27370">Heroku JP Meetup</a>に参加しました。
herokuもWebSolrも知りつつ、手を出していなかったので、いい機会でした。
（SignUpだけ、勉強会直前に済ませましたｗ）</p>
<p><a href="http://www.heroku.com/">Heroku</a>はAWS上に構築されたアプリケーションプラットフォームで、簡単にアプリをデプロイして動作させることができるようです。
Ruby on Railsを使うのが多いみたいですが、他の言語も利用できると。
で、herokuの面白いところは、アドオンとして、開発が簡単にできるようなしくみが用意されていることみたいです。
今回発表のあった、IronMQ、WebSolr、PaperTrailもアドオンとして用意されており、簡単に利用することが可能です。
<a href="http://www.iron.io/products/mq">IronMQ</a>はメッセージキューとして利用できます。</p>
<p><a href="http://websolr.com/">Websolr</a>はSolrを簡単に利用できる形で提供しているものになります。
今、利用できるのは3.5.0のようで、最新版（3.6.0）になるのはまだ未定のようです。
今回の発表はWebSolrの話しもありましたが、基本は全文検索の仕組みとKuromojiの説明でした。
ただ、残念ながら、Kuromojiは3.6.0からの提供となるので、現時点では利用できないようです。
あとで、聞いた話だと、schema.xmlを自分で変更できるようです。
ただ、jarファイルを置いたりはできないようなので、lucene-gosenを利用するとかはできないみたいですが。。。
ほかにも、<a href="http://bonsai.io/home">bonsai.io</a>として、ElasticSearchの提供も行うようです。
まだ、利用はできないようですが。</p>
<p>最後が<a href="https://papertrailapp.com/">Papertrail</a>です。
こちらは、ログを保存して、検索、グラフ化（視覚化）してくれるアドオンです。
まだ、ベータのようですが、ログを保存してくれるようです。無料版もあるようです。
アドオンとしての機能もそうですが、<a href="https://metrics.librato.com/">利用しているグラフ化のツール</a>など、面白そうなものが利用されていました。</p>
<p>LTはRuby使いの方が多かったです。</p>
<p>Ruby使いではないのですが、いろいろなアドオンが用意されており、サービスを簡単に提供することができそうだという印象をうけ、ちょっと使ってみたいなぁと思いました。
普段参加していない勉強会だったので、普段では知りえない興味ある話が聞けて面白かったです。</p>
<p>余談ですがPapertrailの人が利用していた、slideshareのようなサービスの<a href="http://speakerdeck.com/">Speaker Deck</a>もよさそうなので登録してみました。
次に何か発表があった時には資料はこっちにアップしてみようかなぁと</p>
<p>以下は、いつものように自分用のメモです。</p>
<hr>
<pre><code>
日時2012/04/20 19:00 to 21:00
会場：パソナグループ本部　呉服橋
◎オープニング – Ayumu AIZAWA (Heroku Evangelist）
</code></pre><pre><code>
◎新入社員からの挨拶 – Koichi SASADA (Ruby Developer）
　前職：大学教員
　仕事：CRuby開発
　Heroku使った事無いですｗRailsもよく知らないですｗ
　RUby2.0のリリースがゴール。2013/Feb
　性能アップのことやってます。
</code></pre><p>◎<a href="http://www.iron.io/products/mq">IronMQ</a> – Chad Arimura (Iron.IO)</p>
<pre><code>
　「メッセージキューは涼しいです。」
　（Google翻訳による日本語訳付きのスライド）
　Aggregation、Distribution。。。
　IronMQ
　　Elastic、RESTful
　　heroku addon ironmq:rust
　簡単にheroku上にキューが用意できるアドオンです。
　Q：メッセージがキューに到達したのを確認する方法か？
　A：ステータスコードが帰ってくる。
　Q：データのサイズのリミットは？
　A：postのリミットはある。S3とかに巨大データをおいて、ポインタを渡すとかしてほしい。
　Q：キューへの到達の成功の保証は？
　A：アプリケーション側で判断してください？
</code></pre><p>◎Search &amp; Indexing on Heroku – Nick Zadrozny (<a href="http://websolr.com/">Websolr</a>)
<a href="http://www.slideshare.net/nzadrozny/fulltext-search-with-emphasis-on-japanese">スライドはこちら。</a></p>
<pre><code>　
　※ツイートしてて、メモとってなかったので、ツイートをコピペ。
　次はWebsolrのお話
　Bonsai.io？
　Bonsai by onemorecloud - http://bit.ly/JjCuaE
　SQLのLIKE検索はO(n)でおそいねぇと。
　クエリのパースについての話。
　今度は転置インデックスのお話。
　Termへの分割ってどーすんの？というお話。Tokenizeのお話。
　その１：N-GramというTokenizeの方法。N文字ずつ先頭からTermを切り出す。開始位置は1文字ずつずらしていくと。
　N-Gramはノイズがのるし、多くのTermがでてきちゃうよと。
　その２：そこで、次は形態素解析ですね。
　先週、Lucene/Solr 3.6.0がリリースされて、Kuromojiという日本語向けの形態素解析器がでましたよ。
　Kuromojiはこちら。（Lucene版とは少し違うけど。）http://atilika.org/
　Kuromojiのサーチモードのお話。
　通常は、「関西国際空港」という単語になってしまうのを、Kuromojiでは「関西」「国際」　「空港」という切り方の単語も出してくれると。
　ちなみに、lucene-gosenでは、サーチモードはないんですねぇ。。。
　「の」はどこに消えたんだ？？そこの説明は？
　ElasticSearchやSolrのコアの部分でLuceneを使ってるよ。
　ElasticSearch　http://bit.ly/qjjvWp
　Kuromojiはユーザ辞書をサポートしてるよ。

　Q：まだ、3.5.0では？
　A：もうすぐやります
</code></pre><p>◎log analysis for your Heroku app – Eric Lindvall (<a href="https://papertrailapp.com/">Papertrail</a>)</p>
<pre><code>　
　heroku上にログを貯めて、検索したりグラフ化したりできるようになりそうなもの。
　[スライドはこちら](http://speakerdeck.com/u/lindvall/p/log-analysis-for-your-heroku-app)
　ログを貯めて、検索や可視化できるようにするサービスみたいです。
　まだ、アイデアレベルのものも発表資料には含まれていました。
　内部で利用しているツールなど、資料の最後に出てきますが、色々と面白そうなものがありました。
</code></pre><pre><code>
◎Lightning Talks
　◯Receibo ( @shu_0115 )
　　デザイナーｘエンジニアハッカソンでの成果らしい。
　　Webベースの家計簿アプリ。
　　買ったものの名称と料金を入れるだけ。

　◯Heroku + Pusherで作る！リアルタイムアプリケーション ( @satococoa )
　　WebSocketみたいなことが、Pusherでできるらしい。
　　http://www.slideshare.net/satococoa/heroku-pusher　　

　◯Herokuアドオンを作ってみてわかったこと ( @takkam )

　◯heroku client のちょっと進んだ使い方 ( @hsbt )

　◯love heroku? – we love herokuのご紹介 ( @ppworks ）
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>Solr 3.6.0のCJKの設定とSynonymFilterFactoryの気になる点(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/17/solr-3-6-0%E3%81%AEcjk%E3%81%AE%E8%A8%AD%E5%AE%9A%E3%81%A8synonymfilterfactory%E3%81%AE%E6%B0%97%E3%81%AB%E3%81%AA%E3%82%8B%E7%82%B9/</link>
      <pubDate>Tue, 17 Apr 2012 01:16:11 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/17/solr-3-6-0%E3%81%AEcjk%E3%81%AE%E8%A8%AD%E5%AE%9A%E3%81%A8synonymfilterfactory%E3%81%AE%E6%B0%97%E3%81%AB%E3%81%AA%E3%82%8B%E7%82%B9/</guid>
      <description>先日、Solr入門のサンプルschema.xmlの3.6.0対応版の作成をしていて、気になったことがあったので、 メモとして残しておきます。 S</description>
      <content:encoded><p>先日、Solr入門のサンプルschema.xmlの3.6.0対応版の作成をしていて、気になったことがあったので、
メモとして残しておきます。</p>
<p>SynonymFilterFactoryの属性「<span style="color:#FF0000">tokenizerFactory</span>」に関連する話です。
（<a href="http://www.amazon.co.jp/exec/obidos/ASIN/4774141755/johtani-22/ref=nosim/">「Apache Solr入門」</a>の36-37ページに記載があります。）</p>
<p>SynonymFilterFactoryでは、類義語設定ファイルを読み込む際に利用するTokenizerFactoryを「tokenizerFactory」という属性で指定できます。（以下は書籍の記述を抜粋）</p>
<pre><code>
  &lt;filter class=&quot;sold.SynonymFilterFactory&quot; synonyms=&quot;synonyms.txt&quot; ... tokenizerFactory=&quot;solrbook.analysis.SenTokenizerFactory&quot;/&gt;
</code></pre><p>このように、TokenizerFactoryが指定できます。</p>
<p>ただ、<a href="http://johtani.jugem.jp/?eid=76">こちらの記事</a>で書いたように、
Solr 3.6.0のexampleのschema.xmlではCJKのフィールドは次のように設定されています。</p>
<pre><code>
    &lt;!-- CJK bigram (see text_ja for a Japanese configuration using morphological analysis) --&gt;
    &lt;fieldType name=&quot;text_cjk&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&gt;
      &lt;analyzer&gt;
        &lt;tokenizer class=&quot;solr.StandardTokenizerFactory&quot;/&gt;
        &lt;!-- normalize width before bigram, as e.g. half-width dakuten combine  --&gt;
        &lt;filter class=&quot;solr.CJKWidthFilterFactory&quot;/&gt;
        &lt;!-- for any non-CJK --&gt;
        &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt;
        &lt;filter class=&quot;solr.CJKBigramFilterFactory&quot;/&gt;
      &lt;/analyzer&gt;
    &lt;/fieldType&gt;
</code></pre><p>3.6.0以前は、solr.CJKTokenizerFactoryを利用していましたが、3.6.0からはCJKTokenizerFactoryがdeprecatedになってしまい、代わりにStandardTokenizerFactory＋CJKBigramFilterFactoryの組み合わせになっています。
exampleのCJKのフィールドタイプ設定を利用して、かつ、そのフィールドにSynonymFilterを利用する場合に、
StandardTokenizerFactoryを指定してしまうと、類義語が展開できなくなってしまうので注意が必要です。</p>
<p>CJKのフィールドでSynonymFilterを利用する場合は、類義語の設定ファイル内の記述を自力でCJKTokenizerが分割する形で記述する（まぁ、やらないでしょうが）か、deprecatedですが、CJKTokenizerFactoryを利用するのが現時点での対応でしょうか。</p>
<p>なお、これに絡んで、<a href="https://issues.apache.org/jira/browse/SOLR-3359">このようなチケット</a>もできています。</p>
<h5 id="syntaxhighlighterを導入してみました">SyntaxHighlighterを導入してみました。</h5>
<h5 id="ちょっとはみやすくなってますかね">ちょっとはみやすくなってますかね？</h5>
<h5 id="まだsyntaxhighlighterの設定を調べながら使っているのでコロコロ変わるかもしれないですが気にしないでください">まだ、SyntaxHighlighterの設定を調べながら使っているので、コロコロ変わるかもしれないですが、気にしないでください。</h5>
</content:encoded>
    </item>
    
    <item>
      <title>「Apache Solr入門」のサンプルのKuromojiとlucene-gosen対応（2章～4章）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/14/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C2%E7%AB%A04%E7%AB%A0/</link>
      <pubDate>Sat, 14 Apr 2012 02:58:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/14/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C2%E7%AB%A04%E7%AB%A0/</guid>
      <description>先日の続きです。「Apache Solr入門」の2章から4章の説明について、Solr3.6.0で動作させる時の変更点を以下に書いていきます。 な</description>
      <content:encoded><p>先日の続きです。「Apache Solr入門」の2章から4章の説明について、Solr3.6.0で動作させる時の変更点を以下に書いていきます。
なお、前回も説明しましたが、3.6.0からKuromojiという形態素解析器がSolrに同梱されるようになりました。
これから説明する2章の変更点の手順ですが、Kuromojiとlucene-gosenそれぞれの利用方法について説明します。
添付のschema.xmlについては、基本的にKuromojiを利用する形に変更してあります。
それに加えて、lucene-gosen用のフィールドを別途追加で定義しました。
これらのフィールド名については、次の表の用になります。
適宜、書籍のフィールド名と置き換えながら読み進めたり、試したりしてください。</p>
<table class="list_view">
  <thead>
    <tr>
      <th>Kuromojiフィールド</th>
      <th>lucene-gosenフィールド</th>
    </tr> 
  </thead>
<tbody>
<tr>
  <td>title</td>
  <td>title_gosen</td>
</tr>
<tr>
  <td>author</td>
  <td>auther_gosen</td>
</tr>
<tr>
  <td>summary</td>
  <td>summary_gosen</td>
</tr>
<tr>
  <td>intended_reader</td>
  <td>intended_reader_gosen</td>
</tr>
<tr>
  <td>from_author</td>
  <td>from_author_gosen</td>
</tr>
<tr>
  <td>toc</td>
  <td>toc_gosen</td>
</tr>
</tbody>
</table>
<h2 id="2章">2章</h2>
<h3 id="213-schemaxmlのバージョン27ページ">2.1.3 schema.xmlのバージョン（27ページ）</h3>
<p>Solr3.xではschema.xmlのファイルの最新バージョンは<strong>1.<span style="color:#FF0000">5</span></strong>になっています。</p>
<h3 id="223-代表的なトークナイザ35ページ">2.2.3 代表的なトークナイザ（35ページ）</h3>
<p>solrbook.analysis.SenTokenizerFactoryは必要ありません。
<span style="color:#FF0000">Solr 3.6.0からはKuromojiと呼ばれる形態素解析器が用意されています。
solr.JapaneseTokenizerFactoryがそれに該当します。
</span>
これとは別に、lucene-gosenを利用する場合、Solr向けのトークナイザが用意されています。
solr.<span style="color:#FF0000">Gosen</span>TokenizerFactoryがそれに該当します。</p>
<h3 id="224-代表的なトークンフィルタ37ページ">2.2.4 代表的なトークンフィルタ（37ページ）</h3>
<p>以下の2つについては<span style="color:#FF0000">Kuromojiが同等のトークンフィルタを提供しています。</span>
また、lucene-gosenを利用する場合は、lucene-gosenに同等のトークンフィルタが存在します。</p>
<ul>
<li>solrbook.analysis.KatakanaStemFilterFactory</li>
<li>solrbook.analysis.POSFilterFactory</li>
</ul>
<p><span style="color:#FF0000">次のものがSolr 3.6.0に用意されているので、こちらを利用します。</span></p>
<ul>
<li>solr.JapaneseKatakanaStemFilterFactory</li>
<li>solr.JapanesePartOfSpeechStopFilterFactory</li>
</ul>
<p>それぞれ、次のものがlucene-gosenにあるので、こちらを利用します。</p>
<ul>
<li>solr.<span style="color:#FF0000">Gosen</span>KatakanaStemFilterFactory</li>
<li>solr.<span style="color:#FF0000">Gosen</span>PartOfSpeechStopFilterFactory</li>
</ul>
<p>2章向けの<a href="https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/48834b2d0465/schema/schema.xml">schema.xmlはこちら</a>です。その他のtxtファイルについては、特に変更はありません。</p>
<p>3,4章は特に変更はありません。Solrの起動の仕方にだけ注意してください。（-Dsen.homeは必要ありません）</p>
<p>以上が4章までの修正点になります。</p>
<p>昨日に引き続き、眠い目をこすりながら修正したので、おかしいかも。
動かない、意味がわからないなどあれば、コメントorツイートいただければと思います。</p>
<p><span style="color:#FF0000">2012/06/14提供しているschema.xmlに関して修正を加えました。</span><a href="http://johtani.jugem.jp/?eid=92">こちらの記事</a>で説明しているautoGeneratePhraseQueriesの値をtext_gosen、text_cjkのフィールドに対してtrueを設定する記述を追記しました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 3.6.0リリース / 「Apache Solr入門」のサンプルのKuromojiとlucene-gosen対応（1章）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/14/lucene-solr-3-6-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9-apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0/</link>
      <pubDate>Sat, 14 Apr 2012 02:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/14/lucene-solr-3-6-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9-apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0/</guid>
      <description>以前より、アナウンスしていた、Kuromojiという日本語形態素解析が含まれるLucene/Solr 3.6.0がリリースされました。 以下、各</description>
      <content:encoded><p>以前より、アナウンスしていた、Kuromojiという日本語形態素解析が含まれるLucene/Solr 3.6.0がリリースされました。</p>
<p>以下、各リリース内容について簡単に説明されているページへのリンクです。</p>
<p><a href="http://lucene.apache.org/solr/solrnews.html">Solrリリースのお知らせ</a></p>
<p><a href="http://lucene.apache.org/core/corenews.html">Luceneリリースのお知らせ</a></p>
<p>Solr 3.6.0の変更の目玉は各言語のAnalyzer/Tokenizerの設定がexampleのschema.xmlに含まれるようになったことです。
Kuromojiという日本語用の形態素解析器もexampleを起動すればすぐに利用できる形になっています。
Kuromojiを利用する場合は、exampleのschema.xmlが参考になるでしょう。</p>
<p>あと、大きな変更は、Ivyに対応した点です。ソースをダウンロードするとわかりますが、依存するjarファイルが含まれない形に変更されています。
SVNからチェックアウトした場合も同様です。ビルドにはネットワークに接続している環境が必要になりました。</p>
<p>また、このリリースに合わせて、以前書いた「Apache Solr入門」のサンプルについての記事も変更が必要かと思い、
前回の記事をベースに以下に変更した記事を書いたので、参考にしてください。
今回は、Kuromojiという日本語形態素解析がデフォルトで含まれるようになったので、
Kuromojiの利用方法とあわせて、lucene-gosenの利用方法も記載します。
サンプルのschema.xmlについては、Kuromoji、lucene-gosenが同時に利用できる形のものを用意しました。</p>
<hr>
<p>サンプルのschema.xmlを最新版（Solr 3.6 + lucene-gosen-2.0.0-ipadic）のものを用意しました。
なお、あくまでも、3.xでlucene-gosenを利用する場合の「Apache Solr入門」のサンプルプログラムの変更点（とりあえず、4章まで）の違いについて記述します。
申し訳ございませんが、1.4と3.xの違いについての説明はここでは行いません。</p>
<p>以下では、各章でschema.xmlに関連する記載のある部分を抜粋して、変更点と変更したschema.xmlのリンクを用意しました。参考にしてもらえればと思います。</p>
<h2 id="1章">1章</h2>
<h3 id="161-n-gram17ページ">1.6.1 N-gram（17ページ）</h3>
<p>1.6.1の手順に変更はありません。
サンプルプログラムが入っているZip「solrbook.zip」のintroduction/ngram/schema.xmlファイルの代わりに
<a href="https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/48834b2d0465/introduction/ngram/schema.xml">こちらのschema.xml</a>を利用してください。
<span style="color:#FF0000">※なお、Solr 3.6.0から、SOLR_HOME/example/solr/conf/schema.xmlにデフォルトでN-gramで利用しているCJKTokenizerの設定が入るようになっています。
（実際にはCJKTokenizerではなく、CJKBigramFilterとCJKWidthFilterに変更されています。）</span></p>
<h3 id="162-形態素解析18ページ20ページ中盤まで">1.6.2 形態素解析（18ページ～20ページ中盤まで）</h3>
<p><span style="color:#FF0000">CJKと同様、exampleにKuromojiを利用した設定がすでに記述されています。text_jaというフィールドタイプになります。書籍の21ページ1行目に記載のある、
「Field」のテキストボックスに入力する文字列を「text_ja」とすると、Kuromojiを利用した形態素解析結果が表示されます。exampleですでに幾つかのフィルタも設定されているため、書籍の出力結果とは異なる表示となるはずです。</span></p>
<p>lucene-gosenを利用する場合は手順が大きく変わります。
Senを利用する場合、Senの辞書のビルド、Senのjarファイルの配置、Senを利用するためのTokenizerクラスを含んだサンプルjarの配置という作業があります。
lucene-gosenではコンパイル済みの辞書がjarファイルに含まれています。
また、Solr向けのTokenizerもlucene-gosenのjarファイルに含まれています。
lucene-gosenを利用して形態素解析を体験するための手順は次の流れになります。
なお、schema.xmlについては上記N-gramでダウンロードしたschema.xmlに形態素解析の設定もあわせて記載してあります。</p>
<p>jarファイル（<a href="http://lucene-gosen.googlecode.com/files/lucene-gosen-2.0.0-ipadic.jar">lucene-gosen-2.0.0-ipadic.jar</a>）をダウンロードして、$SOLR/example/solr/lib（libディレクトリがない場合は作成）にコピーします。
コピーが終わりましたら、次のように$SOLR/exampleディレクトリでSolrを起動します。
（-Dsen.homeは必要なし）</p>
<pre><code>
$ java -jar start.jar
</code></pre><p>あとは、書籍の記述にしたがって管理画面のAnalysis画面で動作を確認します。
ほぼ、図1-6と同じ結果になっていると思います。
（lucene-gosenで出力される情報には本書のサンプルよりも多くの情報が含まれています。また、サンプルでは、形態素解析の後の単語に基本形を採用しているため、「な」が「だ」として出力されています。基本形を出力する場合は後述するこちらで紹介したTokenFilterを利用すれば可能です。）</p>
<p>2章については後日説明することにします（眠くなってきた。。。）</p>
<p>動作しないなどあれば、コメントください。</p>
<p><span style="color:#FF0000">2012/06/14追記提供しているschema.xmlに関して修正を加えました。</span><a href="http://johtani.jugem.jp/?eid=92">こちらの記事</a>で説明しているautoGeneratePhraseQueriesの値をtext_gosen、text_cjkのフィールドに対してtrueを設定する記述を追記しました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>【メモ】Amazon CloudSearchが出てきましたよ(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/12/%E3%83%A1%E3%83%A2amazon-cloudsearch%E3%81%8C%E5%87%BA%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%82%88/</link>
      <pubDate>Thu, 12 Apr 2012 23:31:09 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/12/%E3%83%A1%E3%83%A2amazon-cloudsearch%E3%81%8C%E5%87%BA%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%82%88/</guid>
      <description>Twitterでこのブログ流れてきて、気になったので、流し読みして簡単にメモ。 Amazonのサービスでスケールする検索サービスみたい。 主に、</description>
      <content:encoded><p>Twitterで<a href="http://aws.typepad.com/aws/2012/04/amazon-cloudsearch-start-searching-in-one-hour.html/?PHPSESSID=fdfe9b1cde84fd75f396f6121de595d6">このブログ</a>流れてきて、気になったので、流し読みして簡単にメモ。
Amazonのサービスでスケールする検索サービスみたい。</p>
<p>主に、機能面です。価格とかはみてないです。</p>
<ul>
<li>データと検索トラフィックに対して自動でスケール（※automaticってのがどういう感じかは調べてないです）</li>
<li><a href="http://aws.amazon.com/jp/documentation/cloudsearch/">CloudSearch APIs</a>でアクセス可能。AWS管理コンソールからオペレーションできる。コマンドツールもある</li>
<li>データ登録にHTTPSも使えますよ。データ形式はJSON、XML、SDF（Search Document Format）と呼ばれるものに準拠したもの</li>
<li>near real-timeで検索可能になる。RAMにインデックスを保持して更新処理が早くなる（Lucene/Solrと一緒か？）</lI></li>
<li>ステミングとかストップワードの設定変えたらre-index</lI></li>
<li>ファセット、フィールド指定検索が可能。（ファセットはファセットクエリみたいなのもできるのかな？）</li>
<li>データ量が増えたら、サーチインスタンスを追加するか、インスタンスタイプを大きくしてスケールする？</li>
<li>トラフィックが増えたら新しいインスタンスにデータをレプリケートしてインスタンスを追加する</li>
<li>簡単に実現できるよ。ファセット検索、全文検索、And/OR検索式、ランキングのカスタマイズ、フィールド指定ソート、フィールド指定検索、テキスト処理オプション（ストップワード、類義語、ステミングのような）</li>
</ul>
<p><a href="http://aws.amazon.com/jp/documentation/cloudsearch/">ここ</a>から詳細のドキュメントが見れるみたいなので、また、見てみます。</p>
<p>あと、WikipediaをCloudSearchで動かしてる<a href="http://wikipedia.searchtechnologies.com/">デモ</a>と<a href="http://www.searchtechnologies.com/wikipedia-cloudsearch.html">それに関する記事</a>もあるみたいです。これも暇を見つけて眺めてみます。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Java One Tokyo 2012 に参加しました。#JavaOneJp(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/06/java-one-tokyo-2012-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-javaonejp/</link>
      <pubDate>Fri, 06 Apr 2012 16:51:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/06/java-one-tokyo-2012-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-javaonejp/</guid>
      <description>JavaOne Tokyo 2012に参加してきました。 4/4-4/5の2日間開催されていたのですが、子供が体調を崩してしまい、4/5のみの参加となりました。 4/4</description>
      <content:encoded><p>JavaOne Tokyo 2012に参加してきました。
4/4-4/5の2日間開催されていたのですが、子供が体調を崩してしまい、4/5のみの参加となりました。
4/4はTwitterのTLを眺めて、羨ましがってました。</p>
<p>7年ぶりに日本で開催されたJavaOneみたいです。
そういえば、7年前くらいに参加した記憶があるなぁと。</p>
<p>会場入りして最初の感想は年齢層が高いなぁと。
最近、勉強会によく顔を出すようになったのですが、勉強会とは年齢層が少し異なりました。
まぁ、私も年齢層を上げている一人ではあるのですが。。。
Javaも熟成してきてるなぁというのが最初の感想でした。
あと、思ったよりもスーツ率は低かったと思いました。（5割はいましたが）
一番違和感を感じたのは、会場の所々の色が「赤い」ことです。。。</p>
<p>以下は私が参加したセッションのカンタンな感想です。</p>
<p><strong>Java EE6 Modeling　JS2-01　#jt12_s201</strong></p>
<hr>
<p>JavaEEから離れてはや数年。私が関わっていた頃はEJB2.0が出たくらいのタイミングだったので、
色々と変わってきているなぁというのが第一印象です。
CDI、Singleton EJBなど、知らない単語もちらほらと。
earではなく、warファイル1つで済む、jQueryをリソースjarとしてパッケージングしてwarファイルに入れられるなどは簡易になっていいなぁと。
web.xmlがアノテーションで記述できて要らなくなるという話は一長一短だと思いました。
一覧性があるからいいと思うこともあるので。
中国の方？の英語だったので少し特徴的でした。すこしは頑張ってみたものの、やはり同時通訳を聞いてしまって反省しています。。。</p>
<p><strong>マルチコアCPU時代のJavaプログラミング　JS2-14　#jt12_s214</strong></p>
<hr>
<p>F社の方が実際に経験されてきた性能関連のプログラミングについての話でした。
現時点のJavaでマルチコアCPUで気をつけるべきプログラミングの観点をわかりやすく説明されたいいセッションでした。
詳細は下のメモを見ていただけるといいですが、色々とプログラミングで気をつけるべき点を説明されていました。
いいおさらいになりましたし、忘れていることや知らないこともあって勉強になりました。
TLでも見かけましたが、モダンJavaプログラミングみたいな、今のJavaでのプログラミングの気をつけるべき点を書いた書籍があるといいかもなぁと思いました。
あと、もうひとつ私が好印象だと思ったのは、F社の製品の紹介などがなかったからかもしれませんｗ</p>
<p><strong>サービス維持・発展を踏まえた楽天オークションのJava EEによる開発と運用　JS2-23　　#jt12_s223</strong></p>
<hr>
<p>楽天オークションの開発をサれている方の立ち上げから現在に至るまでの開発メンバーの管理の仕方や
実際に利用している開発手法、気をつけている点のお話です。
スライドに書いてあること以外のことを発表者の方が話をされており、もう少し、喋る内容に関連した図などを入れてもらえるとわかりやすかったかもしれません。
あとは、WebLogicの管理画面が使いやすくていいという話のあとに、GlassFishに移行していますという話があったりと、少し「？」が浮かぶ部分がありました。。。</p>
<p><strong>JSR 353: Java API for JSON Processing　JS2-33　#jt12_s233</strong></p>
<hr>
<p>最近、Twitterで遊んでもらっている（勝手に絡んでいるだけという話も。。。）@yusukeyさんの発表です。
結構な数の立ち見が出るほどの盛況ぶりで、発表者の@yusukeyさんもびっくりしていました。
JSRがどういった形で実際の仕様として公開されていくのかという話がわかりやすく紹介されていて勉強になりました。
所々で笑いが入るなど、さすがの安定感でした。立ち見が出るはずです。
ようやく、JavaもJSONに関する処理を仕様として取り入れようとしているのはいい傾向かと。
※残念ながら資料は公開されないようです。</p>
<p><strong>UI Controls and Charts: Drag-and-Drop, Filtering, Sorting, Table Hookup with Charts　JS2-42　#jt12_s242</strong></p>
<hr>
<p>最近、Chartに興味を持っていたので、登録したセッションです。
内容としてはJavaFXのコンポーネントをリストアップして紹介していくというお話でした。
グラフ（Chart）周りも数種類が用意されるようです。
JavaFXをあまり知らないのですが、Swingをすこしやっていたのでなんとかついていくことが出来ました。
発表者の方が、実際にJavaFX内部の実装を行われている方だったようで、QAでいくつかこんなコンポーネントはないのか？という質問に、「私のPCでは動いているのですが、ドキュメントなどの整備が追いついていないので公開できていないです」という回答が出て、なかなか大変なのだなぁとｗ
Chartに関しては、グラフの重ね合わせなどがまだできないということで、JFreeChartを触ったことがある身としてはすこし物足りなさそうだなぁというのが印象です。</p>
<p><strong>Learn how the JVM is fundamental to our architecture.　BoF2-03  #jt12_b203</strong></p>
<hr>
<p>今回JavaOneに参加した一番の理由がこのセッションです。
Twitterのアーキテクチャに関するセッションでした。
思った以上に濃い内容で、ツイートしたデータがどんな流れで登録されるのか、登録されたデータを各ユーザのTimelineにどのようにアクセスして表示しているのかが説明されました。
今回は主に<a href="http://twitter.gthub.com/finagle/">Finagle</a>についての話でしたが、非同期処理をどのように活用しているかというお話でした。
負荷分散を行なっているポイントやキャッシュについての考え方などです。
メモを取るのに夢中になってしまい、英語をヒアリングするという目的を見失ってしまいましたが。。。
期待以上の話が聞けて満足しています。</p>
<p><strong>パネルディスカッション</strong></p>
<hr>
<p>最後はJavaに関連するコミュニティを運営されている方々のコミュニティ運営に関するディスカッションです。
コミュニティの運営の苦労している点や、心がけている点など、運営も大変そうだなぁと。
どうしても長くやっているとメンバーが固定化してしまうという話なども出ていました。
これは難しいですよね。会社も同じことが言えますし。
一番印象に残ったのは桜庭さんの「動くものが作れるのが楽しい」という一言でした。
エンジニアをやってるのは確かにそこが面白いからだなぁと。最近忘れかけているので、もっと色々と作っていきたいですね。</p>
<hr>
<p>以上がカンタンな感想です。
少し残念なのですが、事前に参加登録していたセッションについては、マイページと呼ばれるところから資料がダウンロードできるものもあるみたいですが、参加登録してないものについてはダウンロードが出来ない様でした。（4/6現在）
参加できなかったセッションの資料こそ見たいと思うのが普通だと思うのですが、どうでしょう？そのうちどこかに公開されるのかなぁ。</p>
<p>以下は、いつもの通り個人のメモです。</p>
<hr>
<pre><code>
場所：六本木ヒルズ　アカデミーヒルズ49F
日時：2012/04/05 

○Java EE6 Modeling　JS2-01　#jt12_s201
　スーツ禁止。Eva好き？
　・特徴
　　Web Profile Pruing
　　POJO、Annotation、Less XML
　　Java EE6の半分のAPIは変更なし。新規が11%
　・JavaEE6 Web Profile
　　CDI、Interceptors、JSR 250
　　Singleton EJB
　　JPAはEntityBeanの代わりかな？
　・パッケージングがearではなく、war一つで済むように。
　　war（ウォーファイル）
　　web.xmlはアノテーションで行えるようになり、要らなくなる。
　　
◯マルチコアCPU時代のJavaプログラミング　JS2-14　#jt12_s214
　HotSpotVM依存のお話ですよ。
　・GCとか
　・java.util.concurrent.ForkJoinPool
　　デフォルト並列数は論理CPU数
　・メモリアロケーション
　　TLABが非効率に。先にある程度大きめにスレッドに割り当てを行うため、
　　スレッド数が上がるとTLABが非効率になる-&gt;Eden全体では未使用領域があるんだけどGC発生とか
　・JNIでの問題
　　GetStringCriticalとReleaseStringCritical間はGCが抑止されるため。

　・無意識のロック
　　HashTableやStringBufferなど。
　　String#getBytes()も。
　　InetAddress#getAllByName()が内部でキャッシュを持ってる＝ロック
　　File#renameTo()も内部でキャッシュ（ExpiringCache）持ってる。
　　　Javaでは時間がかかる処理にはキャッシュを利用するという仕組みなので。スレッド数を増加させる場合は注意が必要
　・フェアネスロック
　　lock = new Object();
　　for (int i=0;i&lt;100;i++0{
　　　synchronized(lock){
　　　　System.out.println(&quot;i=&quot;+i+&quot;tid=&quot;+Thread.currentThread().getId());
　　　}
　　}
　　上記を実行してもかたよることがあるよ。＝フェアじゃない
　　モニターによるスレッド割り当ての説明。
　　RentrantLockでフェアネスになるよ。
　・性能分析
　　println()で区間分析。複数スレッドで実行したら実行時間にずれが。
　　println内部でもsynchronizedを利用している。＝ロックのせいで処理時間が遅くなるスレッドが出てきたりする。
　　JVMTIで性能分析する場合RawMonitorEnter,RawMonitorExitでロックとりあいなるのでもマルチスレッッドだと正しい値がでない。GetThreadLocalStrage使え。
　・まとめ：
　　ハードウェア更改時にプログラムの変更が必要になることもあるよ。
　　わずかのロックが命取りに（ロックをあんまり使わない、無意識なロックを見極めなさい）
　　ロックポリシーの選択（レスポンス重視なのか、スループット重視なのか） 

　QA
　　Q：事例が合ったという話でしたが、コードの修正しかないんですか？
　　A：はい。やっぱり治すのがいいです。あとは、チューニングですが。。。
　　Q：ReentrantLockが昔はスレッドダンプにでなかったのですが、今は？
　　A：今は出ます。
　　Q：各世代とJavaのバージョンのマッピングは？
　　A：第3世代が1.5くらいから。第2世代が1.3とか1.4？
　　
◯サービス維持・発展を踏まえた楽天オークションのJava EEによる開発と運用　JS2-23　#jt12_s223
　・短期で人材が参加できるようにしているというお話
　・楽天オークションの成り立ち（郵政とdocomoと楽天）
　・商品、コミュニティ系（出品とか検索とか）と流通系（入出金、配送など）の2軸
　・メンバー30人くらい
　　ミドルエンジニア
　　エンジニア
　　プロデューサー
　・楽天市場をベースとして機能追加
　・巨大でリリースが大変に。
　・この5年で機能分解して機能ごとのリリースをめざして改善している
　・JavaEEを使う理由
　　WebLogic上にバッチが動いているなどの仕組みをしている？
　　管理機能が充実している（WebLogic）
　・GlassFishへ移行中
　　あれ？Weblogicの管理機能がいいんじゃなかったっけ？
　　EJB実装はWebLogicに依存してるのでWebLogicもバージョンアップしてる。
　※メモはここまで。電池の持ちを気にして

◯ JSR 353: Java API for JSON Processing　JS2-33　#jt12_s233
　・JSONとは
　　JavaScript Object Notation
　　「軽量な」データ交換フォーマット
　・JSON仕様のグラフ
　・XMLもあるのになんで？
　　シンプルで可読性が高い
　　属性がなく、複雑なスキーマ仕様がない。
　・コンパクト
　・Twitter、GoogleMaps、YahooWebServicesなど
　・JCP、JSRのお話
　　JSRの役割とか。
　　リファレンス実装の話。
　・JSR-353 http://jcp.org/en/jsr/detail?id=353 
　　StAXやDOMに対応するよ。
　・ゴールではないもの
　　JSONをJavaのドメインオブジェクトにバインドするとかもどすとか。
　・メンバー
　　@yusukeyも一人
　・参加になったきっかけ
　　2011/10にTwitterがJCPに参加
　　2012/12/6にJSON JSR提出
　　年越しに採択
　　@yusukey「これTwitterで重要じゃない？」
　　@cra　「やりましょう」
　・JSRのコミュニケーション
　　世界各地の人なので、基本メールベース（今は自己紹介したくらい。。）
　・既存のJSONライブラリ
　　json.org、jackson、google-gson
　・なんで、いろいろあるのにJSR？
　　標準化、クラスパスにデフォ、シンプル、ライセンスで悩まないとか。
　・ロードマップ
　　エキスパートグループ作成→アーリードラフト→パブリックレビュー→ファイナルリリース
　　いま、アーリードラフト
　・最新仕様
　　javax.json.*
　　javax.json.spi.*
　　javax.json.stream.*
　　javax.json.tree.*
　　メソッドチェインできるみたい。
　　JsonObject.create(reader);みたい
　　JsonObject obj = …;
　　JsonWriter writer = obj.accept(writer.)
　・改善案
　　JSON***にしたいな。
　　spiはらないんじゃないかな？（いま、一個しか無い）
　　treeもいらないんじゃないかな？（そんなに増えないし）
　　parseじゃないかな？
　　メソッド追加（asStringで文字列化とか、create(String)とか）
　QA
　　Q：JSONのスキーマ決めないの？バリデータとか
　　A：JSONスキーマはあんまりもてはやされてない。一応あるのはある。

　さすがの安定感。適宜笑いが入ってて楽しかったです。

◯UI Controls and Charts: Drag-and-Drop, Filtering, Sorting, Table Hookup with Charts　JS2-42　#jt12_s242
　◯JavaFX 2.0のControls
　◯Label、Button、ToggleButton、RadioButton、Hyperlink、CheckBox
　◯Slider、ScrollBar、ScrollPane、ProgressIndicator、ProgressBar
　◯TextField、PasswordField、TextArea
　◯新しい機能
　　TitledPaneとAccordion
　　TabPaneとSplitPane
　◯Charts
　　XYChartとPieChart
　　XYChartにはLineChart、AreaChart、BarChart、ScatterChart、BubbleChartがある。 
　◯Menu
　　MenuItemの下に色々ある。
　　CheckMenuItem、RadioMenuItem、CustomMenuItem（SeparatorMenuItemとか)
　◯ChoiceBox、ContextMenu、ToolTip
　◯ListView
　◯2.1の新機能
　　ComboBox、http://fxexperience.com/2012/03/new-in-javafx-2-1-combobox/
　　StackedBarChart、
　　StackedAreaChart
　　MenuBarのMac OSへの対応（窓の中じゃなく、画面上部にちゃんと出る。）
　　LCD Text
　QA：
　　Q：テキストエリアの日本語入力とかは？
　　A：JavaFX3.0で対応
　　Q：FXML（Scene Builder）を使ってコードが生成されるのか？
　　A：ドラッグ・アンド・ドロップでやる場合はScene Builderでもできるでしょう。私はNetBeansとかでやりますけどね。
　　Q：Dialogはいつ対応されるんでしょうか？
　　A：私は使ってるんですけど、JavaFXに入れてもらえませんｗコードは出来てますｗ
　　Q：Chartがありますが、pngやsvgでの出力は可能？
　　A：できません。そのうちできるかも。
　　Q：Chartの結合はできる？Axisの時間を便利に使うツールはありますか？
　　A：できないです。ありません。
　　Q：FX2.0でカスタムコントロール作るの大変？
　　A：私のマシンでは動いてるんですがｗ
　　Q：ChartにCSSを適用することは可能？
　　A：可能です。データはJavaから出力されたものだけですけどね。
　　Q：JavaFX2.0の起動速度が上がったけど、どういう仕組？
　　A：私は上の方のエンジニアなのでよくわからないですが、GPUをうまく使ってるみたいです。

◯Learn how the JVM is fundamental to our architecture.　BoF2-03  #jt12_b203
　Java、Scala、Linuxのカーネルまで担当してます。
　・Twitterのアーキテクチャ
　・スパイクが急にあがる場合にも対応できるスケールの話。
　　なでしこ、#バルス、大震災など
　・Twitterはプラットフォーム

　・最上位層：HTTP Proxy（Routing）
　・中層：Twitter App（Models/Biz Objects、Composition）
　　UserService、TweetService、TimelineServiceに分解（Models/Biz Objects）
　　API（Composition）
　・最下層：TimelineStorage、TweetStorage、SocialGraph、UserStorage（Storage/Retrieval）
　
　・NEEDS
　　　サーバ負荷をハンドルできる必要がある。
　　　言語のフレキシビリティ（JavaとかScalaとか）
　　　成熟したコンカレンシーモデル
　・Java が提供してるもの
　　開発者のエコシステムが素晴らしい。例：Nettyを使っている。Non-blocking I/O
　　world class JITとコンカレンシーモデル
　　メモリ管理
　　　利点：メモリ管理を意識しなくていい
　　　課題：GC。OpenJDKのコミュニティで参加してる。
　　複数の言語が実行可能な環境
　・OpenJDKについて
　・Finagleのお話。
　　Netty上に作られた非同期なRPCサーバクライアントのライブラリ
　　http://twitter.gthub.com/finagle/
　・TimelineServiceのお話
　　340M/day
　　10K/sec（イベント時）
　・ツイートが入ってくる仕組み
　　HTTP Proxy → tweet API → queue → tweet daemon → fanout （social graph）→ delivery → timeline cache → redis
　・fanout
　　4000フォロワーごとにdeliveryにツイートが渡される。
　・Timelineを見る仕組み
　　HTTP Proxy → timeline API（user service、） →timeline service →timeline cache
　・timeline、tweets、users
　・timeline serviceとtimeline cacheの関係
　　Finagleで負荷分散している
　・timeline serviceからtimeline cacheへのリクエストを監視して、
　　高頻度のリクエストについては、timeline serviceのin-process cacheに入れる仕組み
　・Service[Request, Response]
　　Scalaの記述
　　service(request).onSuccess {response =&gt; println(&quot;got response! &quot; + response) } 
　・Finagleのコンポーネント
　　コネクション管理
　　プロトコルデコード　ThriftでService以降がつながってる。
　　分散トレース　オーバーヘッドを抑えて全体を監視（RPCトレース（呼び出しと処理時間のタイムラインチャート））
　　サービスディスカバリ　
　　observability

◯パネルディスカッション
　・JGGUG、JRuby、Scala、Groovyのコミュニティ運営に関係している方々によるディスカッション。
　　
</code></pre><p>　　</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのJava7でのテスト失敗問題の対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/05/lucene-gosen%E3%81%AEjava7%E3%81%A7%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E5%A4%B1%E6%95%97%E5%95%8F%E9%A1%8C%E3%81%AE%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Thu, 05 Apr 2012 00:20:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/05/lucene-gosen%E3%81%AEjava7%E3%81%A7%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E5%A4%B1%E6%95%97%E5%95%8F%E9%A1%8C%E3%81%AE%E5%AF%BE%E5%BF%9C/</guid>
      <description>先日、2.0.0リリースの記事にも記載しましたが、Java7でテストケースが失敗する問題がありました。 @haruyamaさんと@hideak</description>
      <content:encoded><p>先日、<a href="http://johtani.jugem.jp/?eid=72">2.0.0リリースの記事</a>にも記載しましたが、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=28">Java7でテストケースが失敗する問題</a>がありました。</p>
<p>@haruyamaさんと@hideaki_tさんの協力により問題を解消し、trunkと4xブランチにコミットしました。</p>
<p>issueにも記載しましたが、Java6からJava7にバージョンアップするタイミングで変更されたUnicodeのバージョンが原因でした。
Java6ではUnicodeのバージョンが4.0です。Java7ではUnicodeのバージョンが6.0に変更されています。
今回の問題は「・」（0x30FB）の文字列のCharacter.getType()がCONNECTOR_PUNCTUATIONからOTHER_PUNCTUATIONに変更されたのが原因です。（この変更自体はUnicode 4.1で変更されたみたい）
カタカナ文字種の判別をlucene-gosenのnet.java.sen.tokenizers.ja.JapaneseTokenizerのgetCharClass(char c)メソッドで行なっています。
修正前は、ここで、charの範囲が0x30A0～0x30FFにある文字でかつ、Character.getType()がCONNECTOR_PUNCTUATIONでないものがカタカナとして判別されていました。
issueの添付ファイルにJava6とJava7で上記範囲の文字のCharacter.getType()のリストを生成して、該当する文字を探した所、「・」（0x30FB）のみであることがわかりました。
ということで、このコードの意図としては、「・」はカタカナではないと判別したかったのだと。
上記の確認を行えたので、ソースコードを修正してコミットしました。
2.0.1としてリリースするかは、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=29">Issue29</a>のボリュームを見て考えますので、もう少しお待ちください。</p>
<hr>
<p>参考にしたサイト：
<a href="http://www.hos.co.jp/blog/20111004/">JavaSE 7でメソッド名に使えなくなった文字</a>
<a href="http://www.unicode.org/reports/tr44/tr44-4.html#Change_History">UNICODE CHARACTER DATABASE<a/>のHistory</p>
</content:encoded>
    </item>
    
    <item>
      <title>【重要】lucene-gosen 2.0.0リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/02/%E9%87%8D%E8%A6%81lucene-gosen-2-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 02 Apr 2012 19:22:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/02/%E9%87%8D%E8%A6%81lucene-gosen-2-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>先日、宣言したとおり、lucene-gosenのパッケージ名＋クラス名の変更を行ったlucene-gosen 2.0.0をリリースしました。 Lucene/Solr</description>
      <content:encoded><p>先日、宣言したとおり、lucene-gosenのパッケージ名＋クラス名の変更を行ったlucene-gosen 2.0.0をリリースしました。
Lucene/Solr 3.6.0のリリースを待つつもりだったのですが、なかなか出ないので先にリリースを行いました。
現時点では、branches/4xについては、パッケージ名、クラス名の修正が追いついていません。
明日までに4xブランチについても修正を反映する予定です。</p>
<p>参考までに、1.2.1から2.0.0への変更点について以下にまとめました。
また、変更に伴い、Solrのschema.xmlに記述するクラス名も変更になります。
<a href="http://lucene-gosen.googlecode.com/svn/trunk/example/schema.xml.snippet">schema.xmlのサンプルについてはこちらをご覧下さい。</a></p>
<h3 id="変更点"><strong>変更点</strong></h3>
<hr>
<p>まずは、パッケージ名の変更点です。
左が旧パッケージ名、右が新パッケージ名となります。</p>
<table class="list_view">
  <thead>
    <tr>
      <th>旧パッケージ名</th>
      <th>新パッケージ名</th>
    </tr>
  </thead>
  <tbody>
    <tr class="spec">
      <td>org.apache.lucene.analysis.ja</td>
      <td>org.apache.lucene.analysis.gosen</td>
    </tr>
    <tr class="specalt">
      <td class="alt">org.apache.lucene.analysis.ja.tokenAttributes</td>
      <td class="alt">org.apache.lucene.analysis.gosen.tokenAttributes</td>
    </tr>
  </tbody>
</table>
<p>また、パッケージ名とは別に、以下のクラス名も変更になっています。
まずは、<b>org.apache.lucene.analysis.gosen</b>のクラス名の変更点です。</p>
<table class="list_view">
  <thead>
    <tr>
      <th>旧クラス名</th>
      <th>新クラス名</th>
    </tr>
  </thead>
  <tbody>
    <tr class="spec">
      <td>JapaneseAnalyzer.java</td>
      <td>GosenAnalyzer.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseBasicFormFilter.java</td>
      <td>GosenBasicFormFilter.java</td>
    </tr>
    <tr class="spec">
      <td>JapaneseKatakanaStemFilter.java</td>
      <td>GosenKatakanaStemFilter.java</td>
    </tr>
    <tr class="specalt">
      <td>JapanesePartOfSpeechKeepFilter.java</td>
      <td>GosenPartOfSpeechKeepFilter.java</td>
    </tr>
    <tr class="spec">
      <td>JapanesePartOfSpeechStopFilter.java</td>
      <td>GosenPartOfSpeechStopFilter.java</td>
    </tr>
    <tr class="specalt">
      <td>JapanesePunctuationFilter.java</td>
      <td>GosenPunctuationFilter.java</td>
    </tr>
    <tr class="spec">
      <td>なし</td>
      <td>GosenReadingsFormFilter.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseTokenizer.java</td>
      <td>GosenTokenizer.java</td>
    </tr>
    <tr class="spec">
      <td>JapaneseWidthFilter.java</td>
      <td>GosenWidthFilter.java</td>
    </tr>
  </tbody>
</table>
次は<b>**org.apache.solr.analysis**</b>です。
<table class="list_view">
  <thead>
    <tr>
      <th>旧クラス名</th>
      <th>新クラス名</th>
    </tr>
  </thead>
  <tbody>
    <tr class="spec">
      <td>JapaneseBasicFormFilterFactory.java</td>
      <td>GosenBasicFormFilterFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseKatakanaStemFilterFactory.java</td>
      <td>GosenKatakanaStemFilterFactory.java</td>
    </tr>
    <tr class="spec">
      <td>JapanesePartOfSpeechKeepFilterFactory.java</td>
      <td>GosenPartOfSpeechKeepFilterFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>JapanesePartOfSpeechStopFilterFactory.java</td>
      <td>GosenPartOfSpeechStopFilterFactory.java</td>
    </tr>
    <tr class="spec">
      <td>JapanesePunctuationFilterFactory.java</td>
      <td>GosenPunctuationFilterFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>なし</td>
      <td>GosenReadingsFormFilterFactory.java</td>
    </tr>
    <tr class="spec">
      <td>JapaneseTokenizerFactory.java</td>
      <td>GosenTokenizerFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseWidthFilterFactory.java</td>
      <td>GosenWidthFilterFactory.java</td>
    </tr>
  </tbody>
</table>
<p>また、上記クラスに関連するテストクラスの名前も変更になっています。</p>
<p>以上がクラス名、パッケージ名の対応に関する修正ついてでした。</p>
<hr>
<p>また、現在、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=28">Java7にてテストケースが失敗する問題</a>が見つかっています。
こちらの問題の対応版についても近日中にリリースを行う予定です。</p>
<p>問題点、質問などありましたら、コメントしていただくと回答いたします。</p>
<p><b>2012-04-03追記</b>
忘れてました、すみません。今回のリリースで、以下の機能が追加されています。</p>
<ul>
<li>Antのパラメータにproxy.user、proxy.passwordの追加</li>
<li>GosenReadingsFormFilterの追加</li>
<li>TokenAttributeの修正（PronunciationsAttributeImpl、ReadingsAttributeImpl）</li>
</ul>
<p>Antは認証が必要なプロキシ環境で辞書のビルドを実施するときにユーザ名、パスワードを指定できるようにしました。</p>
<p>GosenReadingsFormFilterは単語を読みに変換するTokenFilterになります。
よみは、辞書に登録してある読みになります。オプションとして、romanizedが指定可能です。指定をすると、よみをローマ字に変換します。</p>
<p>TokenAttributeの修正は、バグフィックスになります。<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=26&amp;can=1">Issueはこちら</a>です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>【重要】lucene-gosenの次期リリースについて(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/27/%E9%87%8D%E8%A6%81lucene-gosen%E3%81%AE%E6%AC%A1%E6%9C%9F%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Tue, 27 Mar 2012 02:39:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/27/%E9%87%8D%E8%A6%81lucene-gosen%E3%81%AE%E6%AC%A1%E6%9C%9F%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>lucene-gosenを利用して頂いてる皆様に連絡があります。 連絡事項 次期lucene-gosenのリリース（2.0を予定）にて、org.</description>
      <content:encoded><p>lucene-gosenを利用して頂いてる皆様に連絡があります。</p>
<h3 id="連絡事項">連絡事項</h3>
<hr>
<p><strong>次期lucene-gosenのリリース（2.0を予定）にて、org.apache系のパッケージ名および、クラス名の変更を行います。</strong>
Lucene/Solrの次期リリース版である3.6.0以降では、lucene-gosen 2.0（予定）を利用するようにしてください。</p>
<h3 id="経緯">経緯</h3>
<hr>
<p>Twitterでは少しずつツイートしていますが、Lucene/Solr 3.6から日本語の形態素解析器がLucene/Solrにて用意されることになりました。
ベースとなっているのは、Atilika社が開発した<a href="http://atilika.org/">Kuromoji</a>という形態素解析器です。
Lucene/SolrにコントリビュートされたタイミングではKuromojiAnalyzerなど、Kuromojiという名称が残った形で取り込みが行われました。
その後、<a href="https://issues.apache.org/jira/browse/LUCENE-3909">LUCENE-3909</a>にて、Kuromojiではなく、一般的な名称（Japanese*）に変更する提案が行われました。
この提案で、luene-gosenが利用しているクラス名、パッケージ名と大半のクラスが衝突してしまうこととなりましす。
今後も、lucene-gosenを利用していただけるように、lucene-gosenの<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=27">Issue</a>を発行し、
現在、lucene-gosenのtrunkにてパッケージ名の変更及びクラス名の変更作業を行なっています。
正式にリリースするタイミングになりましたら、再度連絡いたします。</p>
<h3 id="ブランチなどについて">ブランチなどについて</h3>
<hr>
<p>現時点ではパッケージ名、クラス名の変更はリポジトリの以下のものについてのみ作業を行う予定です。</p>
<ul>
<li>trunk</li>
<li>branches/4x</li>
</ul>
<p>現時点でのリリース版（1.2系）のソースについてはbranches/rel-1.2にて、これまで同様のクラス名、パッケージ名のまま変更を行いません。
1.2系についてはこちらをご覧下さい。</p>
<p>また、当ブログにて、提供しているSolr入門のサンプルに利用可能なschema.xmlなどの<a href="http://johtani.jugem.jp/?eid=44">記事</a>についてもLucene/Solr3.6がリリースされた際には再度修正して掲載いたします。
Kuromojiの利用方法もあわせて記載したいです。</p>
<hr>
<p>参考：
lucene-gosenとKuromojiの機能などの比較については<a href="http://www.rondhuit.com/solr%E3%81%AE%E6%97%A5%E6%9C%AC%E8%AA%9E%E5%AF%BE%E5%BF%9C.html">こちら</a>を参考にしてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>久々にMac Miniのことでも(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/25/%E4%B9%85%E3%80%85%E3%81%ABmac-mini%E3%81%AE%E3%81%93%E3%81%A8%E3%81%A7%E3%82%82/</link>
      <pubDate>Sun, 25 Mar 2012 04:47:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/25/%E4%B9%85%E3%80%85%E3%81%ABmac-mini%E3%81%AE%E3%81%93%E3%81%A8%E3%81%A7%E3%82%82/</guid>
      <description>子供の寝かしつけしてたら、寝かしつけされてしまって、２時に目が覚めてしまいました。 TSUTAYAでCD借りてきてウォークマンに入れようと思っ</description>
      <content:encoded><p>子供の寝かしつけしてたら、寝かしつけされてしまって、２時に目が覚めてしまいました。
TSUTAYAでCD借りてきてウォークマンに入れようと思っていたのに。。。
ということで、今作業してます。</p>
<p>MBAを買う前から使っている、もう９年目のWindows自作デスクトップで基本的にはCDの取り込みをしています。
<a href="http://buffalo.jp/products/catalog/storage/hs-dhtgl_r5/?PHPSESSID=1c389a9c0b58221d0102b4b1e8498e2e">ファイルサーバ（BuffaloのTeraStation）</a>がLionから接続できないというのと、
X-アプリで曲管理していたのでという理由です。
あと、ウォークマンなのでMac用のソフトがなかったりというのも理由です。</p>
<p>で、眠い目をこすりながらWindowsマシンを起動すると、ネットワークにつながらないじゃないですか。。。
再起動してもダメでした。</p>
<p>となってしまったので、せっかく、譲ってもらったMac miniも活用しないといけないなと思い、iTunesでアルバムを取り込んでみています。
ここ数年はWAV形式でデータを取り込んでいたので、WAVでの取り込みを設定して、iTunesで取り込みました。</p>
<p>とまぁ、ここまでやってから、件のWindows機を再度、再起動したらネットワークに繋がってしまいました。。。
なので、残りのCDはWindows機で取り込みしてます。
けど、そろそろこのPCも危なそうなので、データをサルベージして、Mac miniで作業できるようにしないとなぁ。
Macでウォークマンを運用している人はどうやってるんだろう？
ちなみに私が利用しているウォークマンは<a href="http://www.sony.jp/walkman/products/NW-A847_V/index.html">NW-A847</a>です。
エンコードはWAV形式で取り込んで、X-アプリでウォークマンに入れています。
iTunes使ってる人は、アップルのロスレスエンコードをつかってるのかなぁ？
ということで、調査・検討したい項目はこんなところですか。</p>
<ul>
<li>調査：TeraStationにMacからつなぐ方法（Lionで利用できそうなソフトなにかないかな？）</li>
<li>調査：iTunesでウォークマンに曲転送（やってみればすぐわかるだろう。。。）</li>
<li>調査：iTunesの曲をCDに焼く方法（車で曲聞くのにCDだから）</li>
<li>検討：ファイルサーバのリプレース対象となる機種（ファイルサーバは導入してまだ５年だから余裕がないと買い換えない）</li>
<li>検討：iPodに鞍替え（Sony好きだし、ウォークマン好きだし、買ったばっかりだからまず無い選択肢）</li>
<li>検討：Mac miniにVirtualBox入れて、WindowsからX-アプリ使うとか。（ファイルサーバが見えるようになるかな？）</li>
<li>検討：Mac miniをWindowsとして運用（もったいなさすぎる気がする）</li>
</ul>
<p>とまぁ、まだ悩んでるだけで、取り留めもない事を書いて、しかも結論が出ないままですが、ご容赦ください。
ご意見、提案を大募集してます！！</p>
</content:encoded>
    </item>
    
    <item>
      <title>第5回 Twitter API勉強会 @渋谷 #twtr_hack(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/22/%E7%AC%AC5%E5%9B%9E-twitter-api%E5%8B%89%E5%BC%B7%E4%BC%9A-%E6%B8%8B%E8%B0%B7-twtr_hack/</link>
      <pubDate>Thu, 22 Mar 2012 00:28:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/22/%E7%AC%AC5%E5%9B%9E-twitter-api%E5%8B%89%E5%BC%B7%E4%BC%9A-%E6%B8%8B%E8%B0%B7-twtr_hack/</guid>
      <description>はい。またまた、Twitter API勉強会に参加してきました。（今回から開催回数の記載がなくなった？） 今回は直前でタイムテーブルが変わってし</description>
      <content:encoded><p>はい。またまた、Twitter API勉強会に参加してきました。（今回から開催回数の記載がなくなった？）
今回は直前でタイムテーブルが変わってしまう波乱がありましたが、個人的には楽しめる内容でした。
Twitterの国際化や形態素解析などの話が聞けたのがすごく面白かったです。
アーキテクチャや利用されている形態素解析器の話など、また、現状の問題点なども話が聞けました。
日々、進化しているんだなぁと。
残念ながら、発表者の方が懇親会にいらっしゃらなかったので、詳しく聞けませんでしたが、挨拶だけは出来ました。
実際のテスト環境や導入方法、A/Bテストとかやってるのかなど、ブログをかきながら色々と気になることが出てきてしまいますｗ
頭の回転がよくないので、話を聞いてる間は質問があんまり思い浮かばなかったなぁ。。。</p>
<p>その他にも（使ったことなくてすみません。。。）「昼会」のサービスの話、他の言語でのTwitterの活用の話も聞けました。</p>
<p>途中で帰ってしまいましたが、懇親会でも数人の方とお話できて、楽しかったです。
（今回の懇親会会場はちょっと人数の割に手狭だったかもなぁ）</p>
<p>次回は4月末を予定しているようでした。今度こそ、利用規約の話になるんですかね？
忙しさを見つつ、また参加しようかと思います。</p>
<p>ということで、いつものメモです。</p>
<hr>
<p>開催日時：2012/03/21 19:00  ～  21:00場所：ハロー会議室shibuya
<a href="http://www.zusaar.com/event/237004">Zusaarのページ</a></p>
<p>◎Twitterの日本語検索、ハッシュタグについて　@keita_f</p>
<pre><code>
　◯アーキテクチャ
　　・Twitterバックエンドの国際化
　　　Before
　　　　各言語ごとに独自実装。
　　　After
　　　　共通テキスト処理ライブラリ：Penguin
　　・アーキテクチャ：検索
　　　Firehose → ingester → Earlybird ⇔ Blender ← ユーザの検索
　　　Firehose：ツイートの受信
　　　ingester：ツイート解析
　　　Earlybird：Luceneベースのインデクサ
　　　Blender：
　　・アーキテクチャ：トレンド
　　　TrendsはGeo（ユーザ位置情報）を利用してるっぽい。
　　　Term Statisticsは過去ツイートからHadoopで頻出フレーズを解析する。
　　　今頻出しているフレーズ＋過去頻出しているフレーズ
　◯日本語関連
　　・ツイートの言語判定
　　　言語判定にはIndigenous Tweets（Kevin Scannell）をベースに実装。
　　　５０言語サポート、Javaで実装
　　　・文字種チェック
　　　・エンコーディングチェック
　　　・Ngram（the→英語、das→ドイツ語）
　　　問題点：
　　　　２つ以上の言語が混じってる場合
　　　　絵文字が入ってる場合
　　　　Unicodeアルファベット
　　・形態素解析
　　　日本語：Gomoku
　　　中国語：LuceneのSmartCN
　　　その他：N-Gram
　　　問題点：
　　　　形態素が小さすぎる
　　　　→そのままだと細かすぎるので品詞情報を元に大きめに区切るようにしている
　　　　iPhone4Sなどが数字で区切られる
　　　　→ASCIIの数字とアルファベットは連結しなおす
　　・フレーズ抽出
　　　他の言語もあったけど、キー入力間に合わず
　　　日本語の場合：
　　　　トークンに分割
　　　　トークンでNgramを生成
　　　　品詞情報を使ってフィルター
　　　　　最初、最後が助詞はNG、接頭詞で終わるのはNG
　　　
　　・やりたい事はまだまだたくさん
　　　・形態素解析の品質向上
　　　　辞書への単語追加
　　　　韓国語、タイ語のトークなイザー
　　　・同義語、類義語、翻訳、音訳、略語などのサポート
　　　・フレーズ・トピックのクラスタリング
　　　・Sentiment Analysis
　　・日本語のできるエンジニア募集中ｗ
　　QA
　　　Q：顔文字を手動で抜き出すと言われたんですが、どのくらいあるんでしょうか？　　　A：正規表現なので、なんとも言えませんが、パターンを
　　　Q：Gomokuの選択の理由は？
　　　A：Javaだから。jarに辞書が入るから。スピードが出るから。
</code></pre><p>◎ランチタイム共有サービス「<a href="http://www.hirukai.jp/">昼会(http://www.hirukai.jp/)</a>」のご紹介　@setomits</p>
<pre><code>　
　◯前提
　　・ネットワーキングは重要
　　・出会いも重要
　　・飲み会だとコスパが悪い
　◯ランチだと
　　・金額が限られる
　　・必ず食べる
　　ということで、昼会だ！
　◯画面でサービスの説明
　　パブリックな昼会、プライベートな昼会がある。
　◯なんでTwitter？
　　LinkedInのOAuthを使ったログイン検討してた。
　　ビジネス目的＋
　　敷居が高いのでTwitterに変更
　◯Twitterのapiで利用しているものなど
　◯ユーザの声
　　・DMを使ってる＝自分からのDMが気持ち悪い
　　　APIの使用制限やフォローしてもらう必要があるのが辛い。
　　　- スパムかと思った
　　　- 乗っ取られたかと思った。
　　　- エンジニアからは、「まぁ、こうするよね」
　　・OAuthについて
　　　- ユーザ登録しなくていいのは嬉しい
　　　- イベント参加のためにOAuthでの書き込み権限まで与えたくない
　◯８ヶ月経って
　　・新しい出会いがあった
　　・昼会を開催する敷居が高い
　　　＝なかなか使ってもらえない
　◯今後の計画
　　募集中！
　　
　QA
　　Q：AppEngineで月額どのくらい？
　　A：料金改定前：数千円/１ヶ月、料金改定後：２万円くらい/１ヶ月
　　　　処理数を減らして、現時点では数千円/１ヶ月に最適化した
　　Q：夜会はないんですか？
　　A：設計していたら、出会い系サイト？になりそうなので、昼にした。
　　　　ドメイン的になぁ。まだ悩み中
　　Q：GAE特化したもの？AWSに移行できる作り？
　　A：できない作りです。（pythonで書いてます。）
　　　　GAEが楽なので、移行可能な作りにするのがつらい。
</code></pre><p>◎LT
　◯Twitter 4 contact　@inda_re
　　・広告！<a href="http://partake.in/events/629f58aa-2407-4ca5-80b3-9050a3680b61">アジャイルサムライの道場</a></p>
<pre><code>
　　・問い合わせフォームをTwitterにしちゃう
　　　今あるHTMLにフォームを入れる方法（MySQLとPHP）
　　　githubにサンプルあるよ。
</code></pre><p>　◯PerlのTwitterモジュールについて　@xtetsuji</p>
<pre><code>
　　・前回出席したら、ムチャぶりが！
　　・いまどきのPerl
　　　進化してるよ、古い情報が検索で出てきてしまうのがネック
　　・perlbrewってのがある
　　　http://perlbrew.pl/
　　・AnyEvent::Twitter::Streamをよく使ってます。
　　・gistにサンプルあげてあるよ！
</code></pre><p>　◯KotlinでもTwitter4J　@ngsw_taro
　　・Kotlinの説明
　　・以下のサイトでインストールしなくても動かせるよ！
　　　<a href="http://kotlin-demo.jetbrains.com/">http://kotlin-demo.jetbrains.com/</a>
　　・NULL安全
　　　Nullを許容する型、許容しない型がある。
　　・一応、Twitter4Jのサンプルも作ったけど、ブログ見てね
　　　 <a href="http://d.hatena.ne.jp/ngsw_taro/">http://d.hatena.ne.jp/ngsw_taro/</a></p>
<p>　◯締め
　Twitter API勉強会ではスピーカーを常に募集中です。登録は<a href="http://bit.ly/twtr-talk">こちら！</a></p>
<p>　
　</p>
</content:encoded>
    </item>
    
    <item>
      <title>「自分の小さな「箱」から脱出する方法」を読みました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/20/%E8%87%AA%E5%88%86%E3%81%AE%E5%B0%8F%E3%81%95%E3%81%AA%E7%AE%B1%E3%81%8B%E3%82%89%E8%84%B1%E5%87%BA%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 20 Mar 2012 23:32:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/20/%E8%87%AA%E5%88%86%E3%81%AE%E5%B0%8F%E3%81%95%E3%81%AA%E7%AE%B1%E3%81%8B%E3%82%89%E8%84%B1%E5%87%BA%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>TLで面白いと見かけて、Amazonで買ってしまいました。 SEやってるのに、つい最近Amazonを使い始めた軟弱者です。 それにしてもAmaz</description>
      <content:encoded><p>TLで面白いと見かけて、Amazonで買ってしまいました。
SEやってるのに、つい最近Amazonを使い始めた軟弱者です。
それにしてもAmazon危険です。スマートフォンにAmazonの
Androidアプリを入れたのですが、これがまた、レコメンドに面白そうな本が出てきて危険です。</p>
<p>話がそれてしまいましたが、面白い本でした。
久々に、小説でも技術書でもない本を短期間で読みました。
「箱」と呼ばれる概念の中と外について、とある会社に転職した管理職の人が学んでいくという物語風の作りです。
いくつか、自分の経験にカブるシーンがあったので、サクサク読めました。
今までの自分になかった考えである「箱」という視点が得られたのがよかったです。</p>
<p>ただ、いくつか気になる点もあるので、また少し時間を開けてからサラっと流し読みしたいと思います。</p>
<p>あと、すこしだけ、キリスト教チックな考え方でもあるかなぁと思う部分もありました。（キリスト教をちゃんと勉強してるわけではないので認識が間違ってるかもしれないです）</p>
<p>人によっては共感出来なかったり、読みにくかったりすると思いますが、私は面白いと思った本でした。
なんとなく、人間関係に違和感を感じていることがある場合は目を通してみるといいかもしれません。</p>
<p>Twitterで読み終えたというツイートをしたら、<a href="http://twitter.com/#!/ledsun/status/182100538727473152?PHPSESSID=43e66de98af6cc0a11e1a6953d2619a2">「自分を変える気づきの瞑想法」を読むとまた面白いですという＠ツイート（これがmentionの日本語の正式名称らしい？）</a>を<a href="http://twitter.com/ledsun">@ledsun</a>いただきました。
箱に入る原理が別の視点で書かれているようです。
読んでみたいです。（本会過ぎてる気がするので、図書館で探そうかなぁ。。。）</p>
<hr>
<p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/490450786X/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=490450786X&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/490450786X/?tag=johtani-22">
      自分を変える気づきの瞑想法【増補改訂版】
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/490450786X/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=490450786X&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/490450786X/?tag=johtani-22">
      自分を変える気づきの瞑想法【増補改訂版】
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>Scala始めました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/14/scala%E5%A7%8B%E3%82%81%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 14 Mar 2012 00:27:01 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/14/scala%E5%A7%8B%E3%82%81%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>えーと、ブログ更新してないなぁとふと思ったので。 １年前くらいからずーっと、つぶやいてましたが、やっとScalaを始めました。 長かった。。。 と</description>
      <content:encoded><p>えーと、ブログ更新してないなぁとふと思ったので。
１年前くらいからずーっと、つぶやいてましたが、やっとScalaを始めました。
長かった。。。
とある、サンプルデータを作成するので、ついでにScalaを勉強してしまえという感じで始めました。</p>
<p>Scalaで学ぶ関数脳入門を買ってもう１年以上経ってました。。。
とりあえず、CSVデータからInsert文を作ってます。
書籍を見ながら、四苦八苦してるところです。
今日は、csvファイルを読み込んで適当に出力した所で終了でした。
現状を忘れないようにというのと、いくつか気になった点があったので、備忘録のため上げておきます。</p>
<ul>
<li>scala IDE for Eclipseを利用（もっさりしてる）</li>
<li>classとは別にobjectというものがある点に慣れない</li>
<li>セミコロンいらないのに、ついつい打ってしまう</li>
<li>usingを利用しようとしたが、Eclipseに怒られた（なんで？）</li>
<li>メソッドの戻り値を省略出来る場合とできない場合がよくわかってない</li>
<li>Javaが混ざる（ファイル出力にBufferedWriterを使う）のがまた戸惑う</li>
</ul>
<p>ということで、四苦八苦してますｗ
７つの言語、７つの世界も参考にしつつ、コップ本をパラパラしながら、ググって書くといった感じです。
Javaだととっくにできていると思うのですが、まだ、読み込んで出力できただけです。</p>
<p>新しいものに触れるのは頭を使って面白いのですが、思い通りに行かず（とくにIDEがもっさりしてたり、上手く使えなかったり）イライラもしていますｗ
まぁ、ちょっとずつ勉強していきます。</p>
<p>（ほかにもいろいろやらないとなぁ。。。）</p>
<p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4844330845/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4844330845&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4844330845/?tag=johtani-22">
      Scalaスケーラブルプログラミング第2版
      </a>
    </p>
  </div>
</div>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4844330845/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4844330845&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4844330845/?tag=johtani-22">
      Scalaスケーラブルプログラミング第2版
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>第0回 Twitter Hack #twtr_hack に遊びに行きました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/09/%E7%AC%AC0%E5%9B%9E-twitter-hack-twtr_hack-%E3%81%AB%E9%81%8A%E3%81%B3%E3%81%AB%E8%A1%8C%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Fri, 09 Mar 2012 03:10:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/09/%E7%AC%AC0%E5%9B%9E-twitter-hack-twtr_hack-%E3%81%AB%E9%81%8A%E3%81%B3%E3%81%AB%E8%A1%8C%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>森ビルに行ったことなかったので、参加してみました。 あと、最近まともにコーディングしてないので、そのへんを矯正するためにもと思って。 （まぁ、遊</description>
      <content:encoded><p>森ビルに行ったことなかったので、参加してみました。
あと、最近まともにコーディングしてないので、そのへんを矯正するためにもと思って。
（まぁ、遊びに行きたかっただけなんですけどね）</p>
<p>すこし遅刻して参加です。
会場は、Twitter Japanさんの@yakitoriでした。ここだけ、撮影可でした。

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20120309/20120309_2267058.jpg" />
    </div>
    <a href="/images/entries/20120309/20120309_2267058.jpg" itemprop="contentUrl"></a>
  </figure>
</div>

で、肝心のハッカソンの中身です（ハッカソン自体、初参加！）。
Twitter APIもTwitter4Jも触ったことがありません。
なので、ソファーに座ってMac Book AirでTwitter4Jのサンプルを眺めながら、とりあえず、Solrに流しこむってのを作ってみました。
今回、Solrに用意したのはlucene-gosenによる形態素解析を行うフィールドとあとは、Solrのexampleについてるダイナミックフィールドたちです。
Twitter4Jからどんなデータが取れるかわからないので、ダイナミックフィールドで型だけ指定してSolrを起動しておきます。
次に、Twitter4Jにある<a href="http://twitter4j.org/en/javadoc/twitter4j/TwitterStream.html#sample()">sample()</a>にてパブリックなツイートの1%を取得（適当なのがこれくらいしか思い浮かばなかったから）。
あとは、SolrJの<a href="http://lucene.apache.org/solr/api/org/apache/solr/client/solrj/impl/StreamingUpdateSolrServer.html">StreamingUpdateSolrServer</a>を利用して、流し込みです。
あんまり時間がなかった（準備とかウォーミングアップで時間かかった）ので、結局、Screan名とツイート本文を
SolrInputDocumentに無理やり詰め込んで登録しただけでした。
ほかにも色々と情報が取れるはずなので、そのへんも格納してファセットとかで遊びたかったなぁ。
とまぁ、こんな感じでタイムアップです。（コードは大したことないので文章だけでｗ）
画面も用意出来なかったので、発表はSolrのダサい管理画面で検索でした。
次回は、もう少し中身を勉強して画面もbootstrapで作ってとかやってみたいですね。
あと、自己紹介スライドに書きましたが、Scalaから使ってみるのもやってみたいです。（その前に、Scalaに入門しないといけないんですけど。。。）</p>
<p>ということで、個人的にいろいろと反省点があるハッカソンでした。
反省点：</p>
<ul>
<li>立ち上がりに時間がかかった（普段からコーディングしてないから）</li>
<li>Twitter4Jについて事前に勉強してない（ごめんなさい）</li>
<li>環境面が整備しきれてない</li>
<li>Twitter API ポケットリファレンス（右の本）を忘れた。。。</li>
<li>スターウォーズのピザの箱の写真取り忘れた＞＜</li>
</ul>
<p>普段、コーディングをしていない仇が出てしまいました。
もっと、すぐにコーディングをできる環境、体質を日頃から鍛えておかないといけないなぁと。
（SolrやTwitter4JのJavaDocをオロオロしながら探すとか情けないです。。。）</p>
<p>場所や雰囲気は全然問題ありませんでした。
ピザを@yusukeyさんからごちそうになったり、飲み物、お菓子が用意されていたりと至れり尽くせりでした。
iPadで音楽も流してくれてたし。
他のハッカソンに参加したことないので比較はできないですが、集中できてよかったです。
あと、ソファーでコーディングもなかなかいいなぁと実感出来ました。（初めて！）
人数的にもこのくらいのほうが話がしやすくていいかなぁと。発表も時間かからないですし。
ただ、帰り道に誰かが言っていましたが、「マラソンというよりは短距離走だったね」というのは否めないかなぁと。
時間は限られちゃうので、準備をしっかりしないとなぁと。
私としては、土日のハッカソンだと参加しづらいので、平日開催がうれしいのですが。
いやぁ、楽しかったです。みなさんの面白いネタも見れたし。（自分のやったのは普通すぎて少し恥ずかしかったです。。。）
あと、GoogleDocsの使い方（自己紹介スライドとかアンケートの集計とか）も学べたのが収穫でした。
@yusukeyありがとうございます！
次回もありそうなので、懲りずに参加しようかな。</p>
<p>さて、他の方たちのブログやtogetterは@yusukeyさんのページからたどってくださいｗ。
<a href="http://samuraism.jp/diary/2012/03/08/1331132580000.html">第0回Twitterハッカソンを開催しました #twtr_hack</a></p>
<hr>
<p>今日購入したCD聴きながらブログ書いてます。
タワレコで視聴して購入したCDです。
なかなか大人な雰囲気でオススメです。

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B006ZCWTXS/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B006ZCWTXS&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B006ZCWTXS/?tag=johtani-22">
      Heaven
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>Clean Coderを読んだ(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/03/clean-coder%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/</link>
      <pubDate>Sat, 03 Mar 2012 01:14:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/03/clean-coder%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0/</guid>
      <description>Clean Coder プロフェッショナルプログラマへの道 Clean Coderを読みました。 理由はTwitterで「Clean Code」がいい本だと流れてきたためです。</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4048860690/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4048860690&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4048860690/?tag=johtani-22">
      Clean Coder プロフェッショナルプログラマへの道
      </a>
    </p>
  </div>
</div>
Clean Coderを読みました。
理由はTwitterで「Clean Code」がいい本だと流れてきたためです。
「Clean Code」はまだ読んでいないのですが、クリーンなコード（メンテナンスしやすく、修正などもやりやすいコード？）を書くために必要な話が書いてあるのだと思います。（まだ妄想）</p>
<p>そして、何も考えずに、「Clean Code”r”」という本が新しく出ていたので、新しい方に手を出しました。
まぁ、軽い勘違いですｗ（コードの構造の話などは出てこなかったです。）
それほど分厚くなく、軽く読めそうだということで読み進めると軽い衝撃を受けました。
Clean Coderはプロのプログラマとして、どのような意識を持つべきか、立ち居振る舞いをするべきかなどが書かれています。
「～したい」はまず守らない約束だという話、ユニットテストを書くことはプロとして当たり前の行為だ、目的意識を明確に持つことなどなど、耳の痛いことが色々と書かれています。
これは、著者の方（パンチカードのころからコーディングをされている！）の実体験を元に、失敗した経験から導きだされているようです。
ところどころ、古くてよくわからない話やちょっとだけしっくりこない表現（ビジネス、QAといった単語）もありましたが、概ねわかりやすい話でした。</p>
<p>基本的にはアジャイルなスタイルの開発を行うプログラマ（設計書に基づいてコーディングするだけの人ではない）について書かれています。
この本を読んでいて、昨年、仕事をご一緒させていただいたRubyistの方たちの開発スタイルを思い出しました。
私よりもこの本に書かれているプロに近いなぁと。
ペアプロやったり、実装方法について相談していたりと。</p>
<p>勘違いでしたが、良い本に出会えて本当によかったです。
私もこの本に書かれているようなチームでのプログラミングをやりたい、またなにかコーディングをしたいという気にさせてくれました。（「～したい」じゃダメって書いてあったのに。。。）
自分を戒めるためにも、定期的に読み返したい本です。
プログラマでいたい方、ある程度プログラミングができるようになってきた方にはぜひ読んでいただきたい本です。
（この流れで、アジャイルサムライやClean Codeを読んだら理解が深まりそうだなぁ）</p>
<hr>
<p>参考URL：
<a href="http://d.hatena.ne.jp/ledsun/touch/20120128/1327759675">35歳定年説をブチ破れ！「Clean Coder プロフェッショナルプログラマへの道 Robert C. Martin」 - ledsunの日記 </a></p>
</content:encoded>
    </item>
    
    <item>
      <title>親子で楽しめる 絵本で英語をはじめる本(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/02/26/%E8%A6%AA%E5%AD%90%E3%81%A7%E6%A5%BD%E3%81%97%E3%82%81%E3%82%8B-%E7%B5%B5%E6%9C%AC%E3%81%A7%E8%8B%B1%E8%AA%9E%E3%82%92%E3%81%AF%E3%81%98%E3%82%81%E3%82%8B%E6%9C%AC/</link>
      <pubDate>Sun, 26 Feb 2012 01:20:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/02/26/%E8%A6%AA%E5%AD%90%E3%81%A7%E6%A5%BD%E3%81%97%E3%82%81%E3%82%8B-%E7%B5%B5%E6%9C%AC%E3%81%A7%E8%8B%B1%E8%AA%9E%E3%82%92%E3%81%AF%E3%81%98%E3%82%81%E3%82%8B%E6%9C%AC/</guid>
      <description>親子で楽しめる 絵本で英語をはじめる本 Twitterでこの本について書かれたブログ記事が流れてきて、購入しました。 最近、英語を身につけておいた</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/479931131X/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=479931131X&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/479931131X/?tag=johtani-22">
      親子で楽しめる 絵本で英語をはじめる本
      </a>
    </p>
  </div>
</div>
Twitterでこの本について書かれたブログ記事が流れてきて、購入しました。</p>
<p>最近、英語を身につけておいたほうがいいなと思うことが多々あり、子供にも英語を勉強してもらいたいなと思っていたところでした。
あとは、私自身が英語が苦手というのもあり、子供をダシにして勉強したいというのもありまして。。。
サラっと読んでみましたが、参考になりました。
特に「多読」というキーワードが面白かったです。<a href="http://www.seg.co.jp/sss/">こちら</a>が元のようですが。
多読とは、文章を分析しないで大意を把握する読書法だそうです。</p>
<ul>
<li>辞書を引かずに楽しめるものを読む</li>
<li>わかるところをつなげて読む</li>
<li>自分が面白いと思う本を選んで読む</li>
</ul>
<p>という原則があるようで、確かにいいなと思いました。
絵本だと絵が書かれているので、辞書を引かなくても想像できそうですし、楽しめそうだなぁと。
また、多読は<a href="http://johtani.jugem.jp/?eid=46">先日読んだ、速読の本に書かれていた本の読み方</a>にも通じるものがあるなと。（まだ、実践できてないんですけどね）</p>
<p>どうしても英語を勉強させたい！、勉強しないと！と思ってしまいがちですが、この本にも書いてあるように楽に楽しんでやったほうがやっぱいいなぁと。
楽しくないと続かないですからねぇ（実際、何度も挫折してるし、押し付けられるとヤル気がなくなるので。。。）</p>
<p>ということで、実践してみようと思います。（平日は子供が寝てしまってから帰宅なので、まずは土日から）子供のためというよりは、自分の英語の勉強のために。
まずは、簡単な絵本を購入して。
この本の後半半分は、著者の方の感想や説明がついた、オススメの絵本50冊が書かれています。
英語の絵本を入手するのは、結構大変（実際に売ってる店もなかなかないし、手にとって見る機会も少ない）だと思うのですごく参考になりそうです。
いくつかピックアップして、あわよくば本屋で手にとってみようかと。なければ、Amazonで購入しようかなぁと思ってる所です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第4回 Twitter API勉強会 @デジタルハリウッド #twtr_hack に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/02/24/%E7%AC%AC4%E5%9B%9E-twitter-api%E5%8B%89%E5%BC%B7%E4%BC%9A-%E3%83%87%E3%82%B8%E3%82%BF%E3%83%AB%E3%83%8F%E3%83%AA%E3%82%A6%E3%83%83%E3%83%89-twtr_hack-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Fri, 24 Feb 2012 01:50:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/02/24/%E7%AC%AC4%E5%9B%9E-twitter-api%E5%8B%89%E5%BC%B7%E4%BC%9A-%E3%83%87%E3%82%B8%E3%82%BF%E3%83%AB%E3%83%8F%E3%83%AA%E3%82%A6%E3%83%83%E3%83%89-twtr_hack-%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>今週も勉強会に参加しました。 @yusukey さんが開催してるTwitter API勉強会です。 前回とは会場が異なりましたが、広くて大画面で良い会場でした。駅</description>
      <content:encoded><p>今週も勉強会に参加しました。
@yusukey さんが開催してるTwitter API勉強会です。
前回とは会場が異なりましたが、広くて大画面で良い会場でした。駅も近いし。（デジタルハリウッドさいこー！）
今回はLT枠のbootstrapの話が聞きたくて参加しました。（ムチャぶり駆動勉強会の現場をTLで目撃してたのでｗ）
あと、バーチャファイター（昔、VF目当てでセガサターンを買ったなぁ、懐かしい）の話も聞きたかったので。
（ごめんなさい、Twitter APIはまだ触る機会がなさそうです。。。）
今回も途中で参加者同士を数グループ（座席が近い人）に分けて自己紹介タイムがあり素敵でした。
ちょっと残念だったのはネームプレートが今回はなかったことでしょうか。
自己紹介でツイッターID教える＋聞くのがちょっと辛かったです。（やっぱり個人名刺作ったほうがいいかなぁ）
私はPCを開いていたのでTwitterのページを見せることで対応出来ましたが。
（次やるときはお手伝いするのでこえかけてください。）</p>
<p>内容はメモをとってあり、下に書いてますので見ていただければ。</p>
<p>Twitter APIの説明はいろいろ新鮮でした。今回はWebページに貼るリンクやボタンの話でした。
カスタマイズも出来るようになってるってのは知りませんでした。Webサイトやってる人は簡単にカスタマイズできるのは便利ですよね。</p>
<p>バーチャファイターのTwitter連携の話は結構詳細な話が出てきてびっくりでした。（テーブル名とかまで出てきましたｗ）
ゲーム筐体の前のユーザの動きまで考えてシステム作るとかすごいなぁとか。違う業界の話って面白いです。</p>
<p>Bootstrap入門はbootstrapの紹介だけじゃなく、Tips（bootstrap弊害とかw）やその他の便利なサイトの紹介までありました。
スライドもあとから読んでもわかるのがすごく嬉しいです。時間を見て試してみます。
TwitterSphereは懇親会でもデモが見れました。Groovyは名前を知ってるだけだったのですが、Swingとかまで動くんですね。（JVMだから当たり前なんだけどなんか新鮮。）
最後はAndroidのAPI Demoのお話。こちらも普段Android携帯使ってるのに知らないことが多くて。。。
Macでもデモが動くんですね。やっぱり動くものを作ってLTするのって素晴らしいですねぇ（なんか作るかー）</p>
<p>とまぁ、最近はキーワードだけ知ってて、いろいろ触れてもいないものが多いなぁと思ってたところに、
今日の勉強会で色々見れて面白かったです。
Twitterがいろんな人に興味を持たれているのがわかります。
（けど、Twitterってどうやって儲かってるんだろう？？？）
懇親会は、入り口からは想像できない貸パーティースペースで素敵でした。
開始の乾杯の音頭を取ることもできましたし（もちろん、ムチャぶり）。
学生が多かったのですが、声をかける暇なく懇親会が終わってしまったのが少し心残りですかねぇ。
毛色が違うメンバーが居て面白いので、次回も空きがあれば参加しようと思います。
スタッフとかお手伝いもしますので。</p>
<p>ということで、いつものメモです。</p>
<hr>
<p>開催日時：2012/02/23 19:00  ～  21:00
場所：デジタルハリウッド東京本校 1Fセミナールーム
<a href="http://www.zusaar.com/event/208101">Zusaarのページはこちら。</a>他にも学生枠とかありました。</p>
<p>◯ Webサイト向けAPI @yusukey</p>
<pre><code>
　・Webサイト向けAPI
　　フォローボタン、ツイートボタン、Web Intents、@Anywhere
　　サーバサイドの実装扶養、htmlのみで構成可能
　・Twitterボタン
　　Twitterのサイトで色々カスタマイズ可能なボタンが作れます。
　　ツイートのカウント表示とか。
　・Web Intents
　　@ツイート、リツイート、お気に入りが簡単にできるものが作成可能。
　　ツイートの埋め込み用リンクがTwitterのWeb版のツイート詳細から生成可能。
　　「このツイートをサイトに埋め込む」リンクをクリックするとダイアログが出てきます。
　・@Anywhere
　　あんまりフォローされてないけど、JavaScriptでサーバサイドなしにTwitterと連携可能。
　　ユーザIDを辞書的に検知してTwitterの情報をホバーで出してくれる。
　・ウィジェット（検索、プロフィール、お気に入り、リストなど）
　　これもTwitterサイトでカスタマイズ可能。
　・ツイートボタンには公式ロゴを利用しましょう
　・注意点
　　charsetを必ず指定しましょう。
　　IE/Firefoxで問題が発生します。
</code></pre><p>◯ グルーブに分かれて簡単に自己紹介</p>
<p>◯ デジタルハリウッドよりご挨拶</p>
<p>◯ 「Virtua Fighter5 Final ShowdownのTwitter連動機能について」@twtrfk</p>
<pre><code>
　部内にAmitterというSNSを構築して自由に使ってもらって、Twitterを疑似体験してもらった。
　VF5FSでの開発中はTwitter4JにAmitterのラッパーを追加してテストしてたらしい。（おもしろいなー）
　当時は、筐体にツイート機能を入れることも考えていたらしい。
　ツイート自体はバッチで処理してます。
　ゲーム筐体にお客さんが座ってる時間もシステムの性能として考慮しないといけない。
　苦労した点：
　　テストアカウントを100個地道に作成（連番、内容がバレるアカウント名はNGだから）
　　Twitter4Jがうまく動かなかった。。。
　今後の展開：
　　まだ未定
　QA
　　Q：FB連携とかは？
　　A：今は考えてないです。
</code></pre><p>◯ LT
　・ @making「Twitter Bootstrap入門」
　　<a href="http://www.slideshare.net/makingx/twitter-bootstrap">スライドはこちら</a></p>
<pre><code>
　　・Bootstrapとは？
　　　Twtter者が提供するWebアプリケーションのフロントエンド
　　　一般的にCSSフレームワークと呼ばれるもの
　　　簡単にかっこいいデザインにするための枠組み
　　　ダウンロードしてきてHTMLで読み込むだけ。
　　・Bootstrapのコンポーネント紹介
　　　いろいろおしゃれなコンポーネントあるよ。（スライド見ましょう）
　　・Tips
　　　examplesがあるので、それをベースに作るのが簡単です！
　　　Initializrも対応してて、こっちで作るのもいいよ！
　　　bootstrap簡単すぎて、みんな同じデザインのサイトに（弊害）
　　　差別化のサービスが出てきてます。
</code></pre><p>　　　
　・ @kimukou_26 「TwitterSphere of Twitter4J」</p>
<pre><code>
　　Griffon？
　　codehausで作られてるGroovy製のMVCイメージのGUIフレームワーク
　　codehausはJetty作ってるとこですね
　　地球儀にツイートを乗っけることができるアプリ。
　　Groovyの記述がちょっと新鮮。
</code></pre><p>　・ @bina1204 「ApiDemos of Twitter4J for Android」</p>
<pre><code>
　　API DemosというAndroidのデモがあり、ここにTwitter4Jを仕込んでみた？
　　Android系は持ってるのに知らない。。。
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenとSynonymFilterを利用するときの注意点（問題点編）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/02/21/lucene-gosen%E3%81%A8synonymfilter%E3%82%92%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AE%E6%B3%A8%E6%84%8F%E7%82%B9%E5%95%8F%E9%A1%8C%E7%82%B9%E7%B7%A8/</link>
      <pubDate>Tue, 21 Feb 2012 01:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/02/21/lucene-gosen%E3%81%A8synonymfilter%E3%82%92%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AE%E6%B3%A8%E6%84%8F%E7%82%B9%E5%95%8F%E9%A1%8C%E7%82%B9%E7%B7%A8/</guid>
      <description>久々にlucene-gosenの話です。 しかも、あんまり嬉しくない話しです。 すでにissueをアップしていますが、lucene-gosenと</description>
      <content:encoded><p>久々にlucene-gosenの話です。
しかも、あんまり嬉しくない話しです。</p>
<p>すでに<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=23">issueをアップ</a>していますが、lucene-gosenとSynonymFilterを併用する場合に、特定の条件下でNullPointerExceptionが発生してしまいます。</p>
<p>条件は以下の組み合わせになります。</p>
<ul>
<li>Solr 3.5.0以前</li>
<li>lucene-gosen1.2.0 - 1.2.1の辞書なしjar</li>
<li>SynonymFilterFactoryにてtokenizerFactoryを指定</li>
</ul>
<p>根本的にはSolr側の問題のようです。<a href="https://issues.apache.org/jira/browse/SOLR-2909">SOLR-2909</a>としてissueが上がっています。</p>
<p>SynonymFilterFactoryでは、類義語の設定ファイルの単語を読み込むときにtokenizerFactoryを指定できます。
このとき、SynonymFilterFactory内部でtokenizerFactoryに指定されたFactoryのクラスが読み込まれ、
インスタンス化されて、Tokenizerが作成されます。
この、Tokenizerのインスタンス化の処理シーケンスに問題があります。
schema.xmlの&lt;tokenizer&gt;タグで指定されたTokenizerFactoryでは、ResourceLoaderAwareインタフェースのinform(ResourceLoader loader)メソッドが実行されます。
このinform()メソッドがSynonymFilterFactoryのToeknizerのインスタンス化の場合に実行されません。
lucene-gosenのJapaneseTokenizerFactoryではこのinform()メソッドでdictionaryDirのパスの読み込みを行なっています。（<a href="http://code.google.com/p/lucene-gosen/source/browse/branches/rel-1.2/src/java/org/apache/solr/analysis/JapaneseTokenizerFactory.java#73">このへん</a>）</p>
<p>上記の条件では、NullPointerExceptionが発生すると書きました。
辞書を内包したjarファイルを利用している場合、NullPointerExceptionが発生しなくても次のような問題点があります。こちらの問題は見た目は動いているように見えてしまうので注意が必要です。
すべて、SynonymFilterを利用する時点でも問題点になります。</p>
<ul>
<li>compositePOS設定が類義語辞書読み込み時に無効</li>
<li>dictionaryDir設定が類義語辞書読み込み時に無効（＝jarに内包されている辞書で動作する）</li>
</ul>
<p>一見動いているように見えるかもしれませんが、望んでいてる動作になっていない可能性があるので注意が必要です。</p>
<hr>
<h3 id="解決策まだ途中">解決策（まだ途中）</h3>
<p>先程書きましたが、基本的にはSolr側の修正をするのが妥当です。
<a href="https://issues.apache.org/jira/browse/SOLR-2909?PHPSESSID=15f554bea5726faaad9185880c7e6a15">SolrのJIRAにパッチもアップされました。</a>
こちらのパッチをSolrに適用し、SynonymFilterFactoryを次のように指定することで問題を回避することが可能になります。</p>
<pre><code>

　&lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; compositePOS=&quot;compositePOS.txt&quot;
　 dictionaryDir=&quot;dictionary/naist-chasen&quot;/&amp;gt;
　...
　　&lt;filter class=&quot;solr.SynonymFilterFactory&quot; synonyms=&quot;synonyms.txt&quot; ignoreCase=&quot;true&quot; 
　　  expand=&quot;true&quot; tokenizerFactory=&quot;solr.JapaneseTokenizerFactory&quot; 
　　  **compositePOS=&quot;compositePOS.txt&quot; dictionaryDir=&quot;dictionary/naist-chasen&quot;**/&amp;gt;
　...
</code></pre><p>SynonymFilterFactoryの設定にcompositePOS、dictionaryDirを追加します。
ここの設定は&lt;tokenizer&gt;タグで指定された設定と同じ物を指定します。以上で問題なく動作することになります。</p>
<p>ただし、この方法はSolrにパッチを当てなければいけません。
Solrにパッチを当てるのもなかなかな作業だと思います。
ということで、どうにかlucene-gosen側だけでも対応出来る形にしたいなぁと考えているところです。
残念ながら、まだ考えているだけですので、もう少し提供できるのは先になってしまいますが。。。
現時点では、次の方法を考え中です。</p>
<ol>
<li>informメソッドを呼ぶフラグを追加して、どうにかしてinformメソッドを呼び出す</li>
<li>SynonymFilterの修正版をlucene-gosenに内包して提供する</li>
</ol>
<p>できれば、a.にて対応できればと思っています。
最悪、b.の方法かと。
悩んでいる間にSolrの次のバージョンが出てしまわないように出来るだけ早く対応しようと思っています。
他にも問題点や気になる点があれば、日本語、英語を問わないので、気兼ねなくissueに上げてもらうか、Twitterで私宛にメンションしてもらえればと。
（あ、issue23へのパッチでもいいですよ！）</p>
<p>追記：
まだ、SOLR-2909のパッチを適用してからの確認はできていません。（ソース見て大丈夫だと思ってるレベル）
あと、現時点での対応方法としては、「lucene-gosenとは別のjarにSynonymFilterFactoryなどを入れて提供」が妥当かなぁと考えているところです。（無理やりinformメソッド呼び出すのは骨が折れそう＋パッチが思いの外早く出て、導入されたのでlucene-gosen本体に特殊処理を入れるのはあまりメリットを感じない。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>Hadoopソースコードリーディング第8回に参加しました。#hadoopreading(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/02/09/hadoop%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E7%AC%AC8%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-hadoopreading/</link>
      <pubDate>Thu, 09 Feb 2012 01:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/02/09/hadoop%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E7%AC%AC8%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-hadoopreading/</guid>
      <description>また、勉強会ログです、すみません。。。 直接業務とは関係ないのですが、今回はリクルートの中野さんが話しをされるというので顔を出してきました。 も</description>
      <content:encoded><p>また、勉強会ログです、すみません。。。
直接業務とは関係ないのですが、今回はリクルートの中野さんが話しをされるというので顔を出してきました。
もちろん、内容も気になりましたというのもありますが。
実際には中野さんは2分くらいしか喋らなかったんですけどね。。。</p>
<p>今回は、最初のセッションはちゃんとしたソースコードリーディングでした。
Hadoopの入力としてデータをHDFSじゃなくて、MongoDBから取得するときに利用するMultipleInputの実装をしてみたというお話でした。
MongoDBやHadoopを知らないと（私はあんまり知らない。。。）少しきつかったかもしれないです。
幸い、HadoopのMultipleInputについては以前、すこしだけ見たことがあったので、かろうじてついていけました。
最初の部分を聞きのがしてしまったので合っているか自信がないのですが、10genから出ている<a href="https://github.com/mongodb/mongo-hadoop">mongo-hadoop</a>の使い勝手、効率の悪そうなところに手を入れた話しがベースになっているんでしょうか？（ツッコミあるとうれしいです）
気になったのは、MongoDBのデータをHadoopから直接取りに行くというユースケースがどういう場合に出てくるのかという点です。
ネットワークやMongoDBの負荷がHadoop処理へのボトルネックになるんじゃないかしらという懸念がありそうだなと。
発表を聞いていて感じたのは、やっぱりソース書いて動かさないとダメだよねぇ、自分も手を動かそうというところです。
リクルートさんの発表は波乱万丈（？）でした。
最後まで話しを聴いたあとに思った感想は「こんなに話して大丈夫なのかな？」「いろんな人を敵に回してない？」というものです。
実際に昨年の夏くらいからEMCの草薙さんも検証メンバーに入って、リクルート社内での活用シーンをベースにMapRの検証をいろんな項目でやられたようです。（ある程度の詳細は以下のメモ参照）
で、最終的な結論はまだ出ていないのですが、最後に出てきた検討中の比較対象が某C社のCDH。
どちらかというとCDHの方に傾いてる感じに取れる終わり方で心配になりました。。。
会場が会場だからというのもあったのかなぁ？
mesosというキーワードが出てきたのが気になりました。単語だけは以前Twitterで流れてきたのを目にしてたのですが、どんなものかという概要がわかったのは収穫でした。
リクルートさんの資料はどこまで公開されるかわからないですが、もう一度見たい気がします。（mesosの部分だけでも公開して欲しい）</p>
<p>ということで、以下はいつもの自分用メモです。</p>
<hr>
<p>日時： 2012年2月8日（水） 19:00～21:30 （受付開始 18:40）
場所： 豊洲センタービルアネックス（NTTデータ、豊洲駅直通）</p>
<p>◎日本OSS貢献者賞・奨励賞のご案内　@hamaken
　<a href="http://ossforum.jp/ossaward7th">日本OSS貢献者賞・奨励賞</a></p>
<p>◎『オレオレMultipleInputを作る方法（仮）』@muddydixon さん
<a href="http://www.slideshare.net/muddydixon/multipleinput">http://www.slideshare.net/muddydixon/multipleinput</a></p>
<pre><code>
　hadoopからMongoDBを利用する方法を紹介
　CDH3u2かu3で試しました。
　10genからも出てるけど使いにくい？から作った？
　MongoMultipleInputs.addInputPath
　　で色々簡単に使えるようにしたかった。
　　絞り込みしてデータをMapperにとり込みたかったため。
　　　＝絞り込みできないと必要なデータ以外も取得できてしまいJSONのパース処理がオーバヘッドになるから。
　com.mongodb.hadoop.input.DelegatingInputFormat
　　mongodbの環境にあわせて多くのsplitを生成する。
　mongosへのアクセスはメタ情報の取り出しだけ。
　あとは、個々のサーバにアクセスしてるのでデータ取り出しが高速？
</code></pre><p>◎『MapRってどうよ？ - 実際に使ってみた感触を紹介します』
　 - リクルート 中野さん、高林さん、大坪さん</p>
<pre><code>
　MapRの説明資料がなかったので、EMCの草薙さんが出てきて説明を開始。

　Greenplum MRはMapRテクノロジが開発した実装のOEM版。
　EMCは特に手を加えず。
　※MapRについては前回のMapR勉強会の記事を御覧くださいな。
　Q：JavaのAPI互換と言われている部分はHadoopのどのバージョン？
　A：CDH3u2に近いパッチが当たってるらしい。
　Q：DFSIO性能の比較対象のHadoopはどのバージョン？
　A：手元に資料は残念ながらないです。

　ここからリクルートさん。
　検証内容：性能検証
　　中古車サイトで利用されている3つのHiveに置き換えてみた。
　結果（パーティション圧縮）
　　MapRのほうが約1.3倍高速に（そこまで速くならなかった？）
　結果（非パーティション圧縮）
　　MapRのほうが約1.7倍高速に（そこまで速くならなかった？）
　　ビルドイン圧縮はGzip圧縮と比較して高速
　※チューニング次第では結果変わるかもね。

　機能検証：マルチテナント検証
　　目的：
　　　複数ユーザに対して1つのクラスタを提供する
　　　セキュリティの担保（）
　　　余剰リソースの有効活用
　　　複数ユーザのJobの並列実行？
　　結果１（ユーザ権限まわり）
　　　MapRのユーザはLinuxユーザに準拠。
　　　同じmaprユーザでもUIDがずれてるとエラーが起きる。
　　　MapR内のPermissionは内部のクラスタ、ボリュームの権限定義が可能
　　　※Volumeは管理者に付加的な情報を与えるような存在
　　　Job実行はHadoopと一緒のアクセス権。
　　　
　　結果２（FairScheduler）
　　　MapRではデフォルトとなっている。
　　　複数テナントからの同時ジョブ実行の動作を確認。
　　　poolをベースにタスクを割り当ててく。
　　　min/max値にあわせてslotが利用される模様
　　結果３（Load処理（Hive）、Job実行（HiveのSelect））
　　　じゃらんPVデータ１ヶ月分をロード。
　　　NFS→MFS
　　結果４（フェイルオーバー）
　　　HA機能
　　　ノード切り替え時間、タスクの復旧までの時間とか。
　　結果５（DirectNFSでの転送）
　　　NFSGatewayをクライアント側にした場合との違いなど。
　　　クライアント側に載せた場合、通信量が減るから速度が出てる。
　Mesos概要
　　Apache IncubatorのOSS。C++により実装
　　効率的なリソース分離、リソース共有機能を提供するクラスタマネージャ。
　Mesosでの検証
　　マルチテナント（ユーザ権限）硬い。
　　リソース制限ができない。などなど。
　MapRとMesosの比較
　　資料期待！
　リクルートとしてのMapRの評価
　　保守/サポート/教育が充実してるのが重要。速度だけじゃない。
　　けど、まだ検討中です。
　Q：クラスタに対するパーミッションの権限情報はどこで管理されてるの？
　　　どこが落ちたときにそのパーミッションの情報がとれなくなる？
　A：CLDBで管理されてます。
　Q：CLDB内部でのデータはバイナリ？DBを別途持ってる？
　A：HDFS上にボリュームが作られてそこに吐き出されてる。
　Q：Jobのスケジュールの情報とかはどこに？
　A：Job管理データはJobTracker用のボリュームに保存されてる。
　　　それを別の人が引き継ぐ。
　Q：ボリュームが壊れたら？
　A：さすがに壊れたら引き継げない。
　　　けど、壊れにくくしてあるのが売り？
　Q：MapRのチューニングのポイントはどんなものがあるの？
　A：チューニングに関しては256Mのchunkサイズを64Mに減らした場合の化粧しかしてない。Apache Hadoopの方は若干チューニングした。
　Q：DFSのシリアライズは？ボンディングとかでもできるんじゃないの？
　A：そこはやってないのでやってみます。
　Q：MapRでの8台構成で全部のせてみた場合、CLDBのリソースはどのくらいもっていってるの？
　A：結構使ってる。CLDBを3台以上にしたらサポート対象外。
　Q：CLDBを3台に制限してる理由は？
　A：わからない。
　Q：本番MapR、検証CDHというのはあり？
　A：M3を開発環境で利用して欲しいですね。（ステマ？）開発目的だとM5を無償提供してほしいなー（EMCへのお願い？）
　Q：Pigの検証は？
　A：リクルートではHiveを利用してたので検証してない。


次回は3月ころかなぁ？だそうです。
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>Fluentd Meetup Japanに参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/02/05/fluentd-meetup-japan%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Sun, 05 Feb 2012 00:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/02/05/fluentd-meetup-japan%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>Fluentd meetup in Japanに参加しました。いつも面白そうな話を聞いてばっかりなので、役に立つためにスタッフとしても参加してみました。 まずは、会場が綺麗</description>
      <content:encoded><p><a href="http://www.zusaar.com/event/193104">Fluentd meetup in Japan</a>に参加しました。いつも面白そうな話を聞いてばっかりなので、役に立つためにスタッフとしても参加してみました。
まずは、会場が綺麗でびっくりしました。しかも電源タップまで用意されていてかなり充実してました。Ustはまだ見ていないのですが、Ustも録画までされていて素晴らしい運営メンバーでした。</p>
<p>開発者の古橋さんほか、Treasure Data Inc.の方たち（太田さんはいなかった）が集まり、
Fluentdの仕組みから、プラグイン開発の方法、実際にプラグイン開発してしかも本番で利用してる実例まで
「パーフェクトFluentd」という本ががそのまま書けてしまうのではというくらい密度の濃い話が聞けました。
私個人は、いつものごとく興味を持ってるんだけど、仕事、私用では触っていないという腰抜けなので話についていくのが大変でしたが。。。
各セッションごとに、しっかりとしたQA時間が取られていて、質問者も質が高く、中の人や発表者の方も誠実に回答されている感じがすごく良かったです。
（残念ながらLTの時は懇親会の準備を手伝っていたので聞けてないです）
基本はログの中継サーバとして利用するという話でした。
個人的には性能系のデータやログ（遅いクエリのログに関するプラグインの話などもあった）をストレージに保存して、時間を軸にログとグラフを表示するとかいう仕組みに使うと面白いかもなぁと感じました。
今回はFluentdがメインなので、ログを集める部分の話が主だったところでしたが、懇親会直前の@doryokujinさんのスライドでも上がっていた集計後のビジュアライゼーションに関する部分のはなしも聞いてみたいなぁと思ってるところです。</p>
<p>あとは、いくつか知りたかったことが。
Treasure Data Serviceというサービスをやってるみたいで、SQLライクな問い合わせやビジュアライゼーションされた画面があるみたいな話があがっていたんですが、画面が見てみたかったです。
また、@just_do_neetさんの構想図のその後を聞いてみたいですねぇ。</p>
<p>それにしても実例も構成から性能値まで上がっていたので、実際に利用する場合に参考になる資料だらけで普及にはずみがつきそうです。
Ust録画が上がっているようなので復習したいと思います。
（あ、内容とは関係ないけど、Ust録画にハッシュタグつきのツイートを時間軸で重ねて表示するサービスとか機能ってないのかな？）</p>
<p>ということで、以下はいつものメモ書きです。</p>
<hr>
<p>2012/02/04 12:30  ～  19:00</p>
<p>フューチャーアーキテクト セミナールーム</p>
<p>参考URL：<a href="http://fluentd.org">http://fluentd.org</a></p>
<p>◎13:00～14:00     「What&rsquo;s Fluentd? - The missing log collector」
<a href="http://www.slideshare.net/treasure-data/fluentd-meetup-in-japan-11410514">http://www.slideshare.net/treasure-data/fluentd-meetup-in-japan-11410514</a>
　Treasure Data, Inc.　古橋 貞之 (@frsyuki)</p>
<pre><code>
　まずは、Fluentdの概要。syslogdみたいなもの
　Fileのtailで流すことが基本。
　Clientライブラリから流すことも可能。
　他との比較
　　ScribeとFluentd
　　　構造化されたデータを扱う
　　　gemとかapt-get，yumでインストールあg可能
　　　Rubyで書かれてるからカスタマイズもプラグインの追加も簡単。
　　FluentdとFlume
　　　FlumeはZKやマスタノードが必要＝セットアップが面倒
　　　JVMだから面倒、設定も簡単。
　Fluentdのアーキテクチャ
　　Bufferプラグイン：スレッドセーフ、バッファ処理など
　　Outputプラグイン：出力だけ
　　out_forwardプラグイン：Cassandraと同じアルゴリズムでHeartBeatしてる
　　out_copyプラグイン：同じ出力を別の出力先にコピー
　　bof_fileプラグイン：バッファ先をファイルにする。Fluentdがこけても大丈夫
　　out_exec_filterプラグイン：外部プログラムからの入出力を使える
　　in_tailプラグイン：ログのパーサーが内蔵されてるみたい。
　開発者用API
　　Unitテストフレームワーク（MRUnitに似てる）
　　TailInput（パーサカスタマイズも簡単）
　ドキュメントとか
　　http://fluentd.org
　　にあります。
　会社紹介！
　　Treasure Data Service
　　Fluentdでは出力先がいろいろできるけど、自分で解析しないといけない
　　！ここで、Treasure Data Service（クラウド）に保存すると。。。
　　データのVisualizationとかがリアルタイムで出来る。


　QA：
　　Q：ログ転送にUDP使ってますか？
　　A：TCPです。UDPはハートビートに利用してます。
　　Q：文字コードはどういう扱い？
　　A：バイナリ形式であつかってます。
　　　　文字列で扱う場合はプラグインで対応しましょう。
　　Q：性能大丈夫？
　　A：Twitterで議論があがって、いろいろ対応しました。
　　Q：TreasureDataServiceはクラウド対応だけなの？
　　A：データはクラウド上で管理してるので上げても大丈夫なものだけが現在は対応。暗号化などは行なっている。
　　Q：TreasureDataServiceはどこで動いてる？
　　A：AWS上（S3にデータ保存）
　　Q：解析プラットフォームを提供している？SQLだけ？
　　A：HiveのSQLで解析可能。Visualizationのツールもある。
　　Q：コンサルタントもあるの？ログ出力系とか。
　　A：あとから解析できるように。
　　Q：in_tailでログローテート（iノードが変わって）も大丈夫？　　A：大丈夫。動かなかったら連絡ください。
　　Q：ファイルバッファリングされるデータの順序は保証される？
　　A：キュー形式なので、保証される。けど、S3とかだと出力先が遅いので、
　　　　パラレルモードで対応が可能。だけど、パラレルだと順序は保証されない。
　　Q：実際のシステムが投げるときにFluentd側で認証とかできる？
　　A：認証はないです。推奨は同一アプリサーバでFluentdを起動。
　　Q：Fluentd間のデータ通信のセキュリティ対応は？
　　A：今はない。プラグインとか作ればOK。
　　Q：中間層のFluentdを置くのは推奨？なしで直接ストレージ保存も可能？
　　A：柔軟な対応（送信先、バッファリングとか）が可能にできるように中間層を置くほうがいい。直接出すことも可能。
　　Q：TDAgentでのセットアップが推奨？gemは？
　　A：gemはOSS版でtrunkに近い。TDAgentはテストとかしっかりやってる。
　　　　バージョンの対応表はリリースノート見ないとわからない。
　　Q：Flumeのほうが優れているのは？
　　A：設定ファイルが一元管理ができる。大規模はFlumeのほうがいいかも
　　　　ただし、includeでHTTPサーバ経由で設定ファイルを取得可能な機能あり。
　　Q：バッファサイズは指定可能？
　　A：可能。
　　Q：ストレージがずっと停止している場合にログがあふれるのを防ぎたい場合の対処法は？
　　A：セカンダリという機能がある。（ただし、対応できてないプラグインもある）
　　Q：フェイルオーバーでローカルファイル出力とかはできる？
　　A：out_forwardは対応してる？
　　Q：Fluentdからのbuf_fileに出力してる部分で遅延、書き込み不可が発生したら？
　　A：入力と出力がスレッドで分かれてる。
　　　　入力スレッドでのエラーはinput_plugin側にException上がる
</code></pre><p>◎14:15～15:00     「Dive into Fluent Plugin」
　<a href="http://www.slideshare.net/repeatedly/fluentd-meetup-dive-into-fluent-plugin">http://www.slideshare.net/repeatedly/fluentd-meetup-dive-into-fluent-plugin</a>
　Preferred Infrastructure, Inc.　Masahiro Nakagawa (@repeatedly)</p>
<pre><code>
　好きなプラグインとか言いましょう（発表者さん）
　MongoDBのプラグインをベースにプラグイン開発の説明
　http://bit.ly/fluentd-with-mongo
　プラグイン名はfluent-plugin-xxxで統一されてます。
　fluent-plugin-mongoがダウンロード数が1位に！
　（scribeのプラグインがデイリーでテストが走ってダウンロード数が増えるという卑怯なカウント稼ぎしてたらしいｗ）
　
　fluentd.confの説明
　fluentdの起動は以下のとおり。開発中は-vつけるといいよ
　　fluentd -c fluentd.conf
　プラグインをどう作る？
　Fluentdの構成図による説明
　　Cool.io、MessagePack、Rubyなど。
　それぞれの説明を簡単に。
　Cool.ioがファイル監視とかをやってくれるみたい。
　プラグイン開発に関する詳細な説明
　　テスト、gem構成などを書いてくれてる。
　QA：
　　Q：複数行のログを扱うのはどこを作ればいい？tailプラグインの正規表現だとうまく行かなかった。
　　A：tailをoverrideしてparserLineを実装する。複数行のparserは難しいけどね
　　Q：ログのアグリゲーションをやってみたいのだが、どう？CPU利用率とかを1分間バッファしてからアグリゲーションして平均出すとか。
　　A；fluent-plugin-aggregateってのがあって、forward-pluginを書き換えて作ってる。
　　　　TimeSlicedOutputでやると時間の期間指定とかが可能。
　　管理画面つくろうと思ってるので手伝ってください！@repeatedly
　　Q：fluentd自体が処理した統計データの出力とかある？
　　A：古橋さんが頑張ります！協力して！=&gt;@tagomoris先生が書いてる！
</code></pre><p>◎15:15～16:00     「fluent を使った大規模ウェブサービスのロギング」
　<a href="
http://www.slideshare.net/hotchpotch/20120204fluent-logging"><a href="http://www.slideshare.net/hotchpotch/20120204fluent-logging">http://www.slideshare.net/hotchpotch/20120204fluent-logging</a></a>
　COOKPAD, Inc.　舘野 祐一 (id:secondlife, @hotchpotch)</p>
<pre><code>
　COOKPADでの基盤全般を見てる部署の人　PVログ＋MySQL
　　1日分はオンメモリにのるから高速。けど、1週間前とかになると遅くなる
　blackholeエンジン（MySQLの一部？）だと速い（ときどきおそくなる）
　データ保存でも問題
　　直近データしか保存できない。大きすぎて。
　　今後スケールしないときつくなる（1日分もメモリに載らなくなる可能性も出てくる）
　他のログはMySQLへJSONでシリアライズして出力とかもしてた。
　　アプリでDBにログ出力はinsertのコストが厳しい。PVとかなら、1リクエスト＝1insertだけど、1リクエストで多数ログ出力とかが厳しい。
　そこにFluentd登場！！！
　　出力先が色々使える。（MySQL以外もOK）
　パフォーマンス
　　バッファリング＋転送をやってくれるし、非同期で処理可能なのでレスポンスタイムへの影響が最小限にできてる。
　安定性
　　プラグインはちょっと注意が必要。
　　バッファリングしてくれてるので、一時的に停止もできるのがうれしい。
　　6時間止めてもOKだったよ（テヘッｗ）
　今後
　　PV系のログをMySQLから移行検討中
　具体例！！
　　td-agentをrpmで入れてるし、Rubyまで入ってくれる。
　　構成管理はpuppetを利用してる。
　　td-agentの設定の注意点
　　　「retry_limit 9」に変更してる。標準だと17なんだけど、2の16乗＝大体1日かかる。9だと大体10分くらい
　　中央転送用サーバ
　　　gitで設定ファイル管理してる。
　　　　よく変更するため。　　　gemfileで各種fluentd/pluginを利用してる。
　　　　自分たちで手を入れたソースを利用できるようにもしたいため。
　　テスト時にテストが書けるのが嬉しい（Rubyで出来てるアプリからもいじりやすいし、アプリのテストでも簡単に使える）
　　ロガーへの実装追加もできる。
　　Tips
　　　バッファからすぐ処理させたい場合とかの説明
　こんなのあったら嬉しいな。
　　設定ファイルの柔軟な設定記述方式
　　DSLにならないかなぁ（けど、やり過ぎも厳しいし。。。）
　ログの重要性
　　統計を考えられるエンジニアが重要。
　　どのデータにどんな価値があってそれを仕事（お金）に結び付けられるか？
　QA：
　　Q：障害対応のノウハウとか？
　　A：nagios、muninでサーバとか監視してる。
　　　　アラートチェックもしてる。
　　　　Fluentdの安定性は2ヶ月実際に導入してみて特に問題は出てない。
　　Q：MongoDBの構成は？
　　A：master/slave構成。PVログはS3に保存してEMRで処理。
　　Q：パフォーマンスの測定は？
　　A：40Mbit/sが処理できてたのは見てる。あとは、後ろの発表ですごいのがある！
　　Q：設定ミスの防止とかをどうやった？
　　A：今回はpuppetの設定後のリロードをチェックするような仕組みを入れた。
　　　　本来なら、データが流れてるかどうかのチェックを入れるプラグインを書いて欲しいｗ
　　Q：データマイニングエンジニアの定義は？
　　A：データの価値を見いだせるエンジニア。あとは、きちんと統計学を学んできてるエンジニア。両方募集中！！
　　Q：DynamoDBは検討は？
　　A：西海岸でしか動かないのでデータ転送の時間がかかってしまった＋RESTで１件ずつ登録なので今回は見送った。スキーマレスなのでうまく使えそう。
　　A２（古橋さん）：バルクインポートがないので使いにくい。
　　Q：ログ出力の形式をFluentdにあわせて変更したとかありますか？
　　A：もともとアプリで出力してたから特に変更なかった。
</code></pre><p>◎16:15～17:00　「fluentをサービスで使ってみた」
　<a href="http://www.slideshare.net/naverjapan/20120204fluent-public">http://www.slideshare.net/naverjapan/20120204fluent-public</a>
　NHN Japan株式会社　Tetsuya Ohira (@just_do_neet)</p>
<pre><code>
　自己紹介が面白かったｗ
　実例の話。
　事例１：
　　社内向けのログ解析に利用　
　　Fluentd＋Hadoop
　　データノードにFluentdで出力して、データノードからHDFSに書き込んでる。
　事例２：
　　NAVERまとめのまとめ作成者へのデータ解析
　　Fluentd＋Hadopo＋Azkaban
　　Azkaban（個人的にはオススメできないｗ）
　　flutendの負荷は全然上がってない。
　監視
　　tcp portの監視
　　転送されたデータサイズのチェック。HDFS側のデータ量でチェックしてる。
　　なぜか半死半生の状態になった（よくわからない。）
　プラグインを自分で直せるので対応が楽。（Ruby力は必要だけど。）
　demo
　　準リアルタイム解析基盤の構成案（おもろそう）
　　　fluentd、MongoDB、Hadoop、Jubatus、node.js
　　地図にアクセス解析結果をのっけて見せる。（maptail.js？）
　提案or相談：
　　動的に設定ファイルの内容を変更できるようにしたい
　　　log rotateのタイミングでshell実行でfluentd再起動
　　必要な情報だけrelayしたい
　　　フィルタープラグインみたいなものが書きたい。
　　fluentd自体のメトリクスを出して欲しい
　　jubatusプラグインはー？
　QA：　　Q：log rotateへの対応はどーしてる？
　　A：今はログローテートのタイミングでfluentd再起動してる
　　Q：Apps側のFluentdがコケたときにaccess_logをcopyしてるらしいけど、重複しないの？
　　A：失敗した対象のログをストレージから一旦廃棄して、再送してる。
　　Q：Rubyがそこまで得意じゃないのにFluentdを選んだ理由は？
　　A：他のモノが使いにくかった＋既存システムに手を入れなくて済む＋タイムリーだったという理由。
　　Q：構成案のデータの流れは？
　　A：MongoDB、Hadoopへは同一ログを流して、リアルタイムが重要な場合はMongoDB、大量データ回すのはHadoop？
</code></pre><p>◎17:15～18:00     「Distributed message stream processing on fluentd」
　<a href="http://www.slideshare.net/tagomoris/distributed-stream-processing-on-fluentd-fluentd">http://www.slideshare.net/tagomoris/distributed-stream-processing-on-fluentd-fluentd</a>
　NHN Japan株式会社 ウェブサービス本部　Tagomori Satoshi (@tagomoris)</p>
<pre><code>
　sed | grep | wc がメイン。
　Hiveで集計してます。
　なんでFluentd？（not Storm、Kafka or Flume?）
　　Rubyが好き（ライブドアはJavaが入ってない環境もあるし）
　　プラグインやTimeSlicedOutputがあるのも採用理由
　ログ収集の構成図
　　ブログに別の勉強会の資料があるので見てください。
　まだ１０日間しか動いてないので実績は少ない？
　　127サーバからのログを流してる。
　　7万行/sec、ピーク120Mbpsが流れてる。
　　89 fluentd インスタンスが12ノード（4コアHT）で動いてる。
　1行のログを1行のタブ区切りで出力する（Hiveでインポートしやすいから）
　Apacheのログに幾つか追加したデータなどが出力されてて、それをタブ区切りにするため設定一発では変換できない。
　TimeSlicedOutputが重要
　　時刻単位でログ出力できないとキツイ（ログの流量が半端ないから）
　　ログローテートでは時刻単位できちんと出力されてない。
　ログから解析までの流れ図
　　以前は生ログをHadoopに書き出してたので25分くらいのタイムラグが出てしまっていた。（ノード数が少ないという理由もあるけど、お金的に増やしたくない）＋Hadoopで他のジョブが実行されてるとさらに遅くなる。。。
　　fluentd でログを流しながらコンバートもかけられ、Hadoopでのデータコンバートの処理遅延が少なくなってハッピーに！
　ストリーミング処理の重要な点
　　バッチでもストリーミング処理でも同じことが可能。（out_exec_filterとHadoop Streaming）
　　SPOFがないこと
　トポロジ、Fluentdの構成　　
　　ログエージェント（scribeline：お手製）
　　　pythonで書かれたscribelineというものを利用してる。
　　　フェイルオーバーの処理（primary、secondary構成）が可能
　　　fluentdは入れてない。（ログを拾い上げて流すだけには重い）
　　workerノード
　　serializerノードが必要なのは出力先の競合を減らす必要があるため。

　　　出力先が大量のアクセスに向いてない場合があるから。
　　　あと、ログの流れを元に監視したい項目にもここで対応可能。
　　watcherノード
　　　deliver、serializerからのデータを見ながら監視、統計処理を行える。
　設定は自動生成してる。800行くらいになっちゃうから。
　　workerへの分散具合をよしなにしたいから。
　 remove_prefix 、add_prefixでデータの流れ先の交通整理してるのか。 
　GrowthForecastでグラフを出力してる。
　　https://github.com/kazeburo/GrowthForecast
　

　QA
　　Q：大規模環境での移行はどうやったの？
　　A：もともと流すような仕組みはあって、流し先を変えたので
　　　　それほど大変ではなかった。ただ、アプリ側で動いてた部分に手を入れたときは配備が大変だった。
　　Q：workerが1サーバ8の理由は？
　　A：4コアHT＝8というので8としている。
　　Q：deliverのカーネルパラメータをいじってたりしない？
　　A：ライブドア標準の設定がされてる。CentOSのデフォルトでもそれほどきびしくないかな？
　　Q：deliverノードでラウンドロビン＋out_forwardになってる理由は？out_forwardだけじゃだめ？
　　A：バッファのタイミングでの切り替えだと溢れてしまうので、最大でも1秒でラウンドロビンしてworkerを切り替える必要がある。
</code></pre><p>※懇親会設営の手伝いのためあんまり聞けず。
◎18:15～19:00(Plugin紹介LT)(各10分)　@shun0102: 「fluent-plugin-dstatを使ったリアルタイムクラスタモニタリング」
　<a href="http://www.slideshare.net/shun0102/fluent-plugindstat">http://www.slideshare.net/shun0102/fluent-plugindstat</a></p>
<pre><code>
</code></pre><p>　@ixixi: 「fluent-plugin-couch」
　<a href="http://www.slideshare.net/ixixi/fluent-plugins-for-couchdbamazon-sqssns">http://www.slideshare.net/ixixi/fluent-plugins-for-couchdbamazon-sqssns</a></p>
<pre><code>
</code></pre><p>　@chobi_e: 「Fluentd &amp; PHP」
　<a href="http://www.slideshare.net/shuheitanuma/fluentd-and-php">
<a href="http://www.slideshare.net/shuheitanuma/fluentd-and-php">http://www.slideshare.net/shuheitanuma/fluentd-and-php</a></a></p>
<pre><code>
</code></pre><p>　@railute:「Fluent Output Plugin for Cassandra」
　<a href="https://skydrive.live.com/view.aspx?cid=D105615A0F594518&amp;resid=D105615A0F594518%21333">https://skydrive.live.com/view.aspx?cid=D105615A0F594518&amp;resid=D105615A0F594518%21333</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>今興味があること(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/27/%E4%BB%8A%E8%88%88%E5%91%B3%E3%81%8C%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8/</link>
      <pubDate>Fri, 27 Jan 2012 01:50:41 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/27/%E4%BB%8A%E8%88%88%E5%91%B3%E3%81%8C%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8/</guid>
      <description>昨年末に今年の抱負について書いたのですが、 他にも興味あるものが増えたので備忘録＆公約？を兼ねてブログに書いておきます。 （興味あるものがあれば</description>
      <content:encoded><p><a href="http://johtani.jugem.jp/?eid=53">昨年末に今年の抱負について書いた</a>のですが、
他にも興味あるものが増えたので備忘録＆公約？を兼ねてブログに書いておきます。
（興味あるものがあれば、シェアしたりできると楽しいので、ツイート、コメント待ってます。）</p>
<ul>
<li>ZooKeeper</li>
<li>SolrCloud</li>
<li>SenseiDB</li>
<li>Lucene</li>
<li>検索システムのアジャイル開発？</li>
</ul>
<p>まずは、ZooKeeper。このなんとも言えない<a href="http://zookeeper.apache.org/images/zookeeper_small.gif">アイコン</a>でお馴染みのヤツです。
こんな顔してますが（顔関係ない。。。）、いろんな所に出没します。
Katta、Lilly、SolrCloudにも出てくるんです。SenseiDBにも出てくるんです。分散処理にことごとく絡んできます。
で、これは、知っとかないとまずそうだと。</p>
<p>SolrCloud。まぁ、妥当です。一応、Solrな人なんで。
昨年、NewSolrDesignを訳してみてもいます。訳しただけでほったらかし。。。
elasticsearchとかSenseiDBとかSolrに似たLuceneを利用している検索サーバがチラホラ話題になってます。
できれば、このあたりの分散の仕組みを比較しながら調べたいなと思ってみたり。
すべてをカバーするのはキツイので、SolrCloudかSenseiDBあたりを調べていきたいなぁと。</p>
<p>で、SenseiDBです。先日、Publickeyで<a href="http://www.publickey1.jp/blog/12/senseidblinkedin.html">記事</a>が出て飛びつきました。
Zookeeper使ってるし、分散だし、全文インデックスにLucene使ってるし。
なんだか興味あること、全部入りな感じです。</p>
<p>Luceneも興味津津です。
Solrがコアとして使ってますし、昨年、Solrと同時にリリースされるようになりました。
Luceneも奥が深く、時々、Solr調べててLuceneのソースも見るのですが、本格的に全体像を把握したりは残念ながらできていません。
そのくせ、Zookeeper同様、いろんな所に出没するんです、これが。
ということで、足元を固めるためにもLuceneに入門しないとなぁと。</p>
<p>で、最後に検索システムのアジャイル開発です。
lucene-gosenのコミッターをやらせてもらってたり、Solrに絡んだ仕事もしてますが、長期にわたってフィードバックをもらいながら検索システムを育てていくというのが最近興味が出てきてることです。
検索システムは作ったらおしまいとは行かないのが厄介なところです。
データは増加しますし、検索されるキーワードも変わってきます。
また、検索される方法も変わってきたりもします。
ユーザが検索したいものも変化があります。
思ったように検索にヒットしない場合とユーザが離れて行ってしまいますし、検索しにくい場合も同様です。
今日、クックパッドの方の話しを聞いて思い出した次第です。（前に、Solr勉強会でも同じ話しをされて、同じ思いに至ったのですが、いかんせん忘れやすくてｗ）
また、ログ解析の話などをTwitterで見てることもあり、検索という側面でのログ解析ってのも重要だよなぁと。
いくつか思いつくこともあるのですが、実践出来る場も限られてます。
色々と話しをする場を設けるのも面白いかもなぁと妄想してる次第です。</p>
<p>取り留めもなく、現時点で気になってることを書きだして見ました。
また、数ヶ月したら気が変わってるかもしれないですが、とりあえず書き残してみます。
興味がある方がいらっしゃったら声をかけてもらえるとうれしいですね。</p>
</content:encoded>
    </item>
    
    <item>
      <title>モーショノロジー2012#1に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/27/%E3%83%A2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%8E%E3%83%AD%E3%82%B8%E3%83%BC2012-1%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Fri, 27 Jan 2012 00:52:35 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/27/%E3%83%A2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%8E%E3%83%AD%E3%82%B8%E3%83%BC2012-1%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>ということで、いつものように勉強会に参加したメモです。 http://atnd.org/events/23608 日時 :2012/01/26 19:00 to 22:00 会場 :アカデミーヒルズ(六本木ヒルズ内) 49階（タワーホールA） （港区六</description>
      <content:encoded><p>ということで、いつものように勉強会に参加したメモです。</p>
<p><a href="http://atnd.org/events/23608">http://atnd.org/events/23608</a>
日時 :2012/01/26 19:00 to 22:00
会場 :アカデミーヒルズ(六本木ヒルズ内) 49階（タワーホールA） （港区六本木6-10-1）
ハッシュタグ :#mnlgy</p>
<pre><code>
■タイムテーブル
司会：CROOZLabs株式会社 研究所 所長 代表取締役社長　小俣 泰明

モーションノロジーの説明があったけど、他のことしてました。すんません。

1.
発表者: 有限会社未来検索ブラジル　森 大二郎
概要：groongaの索引構築の実装

　全文検索とは。歴史から
　　1971：full-text。。。というキーワードが。
　　現代の全文検索の課題はあんまり変わってない

　全文検索の手法
　　いくつかあるけど、転置索引の説明だけ。groongaも転置索引

　groongaはインデックスの動的構築が得意
　静的構築も考慮してます。

2.
発表者：CROOZ株式会社　長谷川 博紀
概要：MeCabからの脱出？(仮
　
　CROOZ MALLの解説
　MySQLとMeCabでやってます。
　Tritonnつかってるのか？
　groongaとの性能比較検証やってみました。
　MeCabが問題じゃなくて、辞書ありの形態素解析が問題。
　　辞書にない単語もうまく検索したい
　　辞書に単語登録したら形態素やり直し
　　ソート用のデータが。。。（ここはわからなかった）

　QA
　　Q：MeCabの代わりとなっているものは？
　　A：MeCabを改善してみる？

3.
発表者 : グリー株式会社　一井 崇
概要：全文検索のちょっとちがった使い方（仮）

　グリーでの数値指標管理
　　事前集計してKVSに登録してみてみる
　1億パターンのキーが存在する。。。
　　キーがわからなくなる問題とか事前集計できてる？とか迷子になる。
　キーも全文検索に入れてみては？
　　「富士山の標高は何メートル？」とかでデータが取れる。
　構造化されたキーをkey stringとして全文検索する。
　構造化Keyに対応したKVSがあればいいんだけど。
　データストアとしての全文検索もいいのでは。

4.
発表者：株式会社クリアコード　須藤 功平
概要：rroongaによる検索サービスの実装

　groongaとRubyの組み合わせでrroonga。Rubyによるインタフェース？
　groongaとのつながり部分をRubyで書いたライブラリ
　多段ドリルダウン機能
　　絞り込みによる検索結果の比較とかに利用可能
　メタデータ抽出にドリルダウンが便利
　　例：番組説明から出演者を抽出
　サジェスト機能もある
　　コンテンツベース
　　　すぐできる。候補の精度がいい
　　統計情報ベース
　　　元データがある程度必要
　　　候補が増えるけど、精度の問題あり

5.
発表者: 株式会社ぐるなび　塩畑 公一
概要: groongaの位置情報検索関連について。

　2010年4月からgroongaを利用し始めた
　　レストラン検索、地図検索など
　円形、矩形の緯度経度検索が可能。
　距離算出もやってる。ソートとか用かな。
　球面近似もやってる。
　geoの検索速度についてパフォーマンステストしました。
　Q：近似手法をいくつもサポートされてるけど必要？
　A：距離計算を正確にしないと不利になるお店が出てきちゃうから

6.
発表者: クックパッド株式会社 兼山 元太
概要: Solrを使ったレシピ検索のプロトタイピング

　クックパッドとは？
　　女性多いよ
　　AWS上でRubyで組んであるよ
　プロトタイピングって？
　　ユーザの問題を発見してから、プロトタイピング
　　本番デプロイ（一部ユーザに公開）
　　　問題を抱えているユーザーに向けてそれぞれ数十のバージョンが動いている
　　インタビューとかしてフィードバック受ける。
　本番の検索エンジンのフィールドをいっぱい追加
　Solrの紹介（MySQL（Tritonn）から移植しました）

　ダイナミックフィールドやレプリケーションの話。
　JSON型でHTTP経由で検索できるのが便利。
　Solr本の紹介して頂きました！あざっす！
　気になってるSolrの新機能
　　not to cache
　　SurroundQuery
　　BloomFilter
　Q：全部AWS？
　A：全部AWSに移行しました。

</code></pre><p>とまぁ、参加してきました。
初六本木ヒルズで、おしゃれな場所、雰囲気にやられてしまって、あんまりちゃんと聞けてないところがありますが。。。
メモを取りましたので。参考までに。
groongaについては勉強不足のため、いまいちついていけませんでした。
ちょっと英語の記事を読んでいたのもありまして。（内職良くない。。。）
MeCabに関する発表に興味があり、勝手にMeCabの代わりorMeCabよりいいものを使ってます！みたいな発表かと想像していました。（ちょっと違ったみたい、妄想は良くないですね。。。）</p>
<p>グリーの方の発表は興味深かったです。ログ解析とかにも確かに全文検索の技術使えるし。
検討が進んだ次の話も聞いてみたいです
groongaの位置情報の話やrroongaでのgroongaの色々な機能のはなしも出てきてgroongaの理解もちょっと進んだのはありがたかったです。</p>
<p>最後のクックパッドさんの話は、今回もわかりやすくて面白かったです。
検索を成長させていくという姿勢と、実際にどんな形でそれを実施されているかが楽しそうに話されててとても羨ましかったです。
次はSolrで困ってる点とか、検索のログとか利用され方をどのように解析してるのかといった話も聞きたいですね。</p>
<p>どうしてもSolrに思い入れがあるので、感想が偏ってるかもしれません、すみません。
気になる点やいや、こうなんだよ！ってツッコミがあれば、ぜひコメントやTweetしてもらえるとありがたいです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>MapR中身説明会に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/20/mapr%E4%B8%AD%E8%BA%AB%E8%AA%AC%E6%98%8E%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Fri, 20 Jan 2012 00:58:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/20/mapr%E4%B8%AD%E8%BA%AB%E8%AA%AC%E6%98%8E%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>日時：2012/01/19 18:30-20:30 場所：ベルサール三田Room2シアター Inside MapR デモ＋内部のお話。 ・自己紹介 Susheel Kaushik 元YahooのHadoop系の人。製品</description>
      <content:encoded><p>日時：2012/01/19 18:30-20:30
場所：ベルサール三田Room2シアター</p>
<pre><code>
Inside MapR
　デモ＋内部のお話。
　・自己紹介
　　Susheel Kaushik
　　　元YahooのHadoop系の人。製品担当責任者。
　　草薙さん

　3台のベアボーンでMapRが動いてるデモが開始。
　クラスタ管理画面の説明。
　なぜか、NFSのサービスが1台落ちてますがｗ
　MapReduceなんかの実行結果なども出てくるのか。
　ボリューム管理も画面から操作
　レプリケーション、スナップショットも管理画面で操作、動作確認できますよ。
　ミラー先はリードオンリーでボリュームの同期が可能。
　スナップショットによりMapRのクラスタ内部にバックアップが保持可能。
　NFSのHA構成可能。VIPの機能などもあるよ。
　事前定義された各種Alertの発行も可能。
　JobTrackerもMapRで拡張された表示がある。
　例：MapTaskPrefetchCapacity：次のジョブのMapperを起動する準備可能な仕組み
　MapR内で独自に出力してる測定値をGangliaで見ることができるよ。

　MapRはHadoopの置き換えとなる製品。
　HDFS部分を重点的に性能アップさせるために主に置き換えた製品。
　MapReduce部分にも手を入れてる。例：Direct Shuffle（HTTPじゃなくて、RPCでShuffleの通信を行ってるとか）ボリューム活用してるらしい。
　分散NameNode、JavaGCの影響の排除、ビルトイン圧縮によるI/O削減など。
　Mapperの出力をHDFSに置くと、メタデータ更新が多くなり、NameNodeがパンクする。（Apache Hadoop）
　Q：中間データもレプリケーションすると性能劣化しないんですか？
　A：中間データボリュームは特定ノードしか保持されない（＝レプリカ数は１）

　ストレージプール（SP）
　　ソフトウェアでストライピング。RAIDしなくてよい。
　コンテナ（データ、ネームがある。）
　　データブロックをグループ化したもの
　　※ストレージプールの数と同数のボリュームを作成すべき。
　CLDBがコンテナを管理してる。

　Q：トランザクション失敗するのは？
　A：どこかにかければトランザクションは成功。
　　　コンテナが復旧してきたら、データがコピーされる。復旧されない場合は別途コンテナを割り当てることもある？

　Q：ノードが追加される場合の挙動は？
　A：。。。聞き逃した。

　トポロジ
　　ノードを階層的にグループ化してデータ配置をコントロール。
　Q：トポロジ設定などの権限設定は？
　A：Permission画面があるよ。
　Q：ボリューム単位のファイルシステムアクセスに関する設定は？
　A：？？？聞き逃した？

　ボリューム
　　いろいろな設定が可能。
　スナップショット
　　Copy-on-Write方式による差分格納
　ミラー
　　ソースからミラーにコピー。手動orスケジュールによる起動が必要。
　　ミラー側はRead-only
　　※誤解を招きやすいので注意
　　読み出しが多い場合にミラーを利用することで対応が可能。
　ビルトイン圧縮
　　LZZFの一種を高速化してる
　　ネットワークIOにも効いてくる
　JobTracker HA
　　最大３ノードで構成可能。アクティブスタンバイ
　NFS HA
　　すべてのノードで稼働可能。
　NFS機能
　　NFSv3相当
　　クライアント側にNFSサービスをインストールするという構成も可能に。
　Q：NFSマウントして作成したファイルもブロックサイズ分のファイルサイズになるの？
　A：8Kバイト単位で内部的にはファイルを作成してる。8KB単位で圧縮して管理してるので、小さいサイズでもいい。（アロケーションサイズが8KB）
　Q：8KBにしてしまったために大きなブロックサイズの利点がなくなるのでは？
　A：オーバーヘッド内容な構成になってる。シーケンシャルに8KBに並んでるから？
　Q：NFSによるとMapReduceによるアクセスの排他制御とかは？
　A：独自で頑張らないといけない。Job起動時に効果的にスナップショット取ったりはしてない。

　リバランスもバックグラウンドで実行可能
　Apache Hadoopが備えるJava APIは100%語幹
　Q：なんで、HDFSをがりっと書き換えたの？
　A：運用性も、ノード管理も。。。全部です。
　　　なくなっても良いデータなら、別にHadoopでもいいですよね。
　　　けど、基幹システムとかだと、信頼性が必要だし、運用の効率も必要だしいろいろ必要。
　Q：実績が必要なんですが、どのような試験を行われているのかという情報が公開される？EMCでやられてるテストのプロセスを適用しているなどの裏付けは公開されないの？
　A：内部で6ヶ月利用してデータロスはない。
　　　品質については強化していく。
　Q：MapRとしてHadoopコミュニティへの還元していく内容ってどんなもの？
　A：Apacheコミュニティに対して1000台のクラスタを提供してスケーラビリティテストとかやってくださいとしている。
　Q：このクラスタを実際にはどう使ってもらうの？
　A：品質アップするためにテスト環境として使ってもらう？
　Q：ApacheのAPIの互換性を死守するのが必ずしもいいとは思えない場合にどうするの？MapR独自APIとかは出さないの？
　A：ApacheのAPIに準拠するのは非常に重要。他のHadoop上のアプリが動作しなくなるから。
　Q：MapRを容量の大きなファイルシステムとしてだけ利用するなんて想定はありますか？MapReduceを利用しないパターンです。
　A：いや、それはｗ
　Q：MapRはエンタープライズがターゲットだけど、Amazonはパブリッククラウドが対象。マルチテナントなパブリックサービスでMapRを利用するとかは？
　A：。。。
　Q：ジョブ管理にも手を入れてるの？
　A：あんまり手を入れてません。
　Q：EMCのストレージ製品でMapRのMapReduceない版みたいの出てない？
　A：中身はMapRじゃないですよ。
</code></pre><p>想定とは異なり、日本の草薙さんが主に説明されたのですごくわかりやすかったです。
しかもかなり内部まで理解されている方だったので突っ込んだ質問にもきちんと回答されてるので更に理解が進みました。
今回利用された資料は現時点では公開の予定はないという話でした。
ただ、かなりまとまってる資料なので、後悔して欲しいものです。
普通にviとかしてるだけなのに、すごいと思うデモってなんか新鮮でした。
MapR自体を触る機会はまだまだないと思うのですが、MapRとしてHadoopに対する思想が垣間見えたのが面白かったです。
すごいメンツが質問を投げまくるのでいろいろな側面で話が聞けました。
ただ、やっぱり英語のヒアリングがダメダメだというのが露呈しました。。。今年は少し頑張らないと、先が思いやられますね。。。
あと、疑問と言うか、感想ですが、MapR自体が結構多機能で、その機能をどう扱うか、どのようなノード構成やボリューム構成を取るかといった設計が結構大事でしかも大変なんじゃないかなぁという印象を受けました。
特にマルチテナントで利用する場合などは、想定されないミラーの利用などでデータ容量が足りなくなったりといった側面も出てくるのかなぁと。</p>
<p>説明会のあと、Hadoopにあんまり絡んでないのに図々しくも飲み会にまで参加してきました。
これまた濃いメンバーだったので話についていくのも大変でしたが、面白い話が聞けました。やっぱり懇親会重要ですよねぇ。</p>
<p>事前に<a href="http://d.hatena.ne.jp/nagixx/20111216/1324006829?PHPSESSID=9d59999a496d495f782647a94315862d">このブログ</a>を読んでいたので話しを聞くのが楽でした。
MapR勉強するために必須のページです。（どうやら今日説明した人がかいてるきがしますが。。。）</p>
<p>追記：
<a href="http://togetter.com/li/244244">トゥギャってくれた人に感謝。</a>
　</p>
</content:encoded>
    </item>
    
    <item>
      <title>OSSAJのミニセミナーで話しをしてきました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/19/ossaj%E3%81%AE%E3%83%9F%E3%83%8B%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC%E3%81%A7%E8%A9%B1%E3%81%97%E3%82%92%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Thu, 19 Jan 2012 11:46:24 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/19/ossaj%E3%81%AE%E3%83%9F%E3%83%8B%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC%E3%81%A7%E8%A9%B1%E3%81%97%E3%82%92%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>お久しぶりです。インフルエンザで一家全滅という最悪の状況に陥っていた我が家でした。 流行してるみたいなのでみなさんも気をつけてください。 さて、</description>
      <content:encoded><p>お久しぶりです。インフルエンザで一家全滅という最悪の状況に陥っていた我が家でした。
流行してるみたいなのでみなさんも気をつけてください。</p>
<p>さて、そんな中、<a href="http://www.ossaj.org/seminar/120118/ossaj_seminar_20120118_brochure.html">OSSAJのミニセミナー</a>でSolrについて簡単に話しをしてきました。
人生初Ustだったのですが、ぶっ倒れている中作成した資料だったためなんとも情けない発表だった気がします。（言い訳カッコ悪いですね。。。）
関係者の皆様、申し訳ございませんでした。</p>
<p>内容としては、Solrの機能を簡単に紹介する程度の資料となっています。デモなどもないのでスライドだけ見てもなんだこれという感じかもしれませんが。</p>
<p>ということで、恥ずかしながら、録画もされているようなので、後で見て反省しようと思います。。。</p>
<div style="width:425px" id="__ss_11146713"> <strong style="display:block;margin:12px 0 4px">[オープンソースソフトウェア検索サーバ Solr入門](http://www.slideshare.net/JunOhtani/solr-11146713)** <iframe src="http://www.slideshare.net/slideshow/embed_code/11146713" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> <div style="padding:5px 0 12px"> View more [presentations](http://www.slideshare.net/) from [Jun Ohtani](http://www.slideshare.net/JunOhtani) </div> </div>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのLucene/Solr4.0対応ブランチ更新(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/08/lucene-gosen%E3%81%AElucene-solr4-0%E5%AF%BE%E5%BF%9C%E3%83%96%E3%83%A9%E3%83%B3%E3%83%81%E6%9B%B4%E6%96%B0/</link>
      <pubDate>Sun, 08 Jan 2012 23:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/08/lucene-gosen%E3%81%AElucene-solr4-0%E5%AF%BE%E5%BF%9C%E3%83%96%E3%83%A9%E3%83%B3%E3%83%81%E6%9B%B4%E6%96%B0/</guid>
      <description>先日のSolr勉強会でLucene/Solr4.x系のlucene-gosenについて質問を受けていたのを忘れないように（年越しちゃいました</description>
      <content:encoded><p>先日のSolr勉強会でLucene/Solr4.x系のlucene-gosenについて質問を受けていたのを忘れないように（年越しちゃいました、すみません。）先週金曜日（1/6）に<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=22">issueに登録</a>しました。
まずは忘れないようにと思って、登録だけして3連休に突入したのですが、Robertさんが1/7に対応してくれました。
Lucene/Solr 4.x系では3.x系とはパッケージやメソッドが変更されるなど少し異なる部分があります。
lucene-gosenでは、プロジェクトのページにもあるとおり、4.x系にも対応しています。
ただ、<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2F4x">この4x系に対応したブランチ</a>が、2011年5月から放置されていました。</p>
<p>ということで、Lucene/Solr 4.0系でlucene-gosenを利用されている方、これから利用される方は、<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2F4x">この4x系に対応したブランチ</a>を利用してください。
なお、このブランチにはLucene/Solrのtrunk (r1228509)のSNAPSHOT版のjarファイルが利用されています。</p>
<p>今後はlucene-gosen側でバグ修正や機能追加を行った場合にも4xブランチを更新していく予定です。
※ただし、Lucene/Solr 4.0が正式リリースされていないため、頻繁にSNAPSHOTのjarファイルを入れ替えることはこなわないと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>あけましておめでとうございます＋会社のブログ更新(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/05/%E3%81%82%E3%81%91%E3%81%BE%E3%81%97%E3%81%A6%E3%81%8A%E3%82%81%E3%81%A7%E3%81%A8%E3%81%86%E3%81%94%E3%81%96%E3%81%84%E3%81%BE%E3%81%99%E4%BC%9A%E7%A4%BE%E3%81%AE%E3%83%96%E3%83%AD%E3%82%B0%E6%9B%B4%E6%96%B0/</link>
      <pubDate>Thu, 05 Jan 2012 12:56:57 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/05/%E3%81%82%E3%81%91%E3%81%BE%E3%81%97%E3%81%A6%E3%81%8A%E3%82%81%E3%81%A7%E3%81%A8%E3%81%86%E3%81%94%E3%81%96%E3%81%84%E3%81%BE%E3%81%99%E4%BC%9A%E7%A4%BE%E3%81%AE%E3%83%96%E3%83%AD%E3%82%B0%E6%9B%B4%E6%96%B0/</guid>
      <description>あけましておめでとうございます。（もう5日ですが。。。） 今年もlucene-gosenを中心に色々と記事を書いていきますので、ツッコミ、コメ</description>
      <content:encoded><p>あけましておめでとうございます。（もう5日ですが。。。）
今年もlucene-gosenを中心に色々と記事を書いていきますので、ツッコミ、コメント待ってます。</p>
<p>たまには会社のブログを書かないとイカンということで、会社のブログに<a href="http://blog.seamark.co.jp/?p=305?PHPSESSID=1bdf737d8906533c6f92bfbddc12b17a">NearRealTime検索についての記事</a>をアップしました。
読んでやってください。</p>
<p>ということで、今年もいろんな勉強会に出没しますので、よろしくお願い致します。</p>
</content:encoded>
    </item>
    
    <item>
      <title>今年の振り返りと来年の抱負？(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/28/%E4%BB%8A%E5%B9%B4%E3%81%AE%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A%E3%81%A8%E6%9D%A5%E5%B9%B4%E3%81%AE%E6%8A%B1%E8%B2%A0/</link>
      <pubDate>Wed, 28 Dec 2011 02:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/28/%E4%BB%8A%E5%B9%B4%E3%81%AE%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A%E3%81%A8%E6%9D%A5%E5%B9%B4%E3%81%AE%E6%8A%B1%E8%B2%A0/</guid>
      <description>他の方たちよりひと足はやいですが、今年の仕事が終わりました。 せっかくブログを始めたので、振り返りと来年の抱負など書いてみようかなと。 今年の振</description>
      <content:encoded><p>他の方たちよりひと足はやいですが、今年の仕事が終わりました。
せっかくブログを始めたので、振り返りと来年の抱負など書いてみようかなと。</p>
<p><strong><em>今年の振り返り</em></strong>
まずは、今年1年を振り返ってみます。
今年の出来事はこんな感じでした。</p>
<ul>
<li>3月に仕事が一区切り</li>
<li>lucene-gosenに参加</li>
<li>ブログを始める</li>
<li>色々な勉強会に参加</li>
<li>Twitterでつぶやきまくり</li>
<li>Mac Book Airの購入</li>
<li>PSVita買いました</li>
</ul>
<p>3月に一昨年から参加していた仕事が一区切りつきました。
もう少し色々とやり残した感がある中で区切りがついたのが少し心残りでした。
あと、3月は震災もあり、色々と大変でした。ただ、自分の中では震災の記憶が少し薄れつつあるのが気がかりです。
良くも悪くも忘れて行ってしまうのだなと。今でも頑張っている方々もいるので、少しでも心に留めておきたいものです。</p>
<p>個人的に一番大きな今年の変化は「lucene-gosenへの参加」です。
関口さんとここ数年ずっと仕事をご一緒させていただいていることもあり、コミッターとして参加させてもらうことが出来ました。
英語が苦手な中、Robertさんへ自己紹介のメールを書いたり、Issueを英語で書いたりと苦労もしていますが、OSSに参加できることがとても楽しいです。
また、使っていただいている方たちからのフィードバックもいただけているのがすごく励みになります。
やはり、OSSは使ってもらってさらに良いものになっていくのかなぁという感想です。
ただ、自分の腰が重いため、なかなか対応が遅かったりといったところもあるなと反省するところも。</p>
<p>勉強会にも色々と参加しました。Solr勉強会を始めとして、かねてより興味があるHadoopのカンファレンスやソースコードリーディング。MongoDBの勉強会やDSIRNLPといった少し研究よりの勉強会にも参加しました。
Solr勉強会に参加してきた経験もあり、他の勉強会にも気負わずに参加できたので本当にSolr勉強会には感謝しています。
おかげで、社外のいろいろな方と話ができ、様々な刺激をうけることが出来ました。</p>
<p>勉強会以外にもTwitterで色々な方に絡みまくったのもまた社外の方とのつながりが出来るいい機会でした。
Twitter依存症じゃないかというくらい、つぶやいたり、人に絡んだりしてます。。。（迷惑だったら言ってください。）
お陰で@doryokujinさんや@wyukawaさんが主催してくれたログ解析の飲み会に誘ってもらえたり、@nobu_kさんや@
IjokarumawakさんにSolr勉強会で講演してもらえたりしました。
あと、Twitterを通じて勉強会やHadoop関連の情報、lucene-gosenへのフィードバックなどももらえていてかなり助かってます。
@yusukeyさんや@repeatedlyさんに遊んでもらってるのも楽しいですｗ</p>
<p>最後の大きな変化はApple製品の購入でした。
これまでもブログで書いてきましたが、私は今年の中頃までアンチApple派でした。
それがここ半年足らずでAir、Mini、iPadと3つのApple製品に囲まれています。
Emacsのショートカットが使えるのが最大の要因ですが、何ごとも毛嫌いせずためしてみるのがいいなと思いましたね。
今では、Windowsで開発したくないくらいになってきています。</p>
<p><strong><em>2012年の抱負</em></strong>
さて、来年の抱負です。
今年いくつかチャレンジしようとして挫折してるものがあります。</p>
<ul>
<li>MongoDB触ってみる</li>
<li>Scala触ってみる</li>
<li>なんかWebサービス作ってみる（Amazonとかで）</li>
<li>elasticsearch、IndexTankを調べてみる</li>
<li>NewSolrCloudDesignを読み込んで自分なりにまとめる</li>
<li>Junsaiに参加する</li>
<li>「7つの言語7つの世界」の続き</li>
</ul>
<p>最初の3つはセットですかね。コップ本買ったり、NoSQLの本を買ったりしてるのですが、手付かずです。。。
電子書籍の検索サービスでもつくってみようと思いつつ、腰が重い状態です。
来年は余裕を見ながらちょっとずつやりたいですね。</p>
<p>elasticsearchやIndexTankは仕事がらみです。
Solr入門を書いたこともあり、検索関連には興味があります。Solrだけでも2ヶ月おきに新バージョンがリリースされたり、4.0系でいろんな機能が実装されているのですが、それ以外にもOSSの検索サーバがいくつかあります。
気になってはいるものの英語が障壁でなかなか手を出して来ませんでした。
来年はSolr以外についてももう少し知識をつけていきたいです。
個人的ですが、分散処理にも少々興味を持っています。
慣れ親しんでいるSolrでもSolrCloudという壮大な計画が上がっていて、少しずつ実装もされています。
ブログでWikiページを翻訳してもみたのですが、翻訳に注力しすぎて理解ができていないです。
このあたりをもう少し理解しつつどのような実装がされているかも調べたいところです。</p>
<p>JunsaiはSolr入門の著者である武田さんが最新のMeCab（今はMeCabが新しくなってしまったので、少し前のMeCabになってしまいました）をJavaに移植したプロジェクトです。
今年頭のSolr勉強会でお披露目があり、私も一応参画しているのですが、手が出せていない状態です。
lucene-gosenで少しは経験を積んできたので、その部分を生かしつつ、こちらももっと活性化させたいところです。
最後は夏ごろにやっていた「7つの言語7つの世界」の再開です。序盤で止まってしまっています。。。
新しい技術に触れる題材としてちょうどよいので、余裕をつくって続きを再開します。
なんか、抱負と言うよりも積み残しですね。。。
他にもTwitterで情報を仕入れるとやりたいことも変わってくるかもしれませんが、それも含めてブログに少しずつでも書きためていこうと思います。</p>
<p>先ほども書きましたが、今年はTwitterを通じて色々な方たちと知り合いになれた一年でした。
ここには書ききれていませんが、フォローしてもらっている方々や勝手に絡んでも相手をしてくれている方々に感謝しています。
今後も絡みますのでよろしくお願いしますｗ
あと、様々な勉強会に今後も顔を出していくので、かまってやってください。
いやぁ、色々書いたけど恥ずかしいですね。。。
まぁ、何事も経験なので、恥ずかしいと思いつつ今後も色々書いていきます。</p>
</content:encoded>
    </item>
    
    <item>
      <title>第2.1回 Twitter API 勉強会 @東京に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/22/%E7%AC%AC2-1%E5%9B%9E-twitter-api-%E5%8B%89%E5%BC%B7%E4%BC%9A-%E6%9D%B1%E4%BA%AC%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Thu, 22 Dec 2011 01:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/22/%E7%AC%AC2-1%E5%9B%9E-twitter-api-%E5%8B%89%E5%BC%B7%E4%BC%9A-%E6%9D%B1%E4%BA%AC%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>@yusukeyさんにサインをもらう目的で勉強会に参加してきました。前回もらいそびれたのでｗ 残念ながら、まだTwitter APIを触ってない</description>
      <content:encoded><p>@yusukeyさんにサインをもらう目的で勉強会に参加してきました。前回もらいそびれたのでｗ
残念ながら、まだTwitter APIを触ってないし、利用したサービスも思いついてないんですが。。。
けど、勉強になりました。
といことで、いつものごとく、自分メモです。</p>
<p>◎APIの基本と最新動向について @yusukey
　資料：<a href="http://www.slideshare.net/yusukey/21twitter-api-api?PHPSESSID=e79359141a811977c01f64da4592dc44">http://www.slideshare.net/yusukey/21twitter-api-api</a></p>
<pre><code>
　・検索APIについて
　　http://search.twitter.com/search.json?q=***
　　%3Aは「:」
　　詳しくはリファレンスを買ってね。

　・ページ処理のベストプラクティス
　　Pageじゃなくて、MaxIdを利用して取得すると１５００件以上のデータも取れるよ。
　・最近のTwitterAPIアップデートについて
　　- ポケリファでの変更点
　　　ストリーミングのエンドポイントがSSLのみに。（twitter4jは2.2.5以上）

　　- ユーザ名が検索APIのスキーマに追加（１２月以降）
　　- in_reply_toの追加（１２月以降）
　　　=&gt;検索結果から会話を追える。
　・include_entitiesが追加
　　t.coの展開したリンクも取得可能。
　　けど、t.coリンクの飛び先はt.coリンクにしてね。パトロールとかしてるから。

　・本当にあった怖いt.co
　　- URLのつもりじゃないのにリンクになる。
　　→仕様です
　　- 日本語を含むURLがおかしくなる。
　　→少しずつ治ってきてる。
　　- リンク先のURLがわからない（クライアント依存）
　そのたいろいろ？
</code></pre><p>◎Zusaarについて何か @knj77
　資料：</p>
<pre><code>
　・特徴
　　- SNSアカウントでログイン
　　　トラブル抑制など
　　- 事前決済可能。最小催行人数が決められて、払い戻しも可能。
　・利用例
　　旅行とか合宿に使う例がないんだけど、できるんじゃないかな？
　・イベントを依頼する機能
　　欲しいイベントを提案可能
　　http://www.zusaar.com/idea/
　・アーキテクチャ
　　GAE/Java、Twitter4j、Yahoo APIのテキスト解析

　・SNS連携
　　TwitterのDMが送れない。他のSNSのアカウントが主催者とか。
　　なので、自分からDMが届く。

　・PayPal連携
　　小さいアプリほど相性がいい
　　固定費ゼロ、マイクロペイメント（１００円とか可能）
　　銀行口座の登録不要、返金可能。部分返金も可能。

　・トラブル
　　- SNS側のエラーが適当
　　　ステータスコードは信用できないｗ
　　- PayPalの実装を３回変更
　　- GAE料金体系の変更
　　　9$/month。追加料金はほとんど使ってない。Googleのアナウンスが下手。
　　- ブルーオーシャンのはずが...

　Q：Zusaarで返金できるポイントは？
　A：主催者が主導（手動？）で返金可能。
　Q：無料イベントで約束手形的に課金するのはできる？
　A：やろうと思えば出来ます。ただ、システム的には１００円は手数料としてもらっている。
　Q：新しいATNDに対抗意識はありますか？
　A：ZusaarはCtoCをターゲットにしてる。ATNDはBtoCなので、補完できる関係だと思ってる。
　Q：APIでTwitterIDとかで検索したいんですが。
　A：対応予定あります。
　Q：できればATNDのAPIと共通化してほしい。
　A：追加してるだけのつもりです。
</code></pre><p>◎グループに分かれて自己紹介
　6人中、懇親会参加者が5人もいたｗ</p>
<p>◎LT
　- OAuth Echoについて@tkawa
　　資料：<a href="http://www.slideshare.net/tkawa1/oauth-echo-20111221-10657954">http://www.slideshare.net/tkawa1/oauth-echo-20111221-10657954</a></p>
<pre><code>
　　　twitpicで使ってる。
　　　認証をServiceProviderに委譲する仕組み。
　　　クライアントがOAuth登録してあれば、。。。
　　・問題点
　　　非公式仕様
　　　OAuth1.0に基づいてて2.0では変わってそう。
　　・Railsで実装してみたよ。
</code></pre><p>　- GroovyとQuartzとTwitter4Jの甘い生活G @mike_neck
　　資料：</p>
<pre><code>
　　ベトナムから緊急来日らしい。
　　しかもむちゃぶりされたらしい。
</code></pre><p>　- 認証なしで使えるAPIまとめ？@ts_3156
　　<a href="http://www.egotter.com/">えごったー</a>の作者で学生！</p>
<p>　- TDD (Twitter4J Driven Development) @sue445
　　資料：</p>
<pre><code>
　　AZusaarの作者！
　　テストドリブンなはなし。
　　Twitter4Jを利用するときのTDDの簡単に実装できる仕組みの話。
　　ここにソースがあるのかな？
　　http:/github.com/sue445/twtrhack/
</code></pre><pre><code>
　- 放射線を自動計測してTwitterにつぶやくimaocandeの紹介 @imaoca
　　松山の方。
</code></pre><p>とまぁ、APIの話だけでなく、実際にAPIを利用している方のサービスの話など色々と面白い話が聞けました。
厚かましくも懇親会にも参加して、そこでも今までのイベントとは違うクラスタの方たちの話が聞けたのが楽しかったです。
やはり、APIを利用して色々とやってみようと思っている方が多かったのでいつもと違う話が聞けたんですかね。
私自身はサービスを思いついたりしないタイプなので、いい刺激になりました。（まだ、特になにか作る気にはなってないですがｗ）
ということで、次回も余力があれば参加したいなー。
　　</p>
</content:encoded>
    </item>
    
    <item>
      <title>1.2.1リリースしました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/21/1-2-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 21 Dec 2011 12:15:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/21/1-2-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>lucene-gosenの最新版（1.2.1）をリリースしました。 プロジェクトページよりダウンロードが可能です。 今回の修正では、特定文字列で</description>
      <content:encoded><p>lucene-gosenの最新版（1.2.1）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>今回の修正では、特定文字列でメモリの使用量が爆発してしまうバグへの対処となっています。
1.2.1以前のバージョンを利用している場合は最新版を利用するようにしてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Solr勉強会第７回に参加しました。（発表もしました）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/20/solr%E5%8B%89%E5%BC%B7%E4%BC%9A%E7%AC%AC%EF%BC%97%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F%E7%99%BA%E8%A1%A8%E3%82%82%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 20 Dec 2011 01:21:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/20/solr%E5%8B%89%E5%BC%B7%E4%BC%9A%E7%AC%AC%EF%BC%97%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F%E7%99%BA%E8%A1%A8%E3%82%82%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>いつものようにSolr勉強会に参加してきました。 皆勤賞を継続中です。（暇人というはなしも。。。） 今回は話しを聞きたいですねぇといったら、いや</description>
      <content:encoded><p>いつものようにSolr勉強会に参加してきました。
皆勤賞を継続中です。（暇人というはなしも。。。）
今回は話しを聞きたいですねぇといったら、いやいや、話もしてくださいと言われてしまったので、
発表もしてきました。
発表資料はブログの最後に掲載してあります。</p>
<p>日時：2011/12/19(月) 19:00～21:00
場所：Voyage Group 8階</p>
<p>1.Fessについて N2SM菅谷さん
　資料：<a href="http://www.slideshare.net/shinsuke/solr-fess">http://www.slideshare.net/shinsuke/solr-fess</a></p>
<pre><code>
　マルチコアで構成されてる。
　S2Robotでクロールしてますよ。
　※ごめんなさい、あまり聞けなかった。。。あとで資料を読んで質問します！
2.lucene-gosenについて @johtani
　発表しました。
</code></pre><p>3.ApacheConに参加しました @Ijokarumawak
　資料：<a href="http://www.slideshare.net/KojiKawamura/apache-con-2011report">http://www.slideshare.net/KojiKawamura/apache-con-2011report</a></p>
<pre><code>
　日本-&gt;カナダ-&gt;ベルリン-&gt;カナダ-&gt;日本
　50万円。。。
　キーノート１：
　　セキュアな開発について。
　キーノート２：
　　Hortonworksのコミュニティ運営大変だよねぇ、頑張るよという話。
　キーノート３：
　本題：Lucene 4.0の話：Simonさん
　　ヨーロッパの方？
　　PostingsFormatの話。
　　Document内部の情報を使うためにどうする？
　　　StoredField：あんまり効率よくないね。
　　　FieldCache（on RAM）：インデックス作る動作の逆を行う。これも効率よくないね。
　　　IndexDocValue：インデックス作成時に作られるので読み込み性能が１００倍！（DocValue）
　　Document Writer Per Thread！
　　Automaton Query
　　　あいまい検索の処理に利用。
　Solr Flair
　　Solr同梱
　Prism
　　LucidImaginationが出してるJRubyのラッパー
　　GitHubで公開されてるらしい。
　Blacklight
　　RoR
　　図書館むけのパッケージじゃなかったっけ？
　VUFind
　　PHP
　　これも図書館向け？
　TwigKit
　　JSPのタグリブ
　　おぉ。これ直接読んでるのかな？
　Ajax Solr
　　Ajax用

　QA
　　Q：TwigKit、Ajax Solrは直接Solrを呼んでるんですかね？
　　A：たぶん、そうですね。JSPのはSolrJかもしれないですが。
</code></pre><p>4.サフィックスアレイの話 @nobu_k
資料：<a href="http://www.slideshare.net/nobu_k/suffix-arraysolr">http://www.slideshare.net/nobu_k/suffix-arraysolr</a></p>
<pre><code>
　Suffix Arrayの話
　全文検索インデックスの話。
　Suffix Array＝検索漏れがない。
　Suffix＝接尾辞
　RedBullがどの文書に入っているか！
　SuffixArrayのメリット
　　検索漏れがない＝n-gramと同様
　　仕組み上n-gramよりも早くなるケースが多い。
　　長いクエリに対して速い
　　　THIS IS IT：全部ストップワード
　SuffixArrayのデメリット
　　インデックス構築系
　　　アルゴリズムが難しい。
　　メモリ上での構築はちょっとだけ楽（けど、簡単ではない）
　　　SAISなど。けど、これだとメモリがいっぱいないとキツイ
　　HDDでの構築
　　　ランダムアクセスを排除したアルゴリズムが必要（dc3,dc7）
　　インデックス更新、差分更新できない。
　　　頑張って１台100GB/day
　　Sedueでは？
　　　SA＆インメモリn-gramのハイブリッド
　　　更新分はn-gramに
　　　検索時にはn-gram＋SAの内容をマージして出力
　　検索
　　　二分探索はHDDとは相性が悪い。
　　　　メモリ上で検索できればOKだけど、サイズが大きいからきびしい。
　　　　圧縮接尾辞配列なら可能だけど、低速。。。
　　　SSDだとはやいよ！
　　　　SSD対応のクラウドはまだないけど。。。
　　　Sedueでは最初の２０段でキャッシュしてる。
　VSストップワード
　　ストップワード込でインデックス作るみたい。
　　1.SAを二分探索
　　2.該当区間から出現位置をロード
　　3.出現位置をソート
　　　O(n)だけど、CPUのキャッシュミスが激しく影響
　　4.ソートした出現位置からヒット文書を求める。
　　　同じくキャッシュミスがやばい
　　・実際にはmallocした領域のページフォルトが一番やばい
　SAが超活躍する場面
　　遺伝子の検索
　　　n-gramとか死ぬ。形態素解析も無理。区切りがわからんｗ
</code></pre><p>　　
5.P2P検索 ORBIS @ceeflyer
資料：<a href="http://www.slideshare.net/ceeflyer/p2p-search-engine-orbis">http://www.slideshare.net/ceeflyer/p2p-search-engine-orbis</a></p>
<pre><code>
　ORBISとは？ラテン語で「目」
　ORBISとは？
　　リアルタイム検索エンジン
　　自律分散検索エンジン
　もともとはAmebaなうの検索のため
　ノード構成
　　比較的小規模（～１０００台）&lt;=しょ、小規模ですか。すごいな。
　　Master-Slaveの差がなし
　　MessagePack利用
　　フルメッシュネットワークを構築
　インデックス
　　フィールド：
　　　Content（形態素解析対象）
　　　Appendix（形態素解析しない）
　　　Flag（属性）
　　ハッシュレプリケーションで登録。近いハッシュ値に登録（レプリカ数を指定できるのかな？）
　単語に対して転置インデックスを一定数で固定
　　投稿日時が新しいものだけ検索するため。
　検索：
　　マージして結果を返す。
　　壊れてたら取れたものだけ返す。

　QA：
　　Q：ハッシュ値が大きくて１台しか選ばれないとかあるのでは？
　　A：ハッシュ値は循環しているという形で３台とか選ぶ。
　　Q：最大何台で稼働させてる？
　　A：現在はまだ５台程度
　　Q：障害時にレプリケーションの維持はするのか？
　　A：現時点はしてない。
　　Q；Cassandra、Lucendraとかあるけど、それをしなかった理由は？
　　A：単純にConfigurationができるから、１から作った。
</code></pre><p>今回はSolr以外の話も聞けたのがとても面白かったです。
やはり、Solr以外の検索エンジンについても知見があると、色々と比較の話しとかしやすいので。
それにしても、一からP2Pの検索エンジンを作っているのにはびっくりしました。
他にもSuffixArray（Sedue）の話も気になっていたのが少し氷解したし、海外旅行のノウハウがApacheConの話で聞けたしｗ
やはり、発表をすると話しをしてもらえるのもあって、いい機会だなと再認識しました。
今回も紹介ネタだったので、ちゃんと事例とか測定したものも発表できるようにならねばと。。。</p>
<p>ということで、私の発表資料はこちらになります。疑問点とか質問事項とか、指摘事項など、コメントorTwitterで連絡いただけるとすごく嬉しいです。</p>
<div style="width:425px" id="__ss_10635521"> <strong style="display:block;margin:12px 0 4px">[Lucene gosenの紹介 solr勉強会第7回](http://www.slideshare.net/JunOhtani/lucene-gosen-solr7)** <iframe src="http://www.slideshare.net/slideshow/embed_code/10635521" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> <div style="padding:5px 0 12px"> View more [presentations](http://www.slideshare.net/) from [Jun Ohtani](http://www.slideshare.net/JunOhtani) </div> </div>
</content:encoded>
    </item>
    
    <item>
      <title>第2回 データ構造と情報検索と言語処理勉強会に参加しました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/11/%E7%AC%AC2%E5%9B%9E-%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0%E3%81%A8%E6%83%85%E5%A0%B1%E6%A4%9C%E7%B4%A2%E3%81%A8%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Sun, 11 Dec 2011 00:04:35 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/11/%E7%AC%AC2%E5%9B%9E-%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0%E3%81%A8%E6%83%85%E5%A0%B1%E6%A4%9C%E7%B4%A2%E3%81%A8%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>ということで、懲りずに#DSIRNLPに参加してきました。 基礎的な部分をおろそかにしたくないけど、TokyoNLPとかWebMiningでは</description>
      <content:encoded><p>ということで、懲りずに<a href="http://partake.in/events/92c72f87-6717-43ee-a358-413d63badf57">#DSIRNLP</a>に参加してきました。
基礎的な部分をおろそかにしたくないけど、TokyoNLPとかWebMiningではついていけないので。。。
今回もしがみつくのが精一杯かもしれないと思いつつ、聞いて来ました。</p>
<pre><code>
0. DSIRNLPについて　@overlast
　講演者が可能なら実装に関して言及する。
　「こうやって作って結果を出す」系の発表をしましょう。
　開催に際して協力してくださったみなさんへの感謝の辞。

1.自然言語処理はじめました　@phyllo
　資料：http://www.slideshare.net/phyllo/ngram-10539181
　ブログ：http://d.hatena.ne.jp/jetbead/20111210/1323499634
　某Web企業の新入社員！（すごい）
　で、入って自然言語処理をはじめたらしい。
　N-Gramの説明

　Perl？
　　1.ナイーブな方法
　　　Perlでハッシュ使って数えてみたよ。これでOK？
　　　いやいや、Nが大きくなったりデータが大きくなるとキツイでしょ
　　2.SuffixArrayを使う
　　　ということで、Suffix Arrayを使ってみた。
　　　SuffixArrayのデータを作ってから、上からN文字数えれば数えられる！
　　　（数式出てきたけど目も取れなかったｗ）
　　　これでいいじゃん？いやいや、更に大規模だとどーすんの。1TBのメモリ
　　3.近似カウント法
　　　いくつかある中からLossy Countingを説明します。
　　　ストリームデータから数えてみよう。
　　　C++の実装？
　　4.分散処理を使う方法
　　　近似じゃダメ！正確な頻度が欲しい！
　　　＝＞Hadoopつかえ！
　色々やってみた感想が良かった。

　SuffixArrayのメモリ使用量という話がありましたが、ディスクを使う方法もあります。 by ?

　新人から自然言語やり始めたというが、自分が新人の頃にはこんなに出来なかった気がするなぁ。
　発表とかもってのほかだったし。

2.AI で AI 創ってみた（LT）　@uchumik
　資料：
　Luaプログラミングが出てきたので、
　多クラスロジスティック回帰とListNetを書いて見ました。（ただし、本は読んでないよ。）
　学習器のお話。
　ラグナロクオンラインのAIがLuaでかけることを思い出した。
　動画の途中で終了ｗ
　GitHubにあげなさい。ブログ書きなさい。動画もあげなさい。 by @overlast

　資料の割に時間がｗ　

2.5.CMコーナー
　技評の方からWEB+DBのCM！（ただし、商品なし）
　読者層の95%が男性ｗ
　総集編買ってね！WEB+DB plusシリーズも買ってね！
　書いてor買ってね。

　技評さんつぎはオライリーさんに負けないように賞品持ってきてねｗ

3.Mahout にパッチを送ってみた　@issay
　資料：http://www.slideshare.net/issaymk2/mahout-10539755
　人がやめていく会社で検索やHadoopやってます。
　※今日は皆既月食です。
　機械学習関連
　SGDはMapReduceでの分散に対応していないので、パッチを送ってみた。
　MapReduce、Shuffleなどの説明
　これをベースにナイーブベイズの説明をあわせて実装を見てみる。
　　・単語ｗのカテゴリｃにおける確率
　　・文書ｄのカテゴリｃにおける確率
　　　-文書ｄに出てくる単語ｗの確率の積と定義
　　単語の相関を見ない。
　　ゼロ頻度問題の解説
　　文書頻度の逆数（IDF）
　　カテゴリ毎の文書数のばらつきに対応するために、Complement Naive Bayes
　　BayesFeatureDriver
　次はランダムフォレスト
　　決定木とは？
　　ランダムフォレストとは？
　　ここでは素性。
　　Mahoutでの実装は？
　　　決定木の数だけ処理分散が可能。
　　　Reducerは使ってない。＝Mapperで計算が終わっちゃうから？
　次はロジスティクス回帰
　　尤度関数、損失関数、確率的勾配降下法（SGD）
　　先程まではオンライン。バッチ学習もある。
　　Mahoutでの実装はオンラインで、MapReduceに向いてない！
　　Adaptive Logistic Regrettion（アダプティブロジスティクス回帰）というのも実装されてる。
　送ったパッチの概要
　　Iterative Parameter Mixisngというのがあるので、それをSGDに適用するパッチ
　今後
　　MapReduce以外も増えるかも。
　
　Q:パッチがはいったバージョンはいつでる？A:3日前に送ったのでまだです
　Q:ナイーブベイズのMapReduceが一段ムダでは？（文書数のカウントは同時に出来るよね）A:そうですね、ぜひパッチを送ってみてください。
　Q:Mahout、Hadoopの未来は明るいんでしょうか？A:HDFSのデータをMapReduce以外のフレームワークで使えるといい。
　Q:Hadoopのパッチ送るの大変ですか？どのくらいかかりました？A:4,5ヶ月かかりました。

　すみません、途中から頭がパンクしてしまいました。。。
　ただ、MahoutがHadoopに乗っかってるからといって、MapReduce活用できてるわけじゃないという話がわかりました。


4.GraphDBのデータ構造　@kimuras
　資料：http://www.slideshare.net/skimura/graphdatabase-data-structure
　LTみたいな資料にになっちゃいました。すみません。
　mixiではMySQLだけど、いろんなDB触ってます。
　なので、GraphDBに興味持ちました。
　Luceneを教科書的に読んでます。
　Neo4jモダンな作りでよかった。　　http://www.slideshare.net/skimura/ss-8787632
　ノードデータの構造
　　NodeManagerというのがいて、Cache系を管理してる。
　　プロパティのインデックス、トランザクションManagerもいて、PersistenceManager
　　ここからNodeにアクセス（探索、）。
　キャッシュとかいいね。
　Soft LRU Cacheってのもあるよ。
　その他色々ｗ

　Q:トランザクション周りでLRUのキャッシュはどうなるんでしょうか@kumagi A:きちんとロックしてました。
　Q:分散はネットワーク、HDDどちらがキツイ？A:HDDがかなりきつい
　Q:プロパティもキャッシュに乗ってたほうがいい？A:プロパティも必要なら載せたほうがいい。メモリのしてはできたはず。

　Luceneがほめられてました。Neo４Jの

5.冬のLock-free祭り　@kumagi
　資料：https://docs.google.com/viewer?a=v&amp;pid=explorer&amp;chrome=true&amp;srcid=0B72X9w6tG5q0NzMyODgwNDYtNjY4Yy00ZDgwLTliZDMtMjQwYmViMWE5NGU5&amp;hl=en_US
　※資料がうまく見れないのは、アニメーションガチガチって話だからかな？
　http://twitter.com/#!/kumagi/status/145363169718697984

　辻Lock-free活動してます。
　　Lock-freeとツイートすると補足されるらしい。
　CPUの系譜のおさらい
　　ポラックの系譜
　　最新のIntelManyCoreとか東大とかに配られてるらしい。
　なんで、マルチコアのCPUがTSUKUMOとかで売ってないの？
　　僕らが使いこなせてないから。＝マルチスレッドの対応がうまくできてないから。
　CAS＝Compare And Swap
　まずは、Lock-free stack
　　ABA問題？
　Lock-free Queue
　　QueueのEnqueueの場合は2回のCASが必要になる。
　　不変条件を守る。ロックだと守りやすい。
　　2回のCAS処理の間が危険領域
　Lock-free List
　　色々なロックの説明。
　Lock-free hash map
　　これも気が狂ってるアルゴリズム？
　　ConcurrentHashMapなども話が出てきたが、説明しない＝Lock-freeじゃないからｗ
　Lock-free SkipList
　　SkipList：
　　　順序関係のあるデータをO(log n)で検索・挿入・削除ができるデータ構造（2分木ともろかぶり）
　　DougLea先生がjava.util.concurrentに入っています。
　Lock-free Btree
　　論文ないんですよ。簡単すぎて。
　SoftwareTransactionalMemory
　The Art of Multiprocessor Programmingを読みましょう！
　論文のお話。
　　http://labs.gree.jp/Top/OpenSource/Flare.html
　　を利用しているらしい。
　The Art of Multiprocessor Programming関連サイト
　http://www.cs.brown.edu/courses/cs176/lectures.shtml
　
　すごくわかりやすかった。資料をもっと見たいなぁ。
　Q:STMはメモリ使用量が2倍になる？A:
　Q:STM（Clojure）使ってみたけど残念な性能しか出なかったけど、なんで？A:Clojureのものはロックベース。でうーむ。。。

6.機械学習と最適化の基礎（仮）　@tkng
　PFIの徳永さん
　日本語入力を支える技術（技評）2012年2月発売予定
　最適化の話、機械学習の話、学習率の話。
　大域的最適、局所的最適の説明。（言いにくそう、大域的最適ｗ）
　凸最適化問題＝最適解が1つしかない？
　　大域的最適解＝局所的最適解
　機械学習なはなし。
　スパムメールの例で説明

　淡々と語られてましたが、わかりやすい説明でした。もっと昔にこんな形で数学の説明聞きたかった。。。
　
7.機械学習・言語処理技術を用いた誤り検出・訂正（LT）　@mamoruk
　資料：
　スペル誤り訂正とかに機械学習とか使われてます。

8.神の言語による自然言語処理（LT）　@AntiBayes
　資料：http://www.slideshare.net/AntiBayesian/ss-10539347#
　Lispさいこー
　Clojureさいこー
　lucene-gosenもいいよーｗ

9.言語判定へのいざない（LT）　@shuyo
　資料：
　Solrで採用されたlanguage-detection libraryの紹介。
　アゼルバイジャン語とか大変みたい。

12.作ろう！簡潔ビットベクトル　@echizen_tm
　資料：http://www.scribd.com/doc/73565169/Lets-Impl-Sbv
　入門編：
　　簡潔データ構造
　　　データサイズをほぼ情報量的下限にまで小さくしたデータ構造
　　　データは小さいし、速度速いというのがこれ。
　　実用化されてる。
　　　mozc（LOUDSが使われてます）
　　　Sedue（圧縮接尾辞配列（デフォルトではなくなった by @tkng））
　　種類：
　　　ビット列：ここの説明がこのあと
　　　木構造：トライ木をLOUDSが結構で実装するケースが多い。データサイズが小さくなる
　　　文字列：ウェーブレット木が有名。全文検索、転置インデックスにウェーブレット木を使う研究が多い。
　中級編
　　簡潔ビットベクトル：
　　rank(i)関数：i番目より前の立っているビットの数。
　　　疎なデータ列の効率的な実装に使える。検索結果が少ない場合のFacetとかに使える？
　　select(i)関数：？
　　　可変長データ列の効率的な実装
　上級編
　　ダメでした。。。ごめんなさい。。。

　体力的に厳しかったです。。。けど面白いし、わかりやすい。どういった所でこれらを使うべきかを取捨選択するのがすこし難しそうです。Luceneとかの内部ソース見ると実はこのあたりで実装されてる部分があったりするのかも。

11.類似文字列検索の仕組み　@overlast
　資料：
　使い所のはなし
　　例：辞書メンテナンスコストを軽減したい
　　架空の事例から類似検索を適用できるというストーリー
　　既存データが活用されるうれしさの例？
　　Apporoのデータ構造とかの話かな？

11.5.審査委員長から賞品持って帰る人の発表
　@kumagi
　@tkng
　@echizen_tm
　の方たちでしたー

12.懇親会
</code></pre><p>いやぁ、前回も感じましたが、基礎ができてないなぁと実感できた勉強会です。
そして幅も広い。やはり、数学的な要素が時々出てくるのでその部分をもっと身につけないといけないなぁと。。。
ついていけなかったところも多々ありましたが、頭の中にキーワードが入ってればなんとかなるかなぁと。
Neo4jやLuceneのソースコードリーディングをやるのが仕事に役立ちそうなので、少しずつでもやろう。
これから懇親会なので、とりあえず、このへんで。
あとで、見返しながら、リンク貼り直したりすると思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>MBAセットアップ備忘録その５とMac Mini（2009late）セットアップとその他くだらない話(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/01/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%95%E3%81%A8mac-mini2009late%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%81%A8%E3%81%9D%E3%81%AE%E4%BB%96%E3%81%8F%E3%81%A0%E3%82%89%E3%81%AA%E3%81%84%E8%A9%B1/</link>
      <pubDate>Thu, 01 Dec 2011 01:00:28 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/01/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%95%E3%81%A8mac-mini2009late%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%81%A8%E3%81%9D%E3%81%AE%E4%BB%96%E3%81%8F%E3%81%A0%E3%82%89%E3%81%AA%E3%81%84%E8%A9%B1/</guid>
      <description>ひさびさに、MBAのお話です。 セットアップといっても、物理的な方ですね。 以前、ケース（カバー）について記載しました。 この記事にも書いてあった</description>
      <content:encoded><p>ひさびさに、MBAのお話です。
セットアップといっても、物理的な方ですね。
<a href="http://johtani.jugem.jp/?eid=34">以前、ケース（カバー）について記載しました。</a>
この記事にも書いてあったのですが、付属の滑り止めが簡易なシールで、カバンの中から出し入れすると剥がれるは、作業をしてるとずれるわで、悩んでいました。
今日、たまたま、東急ハンズに行くことがあったので、店員さんに相談してすべり止め対策のグッズを購入しました。
相談した所、カバーは傷がつきにくくなるような素材ですから、シールもつきにくいとのこと。（言われてみればそりゃそうか。）
対策としては紙ヤスリで傷つけたところに薄手のシールゴムをつければはがれにくくなりますよとのこと。
ということで、やってみました。まぁ、ゴムにホコリはつくかもしれませんが、今のところ滑らず快適！
いやぁ、ハンズの店員さんすごいですわ。
MBAについてはこんなとこです。最近また、出先に行っていてMBA使えないので、ブログを書くかメールにしか使ってないです。。。</p>
<p>で、もう一つはMac Mini（2009 late：Core2Duo 2.26GHz、メモリ4G、HDD250G）です。
前職の同期から安く譲ってもらいました。音声が出力されないという問題点があるとFacebookでつぶやいているのをみて、興味を持ったらゲット出来ましたｗ
で、セットアップをしたのです。
大変でした。まずつまずいたのが、キーボードとマウスでした。
これまで使っていたデスクトップのキーボードがあるのですが、これがPS/2ということで使えません。（軽いジャブ）
MBAのキー配列にも慣れてきていたので、ビックカメラで悩みながらもApple純正のワイヤレスキーボードとまじっくトラックパッドを購入。
で、まずは、付属のLeopardで初期化。
このときは、ワイヤレスキーボードは認識したのですが、マジックトラックパッドはこの時代に無かったらしく、反応せず（さらに軽いジャブ）
さすがに前のPCのマウスはUSBだったので、これを利用してセットアップ再開
次にSnow Leopard（これもつけてくれた）にアップグレードして最後にLionです。
Snow Leopardでソフトウェアアップデートしたところでようやくトラックパッドが認識できたと。
で、やっと使い始めたのですが、MBA（2011）に慣れていたので、思いの外もっさりした動きをしているMacMiniに
戸惑ってます。（トドメのストレート。現在ここ）
ということで、どうやれば快適に使えるかアイデア募集中です。
とりあえず、Lionの設定のウィンドウ復元機能やアニメーションをオフにするなど対策してます。
HDDのSSD化とかメモリ増設とか考えないとダメかなぁ。
あと、ついでにBootcampも興味があるので、せっかくだから実験する予定です。
Windowsが前のデスクトップより快適に使えるようになるなら、Windowsマシンとするのもありかなぁと。。。</p>
<p>セットアップ系はこのへんで。
あとは、最近興味が有ること2点です。（技術系ではありません）
1つ目は最近発売されたゼルダ。夜な夜なやってます。
もともとゲーム好きでゼルダも欠かさずやってきてますが、ゼルダらしさを残しつつ新しい要素やアクションも取り入れとなかなか楽しいです。
ただ、Wii全般に言えることですが難しい！まぁ、楽しんでますけどね
2つ目は前からつぶやいてるのですが、電子書籍を読むためのタブレットがすごく気になってます。
最初は財布に優しく、MBAを普段持ち歩いてるのもあるから、LenovoのA1（7インチ）で決まりだと思っていたのですが、技術書を読むのは少しキツイかもという印象。
どうしてもオライリー本などの大きめのサイズになるため、7インチの画面でPDFを表示したところ、字が小さくなってしまい拡大しないと見づらいのです。
ただ、拡大してしまうと今度はパッと見るという一覧性が下がってしまいます。。。
少々重くなってもいいので10インチにしたほうがいいのかな、けど、電車で立って読むの辛いかもなどなど考えているところで最終的な決断ができない次第です。。。
最初は毛嫌いしていたiPad2もいいかもと思う始末で困ってるとこです。
安い中華パッド（2万ちょっと）に手を出すか、7インチで我慢するか、Galaxy Tab 10.1のWifi版を輸入業者から購入するかと悩んでるところ。
ただ、長く使うのを考えると、OSのバージョンアップなどを保証してくれるiPad2が実はいいじゃないかなぁなど。
皆さんからしてみればくだらないなぁと思われるかもしれないけどずーっと悩んでつぶやいたりしております。
はぁ、こまった。（そもそもそんなにスペックいらないんじゃないかというのもあるからなぁ。メインはPDF読むことだから。）


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B002C1ARK8/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B002C1ARK8&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B002C1ARK8/?tag=johtani-22">
      ゼルダの伝説 スカイウォードソード (期間限定生産 スペシャルCD同梱)
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>「どんな本でも大量に読める「速読」の本」を読みました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/29/%E3%81%A9%E3%82%93%E3%81%AA%E6%9C%AC%E3%81%A7%E3%82%82%E5%A4%A7%E9%87%8F%E3%81%AB%E8%AA%AD%E3%82%81%E3%82%8B%E9%80%9F%E8%AA%AD%E3%81%AE%E6%9C%AC%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 29 Nov 2011 13:57:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/29/%E3%81%A9%E3%82%93%E3%81%AA%E6%9C%AC%E3%81%A7%E3%82%82%E5%A4%A7%E9%87%8F%E3%81%AB%E8%AA%AD%E3%82%81%E3%82%8B%E9%80%9F%E8%AA%AD%E3%81%AE%E6%9C%AC%E3%82%92%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>どんな本でも大量に読める「速読」の本 Twitterで知り合った方がこの本について書かれていたブログ記事を読んで興味を持ち、読みました。 書籍（</description>
      <content:encoded><p>

<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4479793313/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4479793313&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4479793313/?tag=johtani-22">
      どんな本でも大量に読める「速読」の本
      </a>
    </p>
  </div>
</div>
Twitterで知り合った方がこの本について書かれていた<a href="http://www.metisous.com/2011/10/post-62">ブログ記事</a>を読んで興味を持ち、読みました。
書籍（特に技術書）が山になっていたこともあり、速読に興味を持っていたところちょうど記事を目にしたのは
きっと何かのタイミングなんだろうなと。</p>
<p>悪い癖で、電車で読もうと本を常に持ち歩くのですが、ついついスマートフォンやゲームで遊んでしまい、今回も読むのに時間がかかってしまいました。
３章の途中までを今月頭に読んでいたのですが、そこから少しほったらかしで、読み終わったのが昨日でした。</p>
<p>本の内容ですが、先ほどのブログにも書かれていますが、速読は技術ではない。
<span style="color:#FF0000">### 速読　＝　速読技術　X　ストック（知識、経験、情報）</span>
であると。あとは、わからなくてもいいから、ざっと目を通す感じで繰り返し読みなさいと。
さらに、１回でわからんくてもいいから、とにかく繰り返し読むことが重要だということでした。</p>
<p>確かに「速読＝１回で速く読む」、「読書＝１回で理解する」という意識がどこかにあったなぁと気付かされました。
プログラム組んだり、あることを覚えるときは繰り返しを意識してたのに、読書は１回読んで「はい、おしまい。」という気になってました。（マンガは繰り返し読むんですけどねぇ）</p>
<p>他の速読の本は胡散臭いし、絶対無理だよなぁと思ってたのですが、この本に書かれている話は筋が通っているように感じました。
ただ、考えずにサラサラ読みなさいという部分の実践はなかなか難しいかな。どうしても頭の中で音読してしまうので。
私は間を開けてしまったせいで、時間がかかってしまいましたが普通なら１日あれば読める内容なので速読に関してちょっとと思ってる方は読んでみると面白いかと思います。</p>
<p>ちょっとだけショックだったのは、この本の論理だと電子書籍は速読に向いていないというところです。
せっかく溜まった書籍をPDF化して、タブレット購入して（まだ買ってない。。。）読もうと思っていたところなのに。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Hadoopソースコードリーディング第7回に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/29/hadoop%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E7%AC%AC7%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 29 Nov 2011 01:51:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/29/hadoop%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%83%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E7%AC%AC7%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>Hadoopソースコードリーディング第7回に参加しました。 いつものごとく、自分用のメモをとっていたので。 第6回（2010/12）には参加して</description>
      <content:encoded><p>Hadoopソースコードリーディング第7回に参加しました。
いつものごとく、自分用のメモをとっていたので。
第6回（2010/12）には参加してたのですが、あれからそういえば、話が無いなぁと思っていたところに
再開するという話がTwitterに流れてきたので、即申し込みしました。
思い返せば、Hadoopに興味をもって少し触っているところで参加したのだったなぁと感慨深い思いを思い出しました。</p>
<p>今回は場所を変えて豊洲のNTTデータさんで開催されました。</p>
<p>日時：2011/11/28（月）19:00～22:00
場所：豊洲センタービルアネックス NTTデータ</p>
<pre><code>
◎アジェンダ＋導入（NTTデータ 濱野さん）
　ちょっと間が空きましたが隔月で行う予定。
　2012/01/12くらいに次回を予定。ネタ募集中。

◎Hadoop World NYC 2011 参加レポート
　- NTTデータ 下垣 徹さん （Hadoop徹底入門著者の一人です！）
　　・イベント概要：2011/11/08～09にNYで開催
　　　スポンサー企業の数がどんどん大きくなってる。
　　・会場の様子
　　　最初のキーノートは立ち見が出るほど。
　　　個別に5並列でセッションが開催されてましたと。
　　・キーノート紹介
　　　1.Hadoop World（Michael Olson）
　　　　アンケートからの概観
　　　　HadoopはNext Generation DataCenterだそうで。
　　　2.JP Morgan（Larry Feinsmith）
　　　　コスト削減＋収入増加のためにHadoopつかうぞと。
　　　　BigData分析の戦略
　　　　ユースケース１：ETL（Extract/Transfer/Load）ツールとして
　　　　ユースケース２：共通データ基盤
　　　　　検索頻度が低いデータの低コストストレージなどなど。
　　　　ユースケース３：データマイニング
　　　　　異常値検出とか。
　　　3.eBay（E. Williams）
　　　　Cassini：オークション情報の検索エンジン
　　　　　柔軟な検索＋協力なランキング機能
　　　　Hadoopで転置インデックス作成＝Luceneは使ってるってことかな？
　　　　HBaseも利用。データはHBaseに登録してる。
　　　4.Informatica（James Markarian）
　　　　データとプロセス
　　　　Hadoopが賑わうからRDBMSも活気づく
　　　　Hadoopはプラットフォームになるよと
　　　5.Cloudera（Doug Cutting）
　　　　Hadoopの背景と今後について
　　　　BigDataを分散処理するためのカーネルになりつつあるよと。
　　　　Apacheプロジェクトの良さ
　　　　　類似するプロジェクトも共存させてるよと。
　　　　Hadoopの今とこれから（0.20（今）、0.23（将来））
　　　　0.23はHDFSの性能面改善、スケーラビリティの強化、HA
　　　　　　　MapReduce2.0
　　　　CDH4のはなし。
　　　　Hadoopコミュニティはまだまだ若い
　　・HadoopWorldの傾向
　　　HBaseの利用増加
　　　　利用理由？
　　　　　小さいレコードにも強い。
　　　　　ランダムアクセス
　　　大手ベンダ参加
　　　　Oracleすげー。Hadoopの周りを固めていくぞと。
　　　　Hadoopの周りを各社がどう固めていくかというはなし。
　　　事例が多い
　　　　WibiData：HBaseの利用事例。Fonedoctor
　　　　Walt Disney：Big Dataに対する機会損失があるから、導入するよと。
　　　　　　　　　　テーマパークの交通流解析にも使うぞ！
　　イケメンによる来年（2012年）のHadoop Worldの動向！？
　　　参加者3000人！
　　　ラスベガスで開催？
　　　スポンサー拡大で食事が美味しく！？
　　　全方位戦略vs.特化型
　　　BIツールベンダの巻き込み
　　　利用事例のアピール合戦
　　　HBaseの利用事例が続々！（Salesforce.comあたりがセッション持つんじゃね？）
　　　Hadoop対抗アプリケーション、業界特化型キラーアプリケーションがますます増える？
　　　Mahout事例の増加
　　　データマイニング＋Web勉強会
　- NTTデータ 政谷さんのHadoopNYでの発表のダイジェスト
　　まずは印象。深く使っている人は去年より減っている。バズワードになってるからかな？
　　今後もHadoopコミュニティは健全なコミュニティになりそう。（いろんな所が出てきたけど、大丈夫そうだなぁ）
　　・発表スライドのダイジェスト版
　　　日本でのHadoopの盛り上がりの話。
　　　Hadoopと他との住み分けの話。
　　　Sqoopのお話。PostgreSQLに向けたインテグレーションの話。これはPostしてコミット待ち。
　　　Low-Latency Serving Systemへの受け渡し（前処理はHadoopでGPGPU使ってデータをクラスタリングして処理速度をあげる）
　　　FujitsuのETERNUSにHDFS APIが用意されたという話。
　　　MapRの話に似ているよと。小さめのクラスタの場合に有効？セッションの後に少し話題になった。
　　　扇子を配りましたよと。

　※ビールが配られ始めた！（ビールじゃないという声もチラホラｗ）
◎『Hadoop Troubleshooting 101』 セッションレポート
　 - Cloudera 嶋内さん （@shiumachi）
　Hadoopを壊す7つの方法
　Hadoop設定したりしたことある人？＝結構いた。さすがソースコードリーディング
　Clouderaでは木曜日に重要なミーティング（ボードゲームするかFPSするか）というのがあるらしい。
　チケット分析
　　設定ミス？
　　　HadoopやOSの設定ファイルの変更を必要とするあらゆるチケットの事
　　　35%が該当
　　問題の原因別のグラフ
　　　1番は設定ミス
　　　2番はバグ（これはベータ版リリースからチケット管理してるから）
　　メモリの管理ミス
　　　Task Out Of Memory Error
　　　　io.sort.mb &lt; mapred.child.java.optsとなるように設定すること
　　　　io.sort.mb = mapバッファのサイズ
　　　　mapperとreducerを減らす。
　　　　　mapper＝ノード上のコア数
　　　　　これは机にはっとけ！
　　　　　Total RAM = (Mappers + Reducers) * Child Task Heap + DN heap + 3GB + RS heap + ？
　　　JobTracker Out Of Memory Error
　　　　JTのメモリ使用量の合計　＞　割り当てRAM
　　　　原因は？
　　　　　タスクが小さすぎ
　　　　　jobヒストリが多すぎ
　　　　解決は？
　　　　　maximumを減らしなさい？
　　　Unable to Create New Native Thread
　　　　どういう意味？
　　　　　プロセスが起動中にもかかわらずDNが障害ノードとして表示されている。
　　　　原因は？
　　　　　nprocのデフォルト値が低すぎる
　　　　対応は？
　　　　　/etc/seurity/limits.confの値を考えろ
　　　Too Many Fetch-Failures
　　　　元スライドの発表者はこれが大好きらしい
　　　　どういう意味？
　　　　　Reducerのfetch操作がmapperの出力の取得に失敗
　　　　　ブラックリスト入りのTTで発生するらしい
　　　　原因は？
　　　　　DNSの問題
　　　　対応は？
　　　　　mapred.reduce.slowstart.completed.maps = 0.80
　　　　　　reducerの開始を遅らせることで対応
　　　　　tasktracker.http.threads = 80
　　　　　Jetty 6.1.26は使うな！CDH3u2に上げましょう。
　　　Not Able to Place Enough Replicas
　　　。。。きっとどっかにスライドあるからメモはこのへんでいいか。。。

◎Hadoop World NYC 2011 参加レポート
　- Acroquest Technology 阪本 雄一郎さん （@frutescens） 落合 雄介さん （@taro_x）
　・RとHadoopの融合（Revolution AnalyticsのDavid Champagneさん）
　　R言語の紹介
　　Rとの接続点（rmr、rhdfs、？）
　　R言語の利点がHadoopで使えるから、記述が少なくて済むよと。
　　統計処理がわからなくてもRが使えると分析ができるよ。（そう言われても、その分析で正しいのかとかは結局統計とかをある程度理解してないと厳しいんじゃないか？）

　・Hadoopを使った衛星画像解析
　　スケールとかしたいからHadoopにしたけど、ジョブ起動遅いし、科学計算ライブラリが不十分

　・Hadoopをクラウド上に展開（vmware）
　　Hadoopだけじゃなく、NoSQLとかもスケールしてから使いたいよねぇ。
　　ということで、vmwareの色々なものを使ってHadoopと他をうまく構築しますよ事例紹介。
　　マルチテナントのHadoopの話とかもありますよねと。

◎最後はHadoopには関係ないのが多かったけどおみやげ争奪戦。
　なぜか、MongoDBのシールをもらいました。
</code></pre><p>ということで、HadoopWorld2011の総括＋各スライドの紹介でした。
これまでのHadoopWorldの傾向などから、参加者などのトレンドがどうなっているかなどの話が聞けたのが面白かったです。
ClouderaのスライドはHadoopのトラブルシューティングとして必見になりそうな感じでした。
内容が濃くて、それほど触っていない私にはわからないところもチラホラ。（日本語資料の公開されるかなぁ？）
案の定、ビール（発泡酒）を飲んだので、途中から一部が飲み会状態になってたし、ピザ食べてたので、私のメモも途中から適当になってしまいました。
あとは、とある件についていろんなかたからの知見を入手できたのが良かったですね。
他にもLilyプロジェクト（Solrが利用されているOSSの話）の話もあったようなのですが（HPの方が教えてくれました）、今回の参加レポートでは含まれてなかったです。（話聞きたかったなぁ。余力があれば、スライド読むかなぁ。誰か発表してくれないかなぁ）
今日もまた、Twitterで絡んでいた方たち数人と面識が得られました。
今後は定期開催の流れになるようなので、これからもHadoopにしがみついていくためにも参加するようにがんばりますよと。</p>
<p>会場提供のNTTデータのみなさん、発表者の皆さんお疲れ様でした。次回も期待してます！！</p>
<p>2011/12/01追記
<a href="http://togetter.com/li/221481">Togetterのまとめ</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>「Apache Solr入門」のサンプルのlucene-gosen対応（1章から4章）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/26/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AElucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0%E3%81%8B%E3%82%894%E7%AB%A0/</link>
      <pubDate>Sat, 26 Nov 2011 03:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/26/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AElucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0%E3%81%8B%E3%82%894%E7%AB%A0/</guid>
      <description>先週末から勤労感謝の日まで風邪で寝こんでました。。。 みなさん、朝晩、冷え込みが激しいので風邪には気をつけてください。 季節の言葉も入れたので本</description>
      <content:encoded><p>先週末から勤労感謝の日まで風邪で寝こんでました。。。
みなさん、朝晩、冷え込みが激しいので風邪には気をつけてください。</p>
<p>季節の言葉も入れたので本題です。
つい最近、「Apache Solr入門」のサンプルをlucene-gosenでどうやって動かすんですかー？という質問を受けました。
確かに、「Apache Solr入門」を書いたのはSolrのバージョンが1.4が出る直前でしたし、lucene-gosenは存在せず、
当時はSenを元にした日本語の形態素解析のサンプルとなっていました。
そのSenも入手しづらくなってきており、私もlucene-gosenのプロジェクトに携わるようになってきてある程度時間が
経ちました。
せっかくなので、サンプルのschema.xmlだけでも最新版（Solr 3.4 + lucene-gosen-1.2.0-ipadic）のものを用意しました。
なお、あくまでも、3.xでlucene-gosenを利用する場合の「Apache Solr入門」のサンプルプログラムの変更点（とりあえず、4章まで）の違いについて記述します。
申し訳ございませんが、1.4と3.xの違いについての説明はここでは行いません。</p>
<p>以下では、各章でschema.xmlに関連する記載のある部分を抜粋して、変更点と変更したschema.xmlのリンクを用意しました。参考にしてもらえればと思います。</p>
<h2 id="1章">1章</h2>
<h3 id="161-n-gram17ページ">1.6.1 N-gram（17ページ）</h3>
<p>1.6.1の手順に変更はありません。
サンプルプログラムが入っているZip「solrbook.zip」のintroduction/ngram/schema.xmlファイルの代わりに
<a href="https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/b51b74e8c573/introduction/ngram/schema.xml">こちらのschema.xml</a>を利用してください。</p>
<h3 id="162-形態素解析18ページ20ページ中盤まで">1.6.2 形態素解析（18ページ～20ページ中盤まで）</h3>
<p>手順が大きく変わります。
Senを利用する場合、Senの辞書のビルド、Senのjarファイルの配置、Senを利用するためのTokenizerクラスを含んだサンプルjarの配置という作業があります。
lucene-gosenではコンパイル済みの辞書がjarファイルに含まれています。
また、Solr向けのTokenizerもlucene-gosenのjarファイルに含まれています。
lucene-gosenを利用して形態素解析を体験するための手順は次の流れになります。
なお、schema.xmlについては上記N-gramでダウンロードしたschema.xmlに形態素解析の設定もあわせて記載してあります。</p>
<p>jarファイル（<a href="http://lucene-gosen.googlecode.com/files/lucene-gosen-1.2.0-ipadic.jar">lucene-gosen-1.2.0-ipadic.jar</a>）をダウンロードして、$SOLR/example/solr/lib（libディレクトリがない場合は作成）にコピーします。
コピーが終わりましたら、次のように$SOLR/exampleディレクトリでSolrを起動します。
（-Dsen.homeは必要なし）</p>
<pre><code>
$ java -jar start.jar
</code></pre><p>あとは、書籍の記述にしたがって管理画面のAnalysis画面で動作を確認します。
ほぼ、図1-6と同じ結果になっていると思います。
（lucene-gosenで出力される情報には本書のサンプルよりも多くの情報が含まれています。また、サンプルでは、形態素解析の後の単語に基本形を採用しているため、「な」が「だ」として出力されています。基本形を出力する場合は後述するこちらで紹介したTokenFilterを利用すれば可能です。）</p>
<h2 id="2章">2章</h2>
<h3 id="213-schemaxmlのバージョン27ページ">2.1.3 schema.xmlのバージョン（27ページ）</h3>
<p>Solr3.xではschema.xmlのファイルの最新バージョンは<strong>1.4</strong>になっています。</p>
<h3 id="223-代表的なトークナイザ35ページ">2.2.3 代表的なトークナイザ（35ページ）</h3>
<p>solrbook.analysis.SenTokenizerFactoryは必要ありません。
先ほども説明しましたが、lucene-gosenにはSolr向けのトークナイザが用意されています。
solr.JapaneseTokenizerFactoryがそれに該当します。</p>
<h3 id="224-代表的なトークンフィルタ37ページ">2.2.4 代表的なトークンフィルタ（37ページ）</h3>
<p>以下の2つについてはlucene-gosenに同等のトークンフィルタが存在します。</p>
<ul>
<li>solrbook.analysis.KatakanaStemFilterFactory</li>
<li>solrbook.analysis.POSFilterFactory</li>
</ul>
<p>それぞれ、次のものがlucene-gosenにあるので、こちらを利用します。</p>
<ul>
<li>solr.JapaneseKatakanaStemFilterFactory</li>
<li>solr.JapanesePartOfSpeechStopFilterFactory</li>
</ul>
<p>2章向けの<a href="https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/b51b74e8c573/schema/schema.xml">schema.xmlはこちら</a>です。その他のtxtファイルについては、特に変更はありません。</p>
<p>3,4章は特に変更はありません。Solrの起動の仕方にだけ注意してください。（-Dsen.homeは必要ありません）</p>
<p>以上が4章までの修正点になります。
動作しないなどあれば、コメントください。
サンプルアプリについてはまた後日余裕があれば。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>MongoDB勉強会（第7回）に行って来ました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/16/mongodb%E5%8B%89%E5%BC%B7%E4%BC%9A%E7%AC%AC7%E5%9B%9E%E3%81%AB%E8%A1%8C%E3%81%A3%E3%81%A6%E6%9D%A5%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 16 Nov 2011 02:08:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/16/mongodb%E5%8B%89%E5%BC%B7%E4%BC%9A%E7%AC%AC7%E5%9B%9E%E3%81%AB%E8%A1%8C%E3%81%A3%E3%81%A6%E6%9D%A5%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>今回は、触ろうと思って触れていないMongoDBの勉強会に行って来ました。 2週連続の渋谷で、さすがに今回は出口をすんなりでれました。 今回は初</description>
      <content:encoded><p>今回は、触ろうと思って触れていないMongoDBの勉強会に行って来ました。
2週連続の渋谷で、さすがに今回は出口をすんなりでれました。
今回は初のGMOさんのビルへの潜入です。
ということで、いつものごとく自分のメモを残しておきます。</p>
<p>日時 :2011/11/15 14:30 to 20:00
定員 :140 人
会場 :GMO Yours （セルリアンタワー（11階））
ハッシュタグ :#mongotokyo</p>
<p>1.fluentd plugins for MongoDB    <a href="http://twitter.com/#!/doryokujin">@doryokujin</a>
<a href="http://www.slideshare.net/doryokujin/an-introduction-to-fluent-mongodb-plugins">スライドはこちら</a>。</p>
<pre><code>
　FluentとMongoDBのコラボレーション
　・Fluentとは？
　　解析対象のログについて。データ量が膨大
　　これまでのログの扱い方。日次でS3に送信して、ログ解析サーバにて前日分を解析する。
　　これではリアルタイム性がないのが問題！しかも1日分なので、データ量が半端ない。
　　そこでFluentにてストリーミングアプローチ。
　　リレーサーバ（Fluentd）にログを流すと、適宜、解析サーバに流れていく仕組みが可能。
　　これにより、ネットワークに負荷をあまりかけずにログを定期的（時次とか）で流せる。
　　※splunkに近い考えかな。
　　ここで、Fluentのスライドで説明が入りました。http://www.scribd.com/doc/70897187/Fluent-event-collector-update
　・Out Mongo For Local Back-Up
　　MongoDBへの出力。
　　信頼性をあげるためにローカルにmongoDBを置いておき、スプールしておく。
　　リレーサーバにも配置してみるという仕組み。
　　バッファリングにMongoDBを使うので
　　Aggregation Mongoってのを作ったらしい。そのせいで、スライドの作成が遅れたらしいw
　・Aggregation Mongo
　　Map/Reduceの集約のようなプラグイン。
　　fluentdのアプリを通して、特定のキーをベースに集計してから、outputする仕組み。
　　更に、キーに対してshufleが行われて、次のfluentdにデータが集まる。　
　　fluentdを使ってM/Rっぽいことが可能なのかな。
　QA
　　最終的な流れ込み先のMongoDBはCappedCollectionじゃなくしたほうがいい。検索とかしたくなるから。
</code></pre><p>2.about Server Density and MongoDB    <a href="http://twitter.com/#!/davidmytton">@davidmytton</a>
<a href="http://www.slideshare.net/boxedice/mongodb-tokyo-monitoring-and-queueing">スライドはこちら</a></p>
<pre><code>
　Queuingシステムで利用。
　MongoDBとRabbitMQとの比較？
　MongoDB関連のノウハウかな。
　データ量に関する話。メモリに載せたほうがいいのかとか、ログ出力の設定とか、ジャーナルとか。
　HDDの見積もりに関連しそう。
　健康状態の監視方法。
　コネクションプールの話とか。
　rs＝ReplicaSet？
　mongostatってコマンドがあるのか。
　ServerDensityのツールとダッシュボードなどについて。
　※英語のスライド見ながら話聞くとメモがとれない。　
　まとめ：
　　Keep it in Ram インデックスはメモリに載せましょう。
　　Watch your Strage ストレージのサイズは監視しましょう。（ログ、データ、ジャーナルなどなど）
　　db.serverStatus()　コマンドあるよ。これで見れるデータが重要なのかな？
　　rs.stats()　コマンドあるよ。これで見れるデータが重要なのかな？
　　
　QA：残念ながら聞き取れなかったっすｗ
　　Q：MongoDBのクエリログの統計の質問。DBごと？＝コレクションごと？のクエリ統計のとり方は？
　　A：MongoDBにはないので、ログレベルを下げて統計取ったりする方法しかないかなぁ。
</code></pre><p>3.MongoDB: Case Study for AMN    <a href="http://twitter.com/#!/koyhoge">@koyhoge</a>
<a href="http://www.slideshare.net/koyhoge/mongodb-case-study-for-amn">スライドはこちら</a></p>
<pre><code>
　サービス（アジャイルメディア・ネットワーク）で利用している実例について
　広告配信に関連して利用してる。
　最初にPostgreSQLにて実装。5分おきにCronで再起動するはめに。。。
　PostgreSQLの次にSimpleDBへ（インサートが遅い。分散インサート）
　SimpleDB＋SQSに変更。1日130$で断念。
　SaaSなKVSはやめてMongoDBに移行してみる。
　負荷も軽いし、インサートも速い！
　EC2でMongoDB。しかもレプリカを東京A、B、シンガポールAにしてみよう。
　shardingは残念ながらうまく行かず。
　ストレージはEBSの1TB
</code></pre><p>4.「MongoDBとHadoopの蜜月関係」    <a href="http://twitter.com/#!/muddydixon">@muddydixon</a>
<a href="https://github.com/muddydixon/mongotokyo7th">スライドはこちら</a></p>
<pre><code>
　お父さんエンジニアなので、土曜日の勉強会は無理です！＝同じく辛いです！
　HadoopとMongoDBのつなぎについて。
　MongoDBからデータを読み込んで、Hadoopで計算してMongoDBに戻すものがmongo-hadoop
　データロストがニュースになってて心配してる。データロストはめったにない（by @doryokujin）
　Hadoopを計算だけに利用できるのでクラスタが落ちても気にせず立ち上げができそう。
　MongoDBからデータを取得する時点でフィルタリングが可能なので、Hadoopでの演算が楽。
　AdHocなクエリをMongoDBに投げれるのがうれしいのかな？
　HBaseとかHiveに入れてやったりはダメなんかな？

　QA
　　Q：なんで、みんなMongoDBにログ入れるの？@kzk_mover
　　A：様々な形式を入れやすいから。
　　Q：捨てるのどーするの？
　　A：コンパクションが大変（ただし、2.0以降は良くなる予定）
　　Q：MongoDBのMap/Reduceは？
　　A：時間がかかった上に死ぬというひどい目にあったので。。。
</code></pre><p>5.Fusion-io    <a href="http://twitter.com/#!/hasegaw">@hasegaw</a></p>
<pre><code>
　340の仮想マシンが4台で動きます！
　すごそう。。。一回は触ってみたいかも。
　Fusion-ioすげーーーって感じ。（Ustストップ）
</code></pre><p>6.MongoDB on Cloud Foundry    <a href="http://twitter.com/#!/yssk22">@yssk22</a></p>
<pre><code>
　VMWare社のPaaSオファリングの名称
</code></pre><h3 id="感想など">感想など：</h3>
<p>mongoDBに興味はあるのに、腰が引けてるおじさんになって結局触らずに会場入りしてしまいました。
会場に入っていきなり、Treasure Dataの太田さんがマグカップを売っているという場面に遭遇するというインパクトがあるスタート。（思わず1個購入）
最初はMongoDB JPの主催者でもある@doryokujinさんの話から。今熱いfluentdとmongoDBの組み合わせに関する話で、面白かったです。
途中でfluentdを作っている古橋さんのスライドを用いてfluentdの解説まで入りました。
TL上では、その古橋さんが時々フォローを入れているという贅沢な流れ。
なんとなく仕組みはスライドなどを見ていたのですが、更に理解が深まりました。
次が、イギリスから来日されていた@davidmyttonさん（なんと24歳という若さ！）のお話。
残念ながら英語のヒアリングは微妙な私なので、あまり理解できなかったのですが、どうやら、MongoDBの運用でのTipsのお話だったようです。スライドを後で見なおしたほうがいいかな。
次は、実際のMongoDBを利用した事例の紹介。PostgreSQLから試行錯誤してMongoDB＋AWSの組み合わせのお話。
やはり実例があると面白いですね。試行錯誤された部分があるので、非常にわかりやすかったです。
次が、HadoopとMongoDBの組み合わせのお話。HadoopのMap/Reduceの部分だけを利用して、データ保存先はMongoDBにしましょうという割りきった話でした。
いくつか疑問点がメモにもありますが、残念ながら質問する勇気なしという腰抜けっぷり（もうちょっと積極的にならないと。。。）
で、このあとFusion-ioとCloud Foundryの話になるのですが、体力切れ＋lucene-gosenのjavadocが古いことに気づいてしまい、作業をしながら聞いていたのであまり頭に残っていません。（ほんとに申し訳ない。。。）
とまぁ、最近、Twitter上でいろんな人に絡みまくってまして、@doryokujinにも絡みまくってたというのもあり、今回参加することにしたという次第でした。
懇親会にも多くの人が残っており、良いコミュニティができてるなぁというのが正直な感想です。
各セッションでも@doryokujinさんが適宜QAなどのフォローをされていて感心しっぱなしでした。
あとは、Twitter上で会話をしていた方たちとも顔合わせができたので、大収穫でした。</p>
<p>次は、少しでもいいので、触ってから参加することにしようかと思います。
来年1月には<a href="http://www.10gen.com/events/mongo-tokyo-2012">Mongo Tokyo 2012</a>というイベントも開かれるようで、ますます注目を浴びていきそうですね。主催者もミドルウェアもｗ</p>
<p>あ、そうそう、そんなMongoDBの勉強会でしたが、CouchDBの話もちらほら出てまして、<a href="http://www.couchbase.com/couchconf-tokyo" target="_blank">CouchConf
TOKYO</a>というチラシも<a href="http://twitter.com/#!/Ijokarumawak">@Ijokarumawak</a>さんから頂きました。こちらも1月開催のようです。</p>
<p>追記：

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111116/20111116_2134939.jpg" />
    </div>
    <a href="/images/entries/20111116/20111116_2134939.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
</p>
<p>戦利品の画像です。マグカップ（500円）以外は貰い物です。
なんと、このUSBにはMongoDBのコマンドやクエリのチートシート（PDF）が入ってました。びっくり！</p>
</content:encoded>
    </item>
    
    <item>
      <title>Splunk Live!のイベントに行って来ました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/09/splunk-live%E3%81%AE%E3%82%A4%E3%83%99%E3%83%B3%E3%83%88%E3%81%AB%E8%A1%8C%E3%81%A3%E3%81%A6%E6%9D%A5%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 09 Nov 2011 16:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/09/splunk-live%E3%81%AE%E3%82%A4%E3%83%99%E3%83%B3%E3%83%88%E3%81%AB%E8%A1%8C%E3%81%A3%E3%81%A6%E6%9D%A5%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>Solr本の武田さんから教えていただいたSplunkの イベントに行って来ました。 Splunkとは様々な機器のログなどを一箇所に集めてリアルタ</description>
      <content:encoded><p>Solr本の武田さんから教えていただいた<a href="http://splunk.intellilink.co.jp/">Splunk</a>の
イベントに行って来ました。
Splunkとは様々な機器のログなどを一箇所に集めてリアルタイムに検索、分析できるようにするための製品です。（ざっくりした説明ですが。。。）
ちなみに、データ量が小さければフリー版も用意されています。
以前、話しを聞いていて気になっていた所イベントが開催されるということだったので参加してきました。
以下に、その時取ったメモを記載しておきます。いつものごとく、自分用のメモなので、役に立つかはわかりませんが。</p>
<p>Splunk Live! in toyosu</p>
<p>日時：2011/11/02　10:00-12:00
場所：豊洲</p>
<pre><code>
1.挨拶＋アジェンダ紹介
2.ビッグデータ取り込み、ロードマップ（Splunk Inc.）CEO Godfrey Sullivan
　英語でした。。。
　データの種類と量が大きくなってきてるのに、ツールが追いつかないし、回答するのもはやくしろと言われる。
　非構造化データの例としてApacheのログが出ていた。
　TimeSeriesのフラットファイルがsplunkのデータのインデックス。（No RDB＝スキーマいらないよと。）　
　　※ここが重要な点かもしれない。
　リアルタイムに解析できるのが売り。Jubatusとの違いとか聞くと面白いかも。
　Machine Data Engine＝Splunk
　データの関連付けの方法がどんなものか？
　事前のスキーマが不要＝流しこむ前には定義が必要？だよね？
　？No need to filter/forward？？
　デベロッパーフレームワークってなんだ？？
　Splunkbaseと呼ばれる場所にSplunk Appsと呼ばれるアプリケーションがある。色々な場所、OS、アプリで利用できるものらしい。
　？自動監視も学習する仕組みがある？？
　それとも定義するのか？
　あくまでもMachine Dataと言っている。これは、ユーザのリレーションの解析などはないということか？
　Leading Social Gaming Company＝Zynga
　Introscopeのログバージョンに似てるかもなぁ。
　Cloudベースのアプリの解析にも利用（saleforce）
　色々な利用シーンのお話。

3.適用事例（独立行政法人理化学研究所）
　和光と神戸で利用。10G程度のデータを扱ってる？
　ログから情報基盤を監視するのに利用している。
　syslogベースでsplunkにログを送信している。
　事例１：VPNに接続できない（接続数上限がある？）
　　CISCOのログから解析
　　どの研究室で発生しているかも検索。
　事例２：DHCPの接続ミス？（IPアドレスロスト）
　　ここまでは問題が発生してからログを漁るという使い方。
　事例３：LINK FLAPのアラート
　　短時間にUPDOWNを繰り返す場合にスイッチがおかしくなってるんでは？
　事例４：メール大量送信（ウィルス感染）
　　不特定多数のサーバに短時間でメールを配信している

　※使い方としては情報基盤環境すべて（ネットワークとかサーバなど）のログを集めておいて監視＋解析に利用して障害対応などに利用している？データセンターとかに入れるのか？

　あくまでも、ログを保存して、検索できる仕組みが１箇所にまとめられているという感じ。
　そのログの解析について（どういった問題に対して、どういったクエリを投げるか？どういったトラップを仕込むか？）は利用する側の腕にかかっている印象。
　※実際に触ってみたいなぁ。500M/dayか。ご近所に入れてみるか？syslogで転送設定＋入れるサーバが必要。

4.最新アプリなどの紹介（Splunk Inc.）
　GUIがきれいだなー（最近のgoogleっぽいが。。。）
　リアルタイムにデータが入っているのが見えるのか。おもろいな。
　どこのフィールドにヒットしたかがファセットで表示できるらしい。おもろい。
　flashでできたビューがすごくおもろかったぞ。（ドキュメントどこだ？）

5.最新の取り組み（NTTデータ先端）
　やっぱり武田さんだった。
　複数の監視システムのメッセージを統合して検索、アラートを出せるようにする。
　消費電力を算出するためにログを集めて集計する。

6.QA
　ロードマップ
　　・２年以内にビジネスサイドでの利用に向けての動きを見せていく予定。
　　・アプリケーションフレームワークにして、他のパートナーのアプリを載せていきたい。
</code></pre><p>ということで、感想ですが、事例紹介などを聴いた感じだと今のところはインフラ系のログを一元化して検索、監視、アラートをあげるということに活用するためのツールの用に感じました。
当初の使い方がそういったところにあるためだとは思いますが。
開発者としての視点で話しを聞いていて、活用できそうだというのは次のシーンでしょうか。</p>
<ul>
<li>開発時の開発環境のログ集約</li>
<li>性能試験などでのログ、性能データ集約</li>
</ul>
<p>開発時点もしくは性能試験時ですが、色々なサーバ（DBやアプリサーバ）の時間を横串にして表示検索などができると思うので、問題があった時の各種サーバの状態を一元的に見れるため、どのサーバにどういった負荷がかかっていたか、
どこに問題があったかなどをグラフ化して見ることが簡単にできるのではないかなぁという感想です。
あとは、ログが一元化されているので、問題があったときにまずログを検索すればいいのが楽ですかね。</p>
<p>基本的にはログが集まってるからあとは、どう使うかはご自由にという印象でした。
どのようなログを集めておき、どういったトラップでアラートさせるか、どのような検索をすれば望んでいるログが出てくるか、どのような集計をしたいかなどについては、やはり導入してからノウハウを貯めていくか、導入時にコンサルしてもらうなどが必要かと。
また、このツールを入れることでどのようなフィードバックをどのように活用できるかをイメージしていなければ、
宝の持ち腐れになりそうです。
あとは、使い方次第ですが、サーバログ以外にアプリログ（ユーザの行動履歴とか）などを入れることで、インフラ以外での使い道もありそうです。
とりあえず、保存しておいて、あとから特定の傾向を見出すのに検索できるのはちょっと面白いかも。</p>
<p>あ、そうそう、ストラップとUSBメモリ（4G）のノベルティをお土産にもらいました。無料セミナーなのに。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene Eurocon 2011 Barcelona のスライド読みました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/08/lucene-eurocon-2011-barcelona-%E3%81%AE%E3%82%B9%E3%83%A9%E3%82%A4%E3%83%89%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 08 Nov 2011 13:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/08/lucene-eurocon-2011-barcelona-%E3%81%AE%E3%82%B9%E3%83%A9%E3%82%A4%E3%83%89%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>最近忘れやすいので、記録しておこうかと。 読んだスライドの簡単な内容と感想です。 ちなみに、スライドの一覧はこちらです。 ※スライドへのリンクはす</description>
      <content:encoded><p>最近忘れやすいので、記録しておこうかと。
読んだスライドの簡単な内容と感想です。
ちなみに、スライドの一覧はこちらです。
<span style="color:#FF0000">※スライドへのリンクはすべてPDFへのリンクになっていますので、注意が必要です。</span></p>
<hr>
<p><a href="http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/miller_solr4highlights2_eurocon2011.pdf"><em>**Solr 4 Highlights（PDF）**</em></a></p>
<p>Solrの次期バージョン4.0で採用される機能の紹介でした。
紹介されているのは次の機能。各機能について、JIRAの番号も記載があるので便利ですね。</p>
<ul>
<li>DirectSolrSpellChecker</li>
<li>NRT (<a href="http://wiki.apache.org/solr/NearRealtimeSearch">Near RealTime search</a>)</li>
<li>Realtime Get</li>
<li>SolrCloud - search side</li>
<li>SolrCloud - indexing side (WIP)</li>
</ul>
<p>これまでと異なるSpellChecker、Commit前のデータが検索できるNRT（なんでNRSじゃないんだろう？）、Commit前の登録済みデータを取得することが出来るRealtime Getなどの簡単な紹介です。
あと、個人的に興味のあるSolrCloud周りが絵付きで紹介されてます。ZooKeeperもちょっと出てきます。
まだ、ちゃんとまとめてないですが、NewSolrCloudDesignの翻訳したものも参考までに。（<a href="http://johtani.jugem.jp/?eid=31">その１</a>、<a href="http://johtani.jugem.jp/?eid=32">その２</a>）</p>
<hr>
<p><strong><em><a href="http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/Binns_archiveit_eurocon2011.pdf">Archive-It: Scaling Beyond a Billion Archival Web-pages</a></em></strong></p>
<p>InternetArchiveの事例紹介。1996年からWebページのアーカイブを行なっているサイトですね。
その一部でSolrが利用されています。
「1,375,473,187 unique documents」との記述もあり、データ量が巨大です。
データ量が多いのに、ここでFieldCollapsing/Groupingも利用しているようで、インデックス作成、検索両方に対してカスタマイズしたものをgithubで公開している模様です。</p>
<hr>
[<em>**Scaling search at Trovit with Solr and Hadoop**</em>](http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/MarcSturlese_scalingsearchTrovit_eurocon2011.pdf)
<p>次は、Trovitという会社のSolr＋Hadoopの事例紹介です。
最初はLuceneをベースに検索サーバ作ってたけど、Solrが出てきたので、Solrを使うようになったようで。
データ保存先として最初はMySQLを利用してDataImportHandlerでSolrにデータ登録してたけど、
データ量が増加するが、MySQLのShardingが面倒なので、Hadoop（Hive）でデータをパイプライン処理してSolrのインデックスを作成しましょうという流れになったようです。
私が以前、<a href="http://www.slideshare.net/nabeta/ss-8118052">Solr勉強会で紹介したSOLR-1301</a>のパッチをベースにMap/Reduceの処理を2段階にして性能をアップさせたという話が記載されてました。
ただ、これで早くなるのかはよくわからないんですが。。。
一応、資料では、いきなり大きなSolrのインデックスを作らずに、最初のM/Rで小さなインデックスを作成し（TaskTrackerの数＞＞Solrのshardサーバ数だから小さくしたほうが速い？）、
2段目のM/Rでインデックスをマージしてshardサーバ数のインデックスに集約する？という形みたいです。
（英語力のなさが。。。）
あとは、テキスト処理を幾つかHadoopでやってますよという紹介でした。
SOLR-1301の利用者が他にもいて、違うアプローチをとっていたのが印象的。
毎回全データインデックス生成するときは、SOLR-1301を利用してshard数が増えてもすぐに対応が可能になるので、
かなり便利ですよ。</p>
<hr>
<p><a href="http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/Gio_Kincade-Solr_Etsy_eurocon2011.pdf"><em><strong>Solr @ Etsy</strong></em></a></p>
<p>Etsyは個人の作家（編み物とかシールとか）の方が出店するためのショッピングモールのようなサイトです。
実は、最近、MacBookAirのステッカーを購入したのがここでした。
で、検索にSolrを使っています。
面白いのが、検索サーバとWebアプリ（PHPで書かれている）の間のデータのやり取りにThriftを利用していること。
Solrの前にThriftを話すサーバを別途用意しているようです。ネットワークのデータ量を減らすことが目的らしいです。
そのあとは、少しThriftのサーバでのLoadBalancingの話が続きます。
次にレプリケーションの性能問題のはなし。定期的にレプリケーションに異様に時間がかかるのが問題になったようで、
Multicast-Rsyncを試してみたけどダメでしたというはなし。
Bit Torrent + Solrという組み合わせで回避したらしいのですが、いまいち仕組みがわからなかったです。。。
こちらもgithubに公開されている模様。
あとは、QParser、Stemmerをカスタマイズしたものの話です。</p>
<hr>
<p><a href="http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/Baldeschwieler_HortonWorks_LuceneEurocon20111018.pdf"><strong><em>Architecting the Future of Big Data and Search</em></strong></a></p>
<p>LuceneのカンファレンスにHortonworksが出てきてびっくりしました。
まぁ、Luceneの生みの親＝Hadoopの生みの親ですから、問題ないのかもしれないですが。
大半が予想通り、Hadoopに関する話でした。
知らないApacheのプロジェクト「<a href="http://incubator.apache.org/ambari/">Ambari</a>」というのが出てきました。これは、HadoopConferenceJapan2011 Fallでの発表にもチラッと出てきたようです。
「Ambari is a monitoring, administration and lifecycle management project for Apache Hadoop clusters.」ということで、Hadoopクラスタの統合管理のツールになるんでしょうか？
最後の2枚くらいにLuceneが出てきます。絡めてみたって感じですかね。</p>
<hr>
<p><a href="http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/scholten_Configuring_Mahout_Clustering_Jobs_Eurocon2011.pdf"><em><strong>Configuring mahout Clustering Jobs</strong></em></a></p>
<p>今度はMahoutが出てきました。はやりのものが満載です。
まぁ、MahoutもLuceneのインデックスを利用するという話もありますので。
スライドはクラスタリングとはどういうものか、Mahoutの説明とテキストクラスタリング処理のお話、最後はstuckoverflowでのMahoutとSolrの活用の仕方について。</p>
<hr>
<p>ということで、英語力がない中、かなり流し読みな感じですが、あとで思い出すために書きだして見ました。
何かの役に立てれば幸いです。</p>
<p>他に、こんなスライドが面白かったとか、このスライドについても書いてほしいなどあれば、コメントください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Bookscanを使ってみました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/02/bookscan%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 02 Nov 2011 18:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/02/bookscan%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>Bookscanというサービスがあります。 書籍を電子化（PDF）して原本を破棄してくれるサービスです。 電子書籍にはずっと興味を持っていました</description>
      <content:encoded><p><a href="http://www.bookscan.co.jp/?PHPSESSID=d512d1ff0e4682d92b088c8fc7072623">Bookscan</a>というサービスがあります。
書籍を電子化（PDF）して原本を破棄してくれるサービスです。</p>
<p>電子書籍にはずっと興味を持っていました。
技術書を購入するのですが、技術書は３００ページ超の大きな書籍が大半です。
また、日本語の技術書については、なかなか電子書籍が見つからないもしくは、電子化されるのが遅いとうのが現状です。
海外では、<a href="http://www.manning.com/?PHPSESSID=d512d1ff0e4682d92b088c8fc7072623">Manning</a>など、電子書籍も同時に発売（もしくは、製本前から電子書籍が売られている）サイトがありますが、やはり英語の書籍はハードルが高いなぁと。</p>
<p>そんな中、Bookscanの噂を耳にしました。
ということで、どんどん、積ん読になっていく技術書だったので、思い切って、電子化してみることにしました。
※残念ながらまだタブレット（電子書籍端末）は持っていないのですが。。。もうすぐ発売されるlenovoの7inchタブレットを待ちかねているところです。</p>
<p>Bookscanでは、１冊１００円で書籍をPDFにしてくれます。（ページ数により変わってくる）
この場合、あくまでも書籍をスキャンするだけで、検索などはできません。
OCR処理については別途１冊１００円（これもページ数による）のオプションが必要になります。
詳しいサービス内容は<a href="http://www.bookscan.co.jp/service.php">こちら</a>を御覧ください。
基本的には書籍をBookscanに発送してから約３ヶ月ほど電子化するのに時間がかかります。
これでは、すぐに見たい場合に手元にないという状態になってしまいます。
この電子化までの時間を短くできる、「プレミアム会員」というオプションが存在します。
月額固定の費用（９９８０円）を支払うことで、５０冊（※ページ数により１冊の単位が異なります）まで無料で、「スキャン＋OCR＋書籍名をファイル名」のPDFファイルを書籍到着後１週間以内で作成してもらうことができます。
また、プレミアム会員は月単位での契約が可能なようで、１ヶ月（＝５０冊）だけ早く電子化してもらうといった利用方法もできるようです。</p>
<p>で、実際にプレミアム会員となって４９冊の書籍を送付して電子化してもらいました。
先週金曜日に発送、Bookscanに書籍が到着したのが翌土曜日、電子化が完了したのが火曜日の夜という感じでした。
すごくはやい！かなり感動してます。自炊自体をしたことがないので、出来上がりが自炊と比べてという比較はできないのですが、ざっと見ている感じでは全然問題なく読めそうです。</p>
<p>ただ、１冊の２ページだけ、ページの端が文章の途中で切れているPDFがありました。
この書籍に関しては、サポートサービスにお願いして、再スキャンをしてもらいました。
昨日夜に頼んで今日の午後には問題なく電子化されているというサービスの質でとても満足しています。
なお、原本の破棄が電子化後から１０日後に行われるため、問題ないかの確認はそれまでに行う必要があるようです。
大量の本を送る場合は小分けに送ったほうが確認するのが楽になると思います。（送料は小分けにした分、かかってしまいますが。）</p>
<p>書籍の山になっていた机がすっきりして、とても満足しています。
せっかく電子化したので、早くタブレットが欲しいなと思う今日この頃。。。
基本的には今後は電子書籍を購入するようにしていく予定なので、プレミアム会員を継続することはないと思いますが、
家に溜まっている漫画を電子化するのもありかなぁと思っているところです
漫画の場合はOCRに掛ける必要もないですし。</p>
<p>個人的にはBookscanさんには悪いのですが、もっと電子書籍が増えると助かります。
技術書はページ数が多く、重たいうえ、実際に利用するときは検索したくなることが多々ありますので。
あとは、電子書籍を検索できるサイトがあるといいなと思ってもいるところです。
現在は、各出版社が個別に電子書籍のHPを持っている状況で、会社によっては検索すらできないです。
電子書籍があるかないかだけでも簡単に検索できると、いいなと思っています。（小説などはまた別なのかもしれないですが。）
みんなはどう思ってるんですかねぇ？検索したくないですか？</p>
</content:encoded>
    </item>
    
    <item>
      <title>1.2.0リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/31/1-2-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 31 Oct 2011 15:46:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/31/1-2-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>lucene-gosenの最新版（1.2.0）をリリースしました。 プロジェクトページよりダウンロードが可能です。 新規追加機能についてはこちら</description>
      <content:encoded><p>lucene-gosenの最新版（1.2.0）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>新規追加機能については<a href="http://johtani.jugem.jp/?eid=38">こちらのエントリ</a>を御覧ください。</p>
<p>バグなどありましたら、容赦なく報告をいただけると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書の外部化とLucene/Solr3.4対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/26/%E8%BE%9E%E6%9B%B8%E3%81%AE%E5%A4%96%E9%83%A8%E5%8C%96%E3%81%A8lucene-solr3-4%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Wed, 26 Oct 2011 01:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/26/%E8%BE%9E%E6%9B%B8%E3%81%AE%E5%A4%96%E9%83%A8%E5%8C%96%E3%81%A8lucene-solr3-4%E5%AF%BE%E5%BF%9C/</guid>
      <description>すぐやりますと言いつつ、はや1ヶ月。。。 腰が重い、ダメエンジニアですね。。。 すみませんでした。。。 ようやくtrunkにコミットしました。 すぐ</description>
      <content:encoded><p><a href="http://johtani.jugem.jp/?eid=22?PHPSESSID=744a73e2820c6e94289eb9ba777c1ff2">すぐやりますと言いつつ</a>、はや1ヶ月。。。
腰が重い、ダメエンジニアですね。。。</p>
<p>すみませんでした。。。
ようやくtrunkにコミットしました。
すぐにリリース版を用意すると思います。</p>
<p>1ヶ月もあいてしまったので、追加した機能に関するまとめと、
用途別の利用方法を記載しておきます。
（lucene-gosenのWikiにもそろそろ書かないとなぁ。日本語でもいいから。）</p>
<p><strong>追加した機能</strong></p>
<hr>
これまでのlucene-gosenはjarに辞書を含む形でライブラリを提供していました。
ただ、この場合、カスタム辞書を利用している環境ではカスタム辞書を修正し、ビルドしなおすたびに、
jarファイルを作成しなければなりません。
また、jarファイルをSolrなどに配布する必要も出てきます。
この手間を考慮して、辞書を外部ディレクトリで指定することができるようにしたものが
今回の修正になります。
また、修正の過程で同一VM内で異なる辞書を使えるようにする機能も副産物として生まれました。
今回追加した機能は次のようなものになります。
<ul>
<li>辞書を含まないjarのビルドおよび提供</li>
<li>ディレクトリ指定による辞書の指定</li>
<li>同一VM内での複数辞書の利用</li>
<li>辞書リビルド用のAntターゲットの追加</li>
<li>Lucene/Solr jarファイルの最新化（3.4.0対応）</li>
</ul>
<p>ディレクトリ指定による辞書の指定ですが、以下のような形になります。
まずは、LuceneのTokenizerでの指定方法です。
「辞書のディレクトリ」という引数が追加になっています。
ここに、辞書ディレクトリ（*.senファイルが存在するディレクトリ）を相対/絶対パスで指定します。</p>
<pre><code>
...
Tokenizer tokenizer = new JapaneseTokenizer(reader, null, &quot;辞書のディレクトリ&quot;);
...
</code></pre><p>つぎは、Solrでの設定の方法です。
schema.xmlにて次のような設定を行います。</p>
<pre><code>
...
&lt;fieldType name=&quot;text_ja&quot; ...&gt;
  &lt;analyzer&gt;
    ...
    &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; compositePOS=&quot;compositePOS.txt&quot; dictionaryDir=&quot;辞書のディレクトリ&quot;/&amp;gt;
    ...
  &lt;/analyzer&gt;
&lt;/fieldType&gt;
...  
</code></pre><p>schema.xmlの設定については、<a href="https://code.google.com/p/lucene-gosen/source/browse/trunk/example/schema.xml.snippet">example/schema.xml.snippet</a>にも説明がありますので、こちらもあわせて参考に。
なお、Solrの設定については、先ほどのLuceneでの辞書のディレクトリの指定方法（絶対/相対パス）に加えて、
<em>$SOLR_HOME/conf</em> からの相対パスでの指定も可能になっています。</p>
<p><strong>Antのターゲットについて</strong></p>
<hr>
辞書なしjarファイルを作成するターゲットなどを追加しています。
実際に追加したターゲットは以下のとおりです。
<table class="list_view">
<thead>
<tr>
<th>ターゲット名</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>nodic-jar</td>
<td>辞書なしのjarファイルを生成するためのターゲット。辞書のダウンロード、コンパイルは行いません。</td>
</tr>
<tr class="specalt">
<td class="alt">rebuild-dic</td>
<td class="alt">lucene-gosenのビルド済み辞書（.senファイル）を削除してから辞書のコンパイル（ビルド）を行います。-Ddictypeにより辞書のタイプ（ipadic|naist-chasen）の指定が必要です。また、-Dcustom.dicsによりカスタム辞書の指定もあわせて可能です。</td>
</tr>
<tr class="spec">
<td>build-dic-ipadic</td>
<td>テスト用に追加。-Ddictype=ipadicを指定してbuild-dicを実行。</td>
</tr>
<tr class="specalt">
<td class="alt">build-dic-naist-chasen</td>
<td class="alt">ついでに追加。-Ddictype=naist-chasenを指定してbuild-dicを実行。</td>
</tr>
</tbody>
</table>
<p>最後の２つはあまり関係ありません。内部的に有ると便利だったため、作りました。
重要なのは最初の２つです。
ひとつめの「nodic-jar」は辞書を含まないjarファイルをビルドします。
このjarファイル＋辞書の入ったディレクトリを利用することで、辞書の外部化が可能となります。</p>
<p>そして、「rebuild-dic」です。こちらは、<a href="http://johtani.jugem.jp/?eid=4">以前記事に書きました</a>が、カスタム辞書のコンパイルが思いの外面倒だったので、ターゲットを追加しました。
次のように指定することで、辞書のリビルドが可能です。</p>
<pre><code>
$ ant -Ddictype=naist-chasen -Dcustom.dcs=&quot;custom1.csv custom2.csv&quot; rebuild-dic
</code></pre><p><strong>提供されるjarファイルについて</strong><hr></p>
<p>提供されるjarファイルは次のようになる予定です。
1番目のjarファイルが今回追加になる、辞書なしのjarファイルになります。</p>
<ul>
<li>lucene-gosen-1.x.x.jar</li>
<li>lucene-gosen-1.x.x-ipadic.jar</li>
<li>lucene-gosen-1.x.x-naist-chasen.jar</li>
</ul>
<p><strong>用途別の利用方法</strong><hr>
利用用途別に利用するjarファイルやantのターゲットを利用シーンを交えて想定を書いてみます。
Solrでの利用シーンを想定します。</p>
<p><em>**お手軽に使う。辞書ありjarファイルで一発インストール。**</em>
これまでどおりの使い方になります。
辞書込みのjarファイルを利用すれば、すぐに利用可能になります。</p>
<p><em>**カスタム辞書を使い倒す。定期的に辞書をメンテナンス。**</em>
定期的にシステム固有の単語が増える（例：製品名、新語など）場合です。</p>
<ul>
<li>利用するjar：lucene-gosen-1.x.x.jar</li>
<li>辞書のコンパイル＋配置：ant -Ddictype=naist-chasen -Dcustom.dics=&quot;custom1.csv&rdquo; rebuild-dir</li>
<li>Solrの該当コアのRELOAD</li>
</ul>
<p>Solrのマルチコア環境を利用します。なお、sharedLib設定にlucene-gosen-1.x.x.jarを配置すると、辞書の再読み込みができないので注意してください。
設定は、上記のようにTokenizerFactoryの設定でdictionaryDirにて辞書のディレクトリを設定しておきます。
カスタム辞書に単語を追加後、antにて、辞書のリビルドを行います。
リビルドした辞書ファイルを必要に応じて対象の辞書ディレクトリにコピーします。（ビルド後のディレクトリをそのまま利用している場合はコピーの必要はないです。）
最後に、Solrの該当コアのリロードを行います。（リロードの仕方は<a href="http://wiki.apache.org/solr/CoreAdmin?PHPSESSID=d9edccf00e1c13655f96b005f36819c4">こちらを参考に</a>。）
コアのリロードにより、辞書の再読み込みが行われるので、リロード後から新しい辞書が適用されます。</p>
<p><em><strong>異なる辞書を使い倒す。TokenizerごとにdictionaryDir設定するぞ</strong></em>
1つのSolrで異なる辞書を使ったフィールドを使いたい場合です。
ipadicとnaist-chasenといった異なる場合はあまり想定できないですが、カスタム辞書の部分が異なるという形が想定できるでしょうか。（例：製品名のフィールド、企業名のフィールド。。。など）</p>
<ul>
<li>利用するjar：lucene-gosen-1.x.x.jar</li>
<li>設定：schema.xmlに異なるdictionaryDirを設定したTokenizerFactoryを設定</li>
</ul>
<p>上記、カスタム辞書の定期更新も一緒に行うことも可能です。コアをリロードすれば、リロードしたコアで利用している
辞書がすべてリロードされます。</p>
<p><strong>最後に</strong><hr>
遅くなってしまいましたが、ようやく、trunkにコミットしました。
できるだけ速く、リリースしますので、もう少々お待ちを。</p>
<p>Solrのconfディレクトリからの指定については、<a href="http://twitter.com/shinobu_aoki">＠shinobu_aoki</a>さんにパッチを提供してもらいました。
また、trunkにコミットしていないパッチを適用して記事を書いてくれた方もいらっしゃいました。こちらもあわせて参考に。私より説明が上手です。
<a href="http://www.mwsoft.jp/programming/munou/lucene_gosen.html">Java製形態素解析ライブラリ「lucene-gosen」を試してみる</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>JJUG CCC 2011 Fallに参加してきました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/18/jjug-ccc-2011-fall%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 18 Oct 2011 16:20:43 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/18/jjug-ccc-2011-fall%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>JJUG CCC 2011 Fallに参加してきました。 個人的にはかなり久々のJavaのカンファレンスです。（※あくまで「Javaの」という話で。SolrやHad</description>
      <content:encoded><p>JJUG CCC 2011 Fallに参加してきました。
個人的にはかなり久々のJavaのカンファレンスです。（※あくまで「Javaの」という話で。SolrやHadoopとは別という意味です。）</p>
<p><a href="http://www.java-users.jp/contents/events/ccc2011fall/">概要やタイムテーブルはこちら</a>。
予定があったので、残念ながら最後の２つのセッション（Scala、Twitter）しか参加できませんでした。
Hadoopの話も聞きたかったのですが、しょうがないかと。
いつものごとく、メモを個人的に取ったので。</p>
<pre><code>
★楽々Scalaプログラミング
　浅海

　◎OOPからOFPへの道しるべ
　　・関数型の使い所、プログラミングのコツ
　　・つながりの部分

　◎最近の活動
　　モデリングからモデル駆動によるソース生成などに利用。
　　Scala、2008年から使い始めている

　◎Scalaと他の機能との比較図
　◎Scalaの用途
　　・高い生産性
　　　体感で3倍。コード量が少なくなる。
　　　IDEのサポートを考えるとまだ、Java+Eclipseのほうが高い！
　　・DSL
　　　ドメインモデルの記述
　　　フレームワークAPI
　　・並行プログラミング
　　　Many Core
　　　　消費電力的に複数のコアで動かす＝関数型のほうが並行性が上がる（スレッドさわるJavaだけだときつい）
　　　Parallel Everything

　　Object指向＋関数型が扱いやすい。

　◎関数型言語とは
　　・公開関数をあつかえる
　　　関数を値として扱える、引数と返却地に関数を渡せる。（やさしめ）
　　　　＜＝＞
　　　数学（ラムダ計算、圏論など）的にプログラムを記述できる（きびしめ）
　　ある程度使えるはず。厳しめの部分も
　◎関数型言語の長所と短所
　　長所
　　　・高階関数を使った技が使える
　　　　　List（？？？）

　　短所
　　　・メモリ、関数実行オーバーヘッド、スタックの大きさが読めない（再帰的）

　◎関数型言語の系譜
　　純粋関数型言語
　　　pure Lisp
　　伝統的関数型言語
　　　OCaml
　　新世代関数型言語
　　　Haskell
　　　Scala
　　　型クラス
　　　　代数データ型、モナド
　　　　Parametric polymorphism
　◎関数型でしたいこと
　　DSL（Domain Specific Lanuguage）
　　昨年の資料を参考に！
　◎準備
　　scalaの文法
　◎並列コレクション
　　スレッドを利用しないで並列に動作実行させるか？
　　※List().par.map(sitelen).sum
　　　これで並行実行させられる。
　◎Future
　　（これってJavaでもあるよね？＝あるよ）
　　処理を非同期で先行実行し、あとから結果を取得可能
　　Scalaではアクターライブラリでサポート（cala.actors）
　　　関数合成するとFutureの実行結果を取り出す所でブロックされる！！
　◎Promise
　　Futureより強力
　　関数合成すると、合成関数全体が非同期実行される！
　　scalazを利用してた

　◎5つのコツ
　　コツ１：　y=f(x)
　　　引数がひとつの関数が基本
　　　関数合成のビルディングブロックになるよ
　　　引数が複数ある関数は他の関数（map関数とか）で合成しにくい
　　　　対処方法：カリー化（関数の戻りを関数とする）
　　　　　addc(a: Int)(b: Int) : Int = a + b
　　　　　とすることで、
　　コツ２：分割統治
　　コツ３：演算は転換
　　　flatMapとかfoldLeftとか
　　※永続データ構造
　　　前に作ったデータ
　　コツ４：オブジェクトの世界と関数の世界を分ける
　　　更新指示書の形でObjectに渡す？
　　　関数型データ構造
　　コツ５：新しいデザインパターン
　　　関数型プログラミングの用語
　　　　・Functor（関手）
　　　　・Subgroup（半群）
　　　　・Monoid（モノイド）
　　　　・Monad（モナド）
　　　OOPのデザインパターン的に考えれば、数学的なところまで理解しなくてもよさそう？
★Twitterとオープンソース　@yusukey
　http://dev.twitter.com
　http://bit.ly/tdt-ja
　id：twj_dev

　◎OSSとの関わり
　　支援してます。
　　　Apache、Eclipse、Open Invention、JCP、OpenJDK
　　パフォーマンス系のためにもOpenJDKに参画してる
　◎Twitter API
　　オープンなAPIで無償提供
　　１３言語でAPIライブラリがあるよ
　◎Twitter4Jのこれまで
　　ほぼ全てをカバー
　　APIはだいぶ落ち着いてきていて、追加変更は少なくなってる。
　◎立ち位置
　　Twitterはコミュニティを活発にするためにもライブラリを出さないらしい。
　◎Twitter4Jのこれから
　　キャッシング
　　ストリーミングAPI利用を簡単に
　　モックテスト
　　　レートリミットの影響を受けると辛いから。。。
　　ツイート/ユーザの永続化機能
　　jClouds対応？
　　ライブラリからフレームワークへ
　　半公式ライブラリへ
　　　github/twitter/twitter4jへ
　　JSR Social API
　　　Twitter4Jが参照実装に？？？
　スケーラビリティの話。
　Hadoopはユーザのデータ解析とかに使ってる。
　メッセージはキューイングしている。デモ。
　　memcachedプロトコルなんだけど、値を取ると値が消えるよ。（memcachedとは動きが違うよねー。プロトコル的にはクライアントがいっぱいあっていいよねー）
　　RubyのGCがきつかったのでJavaベースに変更
　Kestrel（Scalaで記述。）
　　Kestrelのフォーカス外
　　　メッセージの順序保証（してないよ）
　　　トランザクション
　　memcachedプロトコルの拡張１
　　　ブロックフェッチ
　　　　コンシューマから取りに行くというのが特徴
　　memcachedプロトコルの拡張２
　　　リライアブルフェッチ
</code></pre><p>ということで感想。</p>
<p>Scalaは前から気になっていて、本までかってるのになかなか手を付けられていない始末でした。
そんな時にこのJJUGの話があったので、ちょうどいいと聞きに。
関数型やScalaのぼんやりしたイメージのみを持って聴いていたのですが、思った以上にキーワードが多くてついていくのがやっとというイメージ。
で、いつものごとく、Twitterでつぶやいていた所、色々な反応が。（わからずにつぶやいたのもあり、波紋を呼んだ模様）
お陰で、Scala関連の方たちをフォローできたので結果オーライでした。
話の内容自体はサラっとながす感じだったので、再度資料を見る＋Scalaをもっと勉強しないと理解出来ないなぁという印象でした。
ちなみに、<a href="http://www.slideshare.net/asami224/scala-9728001">資料はこちら</a>。</p>
<p>次は、最近Twitterの中の人になった山本さん（<a href="http://twitter.com/yusukey">@yusukey</a>）の話しを聞きました。
Twitter4Jの話と、TwitterのOSSへの関わりの話。あとは、実際にTwitterが作成しているOSSのひとつKestrelについて。
TwitterはRubyベースで色々と作ってきていたのだが、Java（JVM上で動く言語（Scalaなど））に徐々にシフトしているというのが一番の印象です。あとは、Lucene、Hadoopなど私の興味のあるOSSも利用しているなど。
Kestrelについてはデモを交えながら、実際の動きを説明されていたのでとてもわかり易かったです。
プレゼン中もTweetを表示しながらという、さすがTwitterの中の人という印象でした。
Twitterでは少し前から知り合いだったのですが、実際にお会いできた（イケメンでしたよ！）のも収穫でした。
次はStormの話も聞きたいかなぁ</p>
<p>で、２セッション（山本さんの話のあとにLT大会にも参加）だけですが、全体の感想を。
「クロスコミュニティカンファレンス」というだけあり、Java以外の話題（CouchDBとか名前がありました）もちらほらあったようです。
ただ、最近の他の勉強会やカンファレンスに比べると人が少ない印象でした。
私が参加したセッションは比較的大きな部屋だったせいもあり、40～50人くらい入っていたと思うのですが、ガラガラな印象でした。良い意味で、Javaも安定してきているため、参加者が少なかったのかなぁと
あとは、年齢層がHadoopなどに比べると高めかなぁという印象（かく言う私も年齢層を上げている一人ですが。。。）。Javaも長いですからねぇ。
午前中から参加していると違った印象だったかもしれないですが。。。
あと、初めて行った会場で、しかも大きめの施設でした。午後の途中から行ったせいもあるかもしれないですが、
案内が出ていなくて若干迷子に。。。
ただ、セッションの部屋自体は各席に机があり、電源も比較的多めにあったので、PC持ち込みでメモを取るには最適でした。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenで文章からキーワード抽出（イレギュラー？）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/12/lucene-gosen%E3%81%A7%E6%96%87%E7%AB%A0%E3%81%8B%E3%82%89%E3%82%AD%E3%83%BC%E3%83%AF%E3%83%BC%E3%83%89%E6%8A%BD%E5%87%BA%E3%82%A4%E3%83%AC%E3%82%AE%E3%83%A5%E3%83%A9%E3%83%BC/</link>
      <pubDate>Wed, 12 Oct 2011 12:17:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/12/lucene-gosen%E3%81%A7%E6%96%87%E7%AB%A0%E3%81%8B%E3%82%89%E3%82%AD%E3%83%BC%E3%83%AF%E3%83%BC%E3%83%89%E6%8A%BD%E5%87%BA%E3%82%A4%E3%83%AC%E3%82%AE%E3%83%A5%E3%83%A9%E3%83%BC/</guid>
      <description>昨日、文章から特定の単語（リストあり）を探したいという話を聞き、lucene-gosenでもできるねぇという話になりました。 まぁ、考えてみれ</description>
      <content:encoded><p>昨日、文章から特定の単語（リストあり）を探したいという話を聞き、lucene-gosenでもできるねぇという話になりました。
まぁ、考えてみればごくごく当たり前なのですが。。。（その筋の方たちにしてみれば常識なのかもしれないですが。。。）
一応やってみたので、こんなこともできるなという一例ですということで、記録を残しておきます。</p>
<p>今回の例文として<a href="http://www.kantei.go.jp/jp/noda/statement/201109/13syosin.html">野田首相の所信表明演説の一部</a>を活用させてもらいます。
単語のリストは次のようにします。</p>
<ul>
<li>内閣総理大臣</li>
<li>正心誠意</li>
<li>東日本</li>
<li>日本</li>
</ul>
<p>今回も結果をわかりやすくするためにSolrのanalysis画面を利用します。
作業手順は以下のとおり。</p>
<ol>
<li>dictionary.csvの編集</li>
<li>辞書のコンパイル</li>
<li>fieldTypeの定義（Solrのschema.xmlの設定）</li>
<li>文章からキーワード抽出（Solrのanalysis画面）</li>
</ol>
<div>
### **1.dictionary.csvの編集**
今回はnaist-chasenディレクトリで作業します。
なお、今回利用するlucene-gosenは[ここ](http://johtani.jugem.jp/?eid=21)で紹介した辞書分離バージョンです。（はやくtrunkにコミットせねば。。。）
dictionary.csvを先ほど上げた単語だけのエントリに変更します。
キーワードだけを抽出したいので、他の単語は必要ないからです。
```
<p>&ldquo;内閣総理大臣&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;内閣総理大臣&rdquo;,&ldquo;ナイカクソウリダイジン&rdquo;,&ldquo;ナイカクソウリダイジン&rdquo;
&ldquo;正心誠意&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;正心誠意&rdquo;,&ldquo;セイシンセイイ&rdquo;,&ldquo;セイシンセイイ&rdquo;
&ldquo;東日本&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;東日本&rdquo;,&ldquo;ヒガシニホン&rdquo;,&ldquo;ヒガシニホン&rdquo;
&ldquo;日本&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;日本&rdquo;,&ldquo;ニホン&rdquo;,&ldquo;ニホン&rdquo;</p>
<pre><code>&lt;/div&gt;

&lt;div&gt;
### **2.辞書のコンパイル**
先ほど作成した辞書をコンパイルし、lucene-gosen用バイナリ辞書を作成します。
</code></pre><p>$ cd $LUCENE_GOSEN_HOME¥dictionary
$ ant -Ddictype=naist-chasen clean-sen compile</p>
<pre><code>&lt;/div&gt;

&lt;div&gt;
### **3.fieldTypeの定義（Solrのschema.xmlの設定）**
Solrのschema.xmlにlucene-gosenを利用するフィールドタイプを定義します。
追加するのは次の通り
</code></pre><pre><code>&lt;fieldType name=&quot;text_ja&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot; autoGeneratePhraseQueries=&quot;false&quot;&amp;gt;
  &lt;analyzer&amp;gt;
    &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; compositePOS=&quot;compositePOS.txt&quot; dictionaryDir=&quot;keyword-dic&quot;/&amp;gt;
    &lt;filter class=&quot;solr.JapanesePartOfSpeechKeepFilterFactory&quot; tags=&quot;keeptags_ja.txt&quot; enablePositionIncrements=&quot;true&quot;/&amp;gt;
  &lt;/analyzer&amp;gt;
&lt;/fieldType&amp;gt;
</code></pre>
<pre><code>
また、ここで定義しているcompositePOS.txt、keeptags_ja.txtは次のようになります。


compositePOS.txt
</code></pre><p>未知語</p>
<pre><code>
keeptags_ja.txt
</code></pre><p>名詞-一般</p>
<pre><code>
未知語がバラバラに出現しないようにして見やすくするためと、必要な単語（今回は「名詞-一般」しか利用しないため。）だけを抽出したいための設定です。

&lt;/div&gt;

&lt;div&gt;
**### 4.文章からキーワード抽出（Solrのanalysis画面）**
あとは、analysis画面で解析して見るだけになります。

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111012/20111012_2085981.png" alt="キーワード抽出結果"/>
    </div>
    <a href="/images/entries/20111012/20111012_2085981.png" itemprop="contentUrl"></a>
  </figure>
</div>


ということで、辞書に登録された単語だけが抽出されてますね。
この例ではインデックスに登録となりますが。
ただし、「東日本」「日本」のような一部を含む単語の場合、「東日本」が見つかった場合は「日本」は抽出されません。
あくまでも、ベストな解が見つかるのみという形です。
すべての単語を出したい場合はもう少しやり方を考えたほうがいいかもしれません。
（まぁ、このやり方でキーワードを抽出するかも考えたほうがいいかもしれませんが。。。）

&lt;/div&gt;
最近、頭が硬くなってきてるなぁと実感してしまいました。まぁ、こんな使い方もあるかなぁと。
もっと頭を柔らかくして問題を解けるけるようになりたいなぁと。


</code></pre></content:encoded>
    </item>
    
    <item>
      <title>「7つの言語　7つの世界」 Io 1日目(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/07/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-io-1%E6%97%A5%E7%9B%AE/</link>
      <pubDate>Fri, 07 Oct 2011 21:22:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/07/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-io-1%E6%97%A5%E7%9B%AE/</guid>
      <description>ちょっと間があいてしまったが、ようやくIoの1日目を終了。 なれない感じの言語なので苦労しました。 今回もセルフスタディの私の回答が最後の方に記</description>
      <content:encoded><p>ちょっと間があいてしまったが、ようやくIoの1日目を終了。
なれない感じの言語なので苦労しました。
<span style="color:#FF0000">今回もセルフスタディの私の回答が最後の方に記載されてます。</span>見たくない人は気をつけてください。
ツッコミ大募集です。コメント欄にどしどしコメントください。そこは違うだろ？こっちのほうがいいのでは？という感じで。</p>
<h2 id="感想">感想：</h2>
<p>プロトタイプ言語であり、すべての言語はクローンだそうです。</p>
<h3 id="手始めに">○手始めに</h3>
<div>　　スロット、オブジェクトのクローンについて。
　　メッセージとその送信について。
　　あくまでもオブジェクトだけの世界。クローンであり、クラスのインスタンスではない。（まだつかめない。。。）
</div>
### ○オブジェクト、プロトタイプ、継承
<div>
　　slotNamesで取れないけど、継承してる。クローンで元をコピーしてるだけか？
　　頭文字が大文字かどうかがtypeかどうかの違い。大文字＝type（クラスもどき＝）
　　オブジェクト＝スロットの入れ物
　　オブジェクトのクローン＝オブジェクトのチェーン？（Object ＝＞ Vehicle ＝＞ Car ＝＞ ferrari）
</div>
<div style="color:#0000FF">　　疑問点：
　　　存在しないスロット名をgetSlot()で呼び出したら？＝＞nilが返る
　　　存在しないスロットをメッセージとして送信したら？＝＞Exceptionが出る
</div>
<h3 id="メソッド">○メソッド</h3>
<div>
　　method()で定義
　　メソッドもオブジェクト＝スロットに入れることができる（まだつかめない。。。）
　　メソッドをスロットから呼び出すと実行される。
</div>
<h3 id="リストとマップ">○リストとマップ</h3>
<div>
　　リスト、マップは簡単。幾つか（スタックやキュー）のメソッドも用意されてる。
</div>
<h3 id="truefalsenilsingleton">○true、false、nil、singleton</h3>
<div>
　　cloneメソッドを最定義することでsingletonの動作にする。（言われてみれば当たり前か）
　　Object cloneをオーバーライドすることもできるが、プロセス停止などしないとダメ。（恐るべし。。。）
</div>
<h3 id="インタビュー">○インタビュー</h3>
<div>
　　SIMD（Single Instruction Multiple Data）http://ja.wikipedia.org/wiki/SIMD
</div>
<h3 id="やりながら感想">やりながら感想：</h3>
<div>
　　シンタックスが全然違うのでかなり戸惑い。
　　ただ、考え方はシンプル。slotとか。
　　まだ、予約語がわかってない（cloneとかprintとかslotNamesとか）ので。
</div>
<h2 id="セルフスタディ">★セルフスタディ</h2>
<h3 id="探してみよう">（探してみよう）</h3>
<p><strong>○Ioのいくつかの問題点の例</strong>
<span style="color:#FF0000">　　見つけられず。。。英語がダメダメ。。。</span>　　　　</p>
<p><strong>○質問に答えてくれるIoコミュニティ</strong></p>
<div>
　　ML：http://tech.groups.yahoo.com/group/iolanguage/
　　twitter：http://twitter.com/#!/iolanguage
</div>
**○Ioのイディオムに関するスタイルガイド**
<div>
　　Io Note（英語）
　　　[http://iota.flowsnake.org/](http://iota.flowsnake.org/)
　　Ioプログラミングガイド
　　　[http://iolanguage.com/scm/io/docs/IoGuide.html](http://iolanguage.com/scm/io/docs/IoGuide.html)
　　　[http://xole.net/docs/IoGuide_ja.html](http://xole.net/docs/IoGuide_ja.html)（日本語）
　　Ioスタイルガイド
　　　[http://en.wikibooks.org/wiki/Io_Programming/Io_Style_Guide](http://en.wikibooks.org/wiki/Io_Programming/Io_Style_Guide)
　　　[http://d.hatena.ne.jp/katzchang/20080819/p1](http://d.hatena.ne.jp/katzchang/20080819/p1)（日本語訳してくれてるブログ）
</div>
<h3 id="確認してみよう">（確認してみよう）</h3>
<p><strong>○1 + 1を評価してから、1 + &ldquo;one&quot;を評価する。Ioは強く型付けされた言語か？</strong></p>
<div>
　　```
<p>Io&gt; 1 + 1
==&gt; 2
Io&gt; 1 + &ldquo;one&rdquo;</p>
<h2 id="exception-argument-0-to-method--must-be-a-number-not-a-sequence">Exception: argument 0 to method &lsquo;+&rsquo; must be a Number, not a &lsquo;Sequence&rsquo;</h2>
<p>message &lsquo;+&rsquo; in &lsquo;Command Line&rsquo; on line 1</p>
<pre><code>ということで、型付けは強いです。
&lt;/div&gt;

**○0は真か偽か？空文字列はどうか？nilは真か偽か？**
&lt;div&gt;
</code></pre><p>Io&gt; 0
==&gt; 0
Io&gt; true and 0
==&gt; true
Io&gt; true and &quot;&rdquo;
==&gt; true
Io&gt; &quot;&rdquo;
==&gt;
Io&gt; true and nil
==&gt; false
Io&gt;</p>
<pre><code>0、空文字列はtrue、nilはfalseでした。
&lt;/div&gt;

**○プロトタイプに存在するスロットを確認するにはどうすればよいか？**

&lt;div&gt;
</code></pre><p>Io&gt; Highlander := Object clone
==&gt;  Highlander_0x1a233a0:
type             = &ldquo;Highlander&rdquo;</p>
<p>Io&gt; hoge := Highlander clone
==&gt;  Highlander_0x1a83630:</p>
<p>Io&gt; hoge proto slotNames
==&gt; list(type)
Io&gt; Highlander name := &ldquo;arnold&rdquo;
==&gt; arnold
Io&gt; hoge slotNames
==&gt; list()
Io&gt; hoge proto slotNames
==&gt; list(type, name)
Io&gt; Highlander slotNames
==&gt; list(type, name)
Io&gt; hoge name
==&gt; arnold
Io&gt; hoge getSlot(&ldquo;name&rdquo;)
==&gt; arnold</p>
<pre><code>こんな感じ。slotNamesでスロット名を確認し、スロット内部についてはgetSlotで確認できます。
&lt;/div&gt;
**○等号（=）、コロン等号（:=）および、コロンコロン等号（::=）の違いはなにか？どのようなときに使うか？**
&lt;div&gt;
&lt;table class=&quot;list_view&quot;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;演算子&lt;/th&gt;
&lt;th&gt;説明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;spec&quot;&gt;
&lt;td&gt;:::=&lt;/td&gt;
&lt;td&gt;スロットへの代入＋setterの追加&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;specalt&quot;&gt;
&lt;td class=&quot;alt&quot;&gt;:=&lt;/td&gt;
&lt;td class=&quot;alt&quot;&gt;スロットへの代入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;spec&quot;&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;スロットへの代入（ただし、スロットが存在しない場合は例外が発生）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
とまぁ、記載したが、実際に動かして確認しました。
</code></pre><p>Io&gt; Highlander := Object clone
==&gt;  Highlander_0x223b800:
type             = &ldquo;Highlander&rdquo;</p>
<p>Io&gt; hoge := Highlander clone
==&gt;  Highlander_0x21acd60:</p>
<p>Io&gt; hoge slotNames
==&gt; list()
Io&gt; hoge name ::= &ldquo;arnold&rdquo;
==&gt; arnold
Io&gt; hoge slotNames
==&gt; list(setName, name)
Io&gt; hoge sword := &ldquo;long sword&rdquo;
==&gt; long sword
Io&gt; hoge slotNames
==&gt; list(setName, name, sword)
Io&gt; hoge clothes = &ldquo;armor&rdquo;</p>
<h2 id="updating">Exception: Slot clothes not found. Must define slot using := operator before
updating.</h2>
<p>message &lsquo;updateSlot&rsquo; in &lsquo;Command Line&rsquo; on line 1</p>
<pre><code>&lt;/div&gt;

### （試してみよう）
**○ファイルからIoプログラムを実行せよ**
&lt;div&gt;
ファイル「great.io」を作成し、以下を入力。
</code></pre><p>Highlander := Object clone
Highlander giveWarCry := &ldquo;Woooooooo!!&rdquo; println
Highlander giveWarCry()</p>
<pre><code>で、以下のコマンドを実行する。

</code></pre><p>$ io great.io
Woooooooo!!</p>
<pre><code>&lt;/div&gt;


**○スロットの名前を指定して格納されているコードを実行せよ**
&lt;div&gt;
</code></pre><p>Io&gt; Masason := Object clone
==&gt;  Masason_0x20ae7d0:
type             = &ldquo;Masason&rdquo;</p>
<p>Io&gt; Masason recieveNiceTweet := method(&ldquo;やりましょう&rdquo; println)
==&gt; method(
&ldquo;やりましょう&rdquo; println
)
Io&gt; Masason getSlot(&ldquo;recieveNiceTweet&rdquo;)
==&gt; method(
&ldquo;やりましょう&rdquo; println
)
Io&gt; Masason getSlot(&ldquo;recieveNiceTweet&rdquo;) call
やりましょう
==&gt; やりましょう</p>
<pre><code>
callとgetSlotが鍵ですね。
&lt;/div&gt;



1日目の途中で間があいてしまったのでシンタックスを忘れてしまう始末。。。
ただ、そんなに込み入った記述もないので、最初に勉強することは少ないかもしれないです。
まだメソッドをあとから追加とか、cloneしたあとにまたプロトタイプを変更して反映可能など、考え方になれないもところがありますが、良い頭の体操になってます。
間を置かずに3日目まで行かないと！！
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>MBAセットアップ備忘録その４(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/07/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%94/</link>
      <pubDate>Fri, 07 Oct 2011 15:25:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/07/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%94/</guid>
      <description>久々に、MBAの備忘録です。 あれから色々入れました。 なので、入れたソフトの一覧を追加。 AIR：TweetDeckをインストールするのに必要だ</description>
      <content:encoded><p>久々に、MBAの備忘録です。
あれから色々入れました。
なので、入れたソフトの一覧を追加。</p>
<ul>
<li>AIR：TweetDeckをインストールするのに必要だったため</lI></li>
<li>TweetDeck（AIR版）：TwitterとFacebookを同じ画面で見ることができるため。</lI></li>
<li>TextWrangler：テキストエディタ（まだあんまり使ってない）</lI></li>
<li>Skitch：Evernoteに買収されて無料に。先日のSolr管理画面の説明書きにもちょっと使用</lI></li>
<li>ClamXav：フリーのセキュリティソフト。</lI></li>
<li>SourceTree：Twitterで知ったMac用のgit、mercurialのクライアントソフト。<a href="http://blogs.atlassian.jp/2011/10/sourcetree-git-hg-client-by-atlassian-j.html">AtlassianがSourceTreeを買収したのを機に一時的に無料公開されていたので。</a></lI></li>
<li>Firefox：基本はSafariを使ってます。WindowsではChromeを使ってます。ただ、Webページを表示されていない部分もまるごとキャプチャするのにChromeのプラグインだとダメだったので入れました。今のところキャプチャ用のブラウザです。</lI></li>
<li>Full Screen enable for Eclipse on Lion：Eclipseをフルスクリーン対応するためのプラグイン</lI></li>
<li>MercurialEclipse：bitbucketにアカウントをもっているので。</lI></li>
<li>mercurial：pipを使ってインストール。MercurialEclipseでコマンドが必要だから。「sudo pip install Mercurial」</lI></li>
<li>pip：mercurialをインストールするためにインストール。「sudo easy_install pip」でインストール</lI></li>
<li>iStat pro（ウィジェット）：<a href="http://www.apple.com/jp/downloads/dashboard/status/istatpro.html">CPUなどの情報を表示するウィジェット</a>。ウィジェットがアップルのサイト（AppStoreではない）から検索できるというのを人から聞いた。ウィジェットってあんまり使われないのか？</li>
<li>Mac Blu-ray Player：現時点で有料ソフトはこれだけ。<a href="http://jp.macblurayplayer.com/index.htm">こちら。</a>もともとNASのデータのバックアップ用にBlu-rayのドライブ買ったんですが、せっかくだからMacにも使ってみようと思い、MacでBlu-rayを見るために購入しました。所用で長距離移動もあったので、新幹線で映画見てました。</li>
</ul>
<p>このくらいです。
ほんとはリンクを貼ればいいのですが、そこまでの元気がないので、ご勘弁を。</p>
<p>Skitchがおすすめです。簡単に画像編集＋説明書きが入れられて、矢印などがよくある画像編集の野暮ったさがないので、画面キャプチャに説明書くのに今後活躍する予定です。</p>
<p>あと、ソフトではなく、サービスなのですが、<a href="https://cacoo.com/">Cacoo</a>というサービスが便利です。
ここ最近、ちょっとした作図にはこのサービスを使うようにしています。オンラインで利用できるので、いろんな場所で編集できます。
また、図の作成もさることながら、図を共有することが可能です。
先日は、lucene-gosenが思ったように動かないという相談を受けて、このCacooでanalysis画面や設定をを貼ってもらったりしながら使いました。結構便利ですね。
ただ、付属のチャットを使ったのですが、このチャットの内容がテキスト形式でコピー出来なかったのが残念でした。今後の改修に期待ということで。</p>
<p>あと、セットアップとは少し違いますが、ジャケット買ってつけました。
ステッカーを直接貼るのがちょっと気になったので、ジャケットつけてステッカーを貼ってます。
ただ、ステッカーは思ったよりチープな感じが。。。ジャケットの中に貼らないとダメだったかも。まぁ、それが嫌でジャケット買ったんですが。。。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B0057BZK8Y/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B0057BZK8Y&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B0057BZK8Y/?tag=johtani-22">
      エアージャケットセット for Macbook Air 13inch(クリア)PMC-81
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>Solrの新しい管理画面（Solr4.x trunk系）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/05/solr%E3%81%AE%E6%96%B0%E3%81%97%E3%81%84%E7%AE%A1%E7%90%86%E7%94%BB%E9%9D%A2solr4-x-trunk%E7%B3%BB/</link>
      <pubDate>Wed, 05 Oct 2011 19:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/05/solr%E3%81%AE%E6%96%B0%E3%81%97%E3%81%84%E7%AE%A1%E7%90%86%E7%94%BB%E9%9D%A2solr4-x-trunk%E7%B3%BB/</guid>
      <description>Lucene/SolrのMLでSolrの管理画面を新しくするというチケットが流れていたのでちょっと触って見ました。 ほんとにちょっと触っただけ</description>
      <content:encoded><p>Lucene/SolrのMLでSolrの管理画面を新しくするというチケットが流れていたのでちょっと触って見ました。
ほんとにちょっと触っただけですが、いくつかキャプチャ撮ってみたので、アップしときます。
※以下ではサムネイル画像に元画像（100Kくらいの画像）へのリンクが設定されています。携帯などでは見づらいかもしれませんが、ご容赦を。</p>
<p>URLは旧管理画面とことなり、http://localhost:8983/solr/になります。</p>
<hr>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075207.jpg" alt="新管理画面：トップ画面"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075207.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
まずはトップ画面
ダッシュボードと呼ばれるトップ画面。メモリの利用率や起動してからの時間、Luceneなどのバージョンが表示されます。<br style="clear:both" />
<br style="clear">
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075141.png" alt="新管理画面：クエリ実行結果"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075141.png" itemprop="contentUrl"></a>
  </figure>
</div>
次は検索画面すっきりしてます。facetが指定できるようになったのは大きいかな。ただし、facet.fieldを複数指定などができないが。結果についてはとくに指定がなければXMLで帰ってきます。ただ、パラメータの追加ができなくなってる気がするなぁ<br style="clear:both" />
<br style="clear"/>
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075230.png" alt="新管理画面：クエリ接続エラー"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075230.png" itemprop="contentUrl"></a>
  </figure>
</div>
ちなみに、Solrを止めて検索したらこんな感じの画面になりました。クエリの実行ならこのようにエラーがわかったのですが、停止後に左のメニューにあるSchemaなどをクリックしても白い画面が出るだけで、エラーかどうかがわかりにくいです。<br style="clear:both" />
<br style="clear"/>
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075122.png" alt="新管理画面：Analysisのサンプル（lucene-gosen）"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075122.png" itemprop="contentUrl"></a>
  </figure>
</div>
Analysis画面。入力画面がシンプルになりました。フィールド名はリストで表示されるので選択するだけです。あとは、これまでどおり。サンプルはlucene-gosenの解析結果です。ハイライトもきちんと表示されます。ただし、長い文章の場合は結果部分だけがスクロールできる形になり、ちょっとわかりにくかったです。<br style="clear:both" />
<br style="clear"/>
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075142.png" alt="新管理画面：Analysis失敗"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075142.png" itemprop="contentUrl"></a>
  </figure>
</div>
Analysisの入力画面を表示したあとにSolrを停止して解析してみたらこんなエラー画面が出ました。ちなみに、その後、画面を切り替えずにSolrを起動して解析したら、赤い帯のエラーは出たままでした。一度別画面にすれば、元に戻りますが。<br style="clear:both" />
<br style="clear"/>
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075140.png" alt="新管理画面：キャッシュの状態確認"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075140.png" itemprop="contentUrl"></a>
  </figure>
</div>
Pluginsの画面（旧管理画面のstatisticsに相当）。
キャッシュの状態が確認できます。今まであった画面と情報的には一緒かと。一段カテゴリ（CACHEとかCOREとか）の選択ができるようになり、見やすくなりました。
<br style="clear:both" />
<br style="clear"/>
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075143.png" alt="新管理画面：updatehandlerの状態。ドキュメント数とか"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075143.png" itemprop="contentUrl"></a>
  </figure>
</div>
同じくPluginsの画面。
こちらはupdateHandlerについての情報です。commit数やoptimizeの回数、updateして、commitする前の状態のドキュメント数などが表示されます。前より表示される項目が多くなってるかな？<br style="clear:both" />
<br style="clear"/>
<hr>


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075208.jpg" alt="新管理画面：スキーマブラウザ"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075208.jpg" itemprop="contentUrl"></a>
  </figure>
</div>
最後はスキーマブラウザこの画面が一番良くなっています。旧管理画面では、フィールド名がすべて大文字で表示され、しかもソートがされていない状態だったため、ダイナミックフィールドを利用しているとフィールドを探すのが一苦労でした。
今回は、プルダウンでフィールドやフィールドタイプのリストが表示され、辞書順で並んでいます。Filterなどもわかりやすい表示になっているかと。
<br style="clear:both" />
<br style="clear"/>
<hr>
おまけ


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111005/20111005_2075181.png" alt="Solritasと呼ばれるサンプル画面"/>
    </div>
    <a href="/images/entries/20111005/20111005_2075181.png" itemprop="contentUrl"></a>
  </figure>
</div>
Solritasと呼ばれるVelocityを使った、3.x系で入ってきた新しいサンプル画面です。URLはhttp://localhost:8983/solr/browseです。ファセットなどを使った簡単なサンプル画面なので、検索結果画面でこんなことができるというデモにも使えるかと。ただ、これも旧管理画面よりはましですが、デザインが。。。<br style="clear:both" />
<p>とまぁ、簡単ですが、4.x系の管理画面をいくつか触ってみて、キャプチャをとって見ました。
デザインは前よりもすっきりしています。ただ、クエリについてはパラメータの追加ができなくなっているので、もう少し改良されるといいかなぁ（自分でやれよと言われそうですが。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>New SolrCloud Designの翻訳（その2）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/04/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE2/</link>
      <pubDate>Tue, 04 Oct 2011 18:32:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/04/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE2/</guid>
      <description>遅くなりましたが、続きです。 さらに英語力のなさを痛感して凹んでいるところですが、何かの役に立てばと恥を晒すところです。。。 一応、訳してみたの</description>
      <content:encoded><p>遅くなりましたが、続きです。
さらに英語力のなさを痛感して凹んでいるところですが、何かの役に立てばと恥を晒すところです。。。</p>
<p>一応、訳してみたのですが、訳すのに必死になってしまい、つながりがわかっていない点もちらほら。
このあと一旦見直しつつ、再度理解する「理解編」をアップしようかと思います。
できれば、シーケンス図とかも交えつつ。（そうしないと理解ができない可能性が。。。）
前回同様、原文は最後に付加しておきます。</p>
<h3 id="boot-strapping">Boot Strapping</h3>
<h4 id="cluster-startupクラスタの起動">Cluster Startup（クラスタの起動）</h4>
<p>ノードはZookeeperのホストとポートを指定することから始めます。
クラスタの最初のノードはクラスタのschema/configとクラスタの設定を指定するとこから開始します。
最初のノードはZookeeperに設定をアップロードしてクラスタをブートします。
クラスタは「ブートストラップ」状態です。
この状態ではノード-&gt;パーティションマッピングは計算されず、クラスタはクラスタ管理コマンド以外のどんなread/writeリクエストも受け付けません。</p>
<p>クラスタの最初のノード集合が起動した後、クラスタ管理コマンド（TBD記述？？？）が管理者によって発行されます。このコマンドは整数「partitions」パラメータを受け取り、次のステップを実行します。</p>
<ol>
<li>Cluster Lockを取得</li>
<li>「partitions」をパーティション数として割り当て</li>
<li>各パーティションのためのノードを取得</li>
<li>ZooKeeperのノード-&gt;パーティションマッピングを更新</li>
<li>Cluster Lockをリリース</li>
<li>全ノードに対して最新版のノード-&gt;パーティションマッピングをZooKeeper経由で更新させる</li>
</ol>
<h4 id="node-startup"><strong>Node Startup</strong></h4>
<p>ノードが起動すると、自分がすでに存在するシャードの一部かどうかZooKeeperでチェックします。
もし、ZooKeeperがノードのレコードを持っていない、またはどのシャードの一部でもないと判断したら、
ノードは後述の「New Node」のステップを実行します。すでに存在するノードの場合は後述の「Node Restart」のステップを実行します。</p>
<p><strong>New Node</strong></p>
<p>新しいノードはクラスタの一部ではなく、クラスタのキャパシティを増強するためのものです。</p>
<p>「auto_add_new_nodes」クラスタプロパティが「false」の場合、新しいノードはZooKeeperに「idle」として登録され、他のノードが参加してくれと言うまで待機します。
そうでない場合（auto_add_new_nods=true）は次のステップを実行します。</p>
<ol>
<li>Cluster Lockを取得します。</li>
<li>適切なnode-&gt;partitionエントリを選び出します。</li>
<li>利用可能なパーティションのリストをスキャンして「replication_factor」のノード数以下のパーティションのエントリを探します。複数ある場合はノード数が最小のエントリを選びます。それも一緒ならランダムに選びます。</li>
<li>全パーティションが「replication_factor」以上のノードを持っている場合、ノードはパーティションが最も多いものをスキャンします。複数ある場合はパーティション内のドキュメント数が最大のエントリを選びます。ドキュメント数が同一なら任意のエントリを選びます。</li>
<li>もし、選んだノード-&gt;パーティションエントリを現在のノードに移動させることでがクラスタのパーティション：ノード比率の最大値を小さくするなら、現在のエントリを返します。。それ以外の場合選ばれたエントリがないので、アルゴリズムは終了です。。</li>
<li>ZooKeeper内のノード-&gt;パーティションマッピングを更新します</li>
</ol>
</li>
<li>ZooKeeper内のノードステータスを「リカバリ」状態にします</li>
<li>Cluster Lockをリリースします</li>
<li>「リカバリ」はパーティションのリーダーから開始します。</li>
<li>リカバリが終了したら、再度、Cluster Lockを取得します。</li>
<li>元のエントリはZooKeeperのノード-&gt;パーティションマッピングから削除されます。</li>
<li>Cluster Lockをリリースします</li>
<li>元のノードはZooKeeperからノード-&gt;パーティションマッピングを更新させられます</li>
<li>ステップ1に戻ります。</li>
</ol>
<h4 id="node-restart"><strong>Node Restart</strong></h4>
<p>ノードの再起動とは次のいずれかを意味しています。</p>
<ul>
<li>JVMがクラッシュし、手動または自動でのリスタート</li>
<li>ノードが一時的にネットワークから切り離された。もしくは、ZooKeeperに接続できなかった（死んでいると思われた）。または、ある一定期間、リーダーからの更新を受信できなかった。</li>
<li>このシナリオが表す書き込み処理のライフサイクルの間にネットワークから分断された</li>
<li>ハード故障もしくはメンテナンスウインドウによりクラスタからノードが分断され、ノードをクラスタにrejoinさせるために起動した。</li>
</ul>
<p>ノードが各パーティションに対してメンバーであるパーティションのリストを読み、パーティションのリーダーがリカバリプロセスを実行する。その時、ノードは「auto_add_new_nods」プロパティをチェックして、「New Node」処理のステップを実行する。
これはクラスタが。。。（元の文章が切れてて意味が不明）</p>
<p>クライアントは標準的なSolrの更新形式を利用して書き込みできます。
書き込み処理はクラスタの任意のノードに送信されます。
ノードはハッシュ関数を利用して、どのパーティションに所属するか決めるためにrange-パーティションマッピングを使います。
ZooKeeperはシャードのリーダーを識別して、書き込み処理をそこに送ります。
SolrJはリーダーに対して書き込みを直接送信するための拡張がされています。</p>
<p>リーダーはPartitionバージョンの操作を割り当て、そのトランザクションログの操作を書き込み、シャードに属する他のノードにドキュメントバージョンハッシュを転送します。
ノードはインデックスにドキュメントハッシュを書き込み、トランザクションログに操作を記録します。
リーダーは、min_writesの最小数のノード以上のノードが「OK」とレスポンスを返したら「OK」とレスポンスを返します。
クラスタプロパティのmin_writesは書き込みリクエスト時に指定することで、異なる値を指定できます。</p>
<p>クラウドモードはコミット/ロールバック操作を明示的には行いません。
コミットは特定の間隔で（commit_within）リーダーによりオートコミットにより管理されます。
また、シャードの全メンバーのコミットはトリガーにより管理されます。
ノードが利用可能な最新バージョンはコミットの時点で記録されます。</p>
<h3 id="transaction-log"><strong>Transaction Log</strong></h3>
<ul>
<li>トランザクションログは2つのコミットの間にインデックスに対して実行された操作全てを記録したもの</li>
<li>コミットはそれ以前に実行された操作の耐久性を保証するために、新しいトランザクションログを開始します。</li>
<li>同期は調整が可能です。例えば、flush vs fsynです。fsyncがデフォルトで、JVMクラッシュに対して保証できるが、電源異常の場合には保証できないが、速度的には早いです。</li>
</ul>
<h3 id="recovery">Recovery</h3>
<p>次のトリガーにより復旧が可能です。</p>
<ul>
<li>Bootstrap</li>
<li>パーティション分割</li>
<li>クラスタの再構築</li>
</ul>
<p>ノードは自身に「recovering」というステータスを設定して復旧を開始します。
このフェーズの間、ノードは読み込みリクエストを受けることができませんが、トランザくkションログに書きこまれるすべての新しい書き込みリクエストを受け取ります。
ノードは自身が持つインデックスのバージョンを調べて、パーティションの最新バージョンのリーダーに問い合わせます。
リーダーはシャード内の残りのノードと同期する前に実行されるべき操作の集合を返します（？？？）。</p>
<p>最初にインデックスをコピーし、最新のノードにあるトランザクションログをリプレイします。
もし、インデックスのコピーが必要ならば、インデックスファイルをローカルにまずコピーし、その後トランザクションログをリプレイします。
トランザクションログのリプレイは通常の書き込みリクエストの流れと同じです。
この時、ノードは新しい書き込みを受け付けるかもしれません。その書き込みはインデックスに再生されるべきです。
ある時点でノードは最新のコミットポイントに追いつき、自身のステータスを「ready」にします。
この時点で、このノードは読み込みリクエストを処理できます。</p>
<h4 id="handling-node-failures">Handling Node Failures</h4>
<p>一時的にネットワークが分断され、幾つかのノードとZooKeeperの間の通信が遮断されるかもしれません。
クラスタはデータの再構築（リバランシング）の前にしばらく待ちが発生します。</p>
<p><strong>Leader failure</strong></p>
<p>ノードが故障し、もしそれがシャードのリーダだった場合、他のメンバーがリーダー選出のプロセスを開始します。
新しいリーダーが選出されるまで、このパーティションへの書き込みは受け付けられません。
この時、これはリーダー以外の故障ステップを処理します。（？？？）</p>
<p><strong>Leader failure</strong></p>
<p>シャードの一部に新しいノードが割り当てられる前にリーダーはmin_reaction_timeの間待ちます。
リーダーはCluster Lockを取得し、シャードの新規メンバーとしてノードを割り当てるためのノード-シャード割り当てアルゴリズムを使用します。
ZooKeeperのノード-&gt;パーティションマッピングが更新され、Cluster Lockがリリースされます。
新しいノードはZooKeeperからノード-&gt;パーティションマッピングを強制的にリロードされます。</p>
<h3 id="splitting-partitions">Splitting partitions</h3>
<p>明示的なクラスタ管理コマンドもしくはSolrによる自動的な分割戦略（ストラテジ）はパーティションを分割することができます。
明示的な分割コマンド（split command）は対象となるパーティションを分割するために実行されます。</p>
<p>パーティションXが100から199のハッシュの範囲を持つものとし、X（100から149）、Y（150～199）に分割するとします。
Xのリーダーは、XとYの新しい値の範囲をZooKeeperに分割アクションを記録します。
ノードはこの分割アクションもしくは新しいパーティションの存在については通知を受けません。（？？？）</p>
<ol>
<li>XのリーダはCluster Lockを取得し、パーティションY（アルゴリズムはto be determined）を割り当てるノードを決定し、新しいパーティションを知らせ、パーティション-&gt;ノードマッピングを更新します。Xのリーダはノードのレスポンスを街、新しいパーティションがコマンドを受付可能な状態になったら次の処理を実行します。</li>
<li>Xのリーダーは分割が完了するまですべてのコミットを停止します。</li>
<li>Xのリーダーは最新のコミットポイント（バージョンVとする）のIndexReaderをオープンし、同じバージョンのIndexReaderもオープンするように命じます</li>
<li>XのリーダーはYのリーダーに対してバージョンV以降のトランザクションログのうちハッシュ値の範囲が150から199のものを流します。</li>
<li>Yのリーダーはトランザクションログの#2（#3の間違い？）で送られたリクエストだけを記録します？？？</li>
<li>Xのリーダーはステップ#2で開いたIndexReaderに対してインデックスの分割を開始します。</li>
<li>#5で作成されたインデックスはYのリーダーに送られ、登録されます。</li>
<li>Yのリーダーは「recovery」プロセスを開始するように（シャードの）他のノード命令し、インデックスのトランザクションログを再生し始めます。</li>
<li>パーティションYのすべてのノードがバージョンVに到達したならば</li>
<li>YのリーダーはXのリーダーに#2で作成されたReaderの上に、ハッシュの範囲が100から149だけに属しているドキュメントを抽出するようにするFilteredIndexReaderを準備するように頼みます。</li>
<li>Xのリーダーは#8aのリクエストが完了したのを検知したら、YのリーダーがCluster Lockを取得し、クラスタ全体の検索/登録リクエストの受信を開始するためにレンジ-&gt;パーティションマッピングを変更します。</li>
<li>YのリーダーはXのリーダーに検索リクエストのために#8aで作成されたFilteredIndexReaderの利用開始を頼みます</li>
<li>YのリーダーはXのリーダーに、ZooKeeperからレンジ-&gt;パーティションマッピングを矯正リフレッシュするように頼みます。この時点で#3で開始されたトランザクションログの流しこみが停止されるのが保証されます。</li>
</ol>
</li>
<li>Xのリーダーは自身のパーティションに存在するべきでないハッシュ値をもつドキュメントを削除し、最新のコミットポイントのsearcherを再度開きます。</li>
<li>この時点で分割は完全に終了し、Xのリーダーはcommit_withinパラメータによるコミットをレジュームします（？？？）</li>
</ol>
<p>Notes:</p>
<ul>
<li>分割操作が完了するまで、commit_withinパラメータによるパーティションの分割は実行されない</li>
<li>#8b開始から#8c終了までの間の分散検索は一貫しない検索結果を帰す場合がある（例えば：検索結果が異なる）</li>
</ul>
<h3 id="cluster-re-balancing">Cluster Re-balancing</h3>
<p>クラスタは明示的なクラスタ管理コマンドにより再構築（リバランシング）できる。</p>
<p>TBD
（to be determined）</p>
<h3 id="cluster-re-balancing-1">Cluster Re-balancing</h3>
<p>TBD
（to be determined）</p>
<h3 id="configuration">Configuration</h3>
<h4 id="solr_clusterproperties"><strong>solr_cluster.properties</strong></h4>
<p>これはクラスタ内の全ノードにわたって適用される一般的なSolr設定ファイルとは別のプロパティファイルの集合である。</p>
<ul>
<li>replication_factor：クラスタによって管理されるドキュメントのレプリカの数</li>
<li>min_writes：書き込み操作が成功になる前の最小の書き込み？？？？。これは書き込みごとに上書き設定可能</li>
<li>commit_within：検索に現れるまでの書き込み操作の最大回数</li>
<li>hash_function：ドキュメントのハッシュ値を計算するための関数の実装</li>
<li>max_hash_value：ハッシュ関数が出力することができる最大値。理論的には、この値はクラスタが保持できるパーティションの最大数でもある</li>
<li>min_reaction_time：起動、停止の後に再配分/分割にかかる時間（？？）</li>
<li>min_replica_for_reaction：レプリカノード数がこの値以下になったら、min_reaction_timeにならなくても分割が実行される。</li>
<li>auto_add_new_nodes：booleanフラグ。もしtrueなら新しいノードは自動的にパーティションからレプリカを読み込む。そうでない場合は新しいノードはクラスタに「idle」状態で登録される</li>
</ul>
<h3 id="cluster-admin-commands"><strong>Cluster Admin Commands</strong></h3>
<p>すべてのクラスタ管理コマンドはすべてのノードでパス（/cluster_admin）を与えることで実行できます。
全ノードは同じコマンドを受け付けることができ、振る舞いも同じものになるでしょう。
以下のコマンドはユーザが利用できるパブリックなコマンドです。</p>
<ul>
<li>init_cluster：（パラメータ：パーティション）このコマンドはノードの集合の初期化後に実施されます。このコマンドが実行されるまで、クラスタは読み込み/書き込みコマンドを受け付けません。</li>
<li>split_partition：（パラメータ：パーティション（任意））パーティションを2つに分割します。もしパーティションパラメータが指定されない場合は、ドキュメント数が最大の</li>
<li>add_idle_nodes：このコマンドはauto_add_new_nodes=falseの場合に利用できます。このコマンドはクラスタに対して「idle」状態のすべてのノードを追加するトリガーとなります。</li>
<li>move_partition：（パラメータ：パーティション、from、to）fromのノードからtoの別のノードに引数で指定されたパーティションを移動します。</li>
<li>command_status：（パラメータ：completion_id（任意））上記コマンドはすべて非同期で実行され、completion_idを返します。このコマンドは特定の実行中のコマンドもしくは全ての実行中のコマンドの状態を表示するために利用できます。</li>
<li>status：（パラメータ：パーティション（任意））パーティションのリストを表示し各パーティションの次の情報を表示します。</li>
<li>リーダーノード</li>
<li>ノードのリスト</li>
<li>ドキュメント数</li>
<li>平均読み込み回数（reads/sec）</li>
<li>平均書き込み回数（writes/sec）</li>
<li>平均読み込み時間（time/read）</li>
<li>平均書き込み時間（time/write）</li>
</ul>
</li>
</ul>
<h3 id="migrating-from-solr-to-solrcloud">Migrating from Solr to SolrCloud</h3>
<p>クラウドに移行するときに幾つかの特徴は不要かもしれないし、サポートされないかもしれません。
既存の（クラウドでない）バージョンでのすべての特徴をSolrCloudでサポートし続けなければなりません。</p>
<ul>
<li>レプリケーション：これは必要ありません。</li>
<li>CoreAdminコマンド：明示的なコアの操作は許可されません。内部にコアがあるかもしれないが、暗黙的に管理されるでしょう</li>
<li>複数スキーマのサポート？：単純化のため、ver1.0ではサポートしないかもしれない</li>
<li>solr.xml：SolrCloudでほんとに必要？</li>
</ul>
<h3 id="alternative-to-a-cluster-lock">Alternative to a Cluster Lock</h3>
<p>リーダーを選出する常設の調停ノード（masterはインデックスレプリケーションで利用している用語なので、「調停」とする）を持つほうが単純かもしれません。
「truth」状態をZookeeperの状態としてみなすような次のパターンでは、将来の柔軟性（クラスタを制御するためのZookeeperの状態を直接変更するような外部管理ツールのような）を考慮に入れることができます。
（毎回ロックを取得するよりも）調停ノードを持つことにより、よりスケーラブルになるかもしれません。
特定条件下でのみCluster Lockを利用するハイブリッドも意味があるでしょう。</p>
<h3 id="single-node-simplest-use-case">Single Node Simplest Use Case</h3>
<p>単一ノードでスタートして、ドキュメントをインデックス登録できないといけません。
また、あとで、クラスタに2番目のノードを追加できないと行けません。</p>
<ul>
<li>1つのノードから開始し、最初にZookeeperに設定ファイルをアップロードし、shard1にノードを作成＋登録します。</li>
<li>他の情報がない状態で設定が作成され、1つのシャードのシステムとなります。</li>
</ul>
</li>
<li>いくつかのドキュメントをインデックスします</li>
<li>他のノードが起動し、「まだ割り当てられていない場合、レプリカの最小の数をもつshardに割り当てられ、「recovery」プロセスを開始します」というパラメータを受け取ります。
* 出来れば、同一ホスト上に同じシャードはコピーしない
* この時点の後で、ノードが停止したら、再起動し、同じ役割が再開されるべきです。（Zookeeperでそれ自身であると判別されれば）
</li>
</ol>
<p>原文はこちらからです。</p>
<h3 id="boot-strapping-1">Boot Strapping</h3>
<h4 id="cluster-startup">Cluster Startup</h4>
<p>A node is started pointing to a Zookeeper host and port. The first node in the cluster may be started with cluster configuration properties and the schema/config files for the cluster. The first node would upload the configuration into zookeeper and bootstrap the cluster. The cluster is deemed to be in the “bootstrap” state. In this state, the node -&gt; partition mapping is not computed and the cluster does not accept any read/write requests except for clusteradmin commands.</p>
<p>After the initial set of nodes in the cluster have started up, a clusteradmin command (TBD description) is issued by the administrator. This command accepts an integer “partitions” parameter and it performs the following steps:</p>
<ol>
<li>Acquire the Cluster Lock</li>
<li>Allocate the “partitions” number of partitions</li>
<li>Acquires nodes for each partition</li>
<li>Updates the node -&gt; partition mapping in ZooKeeper</li>
<li>Release the Cluster Lock</li>
<li>Informs all nodes to force update their own node -&gt; partition mapping from ZooKeeper</li>
<li>The Cluster Lock is acquired</li>
<li>A suitable source (node, partition) tuple is chosen:</li>
<li>The list of available partitions are scanned to find partitions which has less then “replication_factor” number of nodes. In case of tie, the partition with the least number of nodes is selected. In case of another tie, a random partition is chosen.</li>
<li>If all partitions have enough replicas, the nodes are scanned to find one which has most number of partitions. In case of tie, of all the partitions in such nodes, the one which has the most number of documents is chosen. In case of tie, a random partition on a random node is chosen.</li>
<li>If moving the chosen (node, partition) tuple to the current node will decrease the maximum number of partition:node ratio of the cluster, the chosen tuple is returned.Otherwise, no (node, partition) is chosen and the algorithm terminates</li>
<li>The node -&gt; partition mapping is updated in ZooKeeper</li>
</ol>
<li>The node status in ZooKeeper is updated to “recovery” state</li>
<li>The Cluster Lock is released</li>
<li>A “recovery” is initiated against the leader of the chosen partition</li>
<li>After the recovery is complete, the Cluster Lock is acquired again</li>
<li>The source (node, partition) is removed from the node -&gt; partition map in ZooKeeper</li>
<li>The Cluster Lock is released</li>
<li>The source node is instructed to force refresh the node -&gt; partition map from ZooKeeper</li>
<li>Goto step #1</li>
</ol>
<h4 id="node-restart-1"><strong>Node Restart</strong></h4>
<p>A node restart can mean one of the following things:</p>
<ul>
<li>The JVM crashed and was manually or automatically restarted</li>
<li>The node was in a temporary network partition and either could not reach ZooKeeper (and was supposed to be dead) or could not receive updates from the leader for a period of time. A node restart ine node failure.</li>
<li>Lifecycle of a Write Operation this scenario signifies the removal of the network partition.</li>
<li>A hardware failure or maintenance window caused the removal of the node from the cluster and the node has been started again to rejoin the cluster.</li>
</ul>
<p>The node reads the list of partitions for which it is a member and for each partition, starts a recovery process from each partition’s leader respectively. Then, the node follows the steps in the New Node section without checking for the auto_add_new_nodes property. This ensures that the cluster recovers from the imbalance created by th</p>
<p>Writes are performed by clients using the standard Solr update formats. A write operation can be sent to any node in the cluster. The node uses the hash_function , and the Range-Partition mapping to identify the partition where the doc belongs to. A zookeeper lookup is performed to identify the leader of the shard and the operation is forwarded there. A SolrJ enhancement may enable it to send the write directly to the leader</p>
<p>The leader assigns the operation a Partition Version and writes the operation to its transaction log and forwards the document + version + hash to other nodes belonging to the shard. The nodes write the document + hash to the index and record the operation in the transaction log. The leader responds with an ‘OK’ if at least min_writes number of nodes respond with ‘OK’. The min_writes in the cluster properties can be overridden by specifying it in the write request.</p>
<p>The cloud mode would not offer any explicit commit/rollback operations. The commits are managed by auto-commits at intervals (commit_within) by the leader and triggers a commit on all members on the shard. The latest version available to a node is recorded with the commit point.</p>
<h3 id="transaction-log-1"><strong>Transaction Log</strong></h3>
<ul>
<li>A transaction log records all operations performed on an Index between two commits</li>
<li>Each commit starts a new transaction log because a commit guarantees durability of operations performed before it</li>
<li>The sync can be tunable e.g. flush vs fsync by default can protect against JVM crashes but not against power failure and can be much faster</li>
</ul>
<h3 id="recovery-1">Recovery</h3>
<p>A recovery can be triggered during:</p>
<ul>
<li>Bootstrap</li>
<li>Partition splits</li>
<li>Cluster re-balancing</li>
</ul>
<p>The node starts by setting its status as ‘recovering’. During this phase, the node will not receive any read requests but it will receive all new write requests which shall be written to a separate transaction log. The node looks up the version of index it has and queries the ‘leader’ for the latest version of the partition. The leader responds with the set of operations to be performed before the node can be in sync with the rest of the nodes in the shard.</p>
<p>This may involve copying the index first and replaying the transaction log depending on where the node is w.r.t the state of the art. If an index copy is required, the index files are replicated first to the local index and then the transaction logs are replayed. The replay of transaction log is nothing but a stream of regular write requests. During this time, the node may have accumulated new writes, which should then be played back on the index. The moment the node catches up with the latest commit point, it marks itself as “ready”. At this point, read requests can be handled by the node.</p>
<h4 id="handling-node-failures-1">Handling Node Failures</h4>
<p>There may be temporary network partitions between some nodes or between a node and ZooKeeper. The cluster should wait for some time before re-balancing data.</p>
<p><strong>Leader failure</strong></p>
<p>If node fails and if it is a leader of any of the shards, the other members will initiate a leader election process. Writes to this partition are not accepted until the new leader is elected. Then it follows the steps in non-leader failure</p>
<p><strong>Non-Leader failure</strong></p>
<p>The leader would wait for the min_reaction_time before identifying a new node to be a part of the shard. The leader acquires the Cluster Lock and uses the node-shard assignment algorithm to identify a node as the new member of the shard. The node -&gt; partition mapping is updated in ZooKeeper and the cluster lock is released. The new node is then instructed to force reload the node -&gt; partition mapping from ZooKeeper.</p>
<h3 id="splitting-partitions-1">Splitting partitions</h3>
<p>A partition can be split either by an explicit cluster admin command or automatically by splitting strategies provided by Solr. An explicit split command may give specify target partition(s) for split.</p>
<p>Assume the partition ‘X’ with hash range 100 - 199 is identified to be split into X (100 - 149) and a new partition Y (150 - 199). The leader of X records the split action in ZooKeeper with the new desired range values of X as well as Y. No nodes are notified of this split action or the existence of the new partition.</p>
<ol>
<li>The leader of X, acquires the Cluster Lock and identifies nodes which can be assigned to partition Y (algorithm TBD) and informs them of the new partition and updates the partition -&gt; node mapping. The leader of X waits for the nodes to respond and once it determines that the new partition is ready to accept commands, it proceeds as follows:</li>
<li>The leader of X suspends all commits until the split is complete.</li>
<li>The leader of X opens an IndexReader on the latest commit point (say version V) and instructs its peers to do the same.</li>
<li>The leader of X starts streaming the transaction log after version V for the hash range 150 - 199 to the leader of Y.</li>
<li>The leader of Y records the requests sent in #2 in its transaction log only i.e. it is not played on the index.</li>
<li>The leader of X initiates an index split on the IndexReader opened in step #2.</li>
<li>The index created in #5 is sent to the leader of Y and is installed.</li>
<li>The leader of Y instructs its peers to start recovery process. At the same time, it starts playing its transaction log on the index.</li>
<li>Once all peers of partition Y have reached at least version V:</li>
<li>The leader of Y asks the leader of X to prepare a FilteredIndexReader on top of the reader created in step #2 which will have documents belonging to hash range 100 - 149 only.</li>
<li>Once the leader of X acknowledges the completion of request in #8a, the leader of Y acquires the Cluster Lock and modifies the range -&gt; partition mapping to start receiving regular search/write requests from the whole cluster.</li>
<li>The leader of Y asks leader of X to start using the FilteredIndexReader created in #8a for search requests.</li>
<li>The leader of Y asks leader of X to force refresh the range -&gt; partition mapping from ZooKeeper. At this point, it is guaranteed that the transaction log streaming which started in #3 will be stopped.</li>
</ol>
</li>
<li>The leader of X will delete all documents with hash values not belonging to its partitions, commits and re-opens the searcher on the latest commit point.</li>
<li>At this point, the split is considered complete and leader of X resumes commits according to the commit_within parameters.</li>
</ol>
<p>Notes:</p>
<ul>
<li>The partition being split does not honor commit_within parameter until the split operation completes</li>
<li>Any distributed search operation performed starting at the time of #8b and till the end of #8c can return inconsistent results i.e. the number of search results may be wrong.</li>
</ul>
<h3 id="cluster-re-balancing-2">Cluster Re-balancing</h3>
<p>The cluster can be rebalanced by an explicit cluster admin command.</p>
<p>TBD</p>
<h3 id="monitoring">Monitoring</h3>
<p>TBD</p>
<h3 id="configuration-1">Configuration</h3>
<h4 id="solr_clusterproperties-1"><strong>solr_cluster.properties</strong></h4>
<p>This are the set of properties which are outside of the regular Solr configuration and is applicable across all nodes in the cluster:</p>
<ul>
<li><strong>replication_factor</strong> : The number of replicas of a doc maintained by the cluster</li>
<li><strong>min_writes</strong> : Minimum no:of successful writes before the write operation is signaled as successful . This may me overridden on a per write basis</li>
<li><strong>commit_within</strong> : This is the max time within which write operation is visible in a search</li>
<li><strong>hash_function</strong> : The implementation which computes the hash of a given doc</li>
<li><strong>max_hash_value</strong> : The maximum value that a hash_function can output. Theoretically, this is also the maximum number of partitions the cluster can ever have</li>
<li>min_reaction_time : The time before any reallocation/splitting is done after a node comes up or goes down (in secs)</li>
<li><strong>min_replica_for_reaction</strong> : If the number of replica nodes go below this threshold the splitting is triggered even if the min_reaction_time is not met</li>
<li><strong>auto_add_new_nodes</strong> : A Boolean flag. If true, new nodes are automatically used as read replicas to existing partitions, otherwise, new nodes sit idle until the cluster needs them.</li>
</ul>
<h3 id="cluster-admin-commands-1"><strong>Cluster Admin Commands</strong></h3>
<p>All cluster admin commands run on all nodes at a given path (say /cluster_admin). All nodes are capable of accepting the same commands and the behavior would be same. These are the public commands which a user can use to manage a cluster:</p>
<ul>
<li><strong>init_cluster</strong> : (params : partition) This command is issued after the initial set of nodes are started. Till this command is issued, the cluster would not accept any read/write commands</li>
<li><strong>split_partition</strong> : (params : partitionoptional). The partition is split into two halves. If the partition parameter is not supplied, the partition with the largest number of documents is identified as the candidate.</li>
<li><strong>add_idle_nodes</strong> : This can be used if auto_add_new_nodes=false. This command triggers the addition of all ‘idle’ nodes to the cluster.</li>
<li><strong>move_partition</strong> : (params : partition, from, to). Move the given partition from a given node from to another node</li>
<li><strong>command_status</strong> :(params : completion_idoptional) . All the above commands are asynchronous and returns with a completion_id . This command can be used to know the status of a particular running command or all the current running commands</li>
<li><strong>status</strong> : (params : partitionoptional) Shows the list of partitions and for each partition, the following info is provided</li>
<li>leader node</li>
<li>nodes list</li>
<li>doc count</li>
<li>average reads/sec</li>
<li>average writes/sec</li>
<li>average time/read</li>
<li>average time/write</li>
</ul>
</li>
</ul>
<h3 id="migrating-from-solr-to-solrcloud-1">Migrating from Solr to SolrCloud</h3>
<p>A few features may be redundant or not supported when we move to cloud such as. We should continue to support the non cloud version which supports all the existing features</p>
<ul>
<li>Replication. This feature is not required anymore</li>
<li>CoreAdmin commands. Explicit manipulation of cores will not be allowed. Though cores may exist internally and they meay be managed implicitly</li>
<li>Multiple schema support ? Should we just remove it from ver 1.0 for simplicity?</li>
<li>solr.xml . Is there a need at all for this in the cloud mode?</li>
</ul>
<h3 id="alternative-to-a-cluster-lock-1">Alternative to a Cluster Lock</h3>
<p>It may be simpler to have a coordinator node (we avoid the term master since that is associated with traditional index replication) that is established via leader election. Following a pattern of treating the zookeeper state as the &ldquo;truth&rdquo; and having nodes react to changes in that state allow for more future flexibility (such as allowing an external management tool directly change the zookeeper state to control the cluster). Having a coordinator (as opposed to grabbing a lock every time) can be more scalable too. A hybrid model where a cluster lock is used only in certain circumstances can also make sense.</p>
<h3 id="single-node-simplest-use-case-1">Single Node Simplest Use Case</h3>
<p>We should be able to easily start up a single node and start indexing documents. At a later point in time, we should be able to start up a second node and have it join the cluster.</p>
<p>start up a single node, upload it&rsquo;s configuration (the first time) to zookeeper, and create+assign the node to shard1.
in the absence of other information when the config is created, a single shard system is assumed
index some documents
start up another node and pass it a parameter that says &ldquo;if you are not already assigned, assign yourself to any shard that has the lowest number of replicas and start recovery process&rdquo;
avoid replicating a shard on the same host if possible
after this point, one should be able to kill the node and start it up again and have it resume the same role (since it should see itself in zookeeper)</p>
</content:encoded>
    </item>
    
    <item>
      <title>New SolrCloud Designの翻訳（その１）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/28/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE%EF%BC%91/</link>
      <pubDate>Wed, 28 Sep 2011 20:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/28/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE%EF%BC%91/</guid>
      <description>ちょっと興味があるので、訳してみました。（Wikiのページはこちら） 更新されているようなので、もとの文章も残しておきます。（ページ下部の続き</description>
      <content:encoded><p>ちょっと興味があるので、訳してみました。（Wikiのページは<a href="http://wiki.apache.org/solr/NewSolrCloudDesign">こちら</a>）
更新されているようなので、もとの文章も残しておきます。（ページ下部の続きはこちら部分以降）
全部訳そうと思ったのですが、終わらなかったので、まずは前半部分です。まだ、訳しただけで理解できてない。。。
（英語力のなさをさらけ出してしまうのですが、これも修行です。。。おかしいところはツッコミを。）</p>
<h3 id="what-is-solrcloud">What is SolrCloud?</h3>
<p>Solrクラウドはクラウドでの検索サービスとしてのSolrを管理、運用するための既存のSolrを拡張するものです。</p>
<h3 id="用語集">用語集</h3>
<ul>
<li>Cluster：クラスタは1単位として管理されるSolrノードの集合です。クラスタ全体で単一のschema、solrconfigをもたないといけません。</li>
<li>Node：ひとつのJVMインスタンスで起動しているSolrのこと</li>
<li>Partition：パーティションはドキュメント集合全体のサブセット（部分集合）のことです。パーティションは部分集合のドキュメントが単一のインデックスに含まれるような形で作られます。</li>
<li>Shard：パーティションはn（＝replication factor）個のノードに保存される必要があります。このn個のノードすべてでひとつのshardです。1つのノードはいくつかのshardの一部にで有る場合があります。</li>
<li>Leader：各Shardは1つのリーダとなるノードを持っています。パーティションに登録されたドキュメントリーダーからコピーされます</li>
<li>Replication Factor：クラスタによって保持されるレプリカの最小限の数</li>
<li>Transaction Log：各ノードによって保持される書き込み処理の追記ログ</li>
<li>Partition version：これは各shardのリーダーが持っているカウンターで、書き込み処理ごとに増加し、レプリカに送られます。</li>
<li>Cluster Lock：これはrange（※後述されているハッシュ値の範囲のことか？）-&gt;パーティションもしくはパーティション-&gt;ノードのマッピングを変更するために取得しなければいけないグローバルなロックのことです。</li>
</ul>
<p>※用語だけだと関係がわかりづらかったので、図を書いてみました。</p>
<p>
<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110928/20110928_2063185.png" alt="SolrCloudのパーティションについて"/>
    </div>
    <a href="/images/entries/20110928/20110928_2063185.png" itemprop="contentUrl"></a>
  </figure>
</div>

ドキュメントの集合とパーティションについての考え方</p>
<p>

<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110928/20110928_2063186.png" alt="SolrCloudのクラスターについて"/>
    </div>
    <a href="/images/entries/20110928/20110928_2063186.png" itemprop="contentUrl"></a>
  </figure>
</div>

クラスタ、ノード、シャードの考え方。</p>
<h3 id="処理原則"><strong>処理原則</strong></h3>
<ul>
<li>任意の処理はクラスタにある任意のノードに実行可能です。</li>
<li>リカバリできないSPOFはありません。</li>
<li>クラスタは伸縮自在（elastic）でなければならない</li>
<li>書き込みが失われないこと（耐久性）を保証する</li>
<li>書き込み順序が保証されなければならない</li>
<li>2つのクライアントが2つの「A」というドキュメントを同時に送信してきた場合、すべてのレプリカで一貫してどちらか一方が保存されなければならない。</li>
<li>クラスタの設定は中央管理されなければならない。また、クラスタのどのノードからもクラスタ設定が更新できます。</li>
<li>読み込み（検索）の自動的なフェイルオーバー</li>
<li>書き込み（インデクシング）の自動的なフェイルオーバー</li>
<li>ノードの故障が発生しても自動的にrepcation factorの数は守られます。（故障したら動的にレプリカを再配置？）</li>
</ul>
<h3 id="zookeeper"><strong>Zookeeper</strong></h3>
<p>ZooKeeperクラスタは次のために使用されます。</p>
<ul>
<li>クラスタ設定の集中管理</li>
<li>分散同期に必要な操作のコーディネータ</li>
<li>クラスタ構成を保存するためのシステム</li>
</ul>
<h3 id="partitioning"><strong>Partitioning</strong></h3>
<p>クラスタは固定されたmax_hash_value＝「N」が設定されます。
max_hash_valueは1000のような大きな値が設定されます。</p>
<pre><code>
hash = hash_function(doc.getKey()) % N
</code></pre><p>ハッシュ値の範囲がパーティションに割り当てられ、ZooKeeperに保存されます。
次の例のような形で、パーティションに対して範囲が設定されます。</p>
<pre><code>
range  : partition
------  ----------
0 - 99 : 1
100-199: 2
200-299: 3
</code></pre><p>ハッシュはドキュメントにインデックスフィールドとして追加され、変更されない値です。
これは、インデックスを分割するときにも利用します。</p>
<p>ハッシュ関数はプラガブルです。これはドキュメントを受け取り、一貫した正整数ハッシュ値を返します。デフォルトのハッシュ関数として、必須でかつ変更されないフィールド（デフォルトはユニークキーフィールド）からハッシュ値を計算する関数が提供されます。</p>
<h4 id="using-full-hash-range"><strong>Using full hash range</strong></h4>
<p>max_hash_valueは必ずしも必要ではありません。各shardはいずれにしろハッシュ値の範囲持っているので、完全な32 bitsハッシュを使うこともできます。
設定可能なmax_hash_valueを利用しないで、クライアントからの値をもとにハッシュ値を作ることができます。
例えば、電子メールの検索アプリでは次のようにハッシュ関数を作ることができます。</p>
<pre><code>
(hash(user_id)&lt;&lt;24) | (hash(message_id)&gt;&gt;&gt;8)
</code></pre><p>ユーザIDから8bitのハッシュコードの先頭8ビットを利用することで、任意のユーザのメールがクラスタの同じ256番目（のノード？）にあるのを保証します。検索時はこの情報をもとにクラスタのその部分への問い合わせだけで情報が得られます。</p>
<p>おそらく、私たちは最大値から最小値をカバーする範囲を表現するのにハッシュ空間を輪（固定のハッシュではなく）とみなしたいです。（？？？円状のハッシュ空間とすることで、クラスタ内のノード数の増減に耐えられるようにするよということかな？）</p>
<h4 id="shard-naming"><strong>shard naming</strong></h4>
<p>シャードからハッシュ値の範囲へのマッピングを別々に管理するよりも、ハッシュコードによりパーティションを構成するときに実際にはハッシュの範囲をシャード名にします。
（シャード「1-1000」は1から1000の間のハッシュコードを持つドキュメントが含まれるという形）</p>
<p>現時点では（コレクション1つに対してシングルコアの1Solrサーバと仮定して）solrコア名はコレクション名をつけるようになっています。
同一コレクションのためのマルチコアに対してのいい命名規則をつけるという課題が残っています。
（※コレクションに対する説明がここまでないかな？）</p>
<h3 id="shard-assignment">Shard Assignment</h3>
<p>ノード-&gt;パーティションのマッピングはZooKeeperにあるCluster Lockを取得したノードによってのみ変更が可能です。
ノードの追加時に、まず、Cluster Lockを取得し、次にそれがどのパーティションを取得できるかを識別します。</p>
<h4 id="node-to-a-shard-assignment">Node to a shard assignment</h4>
<p>新しいノードを探しているノードはまずCluster Lockを取得しないといけません。
第一に、リーダーはシャードを決めます。
シャードが持つ、すべての利用可能なノード数で最小の値を持つノードが選び出されます。
もし、同値ならランダムにノードを選びます。</p>
<p>原文はこちらからです。</p>
<h3 id="new-solrcloud-design"><strong>New SolrCloud Design</strong></h3>
<p>(Work in progress)</p>
<h3 id="what-is-solrcloud-1">What is SolrCloud?</h3>
<p>SolrCloud is an enhancement to the existing Solr to manage and operate Solr as a search service in a cloud.</p>
<h3 id="glossary">Glossary</h3>
<ul>
<li>Cluster : Cluster is a set of Solr nodes managed as a single unit. The entire cluster must have a single schema and solrconfig</li>
<li>Node : A JVM instance running Solr</li>
<li>Partition : A partition is a subset of the entire document collection. A partition is created in such a way that all its documents can be contained in a single index.</li>
<li>Shard : A Partition needs to be stored in multiple nodes as specified by the replication factor. All these nodes collectively form a shard. A node may be a part of multiple shards</li>
<li>Leader : Each Shard has one node identified as its leader. All the writes for documents belonging to a partition should be routed through the leader.</li>
<li>Replication Factor : Minimum number of copies of a document maintained by the cluster</li>
<li>Transaction Log : An append-only log of write operations maintained by each node</li>
<li>Partition version : This is a counter maintained with the leader of each shard and incremented on each write operation and sent to the peers</li>
<li>Cluster Lock : This is a global lock which must be acquired in order to change the range -&gt; partition or the partition -&gt; node mappings.</li>
</ul>
<h3 id="guiding-principles"><strong>Guiding Principles</strong></h3>
<ul>
<li>Any operation can be invoked on any node in the cluster.</li>
<li>No non-recoverable single point of failures</li>
<li>Cluster should be elastic</li>
<li>Writes must never be lost i.e. durability is guaranteed</li>
<li>Order of writes should be preserved</li>
<li>If two clients send document &ldquo;A&rdquo; to two different replicas at the same time, one should consistently &ldquo;win&rdquo; on all replicas.</li>
<li>Cluster configuration should be managed centrally and can be updated through any node in the cluster. No per node configuration other than local values such as the port, index/logs storage locations should be required</li>
<li>Automatic failover of reads</li>
<li>Automatic failover of writes</li>
<li>Automatically honour the replication factor in the event of a node failure</li>
</ul>
<h3 id="zookeeper-1"><strong>Zookeeper</strong></h3>
<p>A ZooKeeper cluster is used as:</p>
<ul>
<li>The central configuration store for the cluster</li>
<li>A co-ordinator for operations requiring distributed synchronization</li>
<li>The system-of-record for cluster topology</li>
</ul>
<h3 id="partitioning-1"><strong>Partitioning</strong></h3>
<p>The cluster is configured with a fixed max_hash_value (which is set to a fairly large value, say 1000) ‘N’. Each document’s hash is calculated as:</p>
<pre><code>
hash = hash_function(doc.getKey()) % N
</code></pre><p>Ranges of hash values are assigned to partitions and stored in Zookeeper. For example we may have a
range to partition mapping as follows</p>
<pre><code>
range  : partition
------  ----------
0 - 99 : 1
100-199: 2
200-299: 3
</code></pre><p>The hash is added as an indexed field in the doc and it is immutable. This may also be used during an index split</p>
<p>The hash function is pluggable. It can accept a document and return a consistent &amp; positive integer hash value. The system provides a default hash function which uses the content of a configured, required &amp; immutable field (default is unique_key field) to calculate hash values.</p>
<h4 id="using-full-hash-range-1"><strong>Using full hash range</strong></h4>
<p>Alternatively, there need not be any max_hash_value - the full 32 bits of the hash can be used since each shard will have a range of hash values anyway. Avoiding a configurable max_hash_value makes things easier on clients wanting related hash values next to each other. For example, in an email search application, one could construct a hashcode as follows:</p>
<pre><code>
(hash(user_id)&lt;&lt;24) | (hash(message_id)&gt;&gt;&gt;8)
</code></pre><p>By deriving the top 8 bits of the hashcode from the user_id, it guarantees that any users emails are in the same 256th portion of the cluster. At search time, this information can be used to only query that portion of the cluster.</p>
<p>We probably also want to view the hash space as a ring (as is done with consistent hashing) in order to express ranges that wrap (cross from the maximum value to the minimum value).</p>
<h4 id="shard-naming-1"><strong>shard naming</strong></h4>
<p>When partitioning is by hash code, rather than maintaining a separate mapping from shard to hash range, the shard name could actually be the hash range (i.e. shard &ldquo;1-1000&rdquo; would contain docs with a hashcode between 1 and 1000).</p>
<p>The current convention for solr-core naming is that it match the collection name (assuming a single core in a solr server for the collection). We still need a good naming scheme for when there are multiple cores for the same collection.</p>
<h3 id="shard-assignment-1">Shard Assignment</h3>
<p>The node -&gt; partition mapping can only be changed by a node which has acquired the Cluster Lock in ZooKeeper. So when a node comes up, it first attempts to acquire the cluster lock, waits for it to be acquired and then identifies the partition to which it can subscribe to.</p>
<h4 id="node-to-a-shard-assignment-1">Node to a shard assignment</h4>
<p>The node which is trying to find a new node should acquire the cluster lock first. First of all the leader is identified for the shard. Out of the all the available nodes, the node with the least number of shards is selected. If there is a tie, the node which is a leader to the least number of shard is chosen. If there is a tie, a random node is chosen.</p>
</content:encoded>
    </item>
    
    <item>
      <title>Hadoop Conference Japan 2011 Fallに参加してきました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/27/hadoop-conference-japan-2011-fall%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 27 Sep 2011 17:51:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/27/hadoop-conference-japan-2011-fall%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>Hadoop Conference Japan 2011 Fallに行ってきました。 まずは、ユーザ会の方々、運営の方々、発表された方々お疲れ様でした。こんな機会を用意していただき、ありがとう</description>
      <content:encoded><p>Hadoop Conference Japan 2011 Fallに行ってきました。
まずは、ユーザ会の方々、運営の方々、発表された方々お疲れ様でした。こんな機会を用意していただき、ありがとうございます。
Hadoopは昨年触っていたのですが、最近は縁がなくなってしまいました。
ただ、触っていたときに面白かったので参加してきました。
ということで、今回も自分用にメモを取ったので。（今回は英語のヒアリングがあって、メモがひどい事になってます。。。）
いつものことながら、おかしいところとかあれば、ツッコミなどフィードバックをもらえると嬉しいです。</p>
<p>場所：ベルサール汐留
日時：2011/09/26 10:00～18:00＋懇親会：18:30～21:00</p>
<p>オープニングトーク：濱野、太田
　参加者：1100名超！（実際は800名弱？）
　Hadoopの経験：利用経験なしな方が580名
　カンファレンスの認知：Twitterよりも知人、その他が多かった。
　会場提供：リクルート様
　リクルート米谷様より一言。
　　Question VOTE！！サイトを用意。
　　http://mit.recruit.co.jpにて情報提供。
　※残念ながら地下だったため、E-mobileにつながらず、あと、電源確保が難しかったのでMBAをスタンドアロンにしてメモをとっていたので、
　　QAサイトにはアクセスしませんでした。もう少し活用したかったんですが、携帯でTwitterを追っかけるので精一杯。。。</p>
<p><strong>◎The role of the Distribution in the Apache Hadoop Ecosystem：Todd Lipcon（Cloudera）</strong></p>
<pre><code>
　1.Introduction
　　Todd Lipcon：Clouderaエンジニア
　　Hadoop とHBaseのコミッター
　2. Hadoop Overview
　　HDFS＋MapReduceの説明
　　Hadoopの生まれてきた経緯　
　　　様々な形式のデータが大量に存在し、データのハンドリングが難しくなってきたため。
　　　Flexible, Scalable Solutionが必要に。
　　Hadoop の2つのユースケース
　　　1.Advanced Analytics
　　　　SNS（Web）、Content Optimization（Media）、Network Analytics（Telco）、
　　　2.Data Processing
　　　　Clickstream SessionizationEngagement

　3.Cloudera Overview
　　ClouderaCustomers
　　　・Large National Bank
　　　・Web-Based Media
　　　　click-through dataや広告のログ
　　　・Wireless Telecom
　　　　大量のデータ
　　目標：大量データからビジネスを引き出すこと。
　　Cloudera Japan：トレーニング。NTTDと協業して開発支援も
　4.CDH Overview
　　100% pure Apache Hadoop
　　SCM Epress(Service &amp;amp; Configuration Manager)
　　　Free 
　　なぜ、CDHを利用するの？
　　　Linuxを利用する場合にLinux.orgからはダウンロードしないように、Hadoopも同じように提供したい。
　　　　RedHat系を目指しているみたい。
　　　様々なパッケージの依存関係が混乱を招く。
　　　　CDHならテストが終わってるものが提供されてる。
　　※SolrはLWEがLucidWorksEnterpriseがこれを目指してるのか。
　5.Cloudera Enterprise
　　Activity Monitor：Jobのパフォーマンスをリアルタイムに監視
　　SCM：設定のvalidateや管理。
　　Resource Manager：。。。
　6.まとめ
　　CDH：簡単にHadopが使えるよ。
　　SCM Express：簡単にHadoopの設定ができてフリーだよ
　　Cloudera：Hadoopに関していろいろサポートしているよ。（Enterprise用ソフトやトレーニングなど）
</code></pre><p><strong>◎About Hortonworks：Owen O&rsquo;Malley（Hortonworks）</strong></p>
<pre><code>
　1.About Hortonworks
　　2011/2月に設立
　　22人のYahooからのアーキテクトとコミッタにより設立
　2.Credentials
　　Yahooのクラスタの経験者がいますよ。
　　OSSに長けた人達によるチームです。
　3.What is Apache Hadoop
　　Hadoopの説明（別の側面が幾つかあり。）
　　Commodity Hardwareで動く
　　簡単にプログラムできる
　　典型的なアプリケーションのタグクラウド（あとでちゃんと見る）
　　HadoopのほとんどのソースコードはYahooで作られてるよ。
　　Clouderaとか目じゃないよ
　4.Hadoop @ Yahoo!
　　各種サーバや規模など
　　Science Hadoop Cluster &lt;-&amp;gt; Production Hadoop Cluster &lt;-&amp;gt; Serving Systems
　　　という構成で、いろいろやってます。
　5.Hadoop Market
　　ビジネス：ビッグデータを扱って色々やろうね
　　金融系：IT系のコストをOSSとHadoopで削減
　　技術系：
　6.Hortonworks Strategy
　　Hadoopを利用、管理しやすくするためのいろんなことをコミュニティに還元しますよ。
　　性能などについても同様。
　　トレーニングやテクニカルサポートやりますよ。
　QA：
　　Q：42Kのノードの管理ツールはなに？
　　A：手で管理してます。
　　Q：社名の由来は？
　　A：童話でHortonという名前の象の話がある。
　　Q：CDHはおすすめ？それともほかのものがいい？
　　A：※聞き取れず。。。
　　Q：500万Query/月はアドホックQueryもあるの？
　　A：幾つかのクラスタに分けて使ってる。アドホックは不明
</code></pre><p><strong>◎How Hadoop needs to evolve and integrate into the enterprise：Ted Dunning（MapR）</strong></p>
<pre><code>
　・Quick History
　・英語わからない身には辛い。。。
　Zookeeperの人らしい。
　Narrow Foundations
　　HDFSとNASの間には大きな壁があり、大きなデータを移動するのはコストが掛かる。
　Broad Foundation
　　HDFSの代わりにNAS、RDBMSの下に位置するMapRを用意
　　ど、どんな仕組み？-&amp;gt;テクニカルセッションで。
　QA：
　　Q：MapRはOSSにしないのか？
　　A：MapRで開発したものはApacheに還元はしますが、OSSにはしないよ。フリー版は提供するかも
</code></pre><p>ここで昼食。午前中からいた人にはランチボックスが提供されました。
午後からはコミュニティトラックとテクニカルトラックの2トラックがありましたが、LTが聞きたかった（それよりも英語が辛かった？）のでコミュニティトラックのど真ん中、最前列に入り浸りました。
<strong>★コミュニティトラック</strong>
<strong>◎Elastic MapReduce Amazonが提供するHadoop：大谷晋平（Amazon）</strong></p>
<pre><code>
　・Amazonとは
　　Eコマース
　　流通
　　AWS
　　の3つのサービス
　・AWS
　　Low-level、High-level、Cross Service、Tools to access services、アプリケーション
　　色々あるなー
　・Bigデータが大変な理由
　　ケタ違いのデータ量、異なる形式データ、即時性
　　現状システムはスケールしない
　　ビジネスとして成立する？
　　　成立するならすぐスケールだめならすぐ縮小
　・Hadoopとは？
　　これまでのお話。
　　スケーラブル、低価格なハード
　　誰でも入手可能で、実績多数。
　・Amazon EMR
　　AWS上でスケーラブルなインフラ上に構築が可能
　　オンプレミスからMapReduceアプリを移行可能なため、分析、解析に集中可能
　　S3からデータIN/OUTするので、データ欠損がない。
　　Hadoopそのままではチューニングやクラスタサイズ見積もりも難しい
　　　＝＞クラスタサイズを動的に拡張伸縮可能。パフォーマンス最適化もできるよ。
　　0.18、0.20が利用可能。
　　EMRはHDFSとジョブ（タスク）トラッカーを別構成にしている。
　　EC2上にMaster、Core、タスクノードを展開
　　S3にデータを格納
　　SimpleDB（KVS）を利用
　・EMR注目機能
　　ジョブフローの高速化
　　　ジョブの再起動無しにコスト/パフォーマンス比を変更可能。
　　　タスクノードを動的に増やせる
　　柔軟なデータウェアハウスクラスタ
　　　タスクノードをバッチ実行時にのみ増やせる。
　　※増減できるのはタスクノード
　　　Coreノードは増加のみ
　　EMR＋Spotインスタンスの活用
　　　コスト削減効果が非常に高い
　　　AWSの余剰リソースをリーズナブルに提供（Amazon的にもウハハだ）
　・その他の機能
　　東海岸だけだけど、スパコンレベルのインスタンスも利用可能
　　AWS上での最適化設計など
　　※ちょっと時間足りなくなってきたｗ
　・EMRの事例
　　1.Razorfish
　　　ROAS（広告費用対効果）を500%改善（すげー）
　　2.So-net
　・EMR都市伝説
　　物理vs仮想
　　　そりゃ、物理が速いよ
　　EMRの柔軟性・拡張性がセールスポイント
　　　ビッグデータは成功/失敗が読めないのもあるので、
　　　インフラに投資する部分を少なくできるよ。
　　オンプレミスが安い？
　　　いやいや、HWの購入から設定など時間かかる。
　・Beyond Hadoop
　　HadoopのIn/Out含めてスケーラブル、フレキシビリティが重要
　　運用管理の仕組みも重要
　　まだまだやってくよ。
　QA
　　Q：EMRは複数サービスから構築してるけど、一部がダウンするとどーなる？（例：SimpleDBが落ちてるとか）
　　A：ジョブはReRunしてもらうか、SimpleDBの状態をみてリカバリをしてもらう。
　　Q：上記にともない、SLAの計算は？
　　A：SLAはEMRについては定義できてない。S3などはリージョン跨いで
　　Q：S3のセキュリティが心配だが、どう考えてるか？
　　A：いままで脆弱性を晒してデータをロストしてない。
　　　　ユーザコントロールなどもできる。基本的には問題ない
　　Q：EMRでS3を使う＝Hadoopのローカリティの利点が失われるのでは？
　　A：オーバヘッドはあるが、実際のデータはS3で守られているので、HDFSが飛んでも大丈夫。
　　Q：S3からのコピーオーバヘッドはどーなるか？
　　A：Single5で少しずつor分割して全部送るも可能
　　Q：EMRはマルチテナントだけど他のユーザのネットワークの影響を受けないの？
　　A：マルチテナントだと起こる可能性がある。
　　　　M1（？）だと内部ネットワークは専有可能らしい。
</code></pre><p><strong>◎LT</strong></p>
<pre><code>
　・Hadoop and Subsystems in livedoor @tagomoris
　　ピーク時に15Gbps。
　　10ノード（36コア）
　　CDH3b2を利用
　　データマイニングではなく、ログからレポーティングをするのに利用している。
　　hadoop streaming + Hiveで実施
　　580G/dayが96サーバから来る
　　ScribeにてHadoop Streaming（Perl）でper hourでHiveにload
　　scribelineをWebサーバに入れててログ配送してくれる。
</code></pre><p>**　・Lightweight　@stanaka（はてなCTO：田中慎司さん）**</p>
<pre><code>
　　EMR以前
　　　自前20台クラスタ
　　　ジョブがあふれてきた
　　　ナマMapReduceを利用
　　　PerlでMapper/Reducerを記述（Hadoop Streaming）
　　EMR導入
　　　リソース増やせて便利！
　　　必要な台数に伸縮可能
　　　問題点：
　　　　S3にアップしないとダメ
　　　　　＝＞ログデータをS3に展開するlog2s3.plを作成。毎時実行。
　　　　起動、ジョブキック、結果改修どーする？
　　　　　＝＞Net::Amazon::EMR::Wrapperを作成した。
　　　　　　クラスタ起動、ジョブキック、クラスタ停止などできる
　　　　　作ってみて思ったこと
　　　　　　よかったところ：
　　　　　　　Perlでかける。Cronで実行可能。HiveQL便利。
　　　　　　悪かったところ。
　　　　　　　途中で失敗してクラスタ起動しっぱなしとかがある。S3にデータを展開が大変。複雑な計算がきつい。
　　　　慣れてないエンジニアにも触れるようにしたい
　　　　　＝＞Perlで書けるようにした？。
</code></pre><p>**　・HBaseでグラフ構造を扱う（開発中）：アメーバ鈴木**</p>
<pre><code>
　　自己紹介@brfrnl69
　　アメーバのソーシャルグラフ
　　　基本はMySQL
　　　マスタ分散が難しい、シャーディング管理が面倒
　　　　＝＞HBaseでやってみるか。
　　目的　
　　　グラフデータに対して高速に更新追加したい。
　　　隣接ノードが取れればいい（これを高速化したい）
　　　オンライン処理したい
　　　運用コストの削減したい。
　　Not目的
　　　マルチホップはどうでもいい。
　　アーキテクチャ
　　　JavaでGatewayつくってみる。
　・Large-Scale Graph Processing：井上さん（@doryokujin）さん
　　Map/Reduceではグラフ計算だめ？
　　　Vertex基本だとShuffleに問題ががが。
　　BSPの紹介
　　　Bulk Synchronous Processing
　　Google Pregel 
　　SSSP：MapReduce Model
　　　すごい計算時間が掛かりそう。MRの組み合わせが何回回ることやら。
　　　枝が少ないとこっちのほうがいいのか？
　　SSSP：PregelModel
　　　シンプルなアルゴリズム
　　Pregel使えるのあるの？
　　　Hama
　　　GoldenOrb
　　　Giraph
　　　　YERN？YARN？
</code></pre><p><strong>◎リクルート式Hadoopの使い方：石川（リクルート）</strong></p>
<pre><code>
　@ground_beetle
　・導入の課題点
　　バッチ処理時間対策のため。
　　　＝＞実は入れたかっただけｗ
　・導入の障壁
　　現行システムへの影響＋開発工数
　　　＝＞これへの対処がこのあとの話
　・課題の克服と活用シーン
　　Azkaban知らなかった。ジョブスケジューリングツール
　　Hadoop単体ではなく、エコシステム（関連ツール）が魅力
　・活用シーン：Hive
　　SQLゆーがざ多く、HiveQLがSQLライクのため導入が簡単に！
　　既存機能のリプレイス系案件に活躍（低工数＋簡単に高速化）
　　とりあえず、Hive実装
　　　＝＞性能アップのためにMapReduceで書きなおし
　　Hotpepperなどのアトリビューション分析に利用
　・活用シーン：Sqoop＋Hive
　　RDBとHadoop連携のツール：Sqoop
　　現行システムの横にHadoopを配置でき、RDBMSの利点も利用しつつHiveも利用できるようになる。
　　　（※気を付けないといけないけどねぇ。）
　　ゼクシィで活用しようとしてるところ。
　・活用シーン：Mahout
　　マイニング用ツール
　　カーセンサーのレコメンド（同時に閲覧される車種）
　　アソシエーション分析＋クラスタリング
　・活用シーン：BIツールの導入
　　何度か導入しようとして失敗してます。。。
　　BIツールの前処理（クレンジングなど）にHadoopを活用
　・インフラリソースは？
　　全部で118台。
　　最小のクラスタ構成はサーバ6台で構成してる
　・Hadoopで複数処理を回す方法
　　ここまで紹介したものを入れてますよ。Hive、Sqoop、Solr。。。
　・Azkaban
　　TomcatにWar配置で利用可能。
　　LinedInのチームにより作成
　※若干マシントラブルで中断
　・今後のHadoop導入
　　ログ基盤
　　分析エンジン・レコメンドエンジンとして
　　バッチ処理短縮＝トライアンドエラーが簡単にできるようになる。＝色々な気づきが出てくる。
　　アジャイル的な解析が可能に。
　・開発サイクルの短縮ができるエコシステムでビジネスが回りだす。
　・今後の展開
　　MapRが気になってる。
　　　マルチテナント対応ができてうはうはできそう。
　　　EMCと共同して検証中

　QA：
　　Q：性能監視ツールの選別の理由は？
　　A：Zabbix、Cactiです。今から利用しようと思っている段階。
　　Q：CDH、GreenPlum、Apacheをそれぞれ利用してるけど、使い分けのシーンは？比較は？
　　A：CDH3u0です。GreenPlumは使ってないです。理由は言うとクビが飛ぶ可能性ががが。
　　Q：高度なデータ分析ができる技術者はどこからヘッドハントしてるの？
　　A：MITに別途分析チームが存在し、高度な分析をしてくれる。
　　Q：NameNodeの冗長化はどーやってんの？
　　A：象本と一緒DRBD＋HeartBeat
　　Q：EMRの導入を検討してる？
　　A：今後、サーバ数が多くなると導入の検討をしていくと思う。
　　　　現時点は、運用ノウハウも入手するために社内で使ってる。
　　　　EMRになるかRCloud（リクルート社内クラウド）になるかは不明。
　　Q：HBaseを利用してる？
　　A：してないです。スペック的にサーバが買ってもらえないと厳しいです。
</code></pre><p><strong>◎The history and the future of Hadoop use case at Rakuten：Tarje Marthinussen（楽天）</strong></p>
<pre><code>
　・Introduction（self、rakuten）
　　NextGeneration Search Group
　　Norwayから来ましたよ。
　・RakutenのBigDataとは
　・Hadoop at rakuten　
　　Recommendation（2009）
　　ProductRanking、GenreRanking、Log解析、（2010）
　　Enhance Search。。。（2011）
　・PigやHiveのようなものが簡単に利用できるようになってきた
　・新しい検索プラットフォームを構築中
　　index 10k docs/sec
　　search 400qps
　・What's Next Generation Search
　　20%はインデックスとQuery評価の
　　50%がデータの前処理（データとQuery）
　　30%がアナライズ
　・Collecting Data？
　　ログの取得はバッチ処理だった。これをStreamingに（Flume）
　・Flume
　　Zookeeperを利用してFlumeを管理してる？？？
　・気になる部分があったので、改善のコーディングしてるよ。
　　・セキュリティ
　　　マスタで管理できるように
　　・Pools
　　　Agentが送信先がコケてると他のPoolにスイッチ可能に。
　　・MasterlessACKs
　QA
　　Q：パッチ作成は何人でやってるの？
　　A：50人が働いていてOSSコミッターが何人いるかはわからんです。
　　Q：ログはテキストだけど、パース処理はどこで？（古橋さん）
　　A：Flumeは各ノードでうまい構成になっていて
　　　　Decorator
　　　性能問題は？
　　　HiveはJSONデータをうまく活用できてるよ。
　　Q：Masterlessとそうじゃないバージョンの性能比は？
　　A：ノード数が増えると
　　Q：HBaseじゃなくてCassandraなの？
　　A：（※英語きつい。。。聞き取れず）
</code></pre><p><strong>◎マーケティング向け大規模ログ解析事例紹介：原謙治（NTTコミュニケーション）</strong></p>
<pre><code>
　・自己紹介
　・BizCITYというクラウドサービスを展開中
　　まずは、宣伝。
　　Bizストレージ、Bizマーケティングとして大規模データ、分散処理を実施してる。
　・Hadoop in Bizマーケティング
　　CGMデータを解析して口コミ情報抽出
　　アクセスログから行動情報抽出
　・Hadoop in BuzzFinder
　　CGM DBからHDFSにインポートして解析開始。DBはPostgreSQL
　　処理フローは資料参照。
　　リッチインデクシング技術（NTT研究所が開発した日本語解析技術）
　　※検索インデックスってどんなものなんだろう？
　　ポジネガ分析気になる。
　・Fast Map-Reduce for PaaS Services
　　アクセス解析やマーケティング解析を行う上でShuffleコストが大きくなるため大量マシンが必要
　　　＝＞マシン数を減らすことが目的？
　　Map Multi-Reduce、PJoinはNTT研究所が開発した技術！？（子象本にないっけ？？）
　　　＝＞Multi-Reduce。同一ノード上のMap出力をReduceすることで、shuffleフェーズに渡るデータを削減している。
　　　　＝＞PJoin　
　QA
　　Q：Map multi-reduce、PJoinはどう実現してるのか？公開するのか？
　　A：Hadoopの0.19に改造をしてる。
　　　　公開できるかどうかはわかりません。NTT研究所が研究しているものを試しに実装してみてるから。
　　　　なのでパッチにはなりませんかね（研究所に聞いてみないとわからないっす）
　　Q：性能が向上したパターンはあったが、悪化する場合などはあるのか？
　　A：不明
　　Q：Map-side Joinとの違いは？
　　A：。。。
※この辺で少し集中力が途切れてしまいました、すみません。（次に集中したかったので。。。）
</code></pre><p><strong>◎ミクシィにおけるHadoopの利用：伊藤敬彦（mixi）</strong></p>
<pre><code>
　LSH-Based Recommendation engine powered by hadoop
　・実はmixiの話はすくないよ。
　・利用している環境。5or6台/クラスタ
　・Hadoopの活用
　　ログデータをHDFSに保存
　　DBコンテンツをHDFSに入れることで、DBに負荷をかけずに解析する。
　・マイニングも検証中
　　検索クエリログをベースにデータマイニング
　では、本題。
　・LSHを利用した推薦
　・推薦とは？
　　mixiにはいろいろ推薦（レコメンド）を付加できるサービスがある。
　・推薦するには？
　　類似インスタンス集合を抽出する。
　　インスタンスは文書だったりユーザだったり
　・類似インスタンスとは？
　　同じ特徴を持つ集合
　　例：同一商品購入したユーザとか同一単語を持つ文書とか
　・抽出するには？
　　全ユーザごとに全ユーザとの類似性をチェックすると時間がかかりすぎる！
　　＝＞LSHを利用しよう！
　・LSHについて
　　特徴：
　　・速い！
　　・精度はそれほどではない
　　事例：
　　・Google Newsのレコメンド（USロケール）
　・LSHの処理ステップ
　　2つだけ。
　　1.インスタンスベクトルを計算（似ているデータは同じ値が返りやすい関数）
　　2.同一値が帰ってきたデータが類似インスタンス
　・インスタンスベクトル例（あとでスライド見ようね）
　・Likelikeがいち実装。Hadoopでうごくよ
　・実験について
　　トップページに表示されていない記事を知ってもらうために推薦してみるぞ！
　・実験１
　　性能を測ってみた
　・実験２
　　精度を調べてみた。
　　精度はほんとにひどいな。。。
　　＝＞同一カテゴリの遷移を元に計算したらそれなりになってきた。
　・今後
　　データソースを増やしたい。
　　他のアルゴリズムも実装したい
　・CM
　　以下も作ってますよ。
　　Oluolu
　　Anuenue

　QA
　　Q：Mahoutは使わないんですか？
　　A：Likelikeを作った頃にMahoutになかったので作った。
　　　　性能比較したいと思ってる。
　　Q：ログサーバのデータ転送はどーやってる？
　　A：伊藤さんのところにはどういう仕組みかは上がって来てない。

　　Q：ベクトルの次元数はどのくらいまで耐えられる？
　　A：高次元のデータに対して耐えられるように作られてる。
　　　　次元数が低くて良い制度が欲しければ他のアルゴリズムがいいかも。
　　Q：ユーザのベクトルを作るテクニックは？
　　A：画像などであれば大変だけど、ユーザであれば、単語とかキーワードとかで

　　Q：ニュース以外のデータは？
　　A：まだまだ実験中。まだやってない。

　　Q：推薦される記事数のばらつきはどういった理由が考えられる？
　　A：後ほど考察してQAサイトに入れます。

　　Q：LSHをMapReduceに載せたということだけど、関数の計算をさせてる？
　　A：Iterationはしてない。
　　　Map側でLSH関数計算してる。Reduceにて類似インスタンスを導出してる。

　　Q：今後の予定の空間木とは？R-Treeだと高次元できついのでは？
　　A：低次元にて利用できるように用意したい。
　　　
　　Q：HamaとかGiraphで高速化させてみるのはどう？
　　A：イテレーションがいらないからあまり利用用途がないかなぁ。
</code></pre><p><strong>◎懇親会</strong></p>
<pre><code>
　AWSを採用しない企業がいくつかあったのでそのあたりを質問してみた。
　AWSは通常運用に利用すると結構なコストがかかる場合があるので、ノウハウがある企業や
　データ量が多い場合は、オンプレミスの場合を選択しているらしい。
　やはりスポットで利用する方がお得感があるみたいだった。
　Twitter上だけの知り合いだった方々と面識が持てたのが一番の収穫でした。
</code></pre><p><strong>感想＋調べること</strong></p>
<p><strong>感想</strong></p>
<pre><code>
　会場規模とかQAサイトとかオープニングムービーとかすごかったです。しかも無料。さすがリクルートさん。
　ランチボックスとか出てきたし。装飾もとても無料のカンファレンスと思えない出来でした。
　午前中はHadoopをもとにした各社の戦略とCMという感じが色濃かったです。少しずつ各社の立ち位置が違いそれはそれで面白かったです。

　AWSの説明を聞いてやはりAWSを扱える技術は必要だと再認識。
　お金かけないで触ってみれるレベルでまずは触ってみるかなぁ。
　洗脳されてるのかもしれないけど、自社or自前でHadoop運用するのは厳しそうです。（懇親会ですこし認識が変わった）

　カフェコーナーでは、リクルートのMITの冊子が配られていたり、映像や案内に使われていたHadoopマスコットのシールが配られていたりと小技も聞いてました。
　あと、オライリーの販売コーナーも用意されていて、思わず子象本を買ってしまう罠にはまったりもしましたが。。。

　mixiの伊藤さんの話は最後で疲れていたのですが、ストーリーが上手くできていて、実験や事例もあったのでわかりやすかったです。
　Hadoopはエコシステムと呼ばれるHadoopを活用するツール群（Hiveの話が多かった？）、Hadoopの今後、Hadoopを活用したログ解析の話など、
　話題が豊富でそれぞれの話が面白くて困ってしいまうくらいです。
　いくつかのセッションで出てきたのですが、アクセスログなどをHDFS上に集めるための仕組みがまだ乱立（定番がまだない）している感じを受けました。
　Flume、Scribe、MapR。。。などなど

　あとは、テクニカルセッションのビデオがアップされたらまた目を通さないと。。。
　残念ながらHadoopから少し縁遠くなっていますが、これからもネタや参考になりそうな話には事欠かなそうなのでかじりついていこうと思います。
　※一番やらないといけないのは英語の勉強かもしれないです。。。
</code></pre><p><strong>調べること</strong>
一応Solrの人なので、Solrに関連しそうな話に興味がいってしまいます。</p>
<ul>
<li>NTT研究所のリッチインデクシング技術</li>
<li>やっぱりAWS触ってみる。EMRまでとは言わないが。（herokuもちょっと興味あるが。）</li>
<li>Mesos、GreenPlum、Scribe、Storm、Flume、Giraphこれらの概要</li>
</ul>
<p><strong>関連サイト</strong>
　<a href="http://mit.recruit.co.jp/hadoop/conference2011fall/SES000000001?pno=1">公式QAサイト</a>。後日録画していたセッションがアップされるそうです
　<a href="http://d.hatena.ne.jp/okachimachiorz/touch/20110927/1317076207">Hadoop Conferene Japan Fall 2011 - 急がば回れ、選ぶなら近道</a>
　<a href="http://itpro.nikkeibp.co.jp/article/NEWS/20110926/369421/?ST=cloud&amp;P=1">Hadoopカンファレンスが開催、本格普及を見据えた支援サービスや先進事例が充実 - ニュース：ITpro</a>
　<a href="http://infra-engineer.com/hadoop/hadoop-conference-japan-2011-fall%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%81%9F%E8%B3%87%E6%96%99%E3%82%84%E3%81%A4%E3%81%B6%E3%82%84%E3%81%8D-hcj11f/">Hadoop Conference Japan 2011 fallで使用された資料 #hcj11f | インフラエンジニアのつぶやき</a>（スライドをまとめてくれてます。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>「7つの言語　7つの世界」 Io 0日目(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/23/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-io-0%E6%97%A5%E7%9B%AE/</link>
      <pubDate>Fri, 23 Sep 2011 03:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/23/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-io-0%E6%97%A5%E7%9B%AE/</guid>
      <description>台風15号すごかったですね。幸いにも（？）夏休みだったので、通勤などでひどい目に合わずにすみました。 風雨はすごくてちょっと怖かったですが。。</description>
      <content:encoded><p>台風15号すごかったですね。幸いにも（？）夏休みだったので、通勤などでひどい目に合わずにすみました。
風雨はすごくてちょっと怖かったですが。。。</p>
<p>さて、夏休みに進める予定が、子供の寝かしつけで一緒に寝てしまう日が続いてしまい、
間が開いてしまいました。</p>
<p>0日目というタイトルになっているのは、まだ、1日目に入ってないからです。。。
Ioという未知の言語をMBAにそのままインストールするのも抵抗があり、VirtualBox上にLinuxをインストールしてから
進めようとして思いの外手こずってしまったためです。
ということで、0日目として、VirtualBox上にScientific Linux 6.1をインストールしてIoのインストールまでではまった箇所を記録として残しておきます。</p>
<h3 id="罠その１">罠その１</h3>
<p>罠と言うよりは、私の無知に関する部類なのですが。。。
Scientific Linux 6.1のインストールは特に手こずることなくインストールでき、
起動も出来ました。
次にscpコマンドでダウンロードしてきたIoのソースをLinuxに渡そうとしてはまりました。
問題となったのはネットワーク接続が「NAT」のみだったため。
NATのため、Linux（ゲストOS）から外部への接続は可能だったのですが、Mac（ホストOS）からLinux（ゲストOS）への接続ができませんでした。
で、変更したのは以下の2点。</p>
<ul>
<li>VirtualBoxの環境設定-&gt;ネットワーク-&gt;ホストオンリーネットワークの追加</li>
<li>仮想マシン（Linux）の設定-&gt;ネットワーク-&gt;アダプタ2を有効にしてホストオンリーアダプタを割り当て</li>
</ul>
<p>1番目のホストオンリーネットワークの追加をしておかないと、2番目のアダプタ2でホストオンリーアダプタを選択したときにエラーが出て、設定ができませんでした。割り当てるべきアダプタを先に用意しとかないとダメですよね、そりゃあ。</p>
<h3 id="罠その２">罠その２</h3>
<p>これも罠というほどではないのですが。。。
Ioのビルドには<a href="http://www.cmake.org/">cmake</a>が必要なのですが、Scientific Linux 6.1に入っているcmakeはバージョンが古い（2.6.4）ため、必要なバージョン（2.8以上）をインストールしないとダメでした。
インストール自体はcmakeのサイトにある手順通りのため割愛します。</p>
<h3 id="罠その３">罠その３</h3>
<p>これもちゃんとドキュメント読めよというレベルですが。。。
Ioのインストールは以下のコマンドを実行するという話です。
私がcmakeについて知らなかったと言われればそれまで。。。</p>
<pre><code>
$ cd io
$ mkdir build &amp;amp;&amp;amp; cd build
$ ccmake ..
$ make
$ sudo make install
</code></pre><p>3つ目の「ccmake」の部分がIoのGetting Startedの下の方にありました。上の方に記載がある「cmake」ではエラーがでてうまく行かなかったので。
IoのGetting Startedにもありますが、ccmakeの場合はCUI上にGUIのようなものが起動するので、「c」（configure）と実行後、「g」（generate）を実行して最後に「e」（exit）でccmakeを離れます。
するとmakeが実行できるようになりました。</p>
<p>あとは、/etc/ld.so.conf.d/io.confファイルを作成し、「/usr/local/lib」と記述。ldconfigを実行することで、Ioが実行可能になります。
ということで、1日目に入れず終了。。。</p>
<p>明日は出来るかなぁ。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 3.4リリース（速報）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/15/lucene-solr-3-4%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</link>
      <pubDate>Thu, 15 Sep 2011 09:31:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/15/lucene-solr-3-4%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</guid>
      <description>Solr/Lucene 3.4がリリースされました。（速報） 以下、各サイトへのリンクです。 Solrリリースのお知らせ Luceneリリースのお知らせ ちなみに、先日の</description>
      <content:encoded><p>Solr/Lucene 3.4がリリースされました。（速報）</p>
<p>以下、各サイトへのリンクです。</p>
<p><a href="http://lucene.apache.org/solr/#14+September+2011+-+Solr+3.4.0+Released">Solrリリースのお知らせ</a></p>
<p><a href="http://lucene.apache.org/#14+September+2011+-+Lucene+Core+3.4.0+and+Solr+3.4.0+Available">Luceneリリースのお知らせ</a></p>
<p>ちなみに、先日のSolr勉強会で関口さんが話されていた<a href="https://issues.apache.org/jira/browse/LUCENE-3418">インデックスが壊れるバグ</a>ですが、
先日のアメリカのハリケーン（Irene）で実際に電源が落ちて見つかったみたいです。</p>
<p>ということで、3.4がリリースされたので、3.1~3.3は利用しないほうがいいようです。</p>
<p><span style="color:#FF0000">追記：</span>
<a href="http://www.lucidimagination.jp/2011/09/lucene-3-4solr-3-4-%E3%81%8C%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/">lucidimagination.jpのサイトに日本語のリリースノートが公開されていた</a>ので、リンクを記載しておきます。</p>
</content:encoded>
    </item>
    
    <item>
      <title>「7つの言語　7つの世界」 Ruby 3日目（最終日）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/14/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-ruby-3%E6%97%A5%E7%9B%AE%E6%9C%80%E7%B5%82%E6%97%A5/</link>
      <pubDate>Wed, 14 Sep 2011 18:21:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/14/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-ruby-3%E6%97%A5%E7%9B%AE%E6%9C%80%E7%B5%82%E6%97%A5/</guid>
      <description>ということで、Rubyの最終日の感想。 今回もセルフスタディの私の回答が最後の方に記載されてます。見たくない人は気をつけてください。 ツッコミ大</description>
      <content:encoded><p>ということで、Rubyの最終日の感想。
<span style="color:#FF0000">今回もセルフスタディの私の回答が最後の方に記載されてます。</span>見たくない人は気をつけてください。
ツッコミ大募集です。コメント欄にどしどしコメントください。そこは違うだろ？こっちのほうがいいのでは？という感じで。</p>
<h2 id="感想">感想：</h2>
<h3 id="メタプログラミングが特徴">◎メタプログラミングが特徴</h3>
<p>例：ActiveRecordのhas_many、has_oneがいい例</p>
<h3 id="オープンクラス">◎オープンクラス</h3>
<p>クラス定義をいつでも変更可能。
あらゆるクラスやオブジェクトをいつでも再定義できる
書きやすいコードのために再定義が可能＝何でもできるが気を付ける必要あり。
DSLの定義に便利。
確かに便利。ただ、範囲を限定しないと予期せぬ場所で問題が発生しそう。
<span style="color:#0000FF">※解析するための手段はいいのがあるのかな？=&gt;method_missingみたい</span></p>
<h3 id="method_missing">◎method_missing</h3>
<p>対象メソッドが見つからない場合に最後に実行されるメソッド
通常はNoMethodErrorが発行される。</p>
<h3 id="モジュールによるメタプログラミング">◎モジュールによるメタプログラミング</h3>
<p>defやattr_accessorなどその一例。
DSLではモジュール内にメソッドを定義してメソッド＋定数を利用
<span style="color:#0000FF">ActsAs&hellip;ってそういう意味合いだったのか。</span>
親クラスバージョン、親クラス＋マクロ、モジュールそれぞれの実装の仕方の紹介。
<span style="color:#0000FF">※マクロもinclude同様、実行順で、メソッドの上書きが発生するのか？</span>
ActiveRecordではメタプログラミングを利用してDBのカラム名からアクセサを追加。
シンタックスの美しさ＝読みやすさ</p>
<h2 id="感想疑問点">感想＆疑問点：</h2>
<p>メタプログラミングはフレームワークを作成するのが便利そう。
ただし、エラーや問題が起きた時の対処をきちんと準備しておかないとひどい目に合いそう。
<span style="color:#0000FF">クラスやモジュールはわかりやすい単位で1ファイルにまとめるもの？</span>
<span style="color:#0000FF">ファイル名の規則とかあったりする？</span>
<span style="color:#0000FF">ディレクトリ構成でパッケージ構成が可能？</span>
<span style="color:#0000FF">複数のモジュール（gemとか）を組み合わせて使っている場合にincludeの順序がどのようになるかが気になる。</span>
予期せぬ順序でincludeされて利用しようと思ったメソッドがオーバーライドされてるとかありそう。
追っかけるのがまた大変そうだ。
異なるパフォーマンス（開発者の開発速度）の観点が一番おもしろかった。ただ、なれるまでは大変そう。すんなりinjectとかコードブロックをうまく利用するイメージがわかない。
まぁ、思考については反復練習かな。これは他の言語でも一緒かな</p>
<p>ようやく、Rubyの世界が終わりました。楽しかった。次は未知の言語である「Io」です。</p>
<p>　　　</p>
<h3 id="試してみよう">（試してみよう：）</h3>
<dl>
<dt>○eachメソッドがCsvRowオブジェクトを返すようにCSVアプリケーションを変更せよ。そのCsvRowのmethod_mmissingを使って、与えられた見出しの列の値を返すようにせよ。</dt>
<dd>
モジュールにて実装してみた。
```
<p>module ActsAsCsv</p>
<p>def self.included(base)
base.extend ClassMethods
end
　　　　　
module ClassMethods
def acts_as_csv
include InstanceMethods
end
end</p>
<p>module InstanceMethods</p>
<pre><code>def read
  @csv_rows = []
  file = File.new(self.class.to_s.downcase + '.txt')
  headers = file.gets.chomp.split(', ')

  file.each do |row|
    @csv_rows &lt;&lt; CsvRow.new(headers,row.chomp.split(', '))
  end
end

def each(&amp;block)
  csv_rows.each(&amp;block)
end

attr_accessor :csv_rows

def initialize
  read
end
</code></pre>
<p>end</p>
<p>class CsvRow
def initialize(headers, csv_contents)
@headers = headers
@csv_contents = csv_contents
end</p>
<pre><code>attr_accessor :headers, :csv_contents

def method_missing name, *args
  csv_contents.fetch(headers.find_index(name.to_s))
end
</code></pre>
<p>end
end</p>
<pre><code>ここまでが実装したモジュール＋クラス。
以下は実行例。id,name,sizeというheaderをもつCSVを使ってみた。

</code></pre><blockquote>
<blockquote>
<p>require &lsquo;acts_as_csv_module_mod.rb&rsquo;
=&gt; true
class RubyCsv
include ActsAsCsv
acts_as_csv
end
=&gt; RubyCsv
csv = RubyCsv.new
=&gt; #&lt;RubyCsv:0x103c4c530 @csv_rows=[#&lt;ActsAsCsv::CsvRow:0x103c4c030 @csv_contents=[&ldquo;1&rdquo;, &ldquo;RubyCsv.class&rdquo;, &ldquo;20&rdquo;], @headers=[&ldquo;id&rdquo;, &ldquo;name&rdquo;, &ldquo;size&rdquo;]&gt;, #&lt;ActsAsCsv::CsvRow:0x103c4be78 @csv_contents=[&ldquo;2&rdquo;, &ldquo;JRubyCsv.class&rdquo;, &ldquo;50&rdquo;], @headers=[&ldquo;id&rdquo;, &ldquo;name&rdquo;, &ldquo;size&rdquo;]&gt;]&gt;
csv.each {|row| puts row.name}
RubyCsv.class
JRubyCsv.class
=&gt; [#&lt;ActsAsCsv::CsvRow:0x103c4c030 @csv_contents=[&ldquo;1&rdquo;, &ldquo;RubyCsv.class&rdquo;, &ldquo;20&rdquo;], @headers=[&ldquo;id&rdquo;, &ldquo;name&rdquo;, &ldquo;size&rdquo;]&gt;, #&lt;ActsAsCsv::CsvRow:0x103c4be78 @csv_contents=[&ldquo;2&rdquo;, &ldquo;JRubyCsv.class&rdquo;, &ldquo;50&rdquo;], @headers=[&ldquo;id&rdquo;, &ldquo;name&rdquo;, &ldquo;size&rdquo;]&gt;]
csv.each {|row| puts row.id}
(irb):8: warning: Object#id will be deprecated; use Object#object_id
2179096600
(irb):8: warning: Object#id will be deprecated; use Object#object_id
2179096380
=&gt; [#&lt;ActsAsCsv::CsvRow:0x103c4c030 @csv_contents=[&ldquo;1&rdquo;, &ldquo;RubyCsv.class&rdquo;, &ldquo;20&rdquo;], @headers=[&ldquo;id&rdquo;, &ldquo;name&rdquo;, &ldquo;size&rdquo;]&gt;, #&lt;ActsAsCsv::CsvRow:0x103c4be78 @csv_contents=[&ldquo;2&rdquo;, &ldquo;JRubyCsv.class&rdquo;, &ldquo;50&rdquo;], @headers=[&ldquo;id&rdquo;, &ldquo;name&rdquo;, &ldquo;size&rdquo;]&gt;]</p>
</blockquote>
</blockquote>
<pre><code>
&lt;span style=&quot;color:#FF0000&quot;&gt;※idというcsvフィールド名にしたら、object_idとかぶっているようでwarningが出てしまった。&lt;/span&gt;

CsvRowクラスの定義がモジュールの中に入っているが、この実装でも動くみたい。ただ、パッケージみたいな感じ7日までは調査してない。。。
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>第6回Solr勉強会に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/12/%E7%AC%AC6%E5%9B%9Esolr%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Mon, 12 Sep 2011 16:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/12/%E7%AC%AC6%E5%9B%9Esolr%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>「第6回Solr勉強会」に参加しました。 なんだかんだと第6回で、今のところ皆勤賞です。 ということで、主に自分用ですが、メモなどとったので。 概</description>
      <content:encoded><p><a href="http://atnd.org/events/18637">「第6回Solr勉強会」</a>に参加しました。
なんだかんだと第6回で、今のところ皆勤賞です。</p>
<p>ということで、主に自分用ですが、メモなどとったので。</p>
<p><strong>概要</strong></p>
<ul>
<li>日時 :2011/09/12 19:00 to 21:00</li>
<li>定員 :110 人</li>
<li>会場 :株式会社 ECナビ</li>
</ul>
<pre><code>
1.「Lucene/Solr 3.2-3.4」 by  ロンウイット 関口 宏司さん
　※Solr2.9.4、3.1でデグレードし、Solr3.4で修正されたバグがあります。
　　インデックス中にPCがシャットダウンされた場合にインデックスが壊れてしまうものあり。
　　Solr3.1-3.3は使用しないようにとのこと。
　○index
　　・IndexWriter.addDocuments(docs)
　　　複数ドキュメントの更新が可能になった。
　　　※検索で話をする親子関係のNestedQuery向けの登録にも利用する。
　　　この登録では途中でフラッシュされないため、セグメントが分割されない特徴あり。
　　　複数だけでなく、
　　・TieredMergePolicy
　　　※Lucene3.2/Solr3.3からデフォルトになった。
　　　新しいインデックスのマージポリシー。（amebaの開発者ブログに説明があった。）
　　　インデックスの登録順が守られていたが、このマージ処理では保証がされなくなったので注意が必要。
　　・update.chain
　　　update.processorがdeprecatedに。
　　　update.chainというパラメータに変更
　○index(cont'd)
　　・TwoPhaseCommit
　　　
　　・UniqFieldsUpdateProcessor
　　　重複した値を削除するための機能。
　　　UIMAでの提案から取り込まれた
　○search
　　・group=on
　　　検索グルーピング機能を使えるパラメータ
　　・{!term}
　　　クエリパーサを通さないでTermQueryをかける機能。
　　・structured explanation
　　　debug=onの算出根拠を
　　　XMLタグで構造化されたExplanationが取得可能に
　　・ReloadCacheRequestHandler
　　　関口さん担当。ExternalFileField
　　　インデックス外（外部ファイル）を元にFunctionQueryの情報を利用可能にできるのだが、
　　　このファイルのリロードが可能になる。
　　・Carrot2　3.5.0
　　　アップグレード。
　　　デモあり。Carrot2のworkbenchとか　　
　○search(cont'd)
　　・hl.phraseLimit parameter
　　　FastVectorHighlighterの高速化用パラメータ。
　　・{!cache=false}
　　　キャッシュを利用しないためのローカルパラメータ。fqのローカルパラメータに利用可能。
　　　検索セッション内でキャッシュへの処理（参照・登録）をしなくなる
　　・BlockJoinQuery
　　　NestedDocumentQueryの名前が変更された。
　　　（Luceneで登録されたところまで。）
　○schema
　　・KStemFilter
　　　PoterStemmerとは別のstemmer
　　・ReversePathHierarchyTokenizer
　　　パスの構造化インデックスの逆バージョン
　　・ommitPositions=&quot;true&quot;
　　　指定が可能になった。
　　・version 1.4
　　　スキーマのバージョンが新しくなった。
　○admin/tools
　　・action=MERGEINDEXES&amp;srcCore=coreName
　　　コア名指定可能。
　　・action=UNLOAD&amp;deleteIndex=true
　　　UNLOAD時にインデックス削除
　　・action=CREATE&amp;property.name
　　　※ぎゃーーー。打ち込み失敗。
　○ぎゃーーーー失敗。
　○技術者大募集中！！
　Q&amp;A
　　Q：クラスタリングの日本語は大丈夫？入門には制限があると記載があったが。
　　A：analyzerが記載が簡単になってるかは不明
　　　　前処理をして別フィールドにメタデータ的にある程度単語で区切ったようなものを作成したほうがいい。
　　Q：グルーピングで、グルーピング後のファセット件数が取得可能か？
　　A：3.4ではグループ数で表示可能。
　　　　パラメータ指定で可能。

2.「Solr＠cookpad」 　by ＠PENGUINANA_ さん
　○COOKPADとは？
　　・レシピ投稿サイト
　　・105万のユーザのレシピ
　　・30代女性の1/2が利用
　○レシピ検索
　　・PC：1307万UU、1億回強/月
　　・モバイルで利用が多く、スーパの店頭などで利用？
　　・Androidもあるよ。
　○人気順検索（Solrですよ）
　　
　○自己紹介
　　・＠PNGUINANA_
　　・情報可視化＋検索が好き！！
　　　yats、など
　○Solrはどのように利用
　　・レシピ検索
　　・もしかして
　○Tritonnから移行
　　同じ検索結果になるようにして徐々に移行
　○なぜ？
　　パフォーマンスが良い。
　　フィールド追加が簡単
　　レプリケーション-&gt;ファイルベース
　　プロトタイピングが楽
　○Solr4 nightly（on Oracle JVM）
　　・マルチコア利用
　　・Ruby on Railsから
　○簡単な構成＋説明
　　・バッチからSolrへの更新をしている。
　　　※Analyzerを利用せず、バッチ側で分かち書き、正規化、同義語展開を行っている。
　　　Rubyで全部かけるほうが社内に展開しやすい。
　　・バッチ終了後、可能ならoptimize
　　　※検索速度がmax20%高速になる。
　　・マスタ-&gt;スレーブレプリケーションはschema.xmlもレプリケート
　　　新規フィールド追加などはレプリケーションだけで実行できて楽。
　　・アプリはMySQLとSolrのSlaveにアクセス。
　　　Solrにはidのみ。本文はMySQLから取得
　　　インデックスサイズを小さくできる＝レプリケーション時間が短くなる
　　　オンメモリにできるため検索速度も向上
　○監視（munin）
　　・監視項目（コア別）：
　　　クエリ：QTime/QPS
　　　キャッシュ：hit/eviction
　　　　キャッシュから漏れている数をみてキャッシュサイズを定期的に変更して無駄をなくす
　　　インデックス：サイズ/docの数
　　　　運用してから重要。開始当初は気にしてるが、そのうち気にならなくなるため。
　　　レプリケーション：所要時間
　　　　スレーブ間でのズレを検知するため。
　　　※コアごとに監視することで、問題点を把握しやすくしている。
　○監視（nagios）
　　・監視項目（コア別）：
　　　　サーバの基本的なヘルスチェック
　　　　　Solrが動いてるサーバのはなし。
　　　　レプリ：インデックスバージョンのチェック
　　　　　ズレが長いとメールが飛ぶ
　○便利だった機能
　　・DynamicField
　　　フィールドをあとから簡単に追加可能。
　　　例：人気順のアルゴリズムの違うフィールド。検索用フィールド
　　・FacetQuery
　　　絞り込み検索をクエリで記載可能。
　　　現時点では社内向け検索機能で利用。
　　　※ファセットで簡単な解析もやってる。
　　　　例：鍋の季節ごとの登録件数。
　　・HTTP経由で色々可能
　　　検索の並列化
　　　　通常検索画面：3クエリを同時実行
　　　　あるプロトタイプ画面だと8クエリで実行したりしてる。
　　・分散検索（Distributed Search）
　　　簡単にsharding可能
　　　　思いクエリは４shardで投げる
　　　　オーバヘッドが大きいので思いクエリにだけ利用しているらしい。
　○開発の流れ
　　・まずはステージングを更新
　　・問題なければマスターも更新
　　　例外：
　　　　フィールド追加するだけだったら直接マスタへ
　　　　例：ファセット追加など。
　○パフォーマンスとか大丈夫？
　　本番で複数のバージョンを持っており、
　　バグっていても自動フォールバックするらしい。
　　価値があったらパフォーマンス向上＋テスト追加
　　例：スニペット変更
　○気になっている機能
　　・not to cache(SOLR-2429)
　　・SurroundQuery(Solr-2703)
　　・JOIN(SOLR-2272)
　　　SQLのJOINなイメージ
　　・BloomFilter(SOLR-1375)
　　　単語が存在するかどうかのチェック
　○Solr入門おすすめ
　○おすすめ
　　・http://blog.sematext.com
　　　月一で新機能が出てくる
　　・SolrのJIRA
　　・@otisg
　○今後やりたいこと
　　・わかち書きの精度向上
　　・検索セッションの分析
　　　nDCG、クエリ分類、検索意図
　　・デバイス対応
　　・パーソナライズ
　Q&amp;A
　　Q：同義語は誰が集めている？
　　A：外部辞書を利用。０件キーワードから解析して取得
　　　　単語登録も一緒。
　　Q：プチトマトとトマトの違いはどうやってる？
　　A：上位、下位の概念で同義語を利用している。
　　　本にあるよ。
　　Q：もしかしてはSolrの機能？
　　A：Solrではない。訂正候補の単語をSolrに検索してからチェックして表示する。
　　Q：人気順はどういった計算をしてるんですか？計算してから登録？By 大谷
　　A：１フィールドに外から入れている。
　　Q：RubyからSolrの利用方法は？独自？ライブラリ？By 大谷
　　A：Rsolrを利用しており、コネクション管理にだけ利用している。
　　　　あとは、ラッパーを独自で作成。
　　Q：4.0を利用している理由は？なかなかチャレンジャー。By関口
　　A：グループクエリを利用したかったため。
　　　　実際には重くて使えていない。
　　　　今は必要ないかもと思っている。年始のバージョンを利用。いつでも入れ替え可能。
　　　　マスタスレーブ構成のsolrのバージョンアップはサービスアウトしてから入れ替える。

3. 「Solrを用いた検索システムの構築」　by データセクション株式会社 高井さん
　　いろいろな試行錯誤について
　○データセクションについて
　　言語処理を元にデータの解析（Twitterとかブログとか）している会社。
　　・昔は自社で検索サーバを構築していた
　　・Luceneを利用して検索サーバを構築するように変更。
　○構成（過去？）
　　・Lucene＋RMI
　　　3.0から縮小で、4.0で廃止に
　○SolrにするかLuceneにするか？
　　・いろいろ機能があるからSolr使ってみるかｗ
　○Solr導入は1.4.1から
　　スペック
　　　マスタ：
　　　　メモリ：16G
　　　　Disk：2Tｘ12
　　　スレーブ：
　　　　256GのSSDを利用
　　　　JavaVMが32bit
　○ひと月分を1shardにして登録
　○検証＋手探り？
　　メモリが足りない。
　　Solrのキャッシュを全Offに。
　　BOBO SolrPluginってのを利用
　　compressオプションも利用。
　　各スレーブにキューを用意して1つだけしか処理しない。（なんでだ？？）
　○結果
　　キャッシュはフィルタキャッシュのみ利用
　　ユーザが同じクエリを投げるのはほぼないので。
　　フィルタキャッシュのエントリのメモリ量の計算式（あとで資料が出てくるかなぁ。）
　○問題点
　　・レプリケーションでインデックスサイズの2倍の容量が必要になる。
　　・レプリケーションの日時フォーマットのバグ（SOLR-1995）を踏んでしまった。
　　・レプリケーション後にインデックスが消えない場合がある
　○検討（1.4.1-&gt;3.2）
　　うれしい要素
　　　レプリケーションバグがfix
　　　省メモリや範囲検索の高速化など　
　　かなしい要素
　　　compressが使えなく（しかも回答してからriindexしてくれる）
　○検討
　　余計なインデックス消す
　　nearinfinityが出しているlucene-compressionを利用
　　　https://github.com/nearinfinity/lucene-compression
　○残った問題
　　facet.rangeが使えないなど。
　○じゃあ集約サーバつくっちゃえ（すごいな。）
　　facet書き換えコンポーネントなど。
　○シャードのインデックスサイズが下がった
　　130GiB-&gt;90GiB
　　これはlucene-compressionの効果
　○ここまでの変更はプラグインにて対応（この一覧いいかも）
　○感想
　　コンポーネント化されてて、簡単に追加機能が実装可能
　　用途次第であまりスペック高くなくても使える。

　Q&amp;A
　　Q：Twitterのデータのクロール方法は？
　　A：HTMLをスクレイピングしている。publicなTLのみ。
　　Q：いきなりSolrに入れる？Solrの前処理は？
　　A：SolrJを利用して前処理済みのデータを登録している。

4. 「Anuenueの紹介と最近の進捗」 by @takahi_i さん（mixi）
　○自己紹介
　　・NAIST出身
　　・ファストサーチ
　　・今はmixi
　○mixiとは？
　○社内の緊急タスク
　　・内製検索エンジンをメンテナンスしやすいOSSにしたい！
　　　-&gt;Solrを選択
　　　　Anuenueも実装
　○Anuenueの作成理由は？
　　インデックス運用が面倒（検索（distributed search）はあるがインデックスは自前）
　　クラスタ用のコマンドが提供されていない。
　○Anuenueが提供する機能
　　・検索クラスタの簡単設定
　　・クラスタ用コマンド
　　・もしかして機能
　○機能：Anuenueのクラスタ設定
　　Merger：クライアントからのクエリをMasterに分散
　　Master：インデックス作成側
　　Slave：検索用スレーブ。マスタからコピー
　○クラスタの管理コマンド
　　クラスタのコマンドを用意。
　　起動、コミット、登録など
　○SolrCloud向けのAnuenueを検討中
　　branch-cloudにあります。
　　今後の予定
　　　インスタンス追加削除を動的に実行できるようにしたい
　○Hadoop Conferenceの宣伝ｗ
　　Oluolu：Hadoop上で動くクエリログマイニングツール
　　Likelike：LSHをHadoopで実装（Hadoop Conference 2011 Fallで発表）
　　
5. LT
5.1　「solrとRの連携について」　by @yutakashino さん（BakFoo）
　　　Python本、Zope本を書いてます。
　　○NHKの実証実験で利用？
　　○TwitterストリームをSolrにストア
　　　facet.date
　　○Rでキーワード頻度グラフ
　　　Node.js、Redis、R、Solrを使ってる。しかもPython
　　○デモ
　　　キーワード＋日付でグラフが出てくる。
　　　Rでプロット。
　　　GoogleのチャートAPIを利用すると面白いことができる。


5.2　「 Apache ManifoldCF」　by 阿部さん(ロンウィット)
　○Apache Incubator
　○Manifold Connector Framework
　　Solr ＜－ MCF ＜－ web＋non-web repositories
　　すぐに利用可能。
　○概要
　　出力はSolr。
　　接続先はWeb、DB、CMS、などなどいろいろRepositoryConnectorというのがあります。
　○Crawler Agent
　　クロールに関するJobの管理
　　　接続先、スケジュールなど
　○Windowsサーバのクロールもできる
　　社内のナレッジ共有などに使える。権限周りも簡単に対応可能。
　　JCIFS.jarによりWindowsの権限情報を取得
　○クロール設定画面もある
　　デモ
　○導入が簡単なのがおすすめ。
　○ManifoldCFの資料関連
　　http://www.manning.com/wright

</code></pre><p>懇親会についてはあとで記載します！！
ということで追記です。
懇親会でも色々と面白い話を聞けました。
<a href="http://twitter.com/#!/PENGUINANA_">@PENGUINANA_</a>さんにはCOOKPADのCI関連の話を聞けました。
1日に数度リリースするという話もあるようでした。</p>
<p>あとは、Rについても<a href="http://twitter.com/#!/yutakashino">@yutakashino</a>さんから概要が聞けたのが良かったです。</p>
<p>ちょっとだけ追記。まだ成形する予定ですが、とりあえず。</p>
<p>追記：スライドが公開され始めたので。</p>
<p><a href="http://www.slideshare.net/KojiSekiguchi/lu-solr32-3420110912">関口さんのスライドはこちら</a>
<a href="http://www.slideshare.net/ShinichiroAbe/apache-manifoldcf">阿部さんのスライドはこちら</a>
<a href="http://www.slideshare.net/penguinana/solr-at-cookpad">＠PENGUINANA_さんのスライドはこちら。</a>
<a href="http://code.google.com/p/anuenue-wrapper/downloads/list">@takahi_iさんのスライドはこちらからPDFダウンロード可能。</a></p>
<p>その他に関連するブログも見つけましたので。
<a href="http://d.hatena.ne.jp/code46/20110913/p1">「Solr@Cookpad」- Solr勉強会で発表してきました</a>
<a href="http://kshigeru.blogspot.com/2011/09/solr-study-6.html">Ars longa, vita brevis: 第６回 Solr 勉強会に行ってきた</a></p>
<p>勉強会で出てきた各製品などのリンク：
色々とURLが出てきていたので、リンクをまとめておきます。</p>
<ul>
<li><a href="https://github.com/nearinfinity/lucene-compression">lucene-compression</a></li>
<li><a href="http://code.google.com/p/anuenue-wrapper/">Anuenue</a> Apache Solr のラッパー</li>
<li><a href="http://code.google.com/p/likelike/wiki/QuickStart">Likelike</a> LSH (Localtiy Sensitive Hashing)の実装</li>
<li><a href="http://code.google.com/p/oluolu/">OluOlu</a> 検索ログマイニングツール</li>
<li>@takahi_iさんの<a href="http://www.lucenerevolution.org/blog/2011/05/31/they-type-it-how-anuenue-mixi-and-the-difficulties-in-japanese-spell-check/">LuceneRevolution2011のスライド</a></li>
<li><a href="http://atilika.org/">Kuromoji</a> 形態素解析器</li>
<li><a href="http://code.google.com/p/lucene-gosen/">lucene-gosen </a>形態素解析器（Senの後継）</li>
<li><a href="http://incubator.apache.org/connectors/">Apache ManifoldCF</a></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>「7つの言語　7つの世界」 Ruby 2日目(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/10/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-ruby-2%E6%97%A5%E7%9B%AE/</link>
      <pubDate>Sat, 10 Sep 2011 02:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/10/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-ruby-2%E6%97%A5%E7%9B%AE/</guid>
      <description>ということで、Ruby2日目の感想（2日目だけで2日間かかったのは内緒。。。） 今回もセルフスタディの私の回答が最後の方に記載されてます。見た</description>
      <content:encoded><p>ということで、Ruby2日目の感想（2日目だけで2日間かかったのは内緒。。。）
<span style="color:#FF0000">今回もセルフスタディの私の回答が最後の方に記載されてます。</span>見たくない人は気をつけてください。
ツッコミ大募集です。コメント欄にどしどしコメントください。そこは違うだろ？こっちのほうがいいのでは？という感じで。</p>
<h2 id="感想">感想：</h2>
<h3 id="関数の定義span-stylecolor0000ffありゃ利用するのはダメなんだこれじゃspanspan-stylecolorff0000--gt単なるタイプミスでしたお恥ずかしいspan">◎関数の定義：<del><span style="color:#0000FF">ありゃ、利用するのはダメなんだ、これじゃ。</span></del><span style="color:#FF0000">&ndash;&gt;単なるタイプミスでした。お恥ずかしい。。。</span></h3>
<pre><code>
&gt;&gt; def tell_the_truth
&gt;&gt;   true
&gt;&gt; end
=&gt; nil
&gt;&gt; tell_the_trueth()
NoMethodError: undefined method `tell_the_trueth' for main:Object
     from (irb):4
</code></pre><h3 id="配列そしてシンタックスシュガー">◎配列：そしてシンタックスシュガー</h3>
<p>puts animalsで内容が出力されるのはうれしい。JavaだとHashCodeが出てくるから。出力メソッド書かないといけなくなる。
animals[-1]で最後の要素とかかなり便利。
animals[0..1]はRangeクラスを利用する形。Rangeはやはり便利。substringなどもできそう。
カラ配列の定義は必要。a = []
1.9と1.8でinclude?の書き方が異なるので注意！
配列（Array）クラスは中はObjectが入る。
多次元配列もOK。popやpushでキューとしても利用可能。</p>
<h3 id="ハッシュ">◎ハッシュ：</h3>
<p>Mapのようなもの。任意のキーが利用可能。:付きの文字列はシンボルと呼ばれる定数値を簡単に定義する方法。
object_idという属性？関数によりObjectのハッシュコードが取れるらしい。
ブレース＝「{}」のカッコのこと。do～endでも代用可能</p>
<h3 id="コードブロックとyeild">◎コードブロックとyeild</h3>
<p>コードブロック＝名前なし関数。習慣では複数行の場合、do/endで、１行は{}みたい。Javaと混同しそう。
コードブロックは引数も指定可能。
「yield」予約語？を利用してコードブロック自体をメソッド内部などで呼び出し可能。
ということで、コードブロックは引数にも指定できると。<span style="color:#0000FF">コードブロック＝関数もObjectとして扱われてる感じか？</span>
<span style="color:#0000FF">実行遅延、分岐、共通関数とか？Javaだとabstractメソッドを利用して処理するようなイメージか？</span>ちょっと違うなぁ。
なれると、yieldはコードを読みやすくできそう。また、シンタックスハイライトしてくれるツールがあれば、更に便利。</p>
<h3 id="ファイルの実行">◎ファイルの実行</h3>
<pre><code>
ruby ファイル名
</code></pre><p>vi、Emacs、TextMateなどがあるよ。</p>
<h3 id="クラスの定義">◎クラスの定義</h3>
<p>※数字で始まる変数は利用できない！Javaと一緒
Tree.rbとログを参考にすること。
&amp;使うとブロックに名前が付けられる。<span style="color:#0000FF">yieldじゃなくてもいい？</span>
クラス名はキャメルケース。
変数とメソッド名は「_」アンダーバーつなぎ
インスタンス変数の頭は@
クラス変数は@@
定数は大文字
※メソッド名、変数名には違和感が。<span style="color:#0000FF">なんでこんな規則？？</span>
判定用関数とメソッドには「?」（if test?）をつける！！Javaでいう「is」か。</p>
<h3 id="mixin多重継承モジュールと呼ばれるものを利用">◎Mixin（多重継承？モジュールと呼ばれるものを利用）</h3>
<p>多重継承のような類似の振る舞いを伝搬する仕組み。
Javaではインタフェースでやること。
Rubyではモジュールといい、関数と定数の集まり。
クラスに機能を盛り込む場合はincludeする
<span style="color:#0000FF">※複数includeして、includeしたものの中に同じメソッドとかあったらどーなる？</span>
<span style="color:#0000FF">　&ndash;&gt;Overrideされた＝includeがあとにあるもので上書きされる</span>
Abstractにできないものをモジュール化できるの楽。
javaだとstaticメソッドだらけのUtilクラスを別途起こすイメージだけど、内部で呼ばれるメソッド（コードブロック）が同じインタフェースじゃないと行けないから、インタフェースの記述もしないと行けない。
ただし、同一名のメソッドを持ってるとややこしそう。
<span style="color:#0000FF">※モジュールからモジュールは呼べる？</span></p>
<h3 id="モジュールenumerableセット">◎モジュール、Enumerable、セット</h3>
<p>EnumerableとComparable（JavaのCollectionまわりかな。）
宇宙船演算子（&lt;=&gt;）Javaのequalsに似てる
any?とかCollectionUtils？？に似たのあったな。
<span style="color:#0000FF">※今利用しているクラスが何をincludeしてるかってのは分かる仕組みあるのかな？
※そういえば、メソッドごとに戻り値があるが、全部newされてインスタンス化されてGCの対象になってるのか？irbだけ？</span>
injectはすんなり使うイメージが出にくそう。また、ソースをぱっと見て理解出来ない。なれだろうけど。
※injectしながらinjectとかあるんだろうな。</p>
<h3 id="探してみよう">（探してみよう：）</h3>
<dl>
<dt>○コードブロックを使った場合と使わない場合の両方について、ファイルにアクセスするコードを書く。コードブロックの利点は？</dt>
<dd>※コードブロックあり。
```
<p>File.open(&ldquo;tree.rb&rdquo;, &lsquo;r&rsquo;) {|f| f.each {|line| puts &ldquo;#{f.lineno} : #{line}&quot;}}</p>
<pre><code>※コードブロックなし
</code></pre><p>f = File.open(&ldquo;tree.rb&rdquo;, &lsquo;r&rsquo;)
while line = f.gets
puts &ldquo;#{f.lineno} : #{line}&rdquo;
end</p>
<pre><code>コードブロックの利点：
　見通しの良さ。行数が少なくてすむ。繰り返し処理が簡単に記述できる。
　※うーん、まだきちんと理解できてないか？
&lt;/dd&gt;
&lt;dt&gt;○ハッシュを配列に変換するにはどうすればよいか？また、逆に配列をハッシュに変換する方法は？&lt;/dt&gt;
&lt;dd&gt;
※間違えた。。。
</code></pre><blockquote>
<blockquote>
<p>h = [:key1 =&gt; &ldquo;hoge&rdquo;, :key2 =&gt; &ldquo;boke&rdquo;, :key3 =&gt; &ldquo;fuga&rdquo;]
h.each {|key, value| puts &ldquo;#{key} is #{value}&quot;}
=&gt;key1hogekey2bokekey3fuga is</p>
</blockquote>
</blockquote>
<pre><code>
ハッシュ-&amp;gt;配列変換
</code></pre><p>h.to_a</p>
<pre><code>
配列-&amp;gt;ハッシュ変換
</code></pre><p>a = [&ldquo;value1&rdquo;, &ldquo;valu2&rdquo;, &ldquo;value3&rdquo;]
h = {}
a.each {|i| h.store(a.index(i),i) }</p>
<pre><code>※また間違い？なんでそうなる？
</code></pre><p>a.inject(h2) {|hoge, i| hoge[a.index(i).to_s] = i}</p>
<pre><code>こっちならいいみたい。まだinjectがわかってない。
</code></pre><p>a.inject(h2) {|hoge, i| hoge[a.index(i).to_s] = i;puts &ldquo;#{i}&quot;; hoge}</p>
<pre><code>
&lt;/dd&gt;
&lt;dt&gt;○ハッシュの各要素について繰り返すにはどうすればよいか？&lt;/dt&gt;
&lt;dd&gt;
</code></pre><p>h.each {|key, value| puts &ldquo;#{key} is #{value}&quot;}</p>
<pre><code>&lt;/dd&gt;
&lt;dt&gt;○Rubyの配列はスタックとしても使える。スタック以外に配列で実現可能なよくあるデータ構造体を挙げよ。&lt;/dt&gt;
&lt;dd&gt;キュー。
他にある？Treeとか？Treeはハッシュじゃないか？Set？順番が関係ないけど。&lt;/dd&gt;
&lt;/dl&gt;


### **（試してみよう：）**
&lt;dl&gt;
&lt;dt&gt;○最初に、eachだけを用いて、１６個の数値と４個の数値の配列の中身を同時に出力せよ。次に、同じ事をEnumerableのeach_sliceを用いて実行せよ。&lt;/dt&gt;
&lt;dd&gt;
&lt;span style=&quot;color:#FF0000&quot;&gt;&lt;em&gt;※これ日本語がわからないんだが、4個ずつ出せってことか？？&lt;/em&gt;&lt;/span&gt;ということで、「16個の数字の配列の中身を4個ずつ同時に出力せよ。」と解釈して実装してみた
※each利用版
</code></pre><p>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
s = []
a.each do |b|
unless s.length &lt; 4
puts s.inspect
s.clear
end
s &laquo; b
end</p>
<pre><code>
※each_slice
</code></pre><p>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
a.each_slice(4) {|b| puts b.inspect}</p>
<pre><code>また、えらい違いだな。
&lt;/dd&gt;
&lt;dt&gt;○Treeクラスは面白いクラスだったが、きれいなインタフェースを用いて新しいツリーを指定することは出来なかった。そこで、initializerにハッシュと配列が入れ子になった構造体を指定できるようにせよ。具体的には、次のようなツリーを指定できるようにしたい。{'grandpa' =&amp;gt; { 'dad' =&amp;gt; {'child 1' =&amp;gt; [], 'child 2' =&amp;gt; [] }, 'uncle' =&amp;gt; {'child 3' =&amp;gt; [], 'child 4' =&lt; [] } } }&lt;/dt&gt;
&lt;dd&gt;&lt;span style=&quot;color:#FF0000&quot;&gt;※root（ここではgrandpaレベル）が複数あると破綻しないのか？&lt;/span&gt;
</code></pre><p>class Tree
attr_accessor :children, :node_name</p>
<p>def initialize(name, children=[])
@children = children
@node_name = name
end</p>
<p>def initialize(hash)
hash.each do |key, value|
@node_name = key
@children = value.inject([]) do |array, (child_key, child_val)|
puts &ldquo;inject! #{key}&rdquo;
[Tree.new({child_key =&gt; child_val})] + array
end
end
end</p>
<p>def visit_all(&amp;block)
visit &amp;block
children.each {|c| c.visit_all &amp;block}
end</p>
<p>def visit(&amp;block)
block.call self
end
end</p>
<pre><code>
残念ながらちょっとカンニングしてしまいました。。。
&lt;/dd&gt;
&lt;dt&gt; ○ファイル内で、あるフレーズを含む全ての行を出力する簡単なgrepをかけ。簡単な正規表現でマッチングを行い、ファイルから行を読み出す必要がある（この処理はRubyでは驚くほど簡単にかける）。必要なら行番号も出力してみると良い。&lt;/dt&gt;
&lt;dd&gt;※メソッドだけでいいな。&lt;span style=&quot;color:#0000FF&quot;&gt;ファイルクローズはこの記述の場合はコーディングブロックのendのタイミングでクローズされるのか？&lt;/span&gt;
</code></pre><p>class RegGrep
def grep(filename, regexp)
File.open(filename, &lsquo;r&rsquo;) do |f|
f.each do |line|
puts &ldquo;#{f.lineno} : #{line}&rdquo; if line.match(regexp)
end
end
end
end</p>
<pre><code>&lt;/dd&gt;
&lt;/dl&gt;

ということで、2日目終了。
恥ずかしいコードだらけだけど、ぜひツッコミ入れてもらえると助かります。

数年前までは、恥ずかしいからとか見せられるレベルじゃないからと、ほとんどアウトプットしなかったのですが、
最近はそれではものすごく損をしていると思っています。
ホントは発表するとか議論するとかする場もあればいいのですが。
アウトプットすることで、フィードバックが貰えて、いろんなかたの考えが参考になり、糧となり成長していけるのかと。
（これじゃ成長できないレベルだよという話でなければいいのだが。。。）

一度、Rubyのプロフェッショナル各位に見てもらいたいなぁｗ
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>「7つの言語　7つの世界」 Ruby 1日目(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/09/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-ruby-1%E6%97%A5%E7%9B%AE/</link>
      <pubDate>Fri, 09 Sep 2011 14:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/09/7%E3%81%A4%E3%81%AE%E8%A8%80%E8%AA%9E-7%E3%81%A4%E3%81%AE%E4%B8%96%E7%95%8C-ruby-1%E6%97%A5%E7%9B%AE/</guid>
      <description>実に3年ぶりくらいにゆっくりできる日々が訪れたので、積読状態の本を消化しようと「7つの言語 7つの世界」を読み始めました。 せっかくブログも始め</description>
      <content:encoded><p>実に3年ぶりくらいにゆっくりできる日々が訪れたので、積読状態の本を消化しようと「7つの言語　7つの世界」を読み始めました。</p>
<p>せっかくブログも始めたので、備忘録も兼ねて感想などを書いていこうかと。</p>
<p>この本ですが、以下の7つの言語についてエッセンスがまとめられています。</p>
<ul>
<li>Ruby</li>
<li>Io</li>
<li>Prolog</li>
<li>Scala</li>
<li>Erlang</li>
<li>Clojure</li>
<li>Haskell</li>
</ul>
<p>まずはRubyからです。
ここ2年ほどRuby（Rails）に関連する仕事をしていたのですが、Ruby周りはプロフェッショナルな方たちがいたので、きちんと勉強していないことがこの本を読み始めてわかりました。</p>
<p>ということで、前置きはそれなりな記述ですが、感想は適当になりますので、あしからず。</p>
<h3 id="感想">感想：</h3>
<p>irbが便利。簡単に動作確認ができるのが便利。Javaだとコンパイルが必要。
変数を定義する必要がない気軽さはある。
必ず戻り値が帰ってくる「puts &lsquo;hello, world&rsquo;」でも。=&gt;nil
putsは楽かな。まぁ、Eclipse使ってると一緒か。
「4」もオブジェクトとなっている。ここもJavaと異なる。
(x &lt;= 4).classという記述でTrueClassというクラスだとわかる。
unlessが結構便利。ただし、記述方法が多数あるので、可読性は落ちる？場合によってはわかりやすいか？
あと、括弧()がないのも慣れない。（まぁ、これは慣れの問題。ただし、カッコありでもOK）
{}のかわりがif～endなのはわかりやすいかも。
if not はわかりやすくていい。!はだいたい間違えるから。。。
whileも１行形式で書けるのか。「x = x + 1 while x &lt; 10」慣れないと厳しい。個人的には混在すると読めないなぁ。
nilとfalse以外がすべてtrueに評価されるのは厳しいのでは？型のチェックがないので、booleanが入ってると想定してない場合に挙動を読めないかも。実行時に動作が変だなーと思うことが出てきそう。
and、orの記述が使えるのは読みやすい。ただし、混在するとやっかい。
判定結果が明らかになった時点で実行が中止されるのは普通。&amp;、|の挙動はJava同様。</p>
<p>やりながら疑問点：</p>
<ul>
<li>NetBeansとかIDEでフォーマッタやcheckstyleみたいなのはあるのか？</li>
<li>コーディング規約はあるのか？（2日目に「習慣」があるらしいとの記載があった。）</li>
<li>必ず戻り値が戻ってくるのは、必ずGC対象になりうるオブジェクトが生成されるってことか？</li>
</ul>
<p>ここまでが感想と疑問点。で、この本の面白いところは最後に調査、コーディングを行う練習問題的なものがある部分です。
一応、私なりの答えを書いておこうかと。（一覧などで見えないようにはしますが、ネタバレがあるので注意してください。）</p>
<p>ということで、セルフスタディの回答。</p>
<h3 id="探してみよう">（探してみよう）</h3>
<dl>
<dt>○Ruby API</dt>
<dd>http://ruby-doc.org/core/</dd>
<dt>○Programming Ruby: The Pragmatic Programmer's Guideのオンライン版</dt>
<dd>http://www.ruby-doc.org/docs/ProgrammingRuby/参考資料：http://www.swlab.it.okayama-u.ac.jp/man/ruby/uguide/uguide00.html</dd>
<dt>○文字列の一部を置換するメソッド</dt>
<dd>"hello".gsub(/[aeiou]/, '*')</dd>
<dt>○Rubyの正規表現に関する情報</dt>
<dd>日本語：http://www.namaraii.com/rubytips/?%A5%D1%A5%BF%A1%BC%A5%F3%A5%DE%A5%C3%A5%C1
英語の試せるサイト（irbが動けば必要ないかもね）：http://rubular.com/</dd>
<dt>○Rubyの範囲に関する情報</dt>
<dd>日本語：http://doc.okkez.net/static/192/class/Range.html
英語：RDocのRangeクラスに相当するのかな。</dd>
</dl>
<h3 id="試してみよう">（試してみよう）</h3>
<dl>
<dt>○文字列"Hello, world"を出力する。</dt>
<dd>```
<dt>○文字列"Hello, Ruby"の中の"Ruby"という単語のインデックスを検索する。</dt>
<dd>```
<p>s = &lsquo;Hello, Ruby&rsquo;
s.index(&lsquo;Ruby&rsquo;) //indexofで間違えた</p>
<pre><code class="language-</dd>" data-lang="</dd>">&lt;dt&gt;○自分の名前を10回出力する。&lt;/dt&gt;
&lt;dd&gt;```

※まずは、正統派。
i = 0
while i &lt; 10
  puts &quot;johtani&quot;
  i = i + 1
end
</code></pre><pre><code>※Rangeを利用
(1..10).each {|n| puts &quot;johtani #{n}&quot;}
</code></pre><pre><code>
※forもあるよね
for i in 1..10
  puts &quot;johtani&quot;
end
</code></pre><pre><code>
※timesってのもある。（0始まり）
10.times {|n| puts &quot;johtani #{n}&quot;}
</code></pre><pre><code>
※uptoなんてのもあるのか。
1.upto(10) {|n| puts &quot;johtani #{n}&quot;}
</code></pre><pre><code>
※downtoも
10.downto(1) {puts &quot;johtani&quot;}
</code></pre><pre><code>
※stepもある。
10.step(1, -1) {puts &quot;johtani&quot;}//step(上限,ステップ)
```&lt;/dd&gt;
&lt;dt&gt;○文字列&quot;This is sentence number 1&quot;の1を10までカウントアップしながら10回出力する。&lt;/dt&gt;
&lt;dd&gt;```

1.upto(10) {|n| puts &quot;This is sentence number #{n}&quot;}
※あとは上記と一緒
```&lt;/dd&gt;
&lt;dt&gt;○ファイルに格納されているRubyプログラムを実行する。&lt;/dt&gt;
&lt;dd&gt;```

vi hoge.rb
※#上記処理をどれか記述
</code></pre><pre><code>
 $ ruby hoge.rb
</code></pre></dd>
<dt>○ボーナス問題：少し物足りない人は、乱数を選択するプログラムを書いてみてほしい。プレーヤーに数字を選択してもらい、その数字が生成された乱数よりも大きいか小さいかを返す。</dt>
<dd>
```
<h1 id="---coding-utf-8---">-<em>- coding: utf-8 -</em>-</h1>
<p>range = (1..100)
while true
puts &ldquo;#{range.min}から#{range.max}の数字を入力してください&rdquo;
n = gets
n = n.to_i
if range.include?(n)
break;
else
puts &ldquo;範囲外の入力値です。もう一度入力してください&rdquo;
end
end
i = rand(range.max)
if i &lt; n
puts &ldquo;入力「#{n}」は乱数「#{i}」より大きいです&rdquo;
elsif i == n
puts &ldquo;入力「#{n}」は乱数「#{i}」と等しいです&rdquo;
else
puts &ldquo;入力「#{n}」は乱数「#{i}」より小さいです&rdquo;
end</p>
<pre><code class="language-</dd>" data-lang="</dd>">&lt;/dl&gt;

とまぁ、こんな感じ。こんな方法もあるよ、ここおかしくない？などあれば、コメント欄まで。
リアクション大募集です。
はやく、シンタックスライター導入せねば。
</code></pre></content:encoded>
    </item>
    
    <item>
      <title>MBAセットアップ備忘録その３(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/07/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%93/</link>
      <pubDate>Wed, 07 Sep 2011 02:18:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/07/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%93/</guid>
      <description>現在の出先でMBAを使えないので、なかなか進んでいないMBAのセットアップです。。。 とりあえず、Eclipse、homebrewをインストー</description>
      <content:encoded><p>現在の出先でMBAを使えないので、なかなか進んでいないMBAのセットアップです。。。</p>
<p>とりあえず、Eclipse、homebrewをインストールしたので、lucene-gosenの開発やビルドには支障がない程度になってきました。（肝心のSolrがまだ動く状況になかった。。。）</p>
<p>あとは、Windowsで使用頻度の高いソフトを列挙して、対応するMacのアプリを探そうかと。
まずは、Windowsでよく利用するソフトのリストアップ</p>
<ol>
<li>Eclipse</li>
<li>Cygwin</li>
<li>TeraTerm</li>
<li>WinSCP</li>
<li>meadow</li>
<li>サクラエディタ</li>
<li>WinMerge</li>
<li>PDF-XChange Viewer</li>
<li>Evernote</li>
<li>FreeMind</li>
<li>VMWare</li>
</ol>
<p>こんなところかな。
a.～e.についてはMacOS特に問題なし。Eclipseは入れたし、その他は標準で入ってるものでまかなえると。
f.についてもEmacsで当面対応する予定。ただ、OmmWriter やCotEditorがおすすめらしいと聞いてます。
g.についてはFileMergeなるものがあるらしいので、試してみようかと。
h.は現状Adobe Readerです。ほかにいいのあるのかしら。PDF-XChange Viewerはちなみに、Solr入門を書くときに編集の方から教えてもらったPDFのビューワです。コメントとかハイライトできるのが便利でした。
i.はMacOS版をインストールして、すでに活用中。
j.については一時期活用していたのですが、最近利用していないです。なので、保留
k.についてはVirtualBoxをインストールしてみました。フリーだったので。WindowsではVMWareだったのですが。。。</p>
<p>ほんとにこれくらいで大丈夫かどうかはもう少し使ってみてからかなぁと。
その他におすすめ、これがあるとかなり便利などあれば、教えてもらえると助かります。
まだ、MacOSに触り始めたところなので、細かな部分のカスタマイズなどができていないので、その辺りもオイオイ考えていこうかと。</p>
<p>※余談ですが、現在「７つの言語　７つの世界」を読み始めました。いろいろな言語の特徴について学べる本のようでかなり楽しく感じています。どうしてもJavaで仕事優先だとコーディングする速度優先でJavaばかり使ってしまうので。
いい頭の体操にもなりそうなのでオススメです。（といってもまだ、最初の言語Rubyなのですが。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書分離のテストケース追加と残タスク(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/07/%E8%BE%9E%E6%9B%B8%E5%88%86%E9%9B%A2%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E3%82%B1%E3%83%BC%E3%82%B9%E8%BF%BD%E5%8A%A0%E3%81%A8%E6%AE%8B%E3%82%BF%E3%82%B9%E3%82%AF/</link>
      <pubDate>Wed, 07 Sep 2011 01:56:28 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/07/%E8%BE%9E%E6%9B%B8%E5%88%86%E9%9B%A2%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E3%82%B1%E3%83%BC%E3%82%B9%E8%BF%BD%E5%8A%A0%E3%81%A8%E6%AE%8B%E3%82%BF%E3%82%B9%E3%82%AF/</guid>
      <description>すぐにテストケース追加しますといいつつ、はや一週間。 ようやく仕事が落ち着いたので、テストケースを追記しました。まだパッチの段階です。 一応、異</description>
      <content:encoded><p>すぐに<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16#c5?PHPSESSID=e73e7c99982a0b86f03ccbef7738d416">テストケース追加します</a>といいつつ、はや一週間。</p>
<p>ようやく仕事が落ち着いたので、テストケースを追記しました。まだパッチの段階です。
一応、異なる辞書の読み込みのテストケースなどを追加し、テストケース追加時点で
いくつか気になったところもあったので、ついでに修正を加えました。
一応、辞書の分離＋複数辞書対応については現時点ではこんなところかと。</p>
<p>あと、もう１項目対応してからtrunkに取り込む予定です。
対応する項目とは以下の項目です。</p>
<ul>
<li>カスタム辞書ビルドの簡易化</li>
</ul>
<p>以前の記事にも書いていますが、カスタム辞書のビルドが思いのほか手間がかかります。
辞書の外部化は対応したのですが、せっかく辞書が外部化できたのですから、コマンド（ant？）で一発で
カスタム辞書を含んだビルドをしたくなるのが人情です。
辞書なしのjarファイルもあることなので、すぐに対応可能かと。
ということで、今週中には対応する予定です（宣言しないとまた先延ばしになりそう）</p>
<p>忘れてそうだったらTwitterやコメントでツッコミを入れてもらえると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>複数辞書の読み込み機能追加（仮）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/30/%E8%A4%87%E6%95%B0%E8%BE%9E%E6%9B%B8%E3%81%AE%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E6%A9%9F%E8%83%BD%E8%BF%BD%E5%8A%A0%E4%BB%AE/</link>
      <pubDate>Tue, 30 Aug 2011 10:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/30/%E8%A4%87%E6%95%B0%E8%BE%9E%E6%9B%B8%E3%81%AE%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E6%A9%9F%E8%83%BD%E8%BF%BD%E5%8A%A0%E4%BB%AE/</guid>
      <description>先日、辞書のjarファイルからの分離についてパッチと記事を書きました。 IssueにあげていたパッチをRobertさんが見ていたらしく、次のよ</description>
      <content:encoded><p>先日、<a href="http://johtani.jugem.jp/?eid=20">辞書のjarファイルからの分離</a>についてパッチと記事を書きました。</p>
<p>IssueにあげていたパッチをRobertさんが見ていたらしく、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16#c4">次のようなコメント</a>をもらいました。</p>
<p><em>Maybe if we change SenFactory.getInstance to use a ConcurrentHashMap then you can easily use multiple dictionaries at the same time?
</em></p>
<p>「SenFactory.getInstanceメソッドでConcurrentHashMap使ったら複数辞書対応できるんじゃない？」（訳）
たしかに。。。なんで思いつかなかったのだろう。。。</p>
<p>ということで、実装してみました。
<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16#c5">パッチはこちら</a>。</p>
<p>使い方ですが、先日の記事と代わりはありません。
ただし、あった制限事項が次のようになります。</p>
<ul>
<li>マルチコアの設定でsharedLibにlucene-gosenのjarを含まない</li>
<li><del>同一コア内で異なるdictinaryDirの指定はできない</del></li>
</ul>
<p>ソースの変更点ですが、ものすごく単純です。
dictionaryDirに指定された文字列をキー、その辞書ディレクトリを利用したSenFactoryのインスタンスを値に持つmapをSenFactory内に保持します。あとは、SenFactoryのgetInstance(String dictionaryDir)メソッドで取得する際にmapに対応するインスタンスがあれば、そのインスタンスをなければ、dictionaryDirから辞書を読み込んでインスタンス生成してmapにキャッシュしつつ返すという実装に変えただけです。
ということで、次のようなIPADICとNAIST-JDIC for ChaSenを同時に使う設定も可能となります。</p>
<pre><code>
    &lt;fieldType name=&quot;text_ja_ipadic&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
      &lt;analyzer&amp;gt;
        &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; dictionaryDir=&quot;/tmp/lucene-gosen/dictionary/ipadic&quot;/&amp;gt;
      &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
    &lt;fieldType name=&quot;text_ja_naist_chasen&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
      &lt;analyzer&amp;gt;
        &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; dictionaryDir=&quot;/tmp/lucene-gosen/dictionary/naist-chasen&quot;/&amp;gt;
      &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
</code></pre><p>あと、注意事項です。
普通に考えるとわかることですが、辞書を複数読み込めるようになったことで、読み込んだ複数の辞書をメモリに保持することになります。ですので、今までよりも多くのメモリを利用するので、Heapのサイズには注意が必要です。
例のごとく（ほんとよくない。。。）テストコードを書いていない状態のパッチをまずはアップしました。
テスト書かないと。。。次回はテストコードかきましたと言う報告をしたいな</p>
<p>あと、Robertさんのコメントの前に<a href="http://twitter.com/#!/shinobu_aoki">@shinobu_aoki</a>さんからJapaneseTokenizerFactoryの設定では辞書のディレクトリを$SOLR_HOME/confからの相対パスで記述できるというパッチもいただいています。
この部分については先日と使い方が異なります。
（すみません、まだきちんとソースを見れてないです。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書のjarファイルからの分離(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/23/%E8%BE%9E%E6%9B%B8%E3%81%AEjar%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%8B%E3%82%89%E3%81%AE%E5%88%86%E9%9B%A2/</link>
      <pubDate>Tue, 23 Aug 2011 10:26:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/23/%E8%BE%9E%E6%9B%B8%E3%81%AEjar%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%8B%E3%82%89%E3%81%AE%E5%88%86%E9%9B%A2/</guid>
      <description>ひさびさに、lucene-gosenの話題です。 lucene-gosenはjarファイルに辞書も同梱されており、jarファイルをクラスパスに</description>
      <content:encoded><p>ひさびさに、lucene-gosenの話題です。</p>
<p>lucene-gosenはjarファイルに辞書も同梱されており、jarファイルをクラスパスに取り込むだけで、
簡単に形態素解析器が利用できるといお手軽さがあり、便利です。</p>
<p>ですが、以前<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16">カスタム辞書の登録</a>について記事を書いたように、カスタム辞書の登録は思いのほか手間がかかります。
lucene-gosenのソースをダウンロードし、lucene-gosenを一度コンパイルし、カスタム辞書のcsvファイルを作成し、カスタム辞書を取り込んだ辞書のバイナリを生成し、最後にjarファイルにするという作業です。（書くだけでいやになってきました。。。）さらに作成したjarファイルをSolrや各プログラムに再度配布するという具合です。</p>
<p>そこで、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16">辞書ファイルの外部化ができないかという話</a>があがっていました。
すこし時間ができたので、山積みになっているissueを横目に軽く実装をしてpatchをissueにアップしました。</p>
<p>機能としてはごく簡単で、JapaneseTokenizerのコンストラクタに辞書のディレクトリ（*.senファイルのあるディレクトリ）を指定可能にしただけです。
また、JapaneseTokenizerFactoryでもdictionaryDir属性で指定可能になっています。
まずは、コンパイルの方法から。
trunkをSVNでcheckoutし、issueにあるpatchをダウンロードして適用します。（svnのチェックアウトについては<a href="http://johtani.jugem.jp/?eid=3">こちら</a>を参考にしてください。）</p>
<pre><code>
$ cd lucene-gosen-trunk
$ patch -p0 --dry-run &lt; lucene-gosen-separate-dictionary.patch
$ patch -p0 &lt; lucene-gosen-separate-dictionary.patch
</code></pre><p>次に、antを実行し辞書なし版のjarファイルをビルドします。</p>
<pre><code>
$ ant nodic-jar
</code></pre><p>これで、dictディレクトリに「lucene-gosen-1.2-dev.jar」というjarファイルが出来上がります。
（※ただし、これだけでは動作しないので、別途辞書のコンパイルは必要です。）</p>
<p>次に、指定の仕方です。JapaneseTokenizerのコンストラクタは第3引数に辞書のディレクトリ（フルパスor実行ディレクトリからの相対パス）を渡すだけです。</p>
<pre><code>
  Tokenizer tokenizer = new JapaneseTokenizer(reader, compositeTokenFilter, dictionaryDir);
</code></pre><p>最後に、Solrのtokenizerタグでの指定方法です。</p>
<pre><code>
    &lt;fieldType name=&quot;text_ja&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
    &lt;analyzer&amp;gt;
        &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; dictionaryDir=&quot;/hoge/dictionarydir&quot;/&amp;gt;
    &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
</code></pre><p>以上が、簡単な設定の仕方です。なお、辞書を内包したjarファイルでもdictionaryDirは利用可能です。優先度としては、dictionaryDirが指定されている場合はdictionaryDirを探索しファイルがなければRuntimeExceptionです。指定がnullもしくは空文字の場合はjarファイルの辞書の読み込みを行います。</p>
<p>次に利用シーン、制限事項についてです。
利用シーンとしてはカスタム辞書を定期的にメンテナンス（追加更新）しながらSolrを運用するというのが想定されます。定期的に辞書の再読み込みをしたい場合です。
利用方法は次のようになります。</p>
<ul>
<li>Solrのマルチコア構成を利用する</li>
<li>各コアごとにlib/lucene-gosen-1.2-dev.jarを用意</li>
<li>辞書の更新が終わったらコアのRELOADを実施</li>
</ul>
<p>コアをリロードすることで、lucene-gosenが辞書を再読み込みようになります。（現状でも再読み込みするが、jarファイルを再配置しないといけない。）あとは、定期的に辞書ファイルを更新、再構築しコアをリロードすれば、
リロード後に新しい辞書が利用できるという具合です。
（もちろん、辞書更新後に入った単語は辞書更新以前に作成したインデックスにはでてこないですが。。。）
また、コアごとにdictinaryDirを別々に指定することも可能です。</p>
<p>制限事項は次のようになります。</p>
<ul>
<li>マルチコアの設定でsharedLibにlucene-gosenのjarを含まない</li>
<li>同一コア内で異なるdictinaryDirの指定はできない</li>
</ul>
<p>以上が、辞書の外部ファイル化のパッチについてでした。
少しテストケースを追加したら、trunkにコミットする予定です。興味があれば、パッチを利用してみてください。</p>
<p><a href="http://code.google.com/p/syntaxhighlighter/">SyntaxHighlighter</a>の導入をしないとソースコードが見にくいですね。。。導入を検討しないと。。。どこかにWebサーバ用意しないとダメかも</p>
</content:encoded>
    </item>
    
    <item>
      <title>MBAセットアップ備忘録その２(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/15/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%92/</link>
      <pubDate>Mon, 15 Aug 2011 09:51:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/15/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2%E3%81%9D%E3%81%AE%EF%BC%92/</guid>
      <description>すみません、また、MBA関連の記事になってしまいました。 ということで、備忘録その２です。 前回からいくつかインストールや環境の設定をしたので、</description>
      <content:encoded><p>すみません、また、MBA関連の記事になってしまいました。
ということで、備忘録その２です。</p>
<p>前回からいくつかインストールや環境の設定をしたので、リストアップ。</p>
<ul>
<li>VirtualBoxとWindowsのインストール</li>
<li>homebrewのインストール</li>
<li><a href="http://pqrs.org/macosx/keyremap4macbook/index.html.ja">KeyRemap4MacBook</a>のインストール</li>
<li>Growlのインストール</li>
<li>Time Machine用HDDの購入と設定</li>
</ul>
<p>VirtualBoxのインストールとWindowsXPのセットアップ。会社関連でどうしてもWindowsが必要なので、
とりあえず、VirtualBoxとWIndowsを入れました。これまでのWindows環境ではVMWareを利用してたのですが、
Macだと有償ということで、無償版が存在するVirtualBoxを選択。
MBAにはDVDのドライブがなく、また、ケチって購入しなかったので、ちょっと手間取りました。
自宅のデスクトップでXPのイメージをisoにして、MBAにコピーしてからインストールと。。。
ただ、MBAのSSDの効果なのか、XPの起動がすごく速くてびっくりです。</p>
<p>homebrewのインストールは、まだインストールしただけの状態。。。
MacPortsってのがあるというのは知っていたのですが、最近はこちらがいいとのこと。
ちなみに、MacPortsにはSolrがあるらしいです。デフォルトのため、lucene-gosenとか入れる必要がありますが。</p>
<p>KeyRemap4MacBookのインストール。入れた方がいいですよと言われてたのだが、きちんと調査してなく、
キーの入れ替えができる程度だと思ってました。昨日、EmacsModeなるものが存在するというのをKeyRemap4MacBookのドキュメントを読んでいて発見し即インストール。
とりあえず、Ctrl-x,Ctrl-cでEvernoteが終了できて喜んでるところです。</p>
<p>Growlのインストール。KeyRemap4MacBookのCtrl-xを通知してくれます。ほかにも使い方があるかは調査が必要。</p>
<p>あとは、昨日、ビックカメラでWDの2TBのHDDを購入してTimeMachineの設定をしてバックアップをとりました。
まだバックアップをとっただけで、差分を見たりはしていないです。帰ってからやってみようかと。</p>
<p>最後に、ケースを買いました。封筒とかいろいろ考えたのですが、やはり機能重視ということで、以下のケースです。
できれば、ファスナーが２つついていて、両方からあけられるとよかったのですが。リュックとショルダーバッグを使い分けるので、横でも縦でも取り出しが楽なものがよかったので、このケースを買いました。
一応、がんばれば縦でも出せるので。あとは、PCだけもって移動することもあるため、取っ手があるのが便利かと。</p>
<p>とまぁ、お盆なので、このエントリーで許してください。次回は技術的なことを書きますので。。。


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/B003WU4FQU/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=B003WU4FQU&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/B003WU4FQU/?tag=johtani-22">
      CaseLogic 機能的なモバイルコンピュータケース(13~14.1インチPC対応)、MacBookやMacBook Proにも対応 ダークグレー UNS-114J-DG
      </a>
    </p>
  </div>
</div></p>
</content:encoded>
    </item>
    
    <item>
      <title>MBAセットアップ備忘録(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/10/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2/</link>
      <pubDate>Wed, 10 Aug 2011 00:46:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/10/mba%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E5%82%99%E5%BF%98%E9%8C%B2/</guid>
      <description>Mac Book Airのセットアップ関連の備忘録 （今回は備忘録なので、文体も変かも） インストールしたもの（順不同）だいたい、このサイトを参考にした。 Evernote Dropbox</description>
      <content:encoded><p>Mac Book Airのセットアップ関連の備忘録</p>
<p>（今回は備忘録なので、文体も変かも）</p>
<p>インストールしたもの（順不同）だいたい、この<a href="http://d.hatena.ne.jp/tomoya/touch/20110806/1312613527">サイト</a>を参考にした。</p>
<ul>
<li>Evernote</li>
<li>Dropbox</li>
<li>XCode</li>
<li>Twitter</li>
<li>YoruFukurou</li>
<li>GNU Emacs For Mac OS X</li>
<li>Java</li>
<li>Office for mac</li>
</ul>
<p>現状はこんなところ。</p>
<p>まだ、キーボードの配列とショートカット、ウィンドウの切り替えなどがすぐにできないので、かなりおぼつかない状態。</p>
<p>Ctrl+aなどの操作は快適なのだが、ウインドウを閉じたりする場合のcommandキーの扱いがまだなれない。キー配置を切り替えたりした方がいいのか悩み中。
Windowsでは長年XKeymacsを使ってきたため、通常のOfficeなどもCtrl+x　Ctrl+sなどで閉じていたので、command＋qなどがすんなり出てこない。。。</p>
<p>あとは、VirtualBoxにWindowsを入れて、Eclipseを入れればなんとか、WindowsPCからMBAへの切り替えができそう。</p>
<p>残っている課題は以下の通り。</p>
<ul>
<li>Time Machine環境の整備USB接続のHDDの導入（ミラーリング対応にするか悩み中）※ちなみにうちで使ってるNASは古い＋NFSマウントできない機種だった。。。　対応可能だったらScientific Linux＋Netatalk環境などを用意するのも考えたんだが。</li>
<li>homebrewのインストール</li>
<li>日本語入力（GoogleIME？SKK？）</li>
<li>ウォークマン、Xperia arcの運用方法まぁ、今まで使ってたPCで対応かな</li>
<li>キーの配置Emacsのショートカットに変更できたりするか調査したい</li>
<li>データの移行Windowsに入ってるデータのうち移行するものしないものの選別Macでは利用できないデータやファイルもあると思う。</li>
<li>解凍ツールの選別？いるのかな？</li>
<li>ブラウザのインストールchrome、Firefoxはいれるかな。</li>
<li>メーラーのインストールWindwosではBecky!を使ってたのだが、どうするか。デフォルトメーラで様子見？</li>
<li>ウイルス対策ソフトの導入Macってどんなソフトがあるかがわからない。。。</li>
</ul>
<p>あと、１週間触っての感想</p>
<p>まずは、良かった点。</p>
<ul>
<li>いろんなUIがよくできてるセットアップなどきれいな画面で説明が進むのが新鮮だった。あと、フォントのきれいさも新鮮。これは、今までのPCが古いからかもしれないが。</li>
<li>トラックパッドがいいまだなれてないけど、スイスイ動くのでストレスなく作業できる。スワイプとかも画面の移動が楽でいい。</li>
<li>静かで速いSSDのため、静かだし起動がすごく速い。会社で支給されてるPCもSSD（３年くらい前のもの）だけど、こんなに起動などがはやくない。</li>
</ul>
<p>あとは、気になった点。</p>
<ul>
<li>AppStoreでページ間のスワイプができない。かなり使いづらい。Safariと同じイメージでいるので。</li>
<li>手首がいたい手前のエッジ部分が手首に当たってちょっといたい。</li>
<li>アプリインストール時にファンがうるさい動画などはまだ見てないが、Office、XCodeのインストール時にファンの排気音がうるさくなり、結構な熱を発していた</li>
</ul>
<p>ファーストインプレッションはこんな感じ。</p>
<p>ただ、前回も書いたようにうらやましかったのもあり、かなり満足している。
もっと触る時間が欲しいのだが、なかなかとれないのが歯がゆい。。。</p>
<p>持ち歩くようになったらとりあえず、「７つの言語　７つの世界」を読み進め＋写経をやる予定。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Apple教に入信しました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/04/apple%E6%95%99%E3%81%AB%E5%85%A5%E4%BF%A1%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Thu, 04 Aug 2011 01:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/04/apple%E6%95%99%E3%81%AB%E5%85%A5%E4%BF%A1%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>個人用のPCを８年ぶりに新調しました。 前回購入した家のデスクトップPCがもう８年ものになりつつあるということで、 さすがに今年はPCを買おうと</description>
      <content:encoded><p>個人用のPCを８年ぶりに新調しました。<br />
前回購入した家のデスクトップPCがもう８年ものになりつつあるということで、<br />
さすがに今年はPCを買おうと思い、年初からいろいろと調べていました。<br />
<br />
当初は次もデスクトップPCを購入する予定でした。ただ、次のような状況ということもあり<br />
デスクトップは見送ることにし、代わりにノートPCにすることに。<br />
<br /></p>
<ul>
<li>通勤時間が１時間を超えること<br /></li>
<li>基本、客先に出向いての仕事<br /></li>
<li>家でPCに触る時間が少ない（家族サービスのため。）<br />
<br />
<br />
<br />
で、ここ数週間悩んでいたのですが、Mac Book Air 13インチを購入しました。（そのMBAから初投稿です。）<br />
実は、Apple製品はこれが初の購入となります。<br />
<br />
対抗馬としてあがっていたのはSonyのVAIO Type Zでした。<br />
元々ソニスト（ソニー大好き）であり、携帯（Xperia arc）、ウォークマン（Sony）、PSP go、ハンディカムなど、<br />
ソニー製品に囲まれており、デザインもType Zのほうが好みなのですが、MBAを購入しました。<br />
<br />
この理由なのですが、私が大学時代にさかのぼります。<br />
情報系の大学の出身で、はじめてきちんと触ったマシンがSunOS4.1（最後の数年はSolaris2.6）でした。<br />
その後、大学６年間は基本的にEmacsでメールをみて、Emacsで文章を書いていました。<br />
そのため、社会人になってからもEmacsのキーバインドから抜け出せない日々を送っていました。<br />
社会人ではやはりWindowsでしたが、XKeymacsというフリーソフトのおかげ（せい？）でEmacsの<br />
ショートカットからは抜け出せないまま。（家のPCはXPで、XKeymacsがインストールされてます。）<br />
<br />
今までの流れだと、何も考えずにWindows7+XKeymacsの流れだったのですが。。。<br />
職場でWindows7のPCを使う機会があり、XKeymacsを入れてみたのですが、これまでとことなり、<br />
うまく機能しないことこの上なし（例：chromeのアドレスバーでCtrl+Kとかできないなど）。<br />
さらに、64bit版の場合、XKeymacsを32bit、64bitの２つインストールしないといけないことに。（32bit用アプリのために32bitを入れないといけない）<br />
で、数ヶ月使っているのですが、ちょっとずつストレスがたまっている始末。。。<br />
<br />
今までは、「Mac？今更恥ずかしくてかえない」「Mac？おしゃれすぎて無理」「Apple？MSにかわって独占してるの気に食わん」と公言していたのですが。。。<br />
<br />
やはりUnixベースであり、Ctrl+aやCtrl+kが意識せずに利用できるということでコーディングなどを行う場合のストレスよりも保守的な思考を変える方が今後のためにも良さそうだということで、心機一転MBAを購入しました。<br />
ということで、ついにApple教へ入信です。<br />
<br />
まだ、１時間も触ってないですが、すでに洗脳されつつあります。。。<br />
新しいものに触れるのって楽しいですよね。当分は何をインストールするかなど楽しい悩みでいっぱいになれそうです。（次回はちゃんとした技術的な記事書きたいと、書けるかな、書くぞ、たぶん。。。）<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
とまぁ、いろいろと理由を書いてみましたが、要は（オシャレさに）憧れてて、あまのじゃくになって思ってもないこと言ってたんです！<br />
理由が欲しかったんです！！<br />
Macかっこえーわーw<br />
Mac miniとかiMacとかもほしーわー<br />
<br />
<br />
<br /></li>
</ul>
</content:encoded>
    </item>
    
    <item>
      <title>第1回 データ構造と情報検索と言語処理勉強会に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/27/%E7%AC%AC1%E5%9B%9E-%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0%E3%81%A8%E6%83%85%E5%A0%B1%E6%A4%9C%E7%B4%A2%E3%81%A8%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 27 Jul 2011 12:33:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/27/%E7%AC%AC1%E5%9B%9E-%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0%E3%81%A8%E6%83%85%E5%A0%B1%E6%A4%9C%E7%B4%A2%E3%81%A8%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E5%8B%89%E5%BC%B7%E4%BC%9A%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>かなり遅くなりましたが、第1回 データ構造と情報検索と言語処理勉強会 #DSIRNLPに参加しました。 帰ってから２日半ほど寝込んでしまい、更新が</description>
      <content:encoded><p>かなり遅くなりましたが、<a href="http://atnd.org/events/17032">第1回 データ構造と情報検索と言語処理勉強会</a> <a href="http://twitter.com/#!/search/%23DSIRNLP">#DSIRNLP</a>に参加しました。<br />
帰ってから２日半ほど寝込んでしまい、更新が遅れました。。。<br />
初の土曜日開催の勉強会でしたが、家族を説得してなんとか参加しました。<br />
おかげで色々と面白そうなキーワードが拾えてよかったです。（拾っただけで理解するのはかなり時間がかかりそうですが。。。）<br />
ということで、前回同様、個人的にメモを取ったので。誤記などあるかもしれませんが、その時はコメントいただければと。<br />
<br />
<br />
0.DSIRNLPについて<br />
　※ボランティアで受付してたので聞けず。<br />
<br />
1.TRIEにトライ！～今日からはじめるTRIE入門～<br />
　@echizen_tm さん<br />
　資料：http://www.scribd.com/doc/58688141/Try-for-Trie<br />
　1.TRIEの説明<br />
　　実装についてはあとでスライドをみればいいか。（※ボランティアで受付してたので前半聞けず）<br />
　　・パトリシア木<br />
　　・Double Array<br />
　　・LOUDS<br />
　　・XBW<br />
<br />
　2.作ってみた<br />
　　その1　ベーシック<br />
　　　　Q.ノードのラベルどーする？<br />
　　　　　・固定長（ラベル＝1文字）<br />
　　　　　　定数時間でアクセス可能<br />
　　　　　・可変長（ラベル＝任意文字列）<br />
　　　　　　Patricia木に拡張可能<br />
　　　　A.拡張性を考えて可変長に！<br />
　　　<br />
　3.LOUDSとは？<br />
　　概要<br />
　　　Jacobsonが提案　　　　　　<br />
　　　Level-Order Unary Degree Sequenceの略（いつからLOUDSになったのか不明）<br />
　　　構築済みTRIEからLOUDSを構築<br />
　　　作業領域がO(NlogN)からO(N)に<br />
　　・ノードに幅優先で順番（Level-Order）をつける<br />
　　・子ノードの数を付ける。Unary符号で実装<br />
　　作ってみた<br />
　　・UnaryよりVerticalCodeのほうがよさそう<br />
　　　http://d.hatena.ne.jp/echizen_tm/20100531/1275323074<br />
　　・dag_vectorを使ってもいいかもよ？<br />
<br />
　4.QA<br />
　　Q.可変長を配列にすればいいんじゃね？<br />
　　A.単純にやると効率悪そう？<br />
　<br />
　※LOUDSをlucene-gosenに適用するとなんか嬉しいことあるかな？<br />
　　現状はDouble-Arrayだけど。<br />
<br />
2. ランキング学習ことはじめ（Solr系に利用できそうな予感。。。ごごご。。。むずかしい。。。）<br />
@sleepy_yoshi さん NTTのSuharaさん「話がうまい」<br />
　　数原　良彦<br />
　自己紹介<br />
　　情報検索、ランキング学習をやってる。<br />
　　三浦半島在住<br />
　ねらい<br />
　　ランキング学習の認知度を高める、ざっくり伝える。　<br />
　ごめんなさい　<br />
　　「実装」についてをわすれてた。<br />
　・ランキング学習とは？<br />
　　Learning to rank<br />
　　preference learningとは違う<br />
　　教師あり機械学習で検索ランキングに適用<br />
　・歴史<br />
　　従来は単一ランキング<br />
　　　クエリ・文書関連度（TF-IDF、BM25）<br />
　　　文書の重要度（PageRank、HITS、SALSAなど）<br />
　　近代的なランキングの実装方法<br />
　　　上記データを利用して関数を適用する。<br />
　・何を正解とするのか？<br />
　　適合性評価の作成方法<br />
　　　評価点数を多段階で点数化<br />
　　ランキング評価方法<br />
　　　ランキングを正解データと比較<br />
　　　NDCG（Normalized Discounted Cumulative Gain）　<br />
　※分類問題におけるモデルの生成<br />
　　Training Data＋学習アルゴリズム＝＞モデル<br />
　ランキング学習の訓練データ<br />
　　素性や評価はクエリごとに変わってくるTraining data（ここが違う）<br />
　ここまでのまとめ<br />
　　正解データは適合度至上主義！<br />
<br />
　ランキング学習手法<br />
　　３つのアプローチ<br />
　　教師あり機械学習（識別学習）≒<br />
　　　どのような目的関数/損失関数を<br />
　　　どのように最適化するのか<br />
　　アプローチ<br />
　　　1.Pointwise手法<br />
　　　　単一のデータに対して損失関数を設定<br />
　　　2.Pairwise手法<br />
　　　　同一クエリのペアに対して損失関数を設定<br />
　　　3.Listwise手法<br />
　　　　同一クエリのリストに対して損失関数を設定<br />
　Pointwise：あまりつかえない。<br />
　　　例：二値分類（適合＋１不適合－１）で学習<br />
　　　　補足：Perceptron<br />
　　　例：PRank<br />
　　　　しきい値と重み両方修正する。<br />
　　Pairwise：Pointwiseよりいいらしい<br />
　　　文書ペアで損失を設定<br />
<br />
　　MQ2007、MQ2008データセットの結果<br />
　　　Pairwise手法とListwise手法を比較しても<br />
　　　そんなに悪くないらしい<br />
　　IR-SVMってので偏りなどを排除できて、精度向上になるよ<br />
　　NLP2011でPARankっての発表したよ（手前味噌）<br />
　　QA<br />
　　　Q.じゃんけんみたいに循環しない？（nokuno）<br />
　　　A.半順序のデータだと起きるが、検索の場合は全順序なので大丈夫だよ<br />
　　Listwise：Pairwiseよりいいらしい<br />
　　　ListNetってのあり。<br />
<br />
　　　AdaRank<br />
　　　　検索評価指標を直接最適化するブースティング手法<br />
　　　　性能はいまいち？実装は簡単<br />
　　その他の話題<br />
　　　Click-through logs<br />
　　　Query-dependent ranking<br />
　　　Feature selection<br />
　　　Transfer learning/Domain adaptation<br />
　　　Diversity/Novelty<br />
　　公開Dataset<br />
　　　LETOR3.0/4.0 Dataset<br />
　　　MSLR-WEB Datasetなど<br />
　　実装<br />
　　　RankingSVM<br />
　　　Stoch&hellip;.<br />
　　Learning to Rank教科書<br />
　　　英語の資料が今年複数出たみたい<br />
　　まとめ<br />
　　　近代的な検索エンジンは多数のランキング素性を利用<br />
　　　評価データを用いてランキング素性の最適な重み付けを求める方法<br />
　　　。。。<br />
　　機械学習手法は論文≒実装<br />
　　　なので、ソースコード見るほうが重要（論文にないノウハウがあるよ）<br />
<br />
3.snappy調べてみた<br />
　@machy　町永圭吾（赤いポータルサイト）<br />
　TopCoderなどで活躍中？<br />
　・snappyは圧縮/伸張ライブラリ<br />
　　zlibよりサイズが大きいけど一桁はやいですぞ。<br />
　・インストールなど<br />
　　google－gflagsってのがないと動作しないよ。<br />
　・比較してみた（zlib）<br />
　　一桁は言い過ぎだけど、はやかった。<br />
　・比較してみた（lzo）<br />
　　Hadoopで利用されているlzo<br />
　　はやかった。<br />
　・仕組みは？<br />
　　zlib　辞書式符号化＋ハフマン符号化＝出力<br />
　　snappy　辞書式符号化＋リテラル＝出力<br />
　・辞書式符号化は？<br />
　　前に出てきたデータで同じ文字列の部分について端折る<br />
　・ハフマン符号化<br />
　　よく出てくる記号を短い符号で置き換えることで圧縮する。<br />
　・snappyの符号化<br />
　　基本バイト単位での処理にすることで、高速化してるみたい。<br />
　・snappyの辞書参照元の探し方<br />
　　ハッシュテーブル利用してマッチする位置を検索<br />
　　4byte窓でハッシュ値を計算してハッシュテーブルを更新<br />
　・圧縮しにくいデータ（同じ単語が出てこないパターン）について<br />
　　圧縮をあきらめ始める（32回同じのが見つからないと窓の移動幅が1つずつ大きくなる）<br />
　　16KB（フラグメント内）での比較回数が1008回に抑えられる。<br />
　・特徴<br />
　　ハッシュがしょうと移したらあるはずのマッチが見つからないこともある。<br />
　　メモリ消費量を抑えるためのハッシュのサイズ？<br />
　　最悪ケースのサイズを予め確保してから処理するため、メモリの再確保が起きないのではやいぞ。<br />
<br />
4.類似文字列検索してますか？<br />
　@overlast さん　Yな会社<br />
　1.曖昧な情報を処理する能力<br />
　　曖昧な情報を解決しようとする能力が高い。<br />
　　例：お笑い芸人の芸がサンプルｗ<br />
　　画像検索は曖昧クエリに答える例。。。<br />
　　　スターバックス、スタバ、SUTABA<br />
　　　スータバでも画像が出てきた。-&gt;女子高生とかが「スータバ」で画像をアップしてたりするから。<br />
<br />
　　文字列を使った検索は現代のインフラになってるよね。<br />
　　例：iタウンページ<br />
　　　タイトルとかに「スターバックスコーヒー」って書いてないとだめ。？<br />
　　　「スタバ」800件くらい　-&gt;　正規化で「ー」消してるんじゃないの？<br />
　　　「スータバックスコーヒー」0件　ちがうな。シノニム辞書登録してるな。<br />
　　曖昧なクエリ（キーワード）から検索できないかなぁ？<br />
　　曖昧情報の処理は文字列処理にこそ必要<br />
　　　なんでヒットしたかがわかるのが正解　<br />
　　　なんでヒットしたかわからないけど、ちゃんと出てきたから正解！<br />
　2.ゼロ件ヒット問題（ゼロマッチ）<br />
　　ピクシブ百科事典<br />
　　　「ピングドラム」だと17件<br />
　　　「ピンクドラム」だと0件　しかも真っ白！！<br />
　　　使いにくいよね。けど、一般的な検索システムの問題だよね。<br />
　　　Googleだと出てくる「ピンクドラム」の結果の最初にはピングドラムが出てくる。<br />
　　　　-&gt;リンクがはられているから出てきた？みんなの間違いで助けられてる<br />
　　　食べログ<br />
　　　　「俺のハンバーグ」<br />
　　　　「俺はハンバーグ」で検索したら。。。「俺はハンバーグで連れは。。。」がヒット-&gt;しらねーよｗ<br />
　　まとめ<br />
　　　ゼロ件ヒットは機会損失！！<br />
　3.曖昧な文字列で正しい文字列を探す方法<br />
　　正しい文字列って？<br />
　　　1.表記誤りがない<br />
　　　2.心の底で求めている<br />
　　　※異表記同義、同表記異義、異言語表記（日本語の情報を英語で検索）などもある<br />
　　　正しさはユーザによって変わる。<br />
　　正しい文字列をさがす手法<br />
　　　クエリ表記の正誤という軸<br />
　　　　誤の場合表記をヒントに似た文字列を検索してみる。<br />
　　　探したい対象が正確か曖昧か<br />
　　　　表記意外がヒントで同じ対象に到達できる？<br />
<br />
　　どんな情報を手がかりにする？<br />
　　　編集距離（レーベンシュタイン）<br />
　　　検索ログ<br />
　4.Apporoの紹介<br />
　　チョコ？いや、ロケット？いや、Approximate &hellip;<br />
　　http://code.google.com/p/apporo/<br />
　　中小規模だと使えそう<br />
　　　SimString<br />
　　今後<br />
　　　よみかな、ローマ字表記に基づく類似文字列検索＋高速化の予定<br />
　　技術概要<br />
　　　Suffix Array<br />
　　　N-gram Searchベースの近似文字列照合<br />
　　　Bit Parallel Matchingによる編集距離計算<br />
<br />
5.検索と言語と文化（仮）<br />
　@mizuno_takaaki さん<br />
　　放送禁止のためメモなし。<br />
<br />
7.自然言語処理におけるargmax操作<br />
　@hitoshi_ni さん<br />
　※順番変更<br />
<br />
　・近似解法<br />
　　その１　全探索<br />
　　　いや、無理でしょ、計算が。<br />
　　その２　貪欲法<br />
　　　最適解が出るかは不明。<br />
　　その３　ビームサーチ<br />
　　　上位kこの仮説を保持しながら幅優先探索<br />
　・動的計画法<br />
　　すでに最大値がわかっていて、ゴールから眺めてみる。<br />
　　逆からたどるとルートがわかるんじゃないか。<br />
　　マルコフ性につけ込むことで、わかるんじゃないか。<br />
　　計算量：品詞数の2乗<br />
　　（品詞数の2乗）＊形態素数<br />
　　DPの実装<br />
　　　Viterbiかなぁ？<br />
　・整数計画法（線形-&gt;整数）<br />
　　自然言語処理としては整数計画法でだいたいOK。<br />
　アルゴリズム<br />
　・手元の解空間中から許容解を得る<br />
　・分枝してそこから最大値を求める<br />
　　最大値＞許容解ならそちらを探索。<br />
　プログラム実装する？<br />
　　すごく大変じゃないけど簡単でもないね。<br />
　　問題の弱点がわかってるなら実装してもいいよね。<br />
　　整数計画法の場合lp_solveなどのフリーのソフトで解くのがいいよ。エクセルでも解けるよ<br />
　ILP（整数計画法？）<br />
　　問題の切り分けに使える。<br />
　まとめ<br />
　　ILPで解けたら、いろいろ自分で考えると面白いよ。<br />
　その他のバズワード<br />
　参考資料<br />
　　組み合わせ最適化＝1万円超<br />
　　アリ本<br />
　　　<br />
6.ソーシャルグラフ分析(導入編)<br />
　@kimuras さん　mixiの木村さん<br />
　・グラフ探索部分はまた今度。<br />
　・テキストマイニングや検索エンジンまわりやってる。<br />
　・ノードが数1千万、エッジが数億のオーダー<br />
　・分散技術の発展によるアルゴリズムの多様化<br />
　これまで<br />
　　RDBからDumpしたデータをKVSに入れて頑張ってた。<br />
　　Dump部分でデータ構造を毎回構築<br />
　　問題点<br />
　　　変更による再実装、メンテナンスコスト、<br />
　これから<br />
　　graphDBに取組中。マイニングに最適な感じ<br />
　graphDBの実装<br />
　　OrientDB、Neo4jとか<br />
　Neo4jって<br />
　　ACIDトランザクション可能<br />
　　EmbeddedGraphDatabaseだとAGPLｖ３だから気をつけてね。<br />
　　luceneで全文検索インデックスつくってるみたい。<br />
　Gremlinってのがquery言語を汎化してるらし。<br />
　とあるSNSのデータを使ってみたよ。<br />
　　メモリ64G、CPUは1コアしか使えなかったよ。<br />
　　ノード：15million、枝：600million<br />
<br /></p>
<h2 id="まとめbr-">まとめ<br /></h2>
<p>ということで、色々と面白い話が聞けました。特にTRIEの話はかなり興味を持ちました。<br />
検索周りで嫌というほど出てくるので。まだまだきちんと理解できてないので復習しないと。。。<br />
lucene-gosenにも関係あるので、しっかり資料をみて復習する予定です。<br />
<br />
基本的に検索系の話に関連があることが多くて面白く聞くことができました。<br />
「類似文字列検索してますか？」や「ランキング学習ことはじめ」などは身近な話（だけど、なかなかきちんと学習できてない話）でした。<br />
また、「自然言語処理におけるargmax操作」では大学でやっていた線形計画法などのわかりやすい説明で10年経ってようやく理解ができた次第です。。。（もっとちゃんと勉強しとけば。。。）<br />
懇親会にも参加し、研究に近い分野の方が多い中でいろいろな話が聞けたのはよかったと思います。<br />
<br />
次回もいろいろなキーワードを拾いたいのでぜひ参加したいです。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Kuromojiを調べてみた(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/20/kuromoji%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/</link>
      <pubDate>Wed, 20 Jul 2011 12:29:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/20/kuromoji%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/</guid>
      <description>以前から春山さんのブログ（リンク）や勉強会で耳にはしていたのですがソースは読んでいませんでした。 先日、Luceneにcontributeされ</description>
      <content:encoded><p>以前から<a href="http://blog.livedoor.jp/haruyama_seigo/archives/51806436.html">春山さんのブログ（リンク）</a>や勉強会で耳にはしていたのですがソースは読んでいませんでした。
先日、<a href="https://issues.apache.org/jira/browse/LUCENE-3305">Luceneにcontributeされた（リンク）</a>ので軽くソースを読んでみました。</p>
<p><a href="http://atilika.org/">公式サイトはこちら</a></p>
<p>まずはMeCabのページにある<a href="http://mecab.sourceforge.net/#diff">比較表（リンク）</a>を基準に特徴を調べてみました。せっかくなので、lucene-gosenも隣に。</p>
<table class="list_view">
<thead>
<tr>
<th>&nbsp;</th>
<th>Kuromoji</th>
<th>lucene-gosen</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>解析モデル</td>
<td>なし（学習機能なし）</td>
<td>なし（学習機能なし）</td>
</tr>
<tr class="specalt">
<td class="alt">コスト推定</td>
<td class="alt">なし（学習機能なし）</td>
<td class="alt">なし（学習機能なし）</td>
</tr>
<tr class="spec">
<td>学習モデル</td>
<td>なし（学習機能なし）</td>
<td>なし（学習機能なし）</td>
</tr>
<tr class="specalt">
<td class="alt">辞書引きアルゴリズム</td>
<td class="alt">Double Array Trie</td>
<td class="alt">Double Array Trie</td>
</tr>
<tr class="spec">
<td>解探索アルゴリズム</td>
<td>Viterbi</td>
<td>Viterbi</td>
</tr>
<tr class="specalt">
<td class="alt">連接表の実装</td>
<td class="alt">2次元 Table</td>
<td class="alt">3次元 Table</td>
</tr>
<tr class="spec">
<td>品詞の階層</td>
<td>無制限多階層品詞？ipadic、unidic形式に対応</td>
<td>無制限多階層品詞</td>
</tr>
<tr class="specalt">
<td class="alt">未知語処理</td>
<td class="alt">字種 (動作定義を変更可能)（おそらく。）</td>
<td class="alt">字種（変更不可能）</td>
</tr>
<tr class="spec">
<td>制約つき解析</td>
<td>たぶん、不可？</td>
<td>たぶん、不可？</td>
</tr>
<tr class="specalt">
<td class="alt">N-best解</td>
<td class="alt">不可能</td>
<td class="alt">不可能</td>
</tr>
</tbody>
</table>
<p>こうやって比較してみるとlucene-gosen（Sen）とあまりかわりはありませんが、lucene-gosenが少し古いのがわかりますね。。。</p>
<p>Kuromojiで利用出来る辞書は現時点ではMeCab IPADIC、UNIDICの2種類のようです。</p>
<p>ソースを読んでの感想ですが、やはりMeCabが偉大だということがよくわかりました。
Senよりも新しいMeCabの処理に似ている点が多いです。解探索アルゴリズムや連接コスト表が特に。
MeCab向けの辞書を利用しているためというのもあるかと思います。</p>
<p>Kuromojiが特徴的なのは「searchモード」と呼ばれるモードが用意されていることです。
公式サイトにある例ですと、「関西国際空港」が「関西」「国際」「空港」というTokenで出力されます。
ソースを見たところViterbiアルゴリズムで辞書を探索しているときに特定の条件でコストをカサ増しすることで、結果を変えるという処理を行っているようです。</p>
<ul>
<li>全て漢字の単語：3文字以上の場合に「(単語の長さ-3)*10000」をコストに加算</li>
<li>その他の単語：7文字以上の場合に「(単語の長さ-7)*10000」をコストに加算</li>
</ul>
<p>このようにコストを変化させることで「空港」でも「関西国際空港」という文字を含む文章が検索できる仕組みになっています。
また、「拡張searchモード」と呼ばれるモードも存在し、こちらは、未知語をuni-gramで区切って出力を行うようです。</p>
<p>ソース上で確認しただけで、未確認ですが、GraphvizFormatterというクラスがあるので、Graphvizで読み込める形式で形態素解析の結果が出力されるかもしれません。（すみません、確認してなくて。）</p>
<p>未知語の処理やsearchモードなど面白い機能があるので、試してみるのもいいかもしれません。lucene-gosenを考えていく上でも参考になりそうです。</p>
<p>最後に余談ですが、<a href="http://atilika.org/confluence/display/KUROMOJI/FAQ">FAQのページ（リンク）</a>が面白いです。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>NAIST-JDic for MeCabのPreprocessorの実装に関する備忘録(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/12/naist-jdic-for-mecab%E3%81%AEpreprocessor%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E5%82%99%E5%BF%98%E9%8C%B2/</link>
      <pubDate>Tue, 12 Jul 2011 10:06:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/12/naist-jdic-for-mecab%E3%81%AEpreprocessor%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E5%82%99%E5%BF%98%E9%8C%B2/</guid>
      <description>忘れてしまうので、備忘録を残しておきます。 一応、ソースには少しずつコメントをいれてはいるのですが。 私は残念ながら、自然言語処理は初心者に毛が</description>
      <content:encoded><p>忘れてしまうので、備忘録を残しておきます。
一応、ソースには少しずつコメントをいれてはいるのですが。
私は残念ながら、自然言語処理は初心者に毛が生えた程度（現在、鋭意勉強中）で、対応方法に問題があるかもしれません。気づいた方はコメントをいただけると助かります。</p>
<h3 id="辞書ファイルについて">辞書ファイルについて</h3>
<p>NAIST-JDic for MeCabの辞書ファイルは以下の構成になっています。</p>
<table class="list_view">
<thead>
<tr>
<th>ファイル名</th>
<th>メモ</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>char.def</td>
<td>文字種の設定</td>
</tr>
<tr class="specalt">
<td class="alt">feature.def</td>
<td class="alt">辞書学習用の設定？</td>
</tr>
<tr class="spec">
<td>left-id.def</td><td>左文脈IDのマスタ（左文脈ID、品詞情報）</td>
</tr>
<tr class="specalt">
<td class="alt">matrix.def</td>
<td class="alt">連接コスト表（前件文脈ID,後件文脈ID,連接コスト）</td>
</tr>
<tr class="spec">
<td>pos-id.def</td>
<td>品詞IDのマスタ（品詞情報、ID）</td>
</tr>
<tr class="specalt">
<td class="alt">rewrite.def</td>
<td class="alt">rewrite情報（左右文脈に出現した場合のそれぞれの品詞情報のrewriteルール。辞書学習で主に利用）</td>
</tr>
<tr class="spec">
<td>right-id.def</td>
<td>右文脈IDのマスタ（右文脈ID、品詞情報）</td>
</tr>
<tr class="specalt">
<td class="alt">unk.def</td>
<td class="alt">未知語の品詞情報（文字種ごとに未知語のコスト、左右文脈ID、品詞情報が記載されている）</td>
</tr>
<tr class="spec">
<td>naist-jdic.csv</td>
<td>単語辞書（単語、左右文脈ID、単語コスト、品詞情報、読みなど記載）</td>
</tr>
</tbody>
</table>
<p>現時点では、MeCabDicPreprocessorでは以下のファイルを利用しています。</p>
<ul>
<li>left-id.def</li>
<li>matrix.def</li>
<li>right-id.def</li>
<li>naist-jdic.csv</li>
</ul>
<p>上記以外のファイルは現時点では利用しない実装になっています。
ただし、rewrite.def、unk.def、char.defについては利用したほうがよりMeCabに近い結果が得られるような気がしています。（特に文字種ごとのコストを利用することは有効と思われます。）</p>
<h3 id="preprocessorでの処理について">Preprocessorでの処理について</h3>
<p>lucene-gosenはSenの後継であり、MeCabの昔のバージョンを移植したものがベースとなっています。
lucene-gosenとMeCabの現時点での実装の大きな違いとして、連接コスト表の違いがあります。
ここからは憶測になってしまいますので、注意してください。（論文を探せばどこかにこの実装の変化の過程が記載してあるかもしれないですが、まだ探していません、すみません。）
過去のMeCabではChaSen向けの辞書を利用していました。
ChaSenでは連接コスト表が3つの項（前の前、前、後）から構成されていました。（n項まで定義可能らしい）
ですので、lucene-gosenのViterbiアルゴリズムの引数も3つのノードが引数となっています。
lucene-gosen向けの連接コスト辞書も同様の作りになっています。
一方、現在のMeCabは先ほど書いたとおり、matrix.defでは2項の連接コスト表（前、後）となっています。この違いを保管するために、Preprocessorでは、matrix.defを3項にするために一番左（前の前）については任意の品詞を採用できるように「<em>,</em>,<em>,</em>,<em>,</em>,*」のみを設定しています。</p>
<p>現時点では、Preprocessorの出力である中間ファイルを共通の形式に出力することで、DictinaryBuilder以降の処理に変更を加えることなくNAIST-JDic for MeCabへの対応を行う形を取りました。まずは使えるようにするのが先かと思いまして。
ただ、MeCabの辞書の構成から考えると中間ファイルに落とし込む処理に無駄があると感じています。
matrix.defでせっかく、IDによる連接コスト表を構成しているのに、IDを品詞情報の文字列に戻したconnection.csvを生成していますので。</p>
<p>ということで、備忘録でした。
あとは、テストをどうするか（正解をどう考えるか）なども考える必要があります。現時点での悩みの種です。。。アイデア募集中です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 1.1.1リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/04/lucene-gosen-1-1-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 04 Jul 2011 20:00:40 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/04/lucene-gosen-1-1-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>lucene-gosen 1.1.1をリリースしました。 先日お知らせしたバグ修正を取り込んだjarを用意いしました。 ダウンロードはこちらから</description>
      <content:encoded><p>lucene-gosen 1.1.1をリリースしました。<br />
先日お知らせした<a href="http://johtani.jugem.jp/?eid=10">バグ修正</a>を取り込んだjarを用意いしました。<br />
<br />
<a href="http://code.google.com/p/lucene-gosen/downloads/list">ダウンロードはこちらから</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 3.3リリース（速報）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/01/lucene-solr-3-3%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</link>
      <pubDate>Fri, 01 Jul 2011 15:29:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/01/lucene-solr-3-3%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</guid>
      <description>Solr/Lucene 3.3がリリースされました。（速報） 以下、各サイトへのリンクです。 Solrリリースのお知らせ Luceneリリースのお知らせ リリースのタイミ</description>
      <content:encoded><p>Solr/Lucene 3.3がリリースされました。（速報）<br />
<br />
以下、各サイトへのリンクです。<br />
<br />
<a href="http://lucene.apache.org/solr/#July+2011+-+Solr+3.3+Released">Solrリリースのお知らせ</a><br />
<br />
<a href="http://lucene.apache.org/#1+July+2011+-+Lucene+Core+3.3+and+Solr+3.3+Available">Luceneリリースのお知らせ</a><br />
<br />
リリースのタイミングがどんどん早くなってる。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Hadoopを中心とした分散環境での開発方法論・モデリング・設計手法等についての座談会(第5回）に参加しました。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/29/hadoop%E3%82%92%E4%B8%AD%E5%BF%83%E3%81%A8%E3%81%97%E3%81%9F%E5%88%86%E6%95%A3%E7%92%B0%E5%A2%83%E3%81%A7%E3%81%AE%E9%96%8B%E7%99%BA%E6%96%B9%E6%B3%95%E8%AB%96%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E8%A8%AD%E8%A8%88%E6%89%8B%E6%B3%95%E7%AD%89%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E5%BA%A7%E8%AB%87%E4%BC%9A-%E7%AC%AC5%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 29 Jun 2011 23:10:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/29/hadoop%E3%82%92%E4%B8%AD%E5%BF%83%E3%81%A8%E3%81%97%E3%81%9F%E5%88%86%E6%95%A3%E7%92%B0%E5%A2%83%E3%81%A7%E3%81%AE%E9%96%8B%E7%99%BA%E6%96%B9%E6%B3%95%E8%AB%96%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E8%A8%AD%E8%A8%88%E6%89%8B%E6%B3%95%E7%AD%89%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E5%BA%A7%E8%AB%87%E4%BC%9A-%E7%AC%AC5%E5%9B%9E%E3%81%AB%E5%8F%82%E5%8A%A0%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>「Hadoopを中心とした分散環境での開発方法論・モデリング・設計手法等についての座談会(第5回）」に参加しました。300名入るイベントルー</description>
      <content:encoded><p>「<a href="http://www.zusaar.com/event/agZ6dXNhYXJyDAsSBUV2ZW50GNc_DA">Hadoopを中心とした分散環境での開発方法論・モデリング・設計手法等についての座談会(第5回）</a>」に参加しました。300名入るイベントルームでしたが、後ろの方まで人が埋まっていました。<br />
ということで、主に自分用ですが、メモを取ったので。<br />
※二次会行きたかった。。。<br />
<br />
<br />
1.「鉄道システムへの誘い」<br />
　@ayasehiro（本名無理w）<br />
　Hadoopの話はありません。<br />
<br />
　○鉄道系基幹システムの開発<br />
　　実態：<br />
　　　耐用年数：１０年以上<br />
　　　開発期間：数年～５年程度<br />
　　　開発規模：～10Mステップ、10k人月～<br />
<br />
　　ほとんどテスト、しかも異常系が主体。<br />
　　夜間に実際に鉄道を走らせて試験したり。<br />
<br />
　　開発サイクルが長い<br />
　　　人材育成が難しい、ノウハウがたまらない。<br />
<br />
　　開発自体はほとんど時間がなく、設計・製造・試験など新規技術の採用が難しい。<br />
　　開発４年前の調査・検証自体が２年程度。<br />
　　Hadoopも調査中。<br />
　　<br />
　○鉄道システム３大システム<br />
　　マルス（予約オンラインシステム）（1960～）<br />
　　コムトラック（運行管理システム）（1972～）<br />
　　ヤックス（ヤード自動化システム）（1968～1984）<br />
<br />
　○鉄道輸送システムとは<br />
　　用語：<br />
　　　運行を計画する=&gt;輸送計画<br />
　　　列車を運行する=&gt;運行管理<br />
<br />
　　需要想定＋営業施策＋その他（お召し列車など）＝列車ダイヤ作成<br />
　　　基本計画（長期）＋短期計画（数日～四半期程度）＝列車ダイヤ（重ねあわせてできあがり）<br />
<br />
　　ダイヤの計画（発車時刻など）と車両運用（車両自体の走る組み合わせ（仮の車両））の作成<br />
　　＋乗務員運用（乗務員の運用計画）<br />
<br />
　　運行管理：<br />
　　　なにも起きなければすることなし。（車両故障、天候、人身事故などによる整合性を取る作業）<br />
　　　＝事前に計画した輸送計画をすべて見直し<br />
　　　遅延の検知は？<br />
　　　　昔：人による伝令<br />
　　　　今：システムによる検知（レールに電流流して検知）<br />
<br />
　　　運転整理（実際に遅れた）：<br />
　　　　部分運休、折り返し駅の変更などにより対応<br />
　　　　元の計画になるべく近づく形で修正していく。<br />
<br />
　　　新幹線、山手線、京浜東北線などは速度信号という信号が表示される。<br />
　　　線路上に信号はないらしい。<br />
<br />
　○鉄道システムを取り巻く情勢<br />
　　少子高齢化・人口減少のため凋落産業となっている。<br />
　　社会インフラの責務＝動くのが当たり前<br />
　　2007年問題（ベテランの引退）＝スジ屋さんは最近いないらしい。<br />
　　高度な判断支援をするシステムが必要<br />
　　　連続稼動＝分散技術を適用できない？<br />
　　　関係各所との情報共有<br />
　　　計画立案のための情報支援＝最適化技術を適用できない？<br />
　　<br />
　○分散処理技術の適用<br />
　　個人的な感想<br />
　　　可用性（連続稼動）のための仕組み<br />
　　　バッチ処理<br />
<br />
　○分散技術の適用<br />
　　・連続稼動<br />
　　　active-active構成がメイン<br />
　　　　主系の出力だけを行う。問題が出れば副系の出力。<br />
　　　3系統の出力を比較器にて出力もある<br />
　　　　magiシステム<br />
　<br />
　　　問題点：<br />
　　　　ハードが高い（H社）<br />
　　　　ソフトウェアの作り込みが複雑＝テストが前パターンできない<br />
　　　解決案：<br />
　　　　汎用的なハードが使いたい。<br />
　　　　作り込みも減らしたい<br />
　　・バッチ処理：<br />
　　　Asakusa使えないかなーw<br />
<br />
　○最適化技術の採用<br />
　　コンピュータ技術の発展<br />
　　2007年問題<br />
　　　職人に言わせれば最適化はいらない、俺の言うことを聞けｗ<br />
<br />
　　・車両運用のモデリング<br />
　　　車両数大=&gt;組み合わせ大<br />
　　　制約条件が多い<br />
　　　車両運用の場合、走行累積キロの条件もある<br />
　　　-&gt;有向グラフにモデル化される。（ただし、グラフ化するまでが大変）<br />
　　・乗務員運用のモデリング<br />
　　　車両と違い、乗務員は1回で2人とか運べる（運転士＋移動する人とか）<br />
<br />
　　・車両割当のモデリング<br />
　　　やはり、グラフ化が可能<br />
<br />
　　・乗務員交番のモデリング。。。など<br />
<br />
　　結構一般的なモデルに落とし込める。ただし、落し込みが大変。<br />
　　机上研究だったものが、コンピュータの発展により実証研究になりつつある。<br />
　　<br />
　○まとめ<br />
　　鉄道システムはまだまだ未到の領域が残っている。<br />
　　開発サイクルが長いため、保守的な開発になる（35年前の設計思想からあまり変わってなかったりする）<br />
　　しかし業務要件やシステム利用者の意識は変化している<br />
　　<br />
　　興味を持たれた方は、ぜひ、我社に！（社名は2次会でｗ）<br />
<br />
　○Q&amp;A<br />
　　Q：鉄道システムのカルチャーってイケイケ？保守的？（@okachimachiorgさん）<br />
　　A：最新技術も知らないとだめじゃないかという人が出てきている。<br />
　　　 コア部分（安全第一なところ）＋周辺領域（ある程度融通が効きそうな部分）と考えることができるんじゃないかって人も出てきている。<br />
　　　 JR九○＝先進的<br />
　　　 JR四○、北○道＝お金ない<br />
　　　 JR○海＝超保守（企業的に超保守）<br />
　　　 JR東、西＝うーむ？<br />
　　　 東京メト○（運行計画）、阪○＝先進的<br />
　　　 京○急行＝基本人間で進路制御w<br />
　　　基本的には新しいものには興味をもつ人たちでは。<br />
　　Q：Su○caとかで分散処理は利用出来るんでは？<br />
　　A：匿名なので外側から見ていると分散処理はいろいろ使えるんでは？<br />
　　　 ログデータからいろいろできるんじゃないの？活かすべきでないの？<br />
　　　 使いどころはいっぱいある。<br />
　　Q：鉄道システムでどうしようもなくなったことはあるか？<br />
　　A：保守体制が一番気になる。<br />
　　　 OSSとかならまだ調べられる。ミドルウェアなどの保守契約が必要。<br />
　　　 保守体制が確立されてればある程度の保守費用は飲み込む。<br />
　　　 どうしようもないことはないが、今すごく困ってることは<br />
　　　 Excelで帳票を出したいとかいわれること。（ちょっと前に作ったシステムでExcel2003。バージョンあげると速度が遅くなったりするｗ）<br />
　　　 ilog社のものを使ってたが、IBMに買収されて保守費用があがってこまってるｗ<br />
　　　 保守が10年と長いため、サポートなどの折衝が必要。<br />
　　Q：最適化の適用範囲は？<br />
　　A：走行順序（どこで追い抜くか）の算出に活用。ほぼ完成でユーザ教育中。<br />
　　　 1列車の波及がかなり影響が出る。ダイヤだけ見ると列車だけだが、乗務員も関係しており、大変。<br />
　　　 ある時点から終電までを最適化の対象としたりして割り切っている。<br />
　　　 また、不足分について算出が出来れば、そこで打ち切ったりもする。<br />
<br />
2.「九州電力におけるHadoopの取り組みについて」<br />
　株式会社キューデンインフォコム e－ビジネス部　@hisashi_yano<br />
　概要：2年間関係したOSSをメインにしたシステムの話。<br />
<br />
　○九州電力の概要<br />
　　現在風当たりが強い業界。<br />
　　東電の1/10くらい<br />
　　部門ごとに大手ベンダーが関わってる。<br />
<br />
　○Hadoop採用の経緯<br />
　　部門ごとに個別最適なシステムを導入していてベンダーロックインされてる。<br />
<br />
　　　　・ホストのリプレース<br />
　　・両現用センター構成への対応<br />
　　・スマートグリッドへの対応<br />
<br />
　　問題点<br />
　　・コスト削減<br />
　　・技術革新への中の人の対応（内部でも問題を理解できるように）<br />
　　・商用パッケージのカスタマイズの限界<br />
　　・脱ベンダーロックイン（実は楽なんだけど。。。）<br />
<br />
<br />
　○過去2年間の研究内容<br />
　　・H21年度の結果<br />
　　　テーマ：クラウドの要素技術の研究<br />
　　　　KVM、Eucalyptus、wakame、hadoop　　　<br />
　　　<br />
　　　性能比較：VMWareとKVM-&gt;ベンチマーク比較<br />
　　　　　結論：性能的にあまり問題なし。<br />
　　　MapReduceの耐障害性など<br />
　　　　　ダミーデータにより台数増加による影響を検証<br />
　　　　　結論：台数大-&gt;性能向上<br />
　　　　　　　　ストリーミングは性能劣化する<br />
　　　　　　　　スループットはリニアに向上<br />
　　　　　信頼性は？<br />
　　　　　　実行中にノードを抜いたりして検証。<br />
　　　　　結論：問題なし。<br />
　　　クラウド環境におけるシステム管理手法<br />
　　　　複数ハードで1アプリという構成になる。<br />
　　　　監視対象が膨大になる。<br />
　　　　障害発生時の切り離しや監視対象も膨大。<br />
　　　　データセンター自体を監視する仕組みが必要では？というところで終了。<br />
<br />
　　・H22年度の結果<br />
　　　分散に特化した研究<br />
　　　前年度の課題<br />
　　　　サーバの仮想化・管理に関する課題<br />
　　　　分散処理に関する課題<br />
　　　　分散処理環境の運用監視に関する課題<br />
　　　目的：<br />
　　　　Hadoopを本番への適用（実際にはダミーデータ＋本番の仕組み）<br />
　　　<br />
　　　柔軟なサーバ統合基盤（サーバを起動-&gt;バッチを起動-&gt;回す仕組み）＝MonkeyMagic<br />
　　　　libvirtを使ってる<br />
　　　　<br />
　　　50台の仮想サーバの起動が10数分で完了。<br />
<br />
　　運用監視基盤（monkey magic）<br />
　　　仮想、実サーバ混在の監視<br />
　　　監視状況（サーバの状況）から判断して制御する仕組みを構築<br />
　　　DSLにてルール（判断＋制御）を指定<br />
　　　・ジョブの監視<br />
　　　・ジョブの実行管理<br />
　　　・構成管理の省力化<br />
　　　　volanteと連携が可能＝AmazonWebServiceとも連携可能<br />
　　　・サーバリソース＋アプリケーションの一括監視が可能<br />
　　分散バッチ処理の概要<br />
　　　RDBからKV形式にして抽出し、MRで回してRDBに戻すという研究<br />
　　　対象：<br />
　　　　配電部門（電柱の設備情報の目視検査）のデータの月間バッチ処理<br />
　　　現状：<br />
　　　　19時間程度かかってる。<br />
　　　テスト環境：<br />
　　　　実サーバ2台（仮想10台）<br />
　　　　MySQL、Javaで実装<br />
　　　処理内容<br />
　　　　電柱104万本<br />
　　　　巨大バッチを分割して実装<br />
　　　結果<br />
　　　　MySQL1台では15日以上かかる処理（現行システムで19時間）<br />
　　　　処理が32分で終了！他でも効果でるよね。<br />
　　　バッチ短縮の理由は？<br />
　　　　1.データアクセスが分散された<br />
　　　　2.処理の並列化（多重化出来る部分がうまくできた）<br />
　　　<br />
　　分散処理を書くのに2名死亡。。。<br />
<br />
　　適用基準の策定、開発ガイドライン、フレームワーク整備などが必要。<br />
<br />
　・H23年度は？<br />
　　Asakusaの適用など。<br />
<br />
　・さらに今後は？<br />
　　スマートグリッドへの適用<br />
　　　-&gt;メーターの交換が必要だが、10年くらいかかる<br />
　　　-&gt;スパンが長い（10年）ので商用製品だときつい？<br />
　　　データ量も半端ない。<br />
　　　テネシー州とClouderaでOpenPDCってのやってるらしい。<br />
　　<br />
　　電力と気温の関係は密接な関係あり。<br />
　　　エアコンが割合を占めてるから。<br />
　　　過去実績と予想気温データから分析するのにHadoop使える！<br />
　　　<br />
　・2年間やってきて思うこと<br />
　　将来目指すべき理想像を掲げるのが重要<br />
　　新技術導入は段階を踏むことが必要<br />
　　コミュニケーション大切！<br />
<br />
　○Q&amp;A<br />
　　Q：仮想化環境のオーバヘッドは？（I/O）<br />
　　A：台数を増やしたときにどうなるか？というのを検証したかった。<br />
　　　アプリ配布も考えていたので、物理サーバに縛られたくなかった。<br />
　　Q：仮想化に関して気をつけたM/RのPGで気をつけたことありますか？<br />
　　A：まったくないです。<br />
　　Q：日本でスマートグリッドははやるの？<br />
　　A：電力会社的にはやりたくない。費用対効果があまりない。<br />
　　Q：今後のスケジュールは？<br />
　　A：文書管理システムの組織名変更などの処理時間が540時間とかでてくるらしい。<br />
　　　これをHadoopで対応してみようかと思ってる。<br />
　　Q：Asakusaをどう評価していくのか？<br />
　　A：開発効率性があがるか？は検証する予定。1/3くらい楽になるんじゃないかなぁ？byのぐちさん<br />
　　　バッチの種類などにもよると思うが、標準化も指標にする予定。<br />
　　　結果はまたこの場で報告する予定。<br />
　　Q：Asakusa＋MonkeyMagicの連携はどんなこと考えてる？<br />
　　A：MonkeyMagicを運用基盤として行く予定。合意が取れればだけど。<br />
　　※MonkeyMagicもOSSにするよー<br />
<br /></p>
<h3 id="まとめbr-">まとめ<br /></h3>
<p>Hadoopから少しずつ離れつつありますが、やはり興味があるので、非常に楽しく話を聞けました。<br />
また、今回はインフラ分野のシステムということで、システムに要求されるレベルや<br />
運用周りにも気を配っている話が聞けたのが収穫でした。保守期間が長いため、テストが長い＝<br />
運用もしっかりと考慮を入れた設計、実装が必要になるというのは最もだと思います。<br />
ただ、少しずつ修正が入るアジャイルなども同様かと。<br />
<br />
MonkeyMagicが出来上がってきた背景の話を聞いて、さらに興味が湧いてきたところです。<br />
今後もかかわりが少ないかもしれないですが、ウォッチしていきたいと思いました。<br />
<br />
ただ、興味あるモノが多すぎるので、優先度をつけつつこなして行かないと。。。<br />
少しずつでも身につけていきたいと思う今日このごろです。<br />
<br />
<br />
※追記：Twitterでコメントを頂いたので、忘れないように追記。<br />
コメントを貰えるだけでもうれしい。やはり、アウトプットしたらフィードバック貰いたいし。<br />
ありがとうございます！<br />
<br />
<a href="http://twitter.com/#!/cocoatomo/status/86124705140047872">Twitter / @cocoatomo: あの質問をまとめるとこうなるのかぁ…… 最適化そのも &hellip;</a><br />
<br />
<a href="http://twitter.com/#!/cocoatomo/status/86236412344680448">Twitter / @cocoatomo: @johtani すみません, コメントはコメント欄 &hellip;</a><br />
<br />
<a href="http://twitter.com/#!/cocoatomo/status/86236865174323200">Twitter / @cocoatomo: @johtani そこらへんの理論って最後には計算量 &hellip;</a><br />
<br />
<a href="http://twitter.com/#!/cocoatomo/status/86281140184432641">Twitter / @cocoatomo: @johtani あの話し振りだとどうもまだ本格的に &hellip;</a><br />
<br />
あと、まとめも出来ていたので、ついでに。<br />
<a href="http://togetter.com/li/155628">Togetter - 「2011/06/29_Hadoopを中心とした分散環境での開発方法論・モデリング・設計手法等についての座談会(第5回） #hadoopmodeling」<a/><br />
<br />
関連するブログも見つけたので。<br />
<br />
<a href="http://blog.goo.ne.jp/hishidama/e/a6c5ae00a7a1bacbc88e023691a2eb0c">第5回Hadoop座談会の感想 - ひしだまの変更履歴</a><br />
<br />
<a href="http://d.hatena.ne.jp/torazuka/20110629/hadoop">Hadoopモデリング座談会（第5回）へ行ってきました - 虎塚</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>compositePOS（CompositeTokenFilter）のバグ修正(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/28/compositeposcompositetokenfilter%E3%81%AE%E3%83%90%E3%82%B0%E4%BF%AE%E6%AD%A3/</link>
      <pubDate>Tue, 28 Jun 2011 12:36:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/28/compositeposcompositetokenfilter%E3%81%AE%E3%83%90%E3%82%B0%E4%BF%AE%E6%AD%A3/</guid>
      <description>以前、こちらで話題に上がっていた「未知語」に関するcompositePOSのエラーの件を調査しました。（Twitterでも流れてました。） 次</description>
      <content:encoded><p>以前、<a href="http://blog.livedoor.jp/haruyama_seigo/archives/51801386.html#comments">こちら</a>で話題に上がっていた「未知語」に関するcompositePOSのエラーの件を調査しました。（Twitterでも流れてました。）
次のような条件の場合にエラーが発生するようです。</p>
<ul>
<li>compositePOSの設定に構成品詞として「未知語」が指定されたエントリが存在する。</li>
<li>未知語が連続して出現する文字列をanalyzeする。（例：ニンテンドーDSi）</li>
</ul>
<p>ということで、trunkに修正版をコミットしました。
Issueは<a href="https://code.google.com/p/lucene-gosen/issues/detail?id=11">こちら。</a></p>
<p>※お茶をにごす感じの日記になってしまいました。次回はマシな記事を書く予定です。。。</p>
<p><span style="color:#FF0000">6/29追記</span>：恥ずかしいバグをいれこんでしましました。。。
ということで、trunkに再度修正版をコミットしました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>NAIST-JDic for MeCab対応版（仮実装）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/21/naist-jdic-for-mecab%E5%AF%BE%E5%BF%9C%E7%89%88%E4%BB%AE%E5%AE%9F%E8%A3%85/</link>
      <pubDate>Tue, 21 Jun 2011 07:38:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/21/naist-jdic-for-mecab%E5%AF%BE%E5%BF%9C%E7%89%88%E4%BB%AE%E5%AE%9F%E8%A3%85/</guid>
      <description>lucene-gosenのtrunkbranches/impl-mecab-dicにNAIST-JDic for MeCabの辞書を利用出来るPre</description>
      <content:encoded><p>lucene-gosenの<del>trunk</del>branches/impl-mecab-dicにNAIST-JDic for MeCabの辞書を利用出来るPreprocessorをコミットしました。<br />
<br />
ビルド方法は次のとおりです。<br /></p>
<pre><code>
$ cd lucene-gosen-trunk
$ ant -Ddictype=naist-mecab
</code></pre><p><br />
現在のstable版で利用できる辞書は「ipadic」「naist-chasen」の2種類でした。<br />
<a href="http://johtani.jugem.jp/?eid=4">以前の記事</a>に書きましたが、naist-chasenの辞書でも2008年の更新となっています。<br />
今回コミットしたPreprocessorでは<a href="http://sourceforge.jp/projects/naist-jdic/">NAIST-JDicのサイト</a>で公開されているMeCab向けの辞書である「mecab-naist-jdic-0.6.3-20100801」を利用出来るようになります。<br />
<br />
ただし、lucene-gosenは昔のMeCabから派生したSenをもとにしていますので、最新のMeCabが持っている機能は<br />
利用できません。<br />
MeCab向けの辞書のうち一部のもの（matrix.def、naist-jdic.csvなど）を利用してlucene-gosen向けの辞書の中間ファイルを生成する仕組みになっています。<br />
<br />
まだ、仮実装版ということで、とりあえず動作するバージョンとなっています。<br />
まだテストが不十分ですが。。。<br />
利用してみて問題などあれば、lucene-gosenの<a href="http://code.google.com/p/lucene-gosen/issues/list">issue</a>に登録していただくか、コメントを頂ければと思います。<br />
<br />
※更新が週1回に落ちてきてるので、もう少し頑張らねば。<br />
<br />
<span style="color:#FF0000">※2011/07/04追記</span>
trunkにコミットしていましたが、branchに一旦移動しました。
仮実装として一旦コミットしたので、trunkとは別でテストする必要があるかと思った次第です。
ということで、試してみたい方は、<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2Fimpl-mecab-dic">branches/impl-mecab-dic</a>にありますので、触ってみてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>compositePOSの利用例（naist-chasenでの英単語の出力方法例）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/14/compositepos%E3%81%AE%E5%88%A9%E7%94%A8%E4%BE%8Bnaist-chasen%E3%81%A7%E3%81%AE%E8%8B%B1%E5%8D%98%E8%AA%9E%E3%81%AE%E5%87%BA%E5%8A%9B%E6%96%B9%E6%B3%95%E4%BE%8B/</link>
      <pubDate>Tue, 14 Jun 2011 00:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/14/compositepos%E3%81%AE%E5%88%A9%E7%94%A8%E4%BE%8Bnaist-chasen%E3%81%A7%E3%81%AE%E8%8B%B1%E5%8D%98%E8%AA%9E%E3%81%AE%E5%87%BA%E5%8A%9B%E6%96%B9%E6%B3%95%E4%BE%8B/</guid>
      <description>前回、naist-chasenではアルファベットが別々の単語としてanalyzeされてしまうという話をしました。 ただ、これだと、英単語が含ま</description>
      <content:encoded><p>前回、naist-chasenではアルファベットが別々の単語としてanalyzeされてしまうという話をしました。</p>
<p>ただ、これだと、英単語が含まれた文章を形態素解析すると、英単語がアルファベット単位に区切られてしまい、
単語の意味をなさなくなってしまいます。</p>
<p>lucene-gosenでは、この問題に対応するための方法が提供されています。
CompositeTokenFilter（compositePOS）という機能です。</p>
<p>文字通り「トークン」を「合成」するための機能になります。</p>
<p>利用するためには以下の作業が必要です。（※Solrでのの利用方法を説明します。）</p>
<ol>
<li>compositePOS設定ファイル（composite_pos_ja_naist-chasen.txt）の用意</li>
<li>schema.xmlのtokenizerにcompositePOS設定を追加</li>
</ol>
<p>まずは、compositePOS設定ファイルの記述方法について説明します。
compositePOS設定ファイルには１行につき１つのcompositeの設定を記述していきます。
記述方法は次のようになります。品詞名を半角スペース区切りで記述します。</p>
<pre><code>
連結品詞名 構成品詞名1 構成品詞名2 ... 構成品詞名n
</code></pre><p>それぞれは次のような意味を持ちます。</p>
<ul>
<li>連結品詞名：合成したあとのトークンの品詞として出力する品詞名</li>
<li>構成品詞名：合成したい品詞名（スペース区切りで複数指定可能）</li>
</ul>
<p>TokenizerのcompositePOS機能は、構成品詞に定義されたトークンが連続して出力された場合に、
結合（合成）して１つのトークン（連結品詞名）として出力します。
また、以下のように構成品詞名が１種類で連続品詞名としても利用する場合は次のように省略した記述も可能です。</p>
<p>以下にcompositePOSファイルの設定例を上げます。
※なお、現時点では#によるコメント機能はありません。ので、記述した内容がそのまま利用されます。</p>
<pre><code>
名詞-数 
未知語 記号-アルファベット
</code></pre><p>１行目は連続した数字を1つのトークン（名詞-数）として出力する設定です。（連続品詞名＝構成品詞名として省略して記述した例になります。）
2行目は連続したアルファベットを１つのトークン（未知語）として出力する設定です。</p>
<p>次にSolrのschema.xmlにlucene-gosenのtokenizerを利用するフィールドタイプの設定を記述します。
$SOLR_HOME/conf/schema.xmlに以下を追加します。&lt;types&gt;～&lt;/types&gt;タグの間に記載します。</p>
<pre><code>
...
 &lt;types&amp;gt;
 ...

    &lt;fieldType name=&amp;quot;text_ja&amp;quot; class=&amp;quot;solr.TextField&amp;quot; positionIncrementGap=&amp;quot;100&amp;quot;&amp;gt;
    &lt;analyzer&amp;gt;
        &lt;tokenizer class=&amp;quot;solr.JapaneseTokenizerFactory&amp;quot; compositePOS=&amp;quot;composite_pos_ja_naist-chasen.txt&amp;quot;/&amp;gt;
    &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
 &lt;/types&amp;gt;
...

</code></pre><p>重要なのはtokenizerタグのcompositePOS属性になります。ここに1.で記載したファイルを指定します。指定したファイルはschema.xmlと同じディレクトリに配置します。
以上が利用するための設定です。</p>
<p>前回同様、「このComputerは、10回に1回の割合で起動中に青い画面が表示されます。」という文章をanalyze画面で解析した結果を示します。</p>
<p>
<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110614/20110614_1890705.png" alt="compositePOS設定済み"/>
    </div>
    <a href="/images/entries/20110614/20110614_1890705.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
とまぁ、記事を書きましたが、すでに<a href="http://wiki.livedoor.jp/haruyama_seigo/d/Solr/Tokenizer%C9%BE%B2%C1201105/JapaneseTokenizer">こちら</a>で出ている話ですね。。。</p>
<p>みなさん手が早くて困ってますｗ</p>
<p>ちなみに、上記の設定の場合、「100,000」や「3.14」といった文字列は「100」「,」「000」という形で出力されてしまいます。これらも数字とみなしたい場合は「名詞-数 名詞-数 記号-句点 記号-読点」という設定で１つのトークンとして出力されます。ただし、「。」も「記号-句点」なので注意が必要です。</p>
<h4 id="なお今回はlucene-gosen-110solr32を利用した例になっています">※なお、今回はlucene-gosen-1.1.0、Solr3.2を利用した例になっています。</h4>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 1.1.0 リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/13/lucene-gosen-1-1-0-%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 13 Jun 2011 12:08:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/13/lucene-gosen-1-1-0-%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>lucene-gosenの1.1.0がリリースされました。 大きな目玉はJapaneseTokenizerが出力する形態素に関するデータを遅延</description>
      <content:encoded><p>lucene-gosenの1.1.0がリリースされました。</p>
<p>大きな目玉はJapaneseTokenizerが出力する形態素に関するデータを遅延ロードすることで、パフォーマンスの改善を行ったことです。</p>
<p>詳しくは関口さんの<a href="http://lucene.jugem.jp/?eid=444">ブログ</a>で実測されてます。さすが、早い。。。
あと、先日リリースされたLucene/Solr 3.2への対応も行われています。</p>
<p>lucene-gosen-1.1.0のダウンロードは<a href="http://code.google.com/p/lucene-gosen/downloads/list">こちらから。</a></p>
<p>うーん、中身がない記事だ。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのTokenFilterたち(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/06/lucene-gosen%E3%81%AEtokenfilter%E3%81%9F%E3%81%A1/</link>
      <pubDate>Mon, 06 Jun 2011 16:23:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/06/lucene-gosen%E3%81%AEtokenfilter%E3%81%9F%E3%81%A1/</guid>
      <description>lucene-gosenをSolr/Luceneで利用する場合、TokenFilterを利用してTokenizerが出力したToken対して</description>
      <content:encoded><p>lucene-gosenをSolr/Luceneで利用する場合、TokenFilterを利用してTokenizerが出力したToken対してさまざまな処理（Tokenに対する正規化や展開など）を追加することが可能です。</p>
<p>今回は現在（ver. 1.0.1）用意されているTokenFilterについて説明します。
以下はTokenFilterの一覧です。
「フィルタ名」にはSolrのschema.xmlで記述するクラス名を書いてあります。</p>
<table class="list_view">
<thead>
<tr>
<th>フィルタ名(Factory名)</th>
<th>概要</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>solr.JapaneseWidthFilterFactory</td>
<td>全角のASCII文字を半角に、半角カタカナを全角にするフィルタ。例：「Ｃｏｍｐｕｔｅｒ」-&gt;「Computer」</td>
</tr>
<tr class="specalt">
<td class="alt">solr.JapanesePunctuationFilterFactory</td>
<td class="alt">区切り文字、記号などを除外するフィルタ。[※1](#token_filter_kome_1)</td>
</tr>
<tr class="spec">
<td>solr.JapanesePartOfSpeechStopFilterFactory</td>
<td>設定ファイルに記載した品詞に該当するTokenを除外するフィルタ。ファイルは「tags="ファイル名"」とfilterに記載。なお、ここで記述する品詞とはanalysis画面に表示される「partOfSpeech」の完全一致となります。</td>
</tr>
<tr class="specalt">
<td class="alt">solr.JapanesePartOfSpeechKeepFilterFactory</td>
<td class="alt">設定ファイルに記載した品詞に該当するToken<span style="color:#ff0000">"以外"</span>を除去フィルタ。ファイルは「tags="ファイル名"」とfilterに記載。なお、ここで記述する品詞とはanalysis画面に表示される「partOfSpeech」の完全一致となります。</td>
</tr>
<tr class="spec">
<td>solr.JapaneseBasicFormFilterFactory</td>
<td>Tokenを基本形に変換するフィルタ。例：「悲しき」-&gt;「悲しい」</td>
</tr>
<tr class="specalt">
<td class="alt">solr.JapaneseKatakanaStemFilterFactory</td>
<td class="alt">カタカナの長音（ー）の正規化フィルタ。4文字以上のカタカナのみの文字列の最後の長音（ー）を除去した文字列に変換します。例：「コンピューター」-&gt;「コンピュータ」、「コピー」-&gt;「コピー」</td>
</tr>
</tbody>
</table>
<p>上記のTokenFilterをJapanizeTokenizerを利用するフィールドタイプに設定することで
各フィルタによる機能が有効になります。
schema.xmlの記載に関する詳細については<a href="http://lucene-gosen.googlecode.com/svn/trunk/example/schema.xml.snippet">こちら</a>を参考にしてください。</p>
<p><span style="font-size:x-small;" id="token_filter_kome_1">※1　Characterクラスの以下の定数に相当する文字が。SPACE_SEPARATOR、LINE_SEPARATOR、PARAGRAPH_SEPARATOR、CONTROL、FORMAT、DASH_PUNCTUATION、START_PUNCTUATION、END_PUNCTUATION、CONNECTOR_PUNCTUATION、OTHER_PUNCTUATION、MATH_SYMBOL、CURRENCY_SYMBOL、MODIFIER_SYMBOL、OTHER_SYMBOL、INITIAL_QUOTE_PUNCTUATION、FINAL_QUOTE_PUNCTUATION</span></p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書とカスタム辞書について(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/02/%E8%BE%9E%E6%9B%B8%E3%81%A8%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E8%BE%9E%E6%9B%B8%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Thu, 02 Jun 2011 17:16:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/02/%E8%BE%9E%E6%9B%B8%E3%81%A8%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E8%BE%9E%E6%9B%B8%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>辞書の特性について 現在lucene-gosenでは以下の2つの辞書が利用可能です。 簡単に違いについて説明します。 IPAdicの辞書について バ</description>
      <content:encoded><h3 id="辞書の特性について">辞書の特性について</h3>
<p>現在lucene-gosenでは以下の2つの辞書が利用可能です。
簡単に違いについて説明します。</p>
<p>IPAdicの辞書について</p>
<ul>
<li>バージョン：2.6.0（※IPAdicとして公開されている最新は2.7.0）</li>
<li>最終更新日：2003/06/19</li>
<li>登録単語数：約24万語</li>
<li>NAIST-Jdicができたためか、更新されていない</li>
</ul>
<p>NAIST-Jdic-for-ChaSenの辞書について</p>
<ul>
<li>バージョン：0.4.3（※NAISTとして公開されている最新はMeCab用の辞書0.6.3）</li>
<li>最終更新日：2008/07/07</li>
<li>登録単語数：約28万語</li>
<li>IPAdicの後継として整備。品詞の定義など大まかな仕様は共通。</li>
<li>IPAdicと異なり、アルファベットや数字が1文字ずつ単語として登録されている。</li>
</ul>
<p>IPAdicとNAIST-Jdicで大きな違いはアルファベットと数字の扱いについてです。
次のような文章をそれぞれの辞書で解析した結果は次のようになります。(SolrのField Analysisの画面です。思いの外大きいのでサムネイルのみですが。)
「このComputerは、10回に1回の割合で起動中に青い画面が表示されます。」
○IPAdicの場合

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1866934.png" alt="IPAdicの解析結果"/>
    </div>
    <a href="/images/entries/20110602/20110602_1866934.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
○NAIST-Jdicの場合


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1866935.png" alt="NAIST-Jdicの解析結果"/>
    </div>
    <a href="/images/entries/20110602/20110602_1866935.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
「Computer」と「10」という単語の区切り方が異なることがわかります。
この違いは、辞書のエントリが異なるために発生します。
NAIST-Jdicでは、数字（例：「1」）やアルファベット（例：「a」）が個々のエントリで登録されているため、分割された単語として認識されます。</p>
<p>※この問題への対応方法はまた後日。</p>
<h3 id="カスタム辞書について">カスタム辞書について</h3>
<p>実際のデータを形態素解析したい場合、辞書に存在しない単語を登録して、1単語として認識させたい場合があります。（固有名詞など）
このような場合にカスタム辞書を利用することで、新しい単語を辞書に登録することが可能になります。
カスタム辞書を利用する手順としては次のようになります。</p>
<ol>
<li>カスタム辞書ファイルの作成</li>
<li>作成した辞書ファイルを利用したjarファイルの生成</li>
</ol>
<p>まずは辞書ファイルの作成についてです。
以下では、naist-chasen(NAIST-Jdic)の辞書を例として説明します。（ディレクトリの違いだけで、IPAdicでも同じ方法でOKです。）</p>
<p>lucene-gosenでは辞書のコンパイルに2つのフェーズが存在します。</p>
<ol>
<li>gosen用辞書を生成する前処理（中間csvファイルの生成）</li>
<li>gosen用バイナリ辞書の生成</li>
</ol>
<p>カスタム辞書は1の出力の形式(=中間csvファイル=dictionary.csv)にあわせたCSVファイルとして作成します。
CSVの各カラムは次のような意味を持っています。</p>
<table border="1">
<tr>
<td>単語</td>
<td>単語の生起コスト</td>
<td>品詞</td>
<td>品詞細分類1</td>
<td>品詞細分類2</td>
<td>品詞細分類3</td>
<td>活用型</td>
<td>活用形</td>
<td>基本形</td>
<td>読み</td>
<td>発音</td>
</tr>
</table>
3カラム目以降は「素性（そせい？）」と呼ばれる項目です。ipadic、naist-jdicでは「品詞」「品詞細分類1」「品詞細分類2」「品詞細分類3」「活用型」「活用形」「基本形」「読み」「発音」となります。
※「見出し語」「形態素生起コスト」「素性」と呼ばれる項目を表形式にする。
厳密な品詞の体系に関してはIPAdicやNAIST-Jdicのサイトをご覧ください（説明できるレベルにはまだまだなっていないので。。。）
今回は、固有名詞（人名、地名など）を追加するという例でカスタム辞書について説明します。
固有名詞として「達川」という人名を追加してみましょう。
まずは、次のようなエントリをもつ「custom-dic.csv」ファイルを作成します。ファイルはUTF-8で保存してください。
コストはすでにあるエントリで似たようなエントリのコストを真似します。（今回は固有名詞,人名で似ているものを採用）。ちなみに、コストは小さいほど単語として出てきやすくなります。
※カスタム辞書にはSenで利用していたものが利用できます。
<hr>
<span style="background-color:#FFFFFF; color:#FF0000">**<em>"達川",2245,名詞,固有名詞,人名,名,*,*,"達川","タツカワ","タツカワ"</em>**</span>
<hr>
<p>上記ファイルを、先日紹介した$LUCENE-GOSEN/dictionary/ディレクトリにコピーします。
では、カスタム辞書を含んだlucene-gosenのjarを作成しましょう。
カスタム辞書のビルドは$LUCENE-GOSEN/dictionary/で行います。
また、カスタム辞書の指定はCSVファイル名をantの引数で指定します。次がコマンドの例になります。</p>
<pre><code>
$ cd lucene-gosen-trunk
$ cd dictionary
$ ant -Ddictype=naist-chasen clean-sen
$ ant -Ddictype=naist-chasen -Dcustom.dics=&quot;../custom-dic.csv&quot; compile
$ cd ..
$ ant -Ddictype=naist-chasen
</code></pre><p>上記コマンドの例で&quot;clean-sen&quot;というタスクを実行しています。これは、すでに出来上がっているgosen用のバイナリ辞書を削除するタスクになります。すでにgosen用の辞書が作成されている場合には辞書の再生成が行われないためです。
また、複数のファイルを利用したい場合は-Dcustom.dics=&quot;custom-dic.csv custom-dic2.csv&quot;という形でスペース区切りでファイル名を記述すればOKです。</p>
<p>カスタム辞書を適用する前と適用後の違いは次のとおりです。
適用前


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1867257.png" alt="カスタム辞書適用前"/>
    </div>
    <a href="/images/entries/20110602/20110602_1867257.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
適用後


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1867258.png" alt="カスタム辞書適用後"/>
    </div>
    <a href="/images/entries/20110602/20110602_1867258.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all"></p>
<p>簡単ですが、以上がカスタマイズ辞書を利用する方法でした。
ちなみに、この記事を書く前にすでにカスタム辞書の件を書いている<a href="http://d.hatena.ne.jp/shinobu_aoki/20110525/1306342970">ブログ</a>がありました。。。こちらも参考にしてください。
今回の例でいくつかSolrのanalysis画面を利用して説明してきました。Solrでのlucene-gosenの利用方法についてはまた後日記載したいと思います。
※参考までに。Solrでの利用方法は<a href="http://lucene.jugem.jp/">こちら</a>にも記載してあります。</p>
<p>また、IPAdicなどの辞書について記載のある書籍を見つけましたので、参考になれば。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4625434394/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4625434394&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4625434394/?tag=johtani-22">
      アプリケーションソフトの基礎 (講座ITと日本語研究)
      </a>
    </p>
  </div>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>ソースからのビルドと構成(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/05/26/%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%8B%E3%82%89%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89%E3%81%A8%E6%A7%8B%E6%88%90/</link>
      <pubDate>Thu, 26 May 2011 17:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/05/26/%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%8B%E3%82%89%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89%E3%81%A8%E6%A7%8B%E6%88%90/</guid>
      <description>今回はソースのダウンロードとビルドについてです。 最新版のソースを利用したり、JavaDocを見たい場合はソースをダウンロードしてからビルドす</description>
      <content:encoded><p>今回はソースのダウンロードとビルドについてです。</p>
<p>最新版のソースを利用したり、JavaDocを見たい場合はソースをダウンロードしてからビルドすることになります。
ソースのダウンロードからビルドまでの手順について説明します。</p>
<p>まずはソースのダウンロードです。</p>
<pre><code>$ mkdir ~/work
$ cd work
$ svn co http://lucene-gosen.googlecode.com/svn/trunk/ lucene-gosen-trunk
$ cd lucene-gosen-trunk
</code></pre><p>ダウンロードしたソースは次のようなディレクトリ構成です。</p>
<table class="simple_table">
<tbody>
<tr><th>.classpath</th><td>Eclipse用ファイル</td></tr>
<tr><th>.project</th><td>Eclipse用ファイル</td></tr>
<tr><th>.settings</th><td>Eclipse用ファイル</td></tr>
<tr><th>AUTHORS</th><td>作者のリスト（Sen、GoSen）</td></tr>
<tr><th>CHANGES.txt</th><td>lucene-gosenにおける更新履歴</td></tr>
<tr><th>COPYING.LGPL</th><td>ライセンス</td></tr>
<tr><th>README.txt</th><td>Readme</td></tr>
<tr><th>build.xml</th><td>Antのビルドファイル</td></tr>
<tr><th>dictionary</th><td>辞書コンパイル用ディレクトリ</td></tr>
<tr><th>docs</th><td>APIドキュメント用ディレクトリ</td></tr>
<tr><th>lib</th><td>ライブラリ</td></tr>
<tr><th>prettify</th><td>Google Code Prettify用ディレクトリ（APIドキュメントでの色づけ用）</td></tr>
<tr><th>src</th><td>ソースコード</td></tr>
</tbody></table>
<p>また、辞書やソースのコンパイルにはAntを利用します。
通常利用するAntのタスクには次のようなものがあります。</p>
<table class="simple_table">
<tbody>
<tr><th>clean</th><td>プロジェクトのクリーンアップ</td></tr>
<tr><th>build-dic</th><td>辞書のコンパイル（辞書のダウンロードも行う）</td></tr>
<tr><th>jar</th><td>jarファイル生成</td></tr>
<tr><th>dist</th><td>配布パッケージの生成（2つのjarファイル生成）</td></tr>
<tr><th>javadoc</th><td>JavaDocの生成</td></tr>
</tbody></table>
<p>jarファイルの生成までの大まかな流れは「javaソースのコンパイル」～「辞書のダウンロード」～「辞書のプレコンパイル」～「辞書のコンパイル」～「jarファイルの生成」となります。
Antのタスク以外にjarファイルを生成する場合に利用するオプションは以下の通りです。</p>
<table class="simple_table">
<tbody>
<tr><th>-Dproxy.host</th><td>プロキシのホスト</td></tr>
<tr><th>-Dproxy.port</th><td>プロキシのポート</td></tr>
<tr><th>-Ddictype</th><td>辞書の指定（指定可能なものは次の通り。naist-chasen、ipadic）</td></tr>
</tbody></table>
<p>以下はNaist-Jdicのjarファイルを生成するコマンドの実行例になります。プロキシサーバを利用する環境の場合は-Dproxy.hostと-Dproxy.portも指定してください。（※認証が必要なプロキシの場合はAntのビルドファイルを修正する必要が出てきます。）</p>
<pre><code>
$ ant -Ddictype=naist-chasen
</code></pre><p>jarファイルはdistディレクトリに生成されます。
これで、jarファイルが利用できるようになります。</p>
<p>次回は、ipadicとNaist-chasenの辞書の違いとカスタム辞書を利用する方法について書こうと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenとは(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/05/23/lucene-gosen%E3%81%A8%E3%81%AF/</link>
      <pubDate>Mon, 23 May 2011 15:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/05/23/lucene-gosen%E3%81%A8%E3%81%AF/</guid>
      <description>概要： Lucene/SolrのコミッターであるRobert Muirさんが始めたプロジェクト 歴史： MeCabのJava移植版としてスタートした</description>
      <content:encoded><p>概要：
　Lucene/SolrのコミッターであるRobert Muirさんが始めたプロジェクト
　
</p>
<p>
歴史：
　MeCabのJava移植版としてスタートしたSenがベースになります。
　その後、辞書の構築部分をPerlからJavaに置き換えたGoSenが登場しました。
　が、どちらもメンテナンスされなくなってきたので、Robertさんが引き継いでメンテナンスとLucene/Solr対応をはじめました。そして、現在にいたります。
</p>
<p>
ライセンス：
　LGPLライセンス（ベースになったMeCabのライセンスにならって）
</p>
<p>
特徴：
　以下のような特徴があります。
　・Lucene/Solrですぐに利用可能（3.1、4.0に対応済み）
　・jarファイル1つで利用可能（辞書をjarファイルに内包）
　・LuceneのAttributeをベースにしたTokenの解析
　・その他（パフォーマンス改善、テスト改善など）
</p>
<p>
プロジェクトのサイト：
　http://code.google.com/p/lucene-gosen/
</p>
<p>
ダウンロード：
　http://code.google.com/p/lucene-gosen/downloads/list
　現時点では2つの辞書を内包したjarファイルが用意されています。
<p>　Naist-jdic 0.4.3（for ChaSen） 参考サイト：http://sourceforge.jp/projects/naist-jdic/
　IpaDic 2.6.0　参考サイト：http://sourceforge.jp/projects/ipadic/
　</p>
</p>
</content:encoded>
    </item>
    
    <item>
      <title>ブログはじめます(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/05/20/%E3%83%96%E3%83%AD%E3%82%B0%E3%81%AF%E3%81%98%E3%82%81%E3%81%BE%E3%81%99/</link>
      <pubDate>Fri, 20 May 2011 11:44:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/05/20/%E3%83%96%E3%83%AD%E3%82%B0%E3%81%AF%E3%81%98%E3%82%81%E3%81%BE%E3%81%99/</guid>
      <description>今さらですが、ブログをはじめてみようかと。今さらですが… はじめてみようと思った一番の理由は、自分で調べたことをメモがわりに残すためです。 あと</description>
      <content:encoded><p>今さらですが、ブログをはじめてみようかと。今さらですが…</p>
<p>はじめてみようと思った一番の理由は、自分で調べたことをメモがわりに残すためです。
あとは、自分を追い込むためもありますが。(こっちが一番の理由かも)最近勉強してないなぁと感じているので。
ということで、まずは、lucene-gosenやsolrについて書いていく予定です。</p>
</content:encoded>
    </item>
    
  </channel>
</rss>
