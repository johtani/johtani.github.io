<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">

  <channel>
    <title>search on @johtaniの日記 3rd</title>
    <link>https://blog.johtani.info/tags/search/</link>
    <description>Recent content in search on @johtaniの日記 3rd</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 15 Dec 2023 00:00:00 +0900</lastBuildDate><atom:link href="https://blog.johtani.info/tags/search/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Oramaという検索エンジンでブログ検索を作ってみた</title>
      <link>https://blog.johtani.info/blog/2023/12/15/introduce-orama-to-blog/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2023/12/15/introduce-orama-to-blog/</guid>
      <description>この記事は、情報検索・検索技術 Advent Calendar 2023の15日目の記事です。 昨年に続き2回目の情報検索のAdvent Calendar参戦です。 今年は、夏</description>
      <content:encoded>&lt;p&gt;この記事は、&lt;a href=&#34;https://adventar.org/calendars/8678&#34;&gt;情報検索・検索技術 Advent Calendar 2023&lt;/a&gt;の15日目の記事です。&lt;/p&gt;
&lt;p&gt;昨年に続き2回目の情報検索のAdvent Calendar参戦です。&lt;/p&gt;
&lt;p&gt;今年は、&lt;a href=&#34;https://blog.johtani.info/blog/2023/06/21/attend-berlin-buzzwords-2023/&#34;&gt;夏にオンライン参加&lt;/a&gt;した&lt;a href=&#34;https://berlinbuzzwords.de&#34;&gt;Berlin Buzzwords&lt;/a&gt;の&lt;a href=&#34;https://program.berlinbuzzwords.de/berlin-buzzwords-2023/talk/73UNZD/&#34;&gt;「The Debate Returns (with more vectors) Which Search Engine?」&lt;/a&gt;というセッションでPhilippさんが話題に出した&lt;a href=&#34;https://www.oramasearch.com/&#34;&gt;Orama search&lt;/a&gt;という検索エンジンを紹介してみようと思います（Oramaが正式名称なのかな？）。&lt;/p&gt;
&lt;h2 id=&#34;oramaという検索エンジン&#34;&gt;Oramaという検索エンジン&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.oramasearch.com/&#34;&gt;公式サイト&lt;/a&gt;のトップにも記載がありますが、エッジで動作する全文検索＆ベクトル検索エンジンというもののようです。
なにそれ？という感じがしますが、&lt;a href=&#34;https://docs.oramasearch.com/open-source/&#34;&gt;オープンソース版のドキュメント&lt;/a&gt;を見るともう少しわかりやすい説明になっています。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Orama is a fast, batteries-included, full-text and vector search engine entirely written in TypeScript, with zero dependencies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;OramaはTypeScriptで書かれた、依存なしで利用できる検索エンジンということです（&lt;em&gt;batteries-included&lt;/em&gt;って初めて見たかも。「必要なものがそろってる」とかいう意味みたい（参考：&lt;a href=&#34;https://en.wiktionary.org/wiki/batteries-included&#34;&gt;英語のWikipedia&lt;/a&gt;））。&lt;/p&gt;
&lt;p&gt;以下は公式サイトにあるサンプルです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;create&lt;/code&gt;でスキーマの定義をした検索エンジンのインスタンスを生成します。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;insert&lt;/code&gt;で先ほど生成した検索エンジンにデータをインデックスします（後で紹介しますが、一括でインデックスするメソッドも用意されています）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;search&lt;/code&gt;で検索し、検索した結果が返ってきます。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上です。あとは、必要に応じてGUIを用意するだけです。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-typescript&#34; data-lang=&#34;typescript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; { &lt;span style=&#34;color:#a6e22e&#34;&gt;create&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;insert&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;search&lt;/span&gt; } &lt;span style=&#34;color:#66d9ef&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;@orama/orama&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;db&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;await&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create&lt;/span&gt;({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;schema&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;title&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;string&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;director&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;string&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;isFavorite&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;boolean&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;year&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;number&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;await&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;db&lt;/span&gt;, {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;title&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;The Prestige&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;director&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Christopher Nolan&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;isFavorite&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;year&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;2006&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;searchResults&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;await&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;search&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;db&lt;/span&gt;, {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;term&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prestige&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;where&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;isFavorite&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;year&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;between&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2008&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;サクッと動きそうですよね？依存関係がなくTypeScriptで書かれているということは静的なサイトなどでコンテンツを検索するのに使えるのでは？&lt;/p&gt;
&lt;p&gt;ということで、自分のブログに組み込んでみました。
今回は組み込んだ時の流れと気になった点などを紹介します。
Hugoに組み込む際になるべく簡単にしたかったので、unpkg.comで公開されているJavaScriptだけを利用する形で検索ページを作ってみました。
JavaScriptやTypeScriptは不慣れなので効率がよくなかったり、間違っている点があればコメントなどをいただけると嬉しいです。&lt;/p&gt;
&lt;h2 id=&#34;構成&#34;&gt;構成&lt;/h2&gt;
&lt;p&gt;私のブログサイトはHugoで生成（&lt;a href=&#34;https://blog.johtani.info/blog/2020/01/22/intro-hugo-and-theme/&#34;&gt;参考：Hugo導入時の記事&lt;/a&gt;）して、GitHub Pagesとして公開しています。
現在のブログ検索にはAlgoliaを利用しているのですが、テスト目的としてOramaも使えるようにしてみました（右上のメニューでOramaをクリックすると以下で説明するOramaによる検索を試すことができます。&lt;/p&gt;
&lt;p&gt;今回実装してみたのは次のような流れです（日本語検索対応については後で説明します）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HugoでJSONのリストを生成（これまでと一緒）&lt;/li&gt;
&lt;li&gt;1.で生成したリストをもとにNodeのスクリプトでOramaのインデックスファイルを作成&lt;/li&gt;
&lt;li&gt;2.で作成したファイルを公開&lt;/li&gt;
&lt;li&gt;画面で3.のファイルをダウンロードして検索&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;1-hugoでjsonのリストを生成これまでと一緒&#34;&gt;1. HugoでJSONのリストを生成（これまでと一緒）&lt;/h3&gt;
&lt;p&gt;HugoでJSONのリストを生成します。Algoliaに記事を登録するのにも利用している仕組みです。
検索に利用する記事の情報を1記事を1つのオブジェクトとして出力します。1つの配列に記事ごとのオブジェクトが入っているJSONファイルになります。&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/johtani/3e40c6738424f95f38c117e18181a010.js?file=list.orma.json.gohtml&#34;&gt;&lt;/script&gt;

&lt;p&gt;少しだけ特殊なことをしています。
ブログ記事に&lt;code&gt;Tags&lt;/code&gt;というフィールドのありなしで処理を分岐しています。
Oramaが配列にNullを受け取った場合にうまく動かなかったので出力するJSON側で調整しています（あとで再確認してIssueあげておかないと）&lt;/p&gt;
&lt;h3 id=&#34;2-1で生成したリストをもとにnodeのスクリプトでoramaのインデックスファイルを作成&#34;&gt;2. 1.で生成したリストをもとにNodeのスクリプトでOramaのインデックスファイルを作成&lt;/h3&gt;
&lt;p&gt;最初に紹介した公式のサンプルの場合、毎回Oramaのインスタンスを生成した後にインデックス登録の処理を行わなければいけません。
ただ、Hugoで生成したコンテンツは特に動的に更新があるわけでもないので、検索画面を表示するたびにインデックス登録するのも無駄が多い気がします。
Orama単体で最初に実装した時には、1.で作成した記事一覧のJSONをOramaで実装した検索ページのスクリプトが呼ばれたタイミングでフェッチし、&lt;code&gt;insertMultiple&lt;/code&gt;（複数登録用のメソッド）を呼び出す実装にしていました。&lt;/p&gt;
&lt;p&gt;Oramaでは、あらかじめ作成したOramaのデータベースを永続化・リストアするための&lt;a href=&#34;https://docs.oramasearch.com/open-source/plugins/plugin-data-persistence.html&#34;&gt;Data Persistenceプラグイン&lt;/a&gt;が提供されています。
インデックスに登録する処理コストはHugoでコンテンツを生成した直後に行い、検索画面では作成済みのインデックスファイルをフェッチしてリストアする形にすることにしました。インデックスと記事のJSONデータを含んだファイルになるので、元の記事一覧のJSONよりはファイルサイズが大きくなってしまいますが、処理時間も含めて考えると許容できるサイズではないかなと。&lt;/p&gt;
&lt;p&gt;次のスクリプトは1.で生成したJSONファイルを読み込んで、Oramaでインデックスした後に&lt;a href=&#34;https://www.jsdelivr.com/package/npm/dpack&#34;&gt;&lt;code&gt;dpack&lt;/code&gt;というバイナリ形式&lt;/a&gt;でファイルに保存しています。
こののプラグインで選択できる形式は現時点では&lt;code&gt;json&lt;/code&gt;、&lt;code&gt;binary&lt;/code&gt;（デフォルト＝msgpack）、&lt;code&gt;dpack&lt;/code&gt;の3種類になります。
残念ながら今回試したところ、&lt;code&gt;binary&lt;/code&gt;ではエラーが発生したので、&lt;code&gt;dpack&lt;/code&gt;を使用しました。&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/johtani/3e40c6738424f95f38c117e18181a010.js?file=persist_orama.mts&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;3-2で作成したファイルを公開&#34;&gt;3. 2.で作成したファイルを公開&lt;/h3&gt;
&lt;p&gt;単にGitHub Pagesに公開しているだけです。
ブラウザが2.で作成したインデックスファイルをフェッチしてから利用するためです。&lt;/p&gt;
&lt;h3 id=&#34;4-画面で3のファイルをダウンロードして検索&#34;&gt;4. 画面で3.のファイルをダウンロードして検索&lt;/h3&gt;
&lt;p&gt;あとは検索画面です（長すぎるので、styleの部分は省略しています）。&lt;/p&gt;
&lt;p&gt;まず、必要なライブラリなどを読み込みます（下のソースコードの1行目から14行目）。&lt;/p&gt;
&lt;p&gt;Oramaだけであれば問題ないのですが、OramaのプラグインがOramaをモジュールとして参照しているため、&lt;a href=&#34;https://developer.mozilla.org/ja/docs/Web/HTML/Element/script/type/importmap&#34;&gt;importmap&lt;/a&gt;として定義しています。
Oramaのプラグインがいくつか利用しているものがあったので、まとめてimportmapに定義してあります。&lt;/p&gt;
&lt;p&gt;次に、検索窓（&lt;code&gt;search-searchbar&lt;/code&gt;）や検索結果（&lt;code&gt;search-hits&lt;/code&gt;）などのHTMLタグを用意します。
ヒット件数と検索にかかった時間も取れるので、表示する場所として&lt;code&gt;stats&lt;/code&gt;も用意してあります。&lt;/p&gt;
&lt;p&gt;22行目からが実際の処理になります。&lt;/p&gt;
&lt;p&gt;28行目で、手順2.で作成したインデックスのファイルをフェッチします。
32行目でフェッチしたファイルから取り出した文字列をリストアして、Oramaのインスタンスを生成します。
34行目のtokenizerに関しては日本語対応で説明します。&lt;/p&gt;
&lt;p&gt;48行目の&lt;code&gt;query&lt;/code&gt;関数がクエリの組み立て＋検索の処理です。&lt;code&gt;search-input&lt;/code&gt;で入力されたテキストを&lt;code&gt;term&lt;/code&gt;として渡して検索をしています（56行目）。
検索した結果のリストをもとに検索結果のHTMLタグを組み上げていきます。
検索結果には後述する&lt;code&gt;@orama/highlight&lt;/code&gt;のモジュールを利用してスニペットを追加しています。&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/johtani/3e40c6738424f95f38c117e18181a010.js?file=search-orama.html&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;日本語対応&#34;&gt;日本語対応&lt;/h2&gt;
&lt;p&gt;残念ながらOramaは公式には日本語をサポートしていません（&lt;a href=&#34;https://github.com/oramasearch/orama/blob/7456e32455a22ac7f8d29760dcd149246c4af1a8/packages/orama/src/components/tokenizer/languages.ts#L1&#34;&gt;サポートしているスキーマに設定できる&lt;code&gt;language&lt;/code&gt;の一覧&lt;/a&gt;（＝STEMMERSにあるものだけ&lt;a href=&#34;https://github.com/oramasearch/orama/blob/7456e32455a22ac7f8d29760dcd149246c4af1a8/packages/orama/src/components/tokenizer/index.ts#L86&#34;&gt;指定可能&lt;/a&gt;）。一覧と実装分けたほうがいい気がするけど？）。
ただ、&lt;a href=&#34;https://docs.oramasearch.com/open-source/internals/components.html#tokenizer&#34;&gt;tokenize関数を変更できるようにしてくれています&lt;/a&gt;。これを利用して、少しトリッキーな形で日本語対応しました。&lt;/p&gt;
&lt;p&gt;先ほどの手順2.でOramaの&lt;code&gt;create&lt;/code&gt;関数として、&lt;code&gt;components&lt;/code&gt;に次の&lt;code&gt;tokenizer&lt;/code&gt;を渡します。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;language&lt;/code&gt;は&lt;code&gt;english&lt;/code&gt;ですが、日本語をtokenizeする処理に次のように書き換えました。
&lt;code&gt;stemming&lt;/code&gt;は今回は行わない（＝用意されているstemmerを使えない）ので&lt;code&gt;false&lt;/code&gt;、&lt;code&gt;normalizationCache&lt;/code&gt;は空のMapを用意しておかないとエラーが出るので、&lt;code&gt;new Map()&lt;/code&gt;しています。
この設定を使ってcreateすることで、&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;components&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;tokenizer&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;language&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;english&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;stemming&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;normalizationCache&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Map&lt;/span&gt;(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;tokenize&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;raw&lt;/span&gt;) =&amp;gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tokenize&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;raw&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;少しわかりにくいですが、&lt;code&gt;tokenizer&lt;/code&gt;の&lt;code&gt;tokenize&lt;/code&gt;関数の中で呼び出している&lt;code&gt;return tokenize(raw)&lt;/code&gt;の部分は&lt;a href=&#34;https://github.com/yuhsak/wakachigaki&#34;&gt;wakachigaki&lt;/a&gt;というライブラリのtokenize関数となっています。&lt;/p&gt;
&lt;h3 id=&#34;wakachigakiというライブラリ&#34;&gt;wakachigakiというライブラリ&lt;/h3&gt;
&lt;p&gt;Elasticsearchなどのサーバー上で動作させる検索エンジンではkuromojiなどの辞書を内包した形態素解析器を利用するのが通常です。
ただ、今回はブラウザだけで完結した簡単な検索を行う仕組みをOramaを使って提供しようと考えました（&lt;a href=&#34;https://github.com/takuyaa/kuromoji.js&#34;&gt;kuromoji.js&lt;/a&gt;というのもありますが、辞書のサイズがそこそこのサイズです）。
ですので、辞書ファイルのない軽量の形態素解析ライブラリとして、&lt;a href=&#34;https://github.com/yuhsak/wakachigaki&#34;&gt;wakachigaki&lt;/a&gt;というライブラリ（＝6.2Kbの軽量日本語分かち書きライブラリ）を利用してみました。&lt;/p&gt;
&lt;p&gt;最初は&lt;a href=&#34;https://github.com/leungwensen/tiny-segmenter&#34;&gt;TinySegmenter&lt;/a&gt;の利用をしていたのですが、wakachigakiのサイトにもあるように、TypeScriptやES Moduleでも利用できる形式となっていたのでこちらに切り替えました。&lt;/p&gt;
&lt;p&gt;辞書もっていないため、品詞でのフィルタリングや読みを利用した処理はできませんが、日本語の文章を分かち書きしてくれます。&lt;/p&gt;
&lt;h3 id=&#34;tokenize関数の設定&#34;&gt;tokenize関数の設定&lt;/h3&gt;
&lt;p&gt;Oramaの&lt;a href=&#34;https://github.com/oramasearch/orama/blob/7456e32455a22ac7f8d29760dcd149246c4af1a8/packages/orama/src/types.ts#L741&#34;&gt;Tokenizerインタフェースの&lt;code&gt;tokenize&lt;/code&gt;&lt;/a&gt;は入力の文字列を&lt;code&gt;string[]&lt;/code&gt;として返すことを期待しています。
wakachigakiの&lt;code&gt;tokenize&lt;/code&gt;も文字列を引数にとり、&lt;code&gt;string[]&lt;/code&gt;を返すので、そのまま呼び出すだけにしてあります。&lt;/p&gt;
&lt;p&gt;ただ、今回のブログ検索の実装では、検索窓に入力された文字列も特に何も処理せずに&lt;code&gt;tokenize&lt;/code&gt;に渡す形にしています。
クエリパーサーなどを考える場合は、検索窓に入力された文字列を前処理したりする必要がありますが今回はいったんこれで。。。&lt;/p&gt;
&lt;h3 id=&#34;data-persistenceはcomponentsはpersistしない&#34;&gt;data-persistenceはcomponentsはpersistしない&lt;/h3&gt;
&lt;p&gt;手順2.で導入した、data-persistenceプラグインですが、現在の実装ではOramaが生成したデータベース（転置インデックス）&lt;strong&gt;だけ&lt;/strong&gt;を永続化しています。
独自に設定した&lt;code&gt;tokenizer&lt;/code&gt;は残念ながら含まれていませんでした。
手順4.の検索画面のコードの34行目の部分（下記）で、フェッチしたインデックスデータから&lt;code&gt;restore&lt;/code&gt;した後のOramaのインスタンスの&lt;code&gt;tokenizer&lt;/code&gt;に手順2.と同じ設定の&lt;code&gt;tokenizer&lt;/code&gt;を設定することで検索時にも同じトークナイズがされるようになります（今の仕組みだと修正した場合には、両方のソースコードを修正しないといけないのがちょっと面倒かも？）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;db&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;tokenizer&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;language&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;english&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;stemming&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;normalizationCache&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Map&lt;/span&gt;(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;tokenize&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;raw&lt;/span&gt;) =&amp;gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tokenize&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;raw&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;tokenizer&lt;/code&gt;はOramaの検索エンジンのインスタンスに紐づく設定となっているので、フィールドごとに切り替えるといったことも現在では想定されていない気がします。&lt;/p&gt;
&lt;h2 id=&#34;ハイライト&#34;&gt;ハイライト&lt;/h2&gt;
&lt;p&gt;ハイライトのために、インデックスデータに単語のポジションを保存するためのプラグインも提供されています。
このポジションは検索の単語がヒットしドキュメントのどの位置に出てくるのか？をあらかじめデータとして保存しておくことで、ハイライトのたびに単語の位置を毎回計算しなくてもよくするためのものと思われます。&lt;/p&gt;
&lt;p&gt;これを利用してもよかったのですが、今回は&lt;a href=&#34;https://github.com/oramasearch/highlight&#34;&gt;&lt;code&gt;@orama/highlight&lt;/code&gt;というモジュール&lt;/a&gt;を利用することにしました。
検索にヒットしたドキュメントの元の文章と検索のキーワードを入力すると検索キーワードの周りにハイライトのHTMLタグを埋め込んでくれるライブラリを利用しました。これは普通に他でも便利かも？&lt;/p&gt;
&lt;p&gt;余力があればプラグインを利用した時のインデックスのサイズの違いや検索時の速度の違いを調べてみるのもいいかもしれないです。&lt;/p&gt;
&lt;h2 id=&#34;そのほかの機能&#34;&gt;そのほかの機能&lt;/h2&gt;
&lt;p&gt;今回は利用していませんが、Oramaにはそのほかの機能も用意されています（&lt;a href=&#34;https://docs.oramasearch.com/open-source/usage/search/introduction.html&#34;&gt;公式ドキュメントの検索の紹介のページ&lt;/a&gt;）。詳細は公式ドキュメントをご覧ください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typo tolerance&lt;/li&gt;
&lt;li&gt;Facet/Filter&lt;/li&gt;
&lt;li&gt;Vector Search&lt;/li&gt;
&lt;li&gt;Geo Search&lt;/li&gt;
&lt;li&gt;Grouping&lt;/li&gt;
&lt;li&gt;Threshold&lt;/li&gt;
&lt;li&gt;Preflight&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;oramaでまだできないこと&#34;&gt;Oramaで（まだ？）できないこと&lt;/h2&gt;
&lt;p&gt;今回触ってみて、現在のOramaの実装ではできなそうなことがいくつかあったので書き出しておきます。
あくまでも現時点のことです。今後実装される可能性もあります。&lt;/p&gt;
&lt;p&gt;デフォルトでは、&lt;code&gt;search&lt;/code&gt;の&lt;code&gt;term&lt;/code&gt;に渡した文字列を&lt;code&gt;tokenize&lt;/code&gt;し、出力されたトークン列（単語の配列）のそれぞれのトークン（単語）にヒットしたドキュメントが&lt;strong&gt;全て&lt;/strong&gt;検索結果に含まれます。
検索結果のドキュメントは&lt;a href=&#34;https://docs.oramasearch.com/open-source/usage/search/bm25-algorithm.html&#34;&gt;&lt;code&gt;BM25&lt;/code&gt;でスコアリング&lt;/a&gt;されるので、多くのトークンが含まれるものが上位に来る可能性が高いです。&lt;/p&gt;
&lt;p&gt;ただ、特定の単語を含まない検索、ANDやOR検索など凝ったクエリは現在は書けないようです（&lt;a href=&#34;https://github.com/oramasearch/orama/issues/295&#34;&gt;Issueがある&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;filterは別途用意されているので、例えばファセットで表示したカテゴリで絞り込んだ状態にするといったことはできそうです。&lt;/p&gt;
&lt;h2 id=&#34;残課題&#34;&gt;残課題&lt;/h2&gt;
&lt;p&gt;とりあえず日本語をなんとなく検索できるようにすることはできました。
が、気になることもいっぱいありました。
時間を見つけてTypeScriptなどを勉強しながら探っていく感じでしょうか。&lt;/p&gt;
&lt;h3 id=&#34;インデックスを小さくする方法&#34;&gt;インデックスを小さくする方法&lt;/h3&gt;
&lt;p&gt;今回はブログ記事のサマリー（Hugoで生成されるSummary）でインデックスを作ったので、インデックスのサイズはまだ小さくなっています（といっても1MB超えてるけど）。ブログ記事全体を検索できるようにする場合は、インデックスファイルが大きくなります。&lt;/p&gt;
&lt;p&gt;これは、OramaがデフォルトではインデックスしたJSON自体もインデックス（ドキュメントストア）に保存しているためです。
ですので、Oramaに登録したJSONデータ＋検索用のインデックスがインデックスファイルとなります。
ただ、検索だけができればよい場合は、出来上がったインデックスがあればドキュメントのIDは取得できます。&lt;/p&gt;
&lt;p&gt;公式ドキュメントでもそのことに触れているページがあります（参考：&lt;a href=&#34;https://docs.oramasearch.com/open-source/usage/insert.html#remote-document-storing&#34;&gt;Remote document storing&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;まだちゃんと実装を見ていませんが、ブログ記事本体の文字列はドキュメントストアに保存しないようにすれば検索はできるが、容量が大きくならないといった工夫ができそうです。&lt;/p&gt;
&lt;p&gt;ただ、検索結果にハイライトを含んだスニペットを表示したい場合は、コンテンツ文字列が必要になってくるので、代わりに何かしらの仕組みを考える必要が出てきます（例えばサマリー部分の文字列だけ保存しておいて、ハイライトはそこだけを対象とするなど）。&lt;/p&gt;
&lt;h3 id=&#34;そのほか&#34;&gt;そのほか&lt;/h3&gt;
&lt;p&gt;そのほかに思いつくのはこの辺でしょうか？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;オートコンプリート：Oramaというよりも、JavaScriptやTypeScriptでうまく実装できるのか？という話のような気がしますが。。。&lt;/li&gt;
&lt;li&gt;ファセット対応：機能としては存在していますが、今回は実装が間に合いませんでした。&lt;/li&gt;
&lt;li&gt;Vector Search：検索キーワードのベクトルをどうやって作るのか？という問題が残りそう。コンテンツのベクトルだけで関連ページの表示みたいなことはできるかも？（動的にやる必要がないので、事前計算して各ページに埋め込むほうがよさそうだけど）&lt;/li&gt;
&lt;li&gt;Geo Search：緯度経度があるデータを探し出してこないと？&lt;/li&gt;
&lt;li&gt;relevanceの処理の変更：BM25以外に切り替えることができるかどうか（Oramaのソースとにらめっこしないといけなそう）&lt;/li&gt;
&lt;li&gt;tokenizeの改良：辞書を利用する形態素解析ライブラリやAPIを利用してみたり、「てにをは」といった1文字のひらがなを削ることで、無駄な検索ヒットを減らしてみたり。大文字小文字の正規化も必要。&lt;/li&gt;
&lt;li&gt;検索ログの解析：検索ログを保存する仕組みなどはないので、Google AnalyticsやほかのAPIなどを用意する？&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;ということで、Oramaという検索エンジン？ライブラリ？を利用して日本語のブログ検索を実装してみました。
まだ、機能が少ない部分もありますが、少数のデータをブラウザだけで簡単に検索できる仕組みをGitHub Pagesだけで提供することができそうです。
今回は静的なファイルだけで完結する形にしましたが、TypeScriptで利用できる簡単な検索エンジンライブラリとして使ってみるとまた別の面白さもあるかもしれないです。
まだ触っていない機能は思いついたこと（＝残課題に書いたこと）もあるので、ちょっと試行錯誤してみようかなぁ。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Extreme Multi-label Learning for Semantic Matching in Product Searchという論文を読んだ</title>
      <link>https://blog.johtani.info/blog/2022/01/12/reading-xmc-for-semantic-search-in-prod/</link>
      <pubDate>Wed, 12 Jan 2022 18:41:02 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2022/01/12/reading-xmc-for-semantic-search-in-prod/</guid>
      <description>年末までに知り合いと次の論文を読んだので軽くまとめておきます。 Extreme Multi-label Learning for Semantic Matching in Product Search なんで読んだの？ 前回読んだ理由と一緒ですが、Semantic</description>
      <content:encoded>&lt;p&gt;年末までに知り合いと次の論文を読んだので軽くまとめておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3447548.3467092&#34;&gt;Extreme Multi-label Learning for Semantic Matching in Product Search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;なんで読んだの&#34;&gt;なんで読んだの？&lt;/h2&gt;
&lt;p&gt;前回読んだ理由と一緒ですが、&lt;code&gt;Semantic Search&lt;/code&gt;に絡んだ検索の論文であり、Amazonの製品検索での話も関係していたので読みました。&lt;/p&gt;
&lt;h2 id=&#34;どんな論文&#34;&gt;どんな論文？&lt;/h2&gt;
&lt;p&gt;Amazonの製品検索でセマンティックマッチングを利用する方法、レイテンシを低くしつつ再現率を改善できるような仕組みを検討して評価した論文です。
実際には商品検索のマッチングを大規模なマルチラベル問題（Extreme Multi-label Classification：以下ではXMC問題とする）として定義して、その問題を効率よく解く方法について検討評価しています。&lt;/p&gt;
&lt;p&gt;検索には大きく2つのフェーズがあります。マッチング（クエリにヒットした文書の集合を求める）とランキング（マッチングのデータを関連性の高い順序で並べる）です。
前回のMSの論文では、マッチング後のランキングのフェーズで言語モデルを利用した並び替えを行う話でした。
今回の論文ではマッチングに関してセマンティックサーチを有効活用できないか？という論文になります。
転置インデックスベースの（語彙による）検索では、スペルミスや類義語などを検索することが難しいです。
語彙以外のデータ（クリックや購入といったユーザーの行動や製品の意味的表現など）を学習できる埋め込みベースのニューラルモデルを活用して、マッチングを補完する方法を検討しています。
下図のようにあくまで転置インデックスベースの検索の補完として利用するみたいです（論文に掲載されていたアーキテクチャ）。&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;max-width:600&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20220112/system.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20220112/system.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&#34;xmc問題とは&#34;&gt;XMC問題とは？&lt;/h3&gt;
&lt;p&gt;セマンティックサーチを行なう場合、埋め込みベースのニューラルモデル（例えばBERTとか）を利用することが多いですが、
クエリトークンを埋め込み空間にベクトル化する部分が推論のボトルネック（レイテンシが高くなる）になることが多いです。
これは、ニューラルネットワークの複雑さに依存します。
低レイテンシになるように、複雑ではないエンコーダーを用いることもありますが今度は性能（精度）が良くなくなります。&lt;/p&gt;
&lt;p&gt;そこで、この論文ではセマンティックマッチングをXMC問題として定義し、これを効率よく解くための手法を提案しています。
ここで、XMCの問題とは、入力（今回はクエリ）に対して、最も関連性の高い複数のラベル（今回は製品）を出力することとなります。
Amazonでの製品なのでほんとに大規模（ラベル＝製品＝1億件）です。&lt;/p&gt;
&lt;p&gt;従来のXMC問題を解くための手法として、疎な線形モデルの手法、パーティションベースの手法、グラフベースの手法などが列挙されています。
疎な線形モデルの手法では、ナイーブなOVR(one-versus-rest)だと出力となるラベルの数に対して線形に推論時間も大きくなるため、今回の用途には適しません。
この論文ではパーティションベースの手法として、推論時間を短くできるツリーベースで疎な線形モデルの手法を評価しています。&lt;/p&gt;
&lt;h2 id=&#34;どういう仕組み&#34;&gt;どういう仕組み？&lt;/h2&gt;
&lt;p&gt;ツリーベースのモデルは次のような感じです。
&lt;a href=&#34;https://arxiv.org/abs/2010.05878&#34;&gt;PECOS&lt;/a&gt;のXR-Linearモデルを利用しています。
モデルに関する詳細についてはこちらの論文に詳しく載っているようです（まだ読んでいないです、、、）。&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:600&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20220112/xr-linear.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20220112/xr-linear.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;一番上に入力として与えられる&lt;code&gt;x&lt;/code&gt;がクエリ（ベクトル）で一番下の四角が製品にあたるラベル（&lt;code&gt;Y={1,...,l,...L}&lt;/code&gt;）です。
これらが与えられたときに最も関連性の高い上位&lt;code&gt;k&lt;/code&gt;件のラベル（製品）を出力するモデルを生成します。
モデルのパラメータは、学習データとして&lt;code&gt;(x, y)&lt;/code&gt;(ここで、&lt;code&gt;y∈{0, 1}^𝐿&lt;/code&gt;、すなわちクエリに製品がマッチするかしないかの二値)を使って調整します。&lt;/p&gt;
&lt;p&gt;これは、ツリーのそれぞれの階層のノードで、その下の層に関連するデータがあるかどうか？という分類を学習すればよいことになります。
学習データとしては、入力と最下層のラベルにマッチするかどうかというデータです。
これをもとに、ツリーの最下層のデータをもとにひとつ上の層の分類器の学習をすればよいことになります。&lt;/p&gt;
&lt;p&gt;推論は出来上がったツリーベースのモデルに対して、ビームサーチで行ないます。
この時、それぞれの層のノードの分類器では推論の効率化のために枝刈りの閾値を設定しています。
本論文で閾値の違いによる精度とレイテンシの比較も行われています。
また、ビームサーチのビームサイズを変化させた場合の性能についても実験しています。&lt;/p&gt;
&lt;h2 id=&#34;どうやって検証した&#34;&gt;どうやって検証した？&lt;/h2&gt;
&lt;p&gt;次のようなデータを用いてAWS上にモデル構築、推論のシステムを用意し実験しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;10億件以上のポジティブなクエリ-商品ペアのデータセット
&lt;ul&gt;
&lt;li&gt;クエリ：2億4千万、商品：1億&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ポジティブなペアとは、クリック数や購入数の集計値が閾値以上。ただし、データセットとしては集計値は利用していない。&lt;/li&gt;
&lt;li&gt;トレーニングセット：12か月分の検索ログ&lt;/li&gt;
&lt;li&gt;評価用テストセット：1か月分の検索ログ（12％の製品はトレーニングセットには出てこない）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;というデーセットをもとに上記のモデルを学習しています。
特徴量はクエリテキストと製品のタイトルをもとにTF-IDFをいくつかのパターンで導出したものが使われています。
詳しくは論文の結果に関する表などを見ていただくとして。。。&lt;/p&gt;
&lt;p&gt;結果として、1億件の商品カタログで60.9%のRecall@100を1.25ミリ秒/リクエストという低レイテンシで達成したとなっています。&lt;/p&gt;
&lt;h2 id=&#34;気になる点は&#34;&gt;気になる点は？&lt;/h2&gt;
&lt;p&gt;知り合いと3人で読んでいて次のような気になる点が出てきました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ツリーの構築をするときに、製品の&lt;strong&gt;ラベルの並び順&lt;/strong&gt;（製品のクラスタリング？に他製品が似た場所に来る感じ）が推論の効率の良し悪しに影響するんじゃないの？＝Amazonではないところではうまくいかない？
&lt;ul&gt;
&lt;li&gt;PECOSの論文読まないといけない？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;製品が&lt;strong&gt;増えたとき&lt;/strong&gt;にモデルの組み換えとかどうするの？
&lt;ul&gt;
&lt;li&gt;コールドスタート問題という形で今後の課題として書かれていたので、今後に期待&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;転置インデックスでのマッチングの補完として利用しているということだったけど、&lt;strong&gt;ランキングはどうしてるんだろう&lt;/strong&gt;？
&lt;ul&gt;
&lt;li&gt;別のランキング用の指標があるのかな？上位k件とした場合にどうやって、決めているのか？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;モデル更新のタイミングはどのくらいのスパンなんだろう？&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ感想&#34;&gt;まとめ＆感想&lt;/h2&gt;
&lt;p&gt;セマンティック検索の処理を分類問題に置き換えてその手法を用いて行い、実際のシステムに適用できそうな速度と精度を出しているのが面白かったです。
セマンティック検索にもいろんな手法があるのだなと（そして、いろいろ勉強しないといけないんだなと。。。）。&lt;/p&gt;
&lt;p&gt;気になる点にも書きましたが、ツリーの作り方になにかコツがあるのか？他の検索結果と組み合わせるときに、どのような基準で上位を選択しているのか？など疑問点も残っています。
個人的には特にどのように既存のシステムにくっつけていくのかというところが気になっています。
論文の続編でその辺の話が出てくるのかなぁ？&lt;/p&gt;
&lt;p&gt;ツリーの作り方に関するPECOSの論文を誰か解説してくれると嬉しいかもなぁと思いつつ、とりあえずブログに書き残してみました。参考になるかなぁ。。。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3447548.3467092&#34;&gt;Extreme Multi-label Learning for Semantic Matching in Product Search - ACM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.science/publications/extreme-multi-label-learning-for-semantic-matching-in-product-search&#34;&gt;Extreme multi-label learning for semantic matching in product search - Amazon Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05878&#34;&gt;PECOS： Prediction for Enormous and Correlated Output Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kamujun.hatenablog.com/entry/2018/06/22/180605&#34;&gt;数えきれないほどの分類を行うExtreme Classification - Technical Hedgehog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literatureという論文を読んだ</title>
      <link>https://blog.johtani.info/blog/2021/11/10/reading-ms-biomedical-search-paper/</link>
      <pubDate>Wed, 10 Nov 2021 14:33:16 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2021/11/10/reading-ms-biomedical-search-paper/</guid>
      <description>Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literatureという論文を読んだので軽くメモを残しておきます。論文自体はリンクから参照してください。 読んだ理由 Azure Cognitive S</description>
      <content:encoded>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13375&#34;&gt;Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature&lt;/a&gt;という論文を読んだので軽くメモを残しておきます。論文自体はリンクから参照してください。&lt;/p&gt;
&lt;h2 id=&#34;読んだ理由&#34;&gt;読んだ理由&lt;/h2&gt;
&lt;p&gt;Azure Cognitive SearchでSemantic Searchが利用可能になったこともあり、「Semantic Search」に関するMSのリサーチチームが発表している論文をたまたま見つけたためです。
Elasticsearchとニューラルモデルを利用したランカーでのリランクする仕組みがSemantic Searchと似ていたので読んでみました。&lt;/p&gt;
&lt;h2 id=&#34;ざっくりメモ&#34;&gt;ざっくりメモ&lt;/h2&gt;
&lt;h3 id=&#34;どんなもの&#34;&gt;どんなもの？&lt;/h3&gt;
&lt;p&gt;クリックログなどで関連度を改善できないような場合に、ドメイン固有の言語モデルを利用して検索結果の改善をする方法が提案されているので、バイオメディカル検索で実際に評価してみたという論文です。
特定分野の大量の文書から検索をするときに、ドメイン固有の事前学習した言語モデルを用意して、さらにファインチューニングする方法を評価しています。
言語モデルの生成に利用されたのはBERTになります。&lt;/p&gt;
&lt;h3 id=&#34;技術や手法のキモはどこ&#34;&gt;技術や手法のキモはどこ？&lt;/h3&gt;
&lt;p&gt;医療分野のデータを利用して評価していますが、ほかのドメインにも一般化できる可能性があることや、実際のシステムを提示している部分が肝になりそうです。&lt;/p&gt;
&lt;p&gt;論文から引用した言語モデルを用いてランキングを行う仕組みを構築する部分の構成です。&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;max-width:600&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20211110/system-overview.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20211110/system-overview.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;左半分が、ドメイン固有の事前学習についてです。Wikipediaなどを利用したBERTのモデルが公開されたりしていますが、大量のドメイン固有データが用意できるのであれば、ドメイン固有のデータで事前学習することが有効であるという主張です（&lt;a href=&#34;https://arxiv.org/abs/2007.15779&#34;&gt;参考文献&lt;/a&gt;。これもMS Researchでした。同じチームなのかな？）。
この論文ではBERTの学習データとして、ドメイン固有のテキストを用いています。
実際にはPubMedBERTを利用しています。&lt;/p&gt;
&lt;p&gt;右半分がドメイン固有のデータで生成された言語モデルを利用して、ドメイン固有のニューラルランカー（検索結果のランキングを行なう仕組み）をファインチューニングする処理です。
MS MARCOと呼ばれるMSが公開している機械学習向けのデータセットのうち、ドメイン固有のサブセットを取り出して利用しているようです。&lt;/p&gt;
&lt;p&gt;この論文では、L1検索（第1段階の検索）にBM25を利用して、L2検索にここで作成したニューラルランカーを利用し、検索結果を返す仕組みとなっています。
これは、Azure Cognitive SearchのSemantic Searchのシステムに似ています。&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:600&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20211110/system_detail.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20211110/system_detail.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;この論文では、BM25で検索した結果の上位60件の結果がL2のランカーの入力となっています。&lt;/p&gt;
&lt;p&gt;以下の2つが主な論文の成果となっています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ドメイン固有の事前学習を利用することで、高度な学習コンポーネントなどの追加することなく高い精度が出た。&lt;/li&gt;
&lt;li&gt;既存のBM25の検索エンジンと組み合わせてニューラルランカーを使うことで、計算コストを下げつつより良い上位の検索結果を返す仕組みを構築した。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;どうやって有効だと検証した&#34;&gt;どうやって有効だと検証した？&lt;/h3&gt;
&lt;p&gt;TREC-COVIDデータセットを用いて評価して、TREC-COVIDに参加しているシステムと論文で提案している構成のシステムとの比較をしています。
また、ドメイン固有の事前学習が、一般ドメインなどの事前学習と比較して影響があるかどうかの評価もしています。&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:600&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20211110/table1.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20211110/table1.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;



&lt;div class=&#34;box&#34; style=&#34;max-width:600&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20211110/table2.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20211110/table2.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;実際にAzure上で構築した仕組みをMicrosoft Biomedical Searchとして公開しているようです。
上記のシステム構成のように、Elasticsearchで検索して、ニューラルランカーはKubernetes上に展開されたGPUで計算をしています。
論文には使用しているマシンの構成や台数なども記載があります。&lt;/p&gt;
&lt;p&gt;実際に構築したシステムをユーザーに利用して体験したもらった結果としては、
複雑な意図を持った長いクエリに対してはPubMedなどの他の検索ツールとしても良い結果が返ってきたことが確認されたとなっています。
ただし、一般的な短いクエリの場合は十分な結果にならない場合があったとのことです。&lt;/p&gt;
&lt;h3 id=&#34;議論はある&#34;&gt;議論はある？&lt;/h3&gt;
&lt;p&gt;実際にほかのドメイン（金融、法律、小売りなど）で適用してもうまくいくかは今後の研究に期待だと思います。
また、ファインチューニングするときに利用できるデータが今回の論文にあるようにMS MARCOのサブセットとして抽出できればよさそうです。&lt;/p&gt;
&lt;h3 id=&#34;他に読むべき論文は&#34;&gt;他に読むべき論文は？&lt;/h3&gt;
&lt;p&gt;TREC-COVIDに参加しているほかのシステムがどのような学習や仕組みが必要なのかを見ることで、どのくらいの手間・コストが軽減しているのかがわかるはずです。&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;p&gt;ということで、読んでみました。
細かなほかの手法については調べていませんが、システム構成としてはAzure Cognitive SearchのSemantic Searchの仕組みと同じだと思われます。
違いは、ドメイン固有の事前学習のモデルではないことでしょうか。
ある程度のドメイン固有のモデルを利用できる仕組みができると、さらにSemantic Searchがうまく使えるようになるのかもしれません（前回書いたブログではWikipediaのデータを利用してみましたが、思ったほど良い感じはしなかったです。。。ファインチューニングの仕組みもないしなぁ）。
また、短いクエリでは芳しくない結果もあったというのは、おそらくニューラルランカーで評価するための情報が少なくなってしまうのだと思います。Semantic Searchに向いているクエリの作り方とかが出てくるのかな？&lt;/p&gt;
</content:encoded>
    </item>
    
  </channel>
</rss>
