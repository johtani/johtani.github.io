<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">

  <channel>
    <title>Solr on @johtaniの日記 3rd</title>
    <link>https://blog.johtani.info/tags/solr/</link>
    <description>Recent content in Solr on @johtaniの日記 3rd</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 29 Jan 2014 18:46:00 +0900</lastBuildDate><atom:link href="https://blog.johtani.info/tags/solr/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>第13回Solr勉強会を開催しました</title>
      <link>https://blog.johtani.info/blog/2014/01/29/hold-to-japan-solr-meetup/</link>
      <pubDate>Wed, 29 Jan 2014 18:46:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/29/hold-to-japan-solr-meetup/</guid>
      <description>改訂新版Solr入門出版記念ということで、第13回Solr勉強会 #SolrJP 新Solr本出版記念を開催しました。 出版記念なので、技術評論社様より、プレ</description>
      <content:encoded>&lt;p&gt;改訂新版Solr入門出版記念ということで、&lt;a href=&#34;http://solr.doorkeeper.jp/events/7260&#34;&gt;第13回Solr勉強会 #SolrJP 新Solr本出版記念&lt;/a&gt;を開催しました。&lt;/p&gt;
&lt;p&gt;出版記念なので、技術評論社様より、プレゼント用にSolr本を用意していただきました！ありがとうございます！！
書籍をゲット出来た方は、ツイートしたりブログ書いたり書評書いたりして、宣伝してください！！！&lt;/p&gt;
&lt;p&gt;今回は、私は手を抜いて他の人に喋ってもらいました！&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;今回は、著者陣（関口さんは特別ゲスト）でスピーカーを固めてみました。
以下は、いつもの簡単なメモです。
スライドが集まったらまた更新していきます。&lt;/p&gt;
&lt;h2 id=&#34;1-はじめての検索エンジンsolr-株式会社nttデータccs鈴木-教嗣さん&#34;&gt;1. 「はじめての検索エンジン＆Solr」 株式会社NTTデータCCS　鈴木 教嗣さん&lt;/h2&gt;
&lt;p&gt;スライド：&lt;a href=&#34;http://www.slideshare.net/suzu2525/solr-13&#34;&gt;はじめての検索エンジン＆Solr 第13回Solr勉強会&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;鈴木さんの発表初めて聞きましたｗ。
趣味が多いなぁ。
ちょこちょこと、宣伝を入れてるのが流石ですｗ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入門らしい概要&lt;/li&gt;
&lt;li&gt;クエリの概要とかも。&lt;/li&gt;
&lt;li&gt;スコア計算とか&lt;/li&gt;
&lt;li&gt;導入するとうれしいところとか&lt;/li&gt;
&lt;li&gt;Solr盛り上げましょう！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-solr-searchcomponent-再訪-株式会社ロンウイット関口-宏司さん&#34;&gt;2. 「Solr SearchComponent 再訪」 株式会社ロンウイット　関口 宏司さん&lt;/h2&gt;
&lt;p&gt;スライド：公開待ち&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ベン図で検索の評価指標の説明&lt;/li&gt;
&lt;li&gt;理論的なお話&lt;/li&gt;
&lt;li&gt;Solrのサーチコンポーネントを使って何ができるか。ベン図で。&lt;/li&gt;
&lt;li&gt;サーチコンポーネント以外にも
&lt;ul&gt;
&lt;li&gt;NGramTokenizerも&lt;/li&gt;
&lt;li&gt;SynonymFilterも&lt;/li&gt;
&lt;li&gt;パーソナライズ検索&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;いきなり話をふられたのでちょっとびっくりしましたｗ&lt;/p&gt;
&lt;h2 id=&#34;3-自動補完autocompleteともしかしてdid-you-mean-株式会社-ロンウイット大須賀-稔さん&#34;&gt;3. 「自動補完(Autocomplete)ともしかして？(Did You Mean?)」 株式会社 ロンウイット　大須賀 稔さん&lt;/h2&gt;
&lt;p&gt;スライド：&lt;a href=&#34;http://www.slideshare.net/mosuka/solr-autocomplete-and-did-you-mean&#34;&gt;Solr AutoComplete and Did You Mean?&lt;/a&gt;&lt;br&gt;
デモ：https://github.com/mosuka/solr-suggester-demo-ui&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;職歴が相変わらずおもしろい&lt;/li&gt;
&lt;li&gt;編集距離のお話&lt;/li&gt;
&lt;li&gt;素晴らしいCM！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;候補のランキングを変更できる？
SpellcheckComponentのパラメータで指定できるものなら楽ですが。。。&lt;/p&gt;
&lt;h2 id=&#34;4-lucene-revolution-2013-dublin振り返り-楽天株式会社平賀-一昭さん&#34;&gt;4. 「Lucene Revolution 2013 Dublin振り返り」 楽天株式会社　平賀 一昭さん&lt;/h2&gt;
&lt;p&gt;スライド：公開待ち&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ダブリンどこ？（間違ってベルリンって言っちゃいましたｗ）&lt;/li&gt;
&lt;li&gt;スタジアムで開催。グランドにも入れるのかなぁ？&lt;/li&gt;
&lt;li&gt;まずはTwitter
&lt;ul&gt;
&lt;li&gt;Luceneの改良版&lt;/li&gt;
&lt;li&gt;ちょっと特殊。１４０文字とか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;青いRさんのライバル。Careerbuilder
&lt;ul&gt;
&lt;li&gt;元FASTユーザ&lt;/li&gt;
&lt;li&gt;企業向けに検索キーワードとかの解析画面を用意&lt;/li&gt;
&lt;li&gt;検索精度の改良の話とか&lt;/li&gt;
&lt;li&gt;転職で引っ越す意思があるかとか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最後はLinkedIn
&lt;ul&gt;
&lt;li&gt;Luceneのユーザ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;まとめ&#34;&gt;まとめ&lt;/h3&gt;
&lt;p&gt;ということで、スピーカーの方々のスライドにもありましたが、
&lt;a href=&#34;http://www.amazon.co.jp/dp/4774161632?tag=johtani-22&amp;amp;camp=243&amp;amp;creative=1615&amp;amp;linkCode=as1&amp;amp;creativeASIN=4774161632&amp;amp;adid=11S5FJFPHF9685VRE24M&amp;amp;&amp;amp;ref-refURL=http%3A%2F%2Fblog.johtani.info%2F&#34;&gt;改訂新版Apache Solr入門&lt;/a&gt;は良い本なので、購入していただけると嬉しいです。&lt;/p&gt;
&lt;p&gt;感想、コメントなど、いつでもお待ちしています！&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 4.6.1のリリースに関する注意点</title>
      <link>https://blog.johtani.info/blog/2014/01/28/release-lucene-gosen-4-dot-6-1/</link>
      <pubDate>Tue, 28 Jan 2014 12:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/28/release-lucene-gosen-4-dot-6-1/</guid>
      <description>Lucene/Solr 4.6.1がリリースされそう(バイナリ配布待ち)lucene-gosenの4.6.1対応版をリリースしました。 ライブラリのインタフェースな</description>
      <content:encoded>&lt;p&gt;Lucene/Solr 4.6.1がリリースされそう(バイナリ配布待ち)&lt;a href=&#34;https://code.google.com/p/lucene-gosen/&#34;&gt;lucene-gosen&lt;/a&gt;の4.6.1対応版をリリースしました。&lt;/p&gt;
&lt;p&gt;ライブラリのインタフェースなどは特に変更はないのですが、ライブラリのダウンロード先が変更になっているため、注意喚起です。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;Google Project Hostingの仕様変更により、Downloadsに新規ファイルがアップロードできなくなっています。（2014年から）&lt;/p&gt;
&lt;p&gt;このため、プロジェクトの選択肢としては以下の3点となっています。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Google Driveにファイルをアップロードしてダウンロードしてもらう&lt;/li&gt;
&lt;li&gt;他のソースコード管理サイトなどを利用する。&lt;/li&gt;
&lt;li&gt;他のダウンロードサイトを利用する&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1.と3.は場所が違うだけで、方法は一緒です。
今回は、暫定的に1.を利用してダウンロードするように対応しました。&lt;/p&gt;
&lt;p&gt;ダウンロード先はプロジェクトのページにリンクが有りますが、わかりにくいのでキャプチャを撮ってみました。&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20140128/project_home.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20140128/project_home.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption align=&#34;center&#34;&gt;&lt;h4&gt;ダウンロード先&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;これまでの&lt;code&gt;Featured - Downloads&lt;/code&gt;とは異なり、&lt;code&gt;Links - External links&lt;/code&gt;の下に
&lt;a href=&#34;https://drive.google.com/folderview?id=0B0xz3tf1TTPnYTlSNExkTzBhWnc&amp;amp;usp=sharing&#34;&gt;Downloads lucene-gosen 4.6.1&lt;/a&gt;というリンクを用意してあります。&lt;/p&gt;
&lt;p&gt;フォルダとなっており、各種jarファイルがリストされていますので、こちらからダウンロードをお願いします。
今後は、この下にダウンロードリンクを追加していく予定です。&lt;/p&gt;
&lt;p&gt;ただし、2.で述べたように「別のソースコード管理サイト」も検討中です。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Solrへのプラグインの配置方法について</title>
      <link>https://blog.johtani.info/blog/2013/12/19/add-jar-file-to-solr/</link>
      <pubDate>Thu, 19 Dec 2013 19:09:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/19/add-jar-file-to-solr/</guid>
      <description>Solr本が出てから、質問を受けてブログ書くと言いながら書いてなかったことを思い出しました。。。 プラグインの配置方法についてこんな質問を受け</description>
      <content:encoded>&lt;p&gt;Solr本が出てから、質問を受けてブログ書くと言いながら書いてなかったことを思い出しました。。。&lt;/p&gt;
&lt;p&gt;プラグインの配置方法についてこんな質問を受けてたので、それっぽいエントリを書いておきます。（想像と違ってたらツッコミ入れてください）&lt;/p&gt;
&lt;!-- more --&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;ja&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/johtani&#34;&gt;@johtani&lt;/a&gt; 追加でプラグインの配置方法とかあると便利かなと思いました&lt;/p&gt;&amp;mdash; Tsubosaka (@tsubosaka) &lt;a href=&#34;https://twitter.com/tsubosaka/statuses/407395766471110656&#34;&gt;2013, 12月 2&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;改定前のSolr本では、日本語の形態素解析器をjarファイルとして追加する方法が書かれていました。
ただ、改定後のSolr本では、KuromojiがLuceneで実装されているためサンプルとしてjarファイルを追加するような方法の記載が明確にはありません。&lt;/p&gt;
&lt;p&gt;19ページのcollection1の説明ですこしだけ、libディレクトリについて触れています。&lt;/p&gt;
&lt;p&gt;独自のTokenizer（lucene-gosenなど）はjar形式でSolrに追加し、schema.xmlなどに利用するFactoryを指定してから利用します。&lt;/p&gt;
&lt;p&gt;このとき、追加のjarファイルを配置する先がlibディレクトリです。&lt;/p&gt;
&lt;p&gt;libディレクトリは2つの種類のスコープのディレクトリが存在します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solr全体で利用可能なlibディレクトリ&lt;/li&gt;
&lt;li&gt;コア単位で利用可能なlibディレクトリ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solr全体で利用するlibディレクトリ&#34;&gt;Solr全体で利用するlibディレクトリ&lt;/h2&gt;
&lt;p&gt;これは、起動しているSolrにある全てのコアで利用するようなjarファイルを配置するディレクトリになります。
場所は&lt;code&gt;$SOLR_HOME/lib&lt;/code&gt;です。ここにjarファイルを配置することで、この&lt;code&gt;$SOLR_HOME&lt;/code&gt;を利用するすべてのコアで同じjarファイルを利用することができるようになります。&lt;/p&gt;
&lt;p&gt;ですので、例えば、lucene-gosenはすべてのコアで利用するという場合にはここに配置すれば、1つのjarファイルを配置するだけで済むことになります。&lt;/p&gt;
&lt;h2 id=&#34;コア単位で利用するlibディレクトリ&#34;&gt;コア単位で利用するlibディレクトリ&lt;/h2&gt;
&lt;p&gt;これは、コアごとにlibディレクトリを用意する場合です。
19ページにも記載されていますが、&lt;code&gt;$SOLR_HOME/コアディレクトリ名/lib&lt;/code&gt;となります。&lt;/p&gt;
&lt;p&gt;特定のコアのみで利用するライブラリについてはこちらに配置する形になります。
他のコアで利用してほしくないjarファイルなどを配置するのに利用すればよいかと。&lt;/p&gt;
&lt;p&gt;簡単ですが、補足記事でした。
UIMAやlangidの利用方法などもあるとうれしですかね？
そのうち気が向けば書くかもしれません。（他の人に書いてもらうのもありかも。）&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>改訂版Solr入門のPDF版も発売</title>
      <link>https://blog.johtani.info/blog/2013/12/09/release-introduction-solr-ebook/</link>
      <pubDate>Mon, 09 Dec 2013 11:08:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/09/release-introduction-solr-ebook/</guid>
      <description>少し遅くなってしまいましたが、12/05に電子書籍も発売されました。 技術評論社の電子書籍サイトから購入可能です。 書籍のページへのリンク PDF</description>
      <content:encoded>&lt;p&gt;少し遅くなってしまいましたが、12/05に電子書籍も発売されました。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;技術評論社の電子書籍サイトから購入可能です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://gihyo.jp/dp/ebook/2013/978-4-7741-6240-9&#34;&gt;書籍のページへのリンク&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PDF版となっております。
購入の際は、技術評論社の電子書籍サイトに会員登録後購入可能となります。&lt;/p&gt;
&lt;p&gt;個人的には電子書籍が便利なので、こちらを普段活用しようと思っています。&lt;/p&gt;
&lt;p&gt;もちろん、紙の書籍も発売中です！購入の際は右の書影をクリックしていただければと！&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>改訂版Solr入門を執筆しました</title>
      <link>https://blog.johtani.info/blog/2013/11/26/introduction-to-solr-new-edition/</link>
      <pubDate>Tue, 26 Nov 2013 12:27:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/11/26/introduction-to-solr-new-edition/</guid>
      <description>勉強会で宣伝もしましたが、改めて。 Solr入門の改訂版を執筆しました。 考えてみれば、もう3年も前なんですね、Solr入門は。 Solr勉強会な</description>
      <content:encoded>&lt;p&gt;勉強会で宣伝もしましたが、改めて。&lt;/p&gt;
&lt;p&gt;Solr入門の改訂版を執筆しました。
考えてみれば、もう3年も前なんですね、&lt;a href=&#34;http://gihyo.jp/book/2010/978-4-7741-4175-6&#34;&gt;Solr入門&lt;/a&gt;は。
Solr勉強会などでも何度も新しいのは出ないのですか？と聞かれていましたが、やっと出ました。（お待たせしました。）&lt;/p&gt;
&lt;p&gt;時が立つのは早いものです。前回のSolr入門はバージョン1.4にて執筆していましたが、今回は4.4をベースにし、4.5.1への対応を行っています。&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20131126/intro_solr.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20131126/intro_solr.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;月曜日には手元に見本が届き、今週金曜日に発売予定です！&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;SolrCloud、SoftCommit、Spatial、Joinなど、多彩な機能についても記載してあります。
また、&lt;a href=&#34;http://manifoldcf.apache.org/ja_JP/index.html&#34;&gt;ManifoldCF&lt;/a&gt;というSolrにデータを登録するのに
利用できるコネクタフレームワークについても書いてあります。&lt;/p&gt;
&lt;p&gt;より多彩になったSolrの機能を活用するための一助となれればと思います。
（電子版も出る予定です。詳細についてはもう少々お待ちください）&lt;/p&gt;
&lt;p&gt;また、出版を記念して少し時期が先になりますが、Solr勉強会を開催しようと思います。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;日時：2014年01月29日&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://solr.doorkeeper.jp/events/7260&#34;&gt;第13回Solr勉強会 #SolrJP 新Solr本出版記念&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;今回はせっかくのSolr入門の書籍の出版記念ということで入門的な話をしてもらう予定です。
Solr初心者の方、Solrに興味のある方などに来ていただきたいと思っています。
（プレゼントも用意できるかも！？）&lt;/p&gt;
&lt;p&gt;ということで、「改訂版Apache Solr入門」をよろしくお願いします。
（もちろん、購入は下のリンクからですよね！）&lt;/p&gt;
&lt;iframe src=&#34;http://rcm-fe.amazon-adsystem.com/e/cm?t=johtani-22&amp;o=9&amp;p=8&amp;l=as1&amp;asins=4774161632&amp;nou=1&amp;ref=tf_til&amp;fc1=000000&amp;IS2=1&amp;lt1=_blank&amp;m=amazon&amp;lc1=0000FF&amp;bc1=000000&amp;bg1=FFFFFF&amp;f=ifr&#34; style=&#34;width:120px;height:240px;&#34; scrolling=&#34;no&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
</content:encoded>
    </item>
    
    <item>
      <title>第12回Solr勉強会を主催しました。#SolrJP</title>
      <link>https://blog.johtani.info/blog/2013/10/10/solr-meetup-memo/</link>
      <pubDate>Thu, 10 Oct 2013 11:35:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/10/10/solr-meetup-memo/</guid>
      <description>不定期開催ですが第12回Solr勉強会を主催しました。 今回は、前回ほどの過熱ぶりでは無かったですが、70人ほどの参加者の方がいらっしゃったか</description>
      <content:encoded>&lt;p&gt;不定期開催ですが&lt;a href=&#34;http://atnd.org/events/43532/&#34;&gt;第12回Solr勉強会&lt;/a&gt;を主催しました。&lt;/p&gt;
&lt;p&gt;今回は、前回ほどの過熱ぶりでは無かったですが、70人ほどの参加者の方がいらっしゃったかと。
ありがとうございます！&lt;/p&gt;
&lt;p&gt;今回は聞きたかったYokozunaの話をしてもらいました。あと、リベンジManifoldCF。
&lt;em&gt;一部、追記しました。Bashoさんからツッコミがあったので。あと、4.5.1の話とか。&lt;/em&gt;&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;manifoldcfのとsolrの組み合わせ仮株式会社-ロンウイット大須賀稔さん&#34;&gt;ManifoldCFのとSolrの組み合わせ（仮）株式会社 ロンウイット　大須賀　稔さん&lt;/h2&gt;
&lt;p&gt;前回お休みだったのでリベンジですw。&lt;/p&gt;
&lt;p&gt;英語だ。。。やっぱ英語がいいですか、スライド。。。&lt;br&gt;
ManifoldCFの概要から。
最新版は1.3です。色々サポートしてるなぁ。&lt;/p&gt;
&lt;p&gt;デモもありました。（やっぱりちゃんと動かないので、鬼門みたいですが）&lt;/p&gt;
&lt;h4 id=&#34;デモ&#34;&gt;デモ&lt;/h4&gt;
&lt;p&gt;ManifoldCFのGUIで操作しながら。
いまいちちゃんと動かなかった。。。&lt;/p&gt;
&lt;h4 id=&#34;qa&#34;&gt;QA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Q:Zipはうまく動かなかった&lt;/li&gt;
&lt;li&gt;A:Solr側で処理してくれてる。&lt;/li&gt;
&lt;li&gt;Q:Notes対応するの？&lt;/li&gt;
&lt;li&gt;A:いまのところない。&lt;/li&gt;
&lt;li&gt;Q:ExcelとかPDFはTika？&lt;/li&gt;
&lt;li&gt;A:Tika次第です。&lt;/li&gt;
&lt;li&gt;Q:認証周りどこから取ってくるの？&lt;/li&gt;
&lt;li&gt;A:クローラ側にはなくて、SharePointとかの権限をみてる。&lt;/li&gt;
&lt;li&gt;Q:Web系の認証は？&lt;/li&gt;
&lt;li&gt;A:まだないのでは。。。（調査します）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;あー、デモの続き忘れてましたね。。。&lt;/p&gt;
&lt;h2 id=&#34;solrを組み込んだriak-20の全文検索機能--yokozuna--bashoジャパン株式会社鈴木一弘さん&#34;&gt;Solrを組み込んだRiak 2.0の全文検索機能 -Yokozuna- Bashoジャパン株式会社　鈴木　一弘さん&lt;/h2&gt;
&lt;p&gt;Riak色々使われてるよ！アングリーバードとか、Y!とか。
Riakで提供されている1機能としてのYokozuna。単独製品ではないですよと。&lt;/p&gt;
&lt;p&gt;Riakの説明。スケールするよ、いつでもRead/Writeできるよ、運用にフォーカスしてるよと。
マスターレスですよ。
Riak2.0のリリースは2013年末。Yokozunaもかな？&lt;/p&gt;
&lt;p&gt;ダイナミックフィールド使ってるので、Yokozunaをonにするだけで簡単に使えるよ。&lt;/p&gt;
&lt;p&gt;RiakがSolrのプロセスを管理。&lt;/p&gt;
&lt;p&gt;インデックスの不整合の検知とかってどうやってるのかなぁ？
インデックス比較用のハッシュツリーをノード間でコピーしつつ検査してる。（Active Anti-Entropy）&lt;/p&gt;
&lt;p&gt;(デモには魔物がいるようだ。。。)&lt;/p&gt;
&lt;h4 id=&#34;qa-1&#34;&gt;QA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Q:JSONの属性を元にしてフィールドにインデックス可能か？&lt;/li&gt;
&lt;li&gt;A:可能です。IIJさんの発表で話が出ます。&lt;/li&gt;
&lt;li&gt;Q:ProtocolBufferでSolrにアクセス可能？&lt;/li&gt;
&lt;li&gt;A:&lt;strike&gt;そのうちできそうです。&lt;/strike&gt;リリース時にはできるようになっています。&lt;/li&gt;
&lt;li&gt;Q:コアのスワップは？スキーマの変更は？&lt;/li&gt;
&lt;li&gt;A:事前に設定するのは可能。&lt;/li&gt;
&lt;li&gt;Q:RiakのデータとSolrでデータがずれるってのはあるの？&lt;/li&gt;
&lt;li&gt;A:可能性はありますが、&lt;strike&gt;極力ずれ&lt;/strike&gt;AAEで修復。&lt;/li&gt;
&lt;li&gt;Q:復旧中のインデックスにアクセスが行かないようにする仕組みなどはある？&lt;/li&gt;
&lt;li&gt;A:今はないです。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;yokozuna-ベンチマークしました株式会社インターネットイニシアティブ曽我部崇さん田中-義久さん&#34;&gt;Yokozuna ベンチマークしました　株式会社インターネットイニシアティブ　曽我部　崇さん、田中 義久さん&lt;/h2&gt;
&lt;p&gt;いいとこ取りで楽だなぁと。いうことで、試してみてます。
デモが動いてる。&lt;/p&gt;
&lt;p&gt;extractorでXMLやJSONをパースできる。
ベンチマーク結果。&lt;/p&gt;
&lt;p&gt;Riak Meetup Tokyo #2の時のQAも入ってるので助かります。素晴らしい。&lt;/p&gt;
&lt;h4 id=&#34;qa-2&#34;&gt;QA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Q:スナップショットは両方取れるの？&lt;/li&gt;
&lt;li&gt;A:Riakは取れますが、インデックスは今は無理です。&lt;/li&gt;
&lt;li&gt;フォロー:0.8はYokozunaにボトルネックがあったので、0.9以降だともっと性能が出るはずですとのこと。また次回とかに発表してもらうのもありですかねぇ。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solr-45の新機能など-johtani&#34;&gt;Solr 4.5の新機能など @johtani&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.johtani.info/images/entries/20131009/Solr4_5_Changes.pdf&#34;&gt;発表資料のPDF&lt;/a&gt;です。&lt;/p&gt;
&lt;p&gt;ツイート見てて誤解を招いたなと思ったのですが、7u40は4.5限定ではなく、すべてのバージョンと考えてください。
チケットを見ると分かりますが、影響バージョンの記載はありません。&lt;/p&gt;
&lt;p&gt;※あ、4.5のChangesを紹介しましたが、4.5.1が出るかも。このへんが困ってるらしいです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-5306&#34;&gt;SOLR-5306: can not create collection when have over one config&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-5317&#34;&gt;SOLR-5317: CoreAdmin API is not persisting data properly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.apache.org/jira/browse/LUCENE-5263&#34;&gt;LUCENE-5263: Deletes may be silently lost if an IOException is hit and later not hit (e.g., disk fills up and then frees up)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lt&#34;&gt;LT&lt;/h2&gt;
&lt;h3 id=&#34;haruyama-さん&#34;&gt;@haruyama さん&lt;/h3&gt;
&lt;p&gt;資料：&lt;a href=&#34;http://haruyama.github.io/solr_20131009/#(1)&#34;&gt;http://haruyama.github.io/solr_20131009/#(1)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;記号が捨てられるTokenizer困るので、捨てないのを作ってみました。&lt;/p&gt;
&lt;p&gt;Kuromojiの困ったこと。全角数字を分解しちゃう。→MappingCharFilterFactoryで全角から半角にしましょう。
lucene-gosenデフォで半角記号が未知語になってしまい、半角カナと混ざるので、記号を全角にしましょう。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>MorphlinesのloadSolrをちょっとだけ調べてみた</title>
      <link>https://blog.johtani.info/blog/2013/08/02/morphlines-loadsolr/</link>
      <pubDate>Fri, 02 Aug 2013 18:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/08/02/morphlines-loadsolr/</guid>
      <description>宿題その2？かな。Solr勉強会でCloudera Searchのスキーマ周りってどうなってるの？という質問が出てて、 なんか調べることになって</description>
      <content:encoded>&lt;p&gt;宿題その2？かな。Solr勉強会でCloudera Searchのスキーマ周りってどうなってるの？という質問が出てて、
なんか調べることになってたので、関係しそうなMorphlinesの&lt;a href=&#34;https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/LoadSolrBuilder.java&#34;&gt;LoadSolr&lt;/a&gt;コマンドを調べてみました。
こいつが、Solrへの書き込みを実行するコマンドみたいだったので。&lt;br&gt;
（※Cloudera Searchのスキーマの設定方法とかは調べてないです。）&lt;br&gt;
（※めんどくさかったので、パッケージ名すっ飛ばしてクラス名書いてます。githubへのリンクを代わりに貼ってます。）&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;recordsolrのドキュメント&#34;&gt;Record＝Solrのドキュメント&lt;/h2&gt;
&lt;p&gt;convert()メソッドにて、MorphlinesのRecord（コマンドの処理するデータの１単位）に格納されているKey-ValueデータをSolrInputDocumentクラスのフィールドとして格納しています。
Recordにもフィールドという概念があり、Recordのフィールド＝Solrのフィールドという事みたいです。&lt;/p&gt;
&lt;p&gt;ということで、Solrのフィールドは事前に定義しておき、Morphlinesの処理内部でSolrのフィールド名に値を詰めていく感じでしょうか。
別途、&lt;a href=&#34;https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/SanitizeUnknownSolrFieldsBuilder.java&#34;&gt;sanitizeUnknownSolrFilds&lt;/a&gt;というコマンドが用意されていて、Solrのスキーマにないものはこのコマンドを使って、無視するフィールド名に変えたり、雑多なデータを入れるためのフィールド名にするといった処理ができるようです。このコマンド内部で、Solrのスキーマ設定を元に、Solrのフィールドに合致する物があるかをチェックして処理しています。&lt;/p&gt;
&lt;h2 id=&#34;solrへの登録処理は&#34;&gt;Solrへの登録処理は？&lt;/h2&gt;
&lt;p&gt;Solrへの登録処理自体はLoadSolrクラス内部でDocumentLoaderというクラスのload()メソッドを呼び出しているだけでした。ということで、&lt;a href=&#34;https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/DocumentLoader.java&#34;&gt;DocumentBuilder&lt;/a&gt;クラスを少し調査。&lt;/p&gt;
&lt;h3 id=&#34;documentloader&#34;&gt;DocumentLoader&lt;/h3&gt;
&lt;p&gt;IFでした。。。実クラスは次の条件&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SolrMorphlineContextにDocumentLoaderがあればそちらを採用（他の種類はなにがあるんだろ？）&lt;/li&gt;
&lt;li&gt;なければ、&lt;a href=&#34;https://github.com/cloudera/cdk/blob/b6f98cff4a027af04f97fdec9abf729785d74cf5/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/SolrServerDocumentLoader.java&#34;&gt;SolrServerDocumentLoader&lt;/a&gt;をnewしたものを利用&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2.の場合がおそらくMapReduceではないパターンのloadSolrだと思われます。SolrServerDocumentBuilderはSolrJのAPIを利用して、Solrへデータ登録していく普通のアプリです。（対象とするSolrは外部に起動しているもののはず＝FlumeのSolrSinkではこちらを採用かな？）&lt;br&gt;
Solrへの接続情報とか設定ファイルとかSolrCloud用のZooKeeperとかは&lt;a href=&#34;https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/src/main/java/com/cloudera/cdk/morphline/solr/SolrLocator.java&#34;&gt;SolrLocatorクラス&lt;/a&gt;に設定される内容が利用されます。&lt;/p&gt;
&lt;p&gt;1.のパターンは、どうやら、&lt;a href=&#34;https://github.com/cloudera/search/blob/master/search-mr/src/main/java/org/apache/solr/hadoop/morphline/MorphlineMapper.java&#34;&gt;Cloudera SearchのMapReduceIndexerToolのクラス&lt;/a&gt;にあるMyDocumentLoaderかなぁと。
こちらは、MapReduceを利用する場合に、利用されてるっぽいです（ちゃんと見てないけど）
内部処理は、HadoopのContext.writeメソッドにでSolrInputDocument（＝MorphlinesのRecord）を書きだして、ReducerでSolrOutputFormatでインデックス作成の流れかなと。たぶん、&lt;a href=&#34;https://github.com/cloudera/search/blob/master/search-mr/src/main/java/org/apache/solr/hadoop/morphline/MorphlineMapRunner.java&#34;&gt;MorphlineMapRunner&lt;/a&gt;あたりを読みこめば解読できるかと。
ちなみに、こちらは、2.とは異なり、SolrLocatorの設定は無視されそう。&lt;/p&gt;
&lt;h2 id=&#34;感想妄想&#34;&gt;感想＋妄想？&lt;/h2&gt;
&lt;p&gt;ということで、Morphlinesのデータ流れを考える上で、現時点ではSolrのスキーマを頭の片隅に置きつつ、
Recordの中にあるデータをゴニョゴニョしてデータを形成していくって感じになりそうです。
うまく処理できなかったものとかのカウントとかもとれたりするのかなぁ？とか、また色々と気になるところが出てきますが、一旦ここまでで。。。（だれか、続きを調べて書いてみてくれてもいいんですよ！コマンドもいっぱいあるし！）&lt;/p&gt;
&lt;p&gt;とまぁ、こんなかんじでMorphlinesをちょっとだけ読みました。
よくよく考えたら、こんなの作ったことあるなぁと（こんなに汎用的じゃないけど）。
みんな同じ事考えるんですねぇ。
コマンドパターン？みたいな感じで、I/F決めてSolrとか別のシステムとかにデータ入れる処理を順番に記述できる的なバッチ処理良くかいてます（書いてましたのほうが正解かなぁ）。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Morphlines入門？</title>
      <link>https://blog.johtani.info/blog/2013/07/31/introduction-morphlines/</link>
      <pubDate>Wed, 31 Jul 2013 19:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/31/introduction-morphlines/</guid>
      <description>Morphlinesについてちょっとだけ、さらに調べました。 誤解 Solr勉強会でなんとなく私の認識を話しましたが、ちょっと誤解してたみたいで</description>
      <content:encoded>&lt;p&gt;Morphlinesについてちょっとだけ、さらに調べました。&lt;/p&gt;
&lt;h2 id=&#34;誤解&#34;&gt;誤解&lt;/h2&gt;
&lt;p&gt;Solr勉強会でなんとなく私の認識を話しましたが、ちょっと誤解してたみたいです。スミマセン。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h3 id=&#34;誤解morphlineというプラットフォームミドルウェアがありそうなイメージ&#34;&gt;誤解：Morphlineというプラットフォーム/ミドルウェアがありそうなイメージ&lt;/h3&gt;
&lt;p&gt;まぁ、書いてあるのでちゃんと読めって話ですが、Morphlineはあくまでライブラリだということでした。
私はなんとなくManifoldCFのようなミドルウェアorプラットフォームが存在して、
そこにFlumeのSinkとかMapReduceによるIndexerが動作するのかと思ってました。&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;max-width:300&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20130731/wrong_image.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20130731/wrong_image.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;まぁ、これが間違いでした。正解のイメージはこっちですね。&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:300&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20130731/correct_image.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20130731/correct_image.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;各プラットフォーム（FlumeとかHadoopとか）に組み込んむライブラリで、
それぞれ組み込んだ先でMorphlineの設定を記述することで、パイプライン処理ができるっぽいです。&lt;/p&gt;
&lt;p&gt;Flumeについては、MorphlineSolrSinkというクラスでMorphlineの設定ファイルを読み込み、いろいろ処理出来ます。&lt;/p&gt;
&lt;p&gt;Map/ReduceだとCloudera Searchに含まれてる&lt;a href=&#34;https://github.com/cloudera/search&#34;&gt;MapReduceIndexerTool&lt;/a&gt;がMorphlineの設定を読み込んでコマンド実行してくれるみたいです。
MapReduceIndexerToolはまだちゃんと読んでないのですが、MapperとしてMorphlineのコマンドが実行されるのかなぁ？という感じです。
（結構入り組んでるので、ちゃんと読まないとわからない。。。）&lt;/p&gt;
&lt;p&gt;ということで、Morphlineというプラットフォームがあって、一元的にFlumeやMap/Reduceに対するコマンドをパイプライン化するという話でありませんでした。&lt;/p&gt;
&lt;p&gt;※ちなみに、上の画像ですが、愛用しているNUBoardを使って書いてます。
考えをまとめるのにすごく役立つ一品です。持ち運び可能なノート型ホワイトボードです。&lt;/p&gt;
&lt;iframe src=&#34;http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&amp;bc1=000000&amp;IS1=1&amp;nou=1&amp;bg1=FFFFFF&amp;fc1=000000&amp;lc1=0000FF&amp;t=johtani-22&amp;o=9&amp;p=8&amp;l=as1&amp;m=amazon&amp;f=ifr&amp;ref=qf_sp_asin_til&amp;asins=B00A08IVT4&#34; style=&#34;width:120px;height:240px;&#34; scrolling=&#34;no&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;疑問点&#34;&gt;疑問点&lt;/h3&gt;
&lt;p&gt;ただ、読んでてまだ不明な点があります。まぁ、ぼちぼち調べるかなぁと。。。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solrのschemaはどーなってんの？&lt;/li&gt;
&lt;li&gt;MorphlineにSolrへロードするコマンド（loadSolr）があるけど、FlumeのMorphlineSolrSinkってのもSolrに書き込みそうだけど？&lt;/li&gt;
&lt;li&gt;Map/ReduceでSolrに書き込むもMorphlineのコマンドとの違いは？（前にソースを見たときはSOLR-1301がベースになっていて、SolrOutputFormatってクラスがEmbeddedSolrServer起動してインデクシングしてた）&lt;/li&gt;
&lt;li&gt;GoLiveってなんだろ？（MapReduceIndexerToolに入ってて、M/Rでインデックス作ったあとにSolrのクラスタに配布＋マージするやつっぽい）&lt;/li&gt;
&lt;li&gt;どんなコマンドがあるの？（&lt;a href=&#34;http://cloudera.github.io/cdk/docs/0.4.1/cdk-morphlines/morphlinesReferenceGuide.html&#34;&gt;Cloudera Morphlines Ref Guide&lt;/a&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下は、参考資料と参考資料にあるSlideshareの資料を一部訳したものになります。&lt;/p&gt;
&lt;h3 id=&#34;参考資料&#34;&gt;参考資料&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/cloudera/using-morphlines-for-onthefly-etl&#34;&gt;Using Morphlines for On-the-Fly ETL(slideshare)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudera/cdk/tree/master/cdk-morphlines&#34;&gt;cloudera/cdk/cdk-morphlines(github)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;メモ&#34;&gt;メモ&lt;/h2&gt;
&lt;h3 id=&#34;現在のコマンドライブラリスライド-18-19ページ&#34;&gt;現在のコマンドライブラリ（スライド 18-19ページ）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Solrへのインテグレートとロード&lt;/li&gt;
&lt;li&gt;フレキシブルなログファイル解析&lt;/li&gt;
&lt;li&gt;1行、複数行、CSVファイル&lt;/li&gt;
&lt;li&gt;正規表現ベースのパターンマッチと展開&lt;/li&gt;
&lt;li&gt;Avro、JSON、XML、HTMLのインテグレーション&lt;/li&gt;
&lt;li&gt;Hadoop シーケンスファイルのインテグレーション&lt;/li&gt;
&lt;li&gt;SolrCellとApache Tikaパーサすべてのインテグレーション&lt;/li&gt;
&lt;li&gt;Tikaを利用したバイナリデータからMIMEタイプの自動判別&lt;/li&gt;
&lt;li&gt;動的Javaコードのスクリプティングサポート&lt;/li&gt;
&lt;li&gt;フィールドの割り当て処理、比較処理&lt;/li&gt;
&lt;li&gt;リストやセット書式のフィールド処理&lt;/li&gt;
&lt;li&gt;if-then-else条件分岐&lt;/li&gt;
&lt;li&gt;簡易ルールエンジン（tryRules）&lt;/li&gt;
&lt;li&gt;文字列とタイムスタンプの変換&lt;/li&gt;
&lt;li&gt;slf4jロギング&lt;/li&gt;
&lt;li&gt;Yammerメトリックとカウンター&lt;/li&gt;
&lt;li&gt;ネストされたファイルフォーマットコンテナの解凍&lt;/li&gt;
&lt;li&gt;などなど&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;プラグインコマンドスライド20ページ&#34;&gt;プラグインコマンド（スライド　20ページ）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;簡単に新しいI/Oや変換コマンドが追加可能&lt;/li&gt;
&lt;li&gt;サードパーティや既存機能のインテグレード&lt;/li&gt;
&lt;li&gt;CommandインタフェースかAbstractCommandのサブクラスを実装&lt;/li&gt;
&lt;li&gt;Javaクラスパスに新規作成したものを追加&lt;/li&gt;
&lt;li&gt;登録処理などは必要ない&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;新しいプラグインコマンドとして考えられるもの22ページ&#34;&gt;新しいプラグインコマンドとして考えられるもの（22ページ）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RDBやKVSやローカルファイルなどの外部データソースをレコードにjoin&lt;/li&gt;
&lt;li&gt;DNS名前解決とか短縮URLの展開とか&lt;/li&gt;
&lt;li&gt;ソーシャル・ネットワークからリンクされたメタデータのフェッチ（？？）&lt;/li&gt;
&lt;li&gt;レコードの感情分析とアノテーション？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;31ページの図がわかりやすいかも&lt;/p&gt;
&lt;p&gt;以上。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>第11回Solr勉強会を主催しました。#SolrJP</title>
      <link>https://blog.johtani.info/blog/2013/07/29/study-of-solr/</link>
      <pubDate>Mon, 29 Jul 2013 23:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/29/study-of-solr/</guid>
      <description>不定期開催ですが第11回Solr勉強会を主催しました。 今回も大入り90人くらい？の参加者の皆さんがいらっしゃいました。ありがたいことです！（</description>
      <content:encoded>&lt;p&gt;不定期開催ですが&lt;a href=&#34;http://atnd.org/events/41368/&#34;&gt;第11回Solr勉強会&lt;/a&gt;を主催しました。&lt;/p&gt;
&lt;p&gt;今回も大入り90人くらい？の参加者の皆さんがいらっしゃいました。ありがたいことです！（20時時点で最終的に補欠17人でした。）&lt;/p&gt;
&lt;p&gt;&lt;strike&gt;とりあえず、第一報です。このあと懇親会なので。&lt;/strike&gt;&lt;/p&gt;
&lt;p&gt;ということで、帰りの電車でいくつか感想を（忘れちゃうから）。&lt;/p&gt;
&lt;!--  more --&gt;
&lt;p&gt;小林さんの苦労話は細かいですが、結構はまりがちな点を共有していただいたので良かったかなぁと。
Solrのexampleの設定とか、ManifoldCFとかちょっとずつ罠があったりするので、あるあるネタはありがたいと思いますｗ&lt;/p&gt;
&lt;p&gt;Cloudera Searchについては、安定の嶋内さんの喋りに圧巻でした。検索だけの視点とは異なる観点についての
話は私には足りないしてんだったりするので参考になります。
なんか、気づいたらMorphlineやスキーマ周りを調べてブログ書くことになっちゃったけど。。。
一つ質問しそこねたのがあって、Cloudera社は基本的に公開したOSSについてのトレーニングも立ち上げているイメージです。Cloudera Searchについてもトレーニングが立ち上がるのかなぁと密かに期待をしてみたり（予算の関係上参加できるかは不明ですが。。。）&lt;/p&gt;
&lt;p&gt;牧野さんの話は画像系について、私は詳しくないので、また関口さんのalikeと比較とかしてもらえると面白いかなぁと。とりあえず、青いロボットがちゃんと検索できるようになるといいですねｗｗ&lt;/p&gt;
&lt;p&gt;秀野さんの空間検索は緯度経度以外のPOLYGONなどを利用した検索で、実は私も知らない機能でしたｗ&lt;br&gt;
なとなくは知ってたんですが、そこまでちゃんと検索できるとは！地図以外にも活用できるような気がします（想像つかないんだけど。。。）&lt;/p&gt;
&lt;p&gt;最後は私の発表で、簡単な資料ですみませんでした。しかも発表よりも宣伝が。。。（ブログの宣伝だったりとか。。。）
最後に宣伝した「&lt;a href=&#34;http://www.ipsj.or.jp/dp/cfp/copy_of_copy_of_dp0502s.html&#34;&gt;「ビッグデータ活用を支えるOSS」特集への論文投稿のご案内&lt;/a&gt;」もご検討ください！&lt;/p&gt;
&lt;p&gt;懇親会も楽しかったです。また思いついたら開催しますー&lt;br&gt;
最後に、今回の発表者の皆様、会場提供していただいたVOYAGE GROUPの皆様ありがとうございました！&lt;/p&gt;
&lt;p&gt;以下はいつものメモです。&lt;/p&gt;
&lt;h2 id=&#34;manifoldcfのとsolrの組み合わせ仮株式会社-ロンウイット大須賀さん&#34;&gt;ManifoldCFのとSolrの組み合わせ（仮）株式会社 ロンウイット　大須賀さん&lt;/h2&gt;
&lt;p&gt;残念ながら、発熱のため発表は次回に持ち越しに。&lt;/p&gt;
&lt;p&gt;##社内ファイル及びWEBコンテンツの検索システム構築時に苦労したこと ソフトバンクBB㈱　小林さん&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ManifoldCF＋Solrを使って社内ファイルの検索システム構築&lt;/li&gt;
&lt;li&gt;約1000万ドキュメント&lt;/li&gt;
&lt;li&gt;さまざまなDCにドキュメントがある&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;クロールジョブのハング&#34;&gt;クロールジョブのハング。。。&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ログをDEBUGにしたら。。。ログファイル150GB。。。&lt;/li&gt;
&lt;li&gt;一定時間ごとにAgentをリスタートするバッチを。。。（力技）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mcfエラーによるジョブの停止&#34;&gt;MCFエラーによるジョブの停止&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;CONNECTORS-590&lt;/li&gt;
&lt;li&gt;エラーが発生して止まったジョブを起動するバッチをcronで。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;自作リアルタイムインデクシングの問題&#34;&gt;自作リアルタイムインデクシングの問題&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;MCF使わないでSlaveにインデックス&lt;/li&gt;
&lt;li&gt;openSearcher=falseだとautoCommitが実行されてもSearcherを再起動しないので検索にでてこない&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;リプリケーションのnw負荷&#34;&gt;リプリケーションのNW負荷&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;別DCからのレプリケーションが複数が一度に実施される→ネットワーク負荷が。。。&lt;/li&gt;
&lt;li&gt;cronで別々にレプリすることでNW負荷を分散できてるかな。。。
　　&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cloudera-search-入門仮-cloudera-株式会社嶋内さん&#34;&gt;Cloudera Search 入門(仮) Cloudera 株式会社　嶋内さん&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;マサカリ画像がｗ&lt;/li&gt;
&lt;li&gt;SolrのコミッターMark Millerさんもジョインしてる&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;clouderaとhadoop入門とか&#34;&gt;ClouderaとHadoop入門とか。&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;いろいろあるよ、エコシステム&lt;/li&gt;
&lt;li&gt;4つの分類。
&lt;ul&gt;
&lt;li&gt;データの取り込み&lt;/li&gt;
&lt;li&gt;データの保存&lt;/li&gt;
&lt;li&gt;データの活用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;search&#34;&gt;Search&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;検索エンジンなら数十億人が使い方を知ってる（Clouderaのチャールズ・ゼドルースキ）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cloudera-search&#34;&gt;Cloudera Search&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hadoopのためのインタラクティブな検索&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CDHとSolrの統合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OSS！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利点とか。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;データ解析にも使えるよね、検索&lt;/li&gt;
&lt;li&gt;非構造化データの検索にもいいよね&lt;/li&gt;
&lt;li&gt;単一プラットフォームによるコスパ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cloudera-searchの事例&#34;&gt;Cloudera Searchの事例&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;バイオテクノロジー企業で画像検索とか&lt;/li&gt;
&lt;li&gt;医療系企業でいろんなログイベントの管理とか&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cloudera-searchのアーキテクチャ&#34;&gt;Cloudera Searchのアーキテクチャ&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Flumeでストリーミングで登録&lt;/li&gt;
&lt;li&gt;HBaseデータの登録&lt;/li&gt;
&lt;li&gt;M/Rでバッチ登録&lt;/li&gt;
&lt;li&gt;HueのWebインタフェース&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morphlines、HBaseはLinyプロジェクトのもの&lt;/p&gt;
&lt;p&gt;Solr使うならCDH！！&lt;/p&gt;
&lt;h4 id=&#34;qa&#34;&gt;QA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Q：デモで使われたTwitterの検索のデータ料とかは？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A：デモ環境ですので小さい。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q：スキーマってどうするの？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A：スキーマは。。。私が書こうかなぁ、ブログ。。。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;コンピュータビジョン株式会社-curious-vehicle牧野さん&#34;&gt;コンピュータビジョン　株式会社 Curious Vehicle　牧野さん&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;色々やってます&lt;/li&gt;
&lt;li&gt;コンピュータビジョンの説明（某ネコ型ロボットのいろんな画像がｗ）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;画像検索の流れ&#34;&gt;画像検索の流れ&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;特徴情報の抽出&lt;/li&gt;
&lt;li&gt;特徴情報のクラスタリングによるword化&lt;/li&gt;
&lt;li&gt;Solrによる画像情報の検索&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;1-特徴情報の抽出&#34;&gt;1. 特徴情報の抽出&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;SIFT特徴点解析&lt;/li&gt;
&lt;li&gt;グレースケールしてからSIFT&lt;/li&gt;
&lt;li&gt;注意！SIFTは商用ライセンスが必要です&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;2-特徴情報のクラスタリングによるword化&#34;&gt;2. 特徴情報のクラスタリングによるword化&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;K-meansでクラスタリング&lt;/li&gt;
&lt;li&gt;クラスタ情報をヒストグラム化してSolrへ&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;3-solrによる画像情報の検索&#34;&gt;3. Solrによる画像情報の検索&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;物体認識ベンチマークセット（ケンタッキー大）を使って。&lt;/li&gt;
&lt;li&gt;やっぱり良し悪しある。データセットに特化したチューニングしてます。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;つぎのステップ&#34;&gt;つぎのステップ&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;文字認識とか顔認識&lt;/li&gt;
&lt;li&gt;つぎはドラえもんじゃねぇ、検索とかも。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;ガウシアンによる画像ぼかしの例&#34;&gt;ガウシアンによる画像ぼかしの例&lt;/h5&gt;
&lt;h4 id=&#34;qa-1&#34;&gt;QA&lt;/h4&gt;
&lt;p&gt;マイク回しててメモ取れず。。。&lt;/p&gt;
&lt;h2 id=&#34;国土交通省のデータをsolrで検索株式会社ネクスト秀野さん&#34;&gt;国土交通省のデータをSolrで検索　株式会社ネクスト　秀野さん&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://speakerdeck.com/ryo0301/guo-jiao-sheng-falsedetawosolrdejian-suo&#34;&gt;スライドはこちら&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;評価の関係で。。。&lt;/li&gt;
&lt;li&gt;Spatial検索の話&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;デモの想定機能&#34;&gt;デモの想定機能&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;地図上の小学校を起点に物件検索&lt;/li&gt;
&lt;li&gt;地図上をクリックしたところを中心に検索&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;デモ環境&#34;&gt;デモ環境&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Solr4.3.0、PostGIS 2.0.3、東京都のデータ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;事前知識&#34;&gt;事前知識&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ジオメトリーデータ（点、線、面がある）&lt;/li&gt;
&lt;li&gt;WKB/WKT、Intersects（しらなかった。こんなのもあるのか）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;環境&#34;&gt;環境&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;EC2上にPostGIS＋Solrで構築&lt;/li&gt;
&lt;li&gt;WKT形式でDIHでインポートできるらしい。&lt;/li&gt;
&lt;li&gt;Solr＋S3をJSでGoogleMapへ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solr-44新機能をちょっと紹介johtani&#34;&gt;Solr 4.4新機能をちょっと紹介　@johtani&lt;/h2&gt;
&lt;p&gt;紹介というよりも宣伝。。。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Yokozunaの気になる点というかなんというか</title>
      <link>https://blog.johtani.info/blog/2013/07/11/yokozuna-check-point/</link>
      <pubDate>Thu, 11 Jul 2013 01:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/11/yokozuna-check-point/</guid>
      <description>Yokozunaの気になる点というか、自分だったらこのへん調べるだろうなって観点を上げてみます。 別に調べるわけじゃないので、完全に自己満足な</description>
      <content:encoded>&lt;p&gt;Yokozunaの気になる点というか、自分だったらこのへん調べるだろうなって観点を上げてみます。
別に調べるわけじゃないので、完全に自己満足なメモですけど。&lt;br&gt;
ちなみに、分散システムとかRiakの仕組みは詳しくないので、ズレてる点がいっぱいあるかも。&lt;br&gt;
というか、分散システムでテストというか、検討する点とかってまとまってる資料とかあるのかなぁ？&lt;/p&gt;
&lt;!-- more --&gt;
&lt;ul&gt;
&lt;li&gt;スキーマ変更時の挙動
&lt;ul&gt;
&lt;li&gt;フィールド型変更とか、フィールド追加とか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;既存RiakクラスタにYokozunaの機能を追加する方法と制限
&lt;ul&gt;
&lt;li&gt;タイムラグとかも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Riak＋Yokozunaクラスタに対してノード追加時に発生するオーバーヘッド（ネットワークとかディスクIOとか）&lt;/li&gt;
&lt;li&gt;性能検証のためのシナリオ（どっちが先に悲鳴をあげるかとか）
&lt;ul&gt;
&lt;li&gt;Riakメインで、Yokozunaはおまけ程度に検索するというシナリオ&lt;/li&gt;
&lt;li&gt;Yokozunaメインで使うシナリオ&lt;/li&gt;
&lt;li&gt;更新が多い場合のシナリオ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Riakのみ、Riak＋Yokozunaの各種統計情報（CPU、メモリ、ディスクサイズ、ネットワークIO）&lt;/li&gt;
&lt;li&gt;運用系（監視とか）の手法とか機能？とか&lt;/li&gt;
&lt;li&gt;バージョンアップなどの対応方法&lt;/li&gt;
&lt;li&gt;Solrがコケた時とかの対処&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;とりあえず、こんな感じかなぁ。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Riak Meetup Tokyo #2に参加しました。#riakjp</title>
      <link>https://blog.johtani.info/blog/2013/07/10/riak-meetup-tokyo-no2/</link>
      <pubDate>Wed, 10 Jul 2013 18:57:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/10/riak-meetup-tokyo-no2/</guid>
      <description>先日、Bashoさんにおじゃましたのもあり、Riak Meetup Tokyo #2に参加しました。 Yokozunaの話も聞けるということで。 懇親会も参加しました。</description>
      <content:encoded>&lt;p&gt;先日、Bashoさんにおじゃましたのもあり、&lt;a href=&#34;http://connpass.com/event/2656/&#34;&gt;Riak Meetup Tokyo #2&lt;/a&gt;に参加しました。&lt;br&gt;
Yokozunaの話も聞けるということで。
懇親会も参加しました。Vさん＆リピさんと話し込んじゃってあんまり他の人と話せなかったけど。。。&lt;/p&gt;
&lt;p&gt;以下はいつものメモです。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;freakout-久森さん-riak環境をプロダクションで構築運用してみた仮&#34;&gt;FreakOut 久森さん 「Riak環境をプロダクションで構築＆運用してみた（仮）」&lt;/h2&gt;
&lt;h3 id=&#34;freakoutとrtb&#34;&gt;FreakOutとRTB&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ディスプレイ広告の新しい配信の枠の話&lt;/li&gt;
&lt;li&gt;この人には何出すの？いくらで？みたいな感じ&lt;/li&gt;
&lt;li&gt;純広告：表示保証、期間保証など&lt;/li&gt;
&lt;li&gt;RTB：1回の広告表示ごとに買い付け&lt;/li&gt;
&lt;li&gt;DSP（デマンド・サイト・プラットフォーム）&lt;/li&gt;
&lt;li&gt;広告表示は大体0.1秒で表示しないといけない。この間に色々やってる。
&lt;ul&gt;
&lt;li&gt;50ms or die.で戦ってます。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RTBはCPUバウンド
&lt;ul&gt;
&lt;li&gt;多コアを安く並べたい&lt;/li&gt;
&lt;li&gt;Tokyoなんとかとか使ってた。
&lt;ul&gt;
&lt;li&gt;スケーラビリティがキツイ（クライアント側でアルゴリズム分散してる）&lt;/li&gt;
&lt;li&gt;データ解析もしたいけど、検索ができない&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RTBに適したRiakがうまくハマるのではと。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;構成とかとか&#34;&gt;構成とかとか&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;アプリはPerlなので、PerlでRiakクライアントが必要。Memcached互換とかあると嬉しい。&lt;/li&gt;
&lt;li&gt;ProtobufサポートもPurePerlしかなかった。&lt;/li&gt;
&lt;li&gt;ないなら、作ろうと。&lt;a href=&#34;https://github.com/myfinder?tab=repositories&#34;&gt;githubに上がってます。このへんかな？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;監視はcloudforecastとかでやってる。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;課題&#34;&gt;課題&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Redirectがつらい（haproxy？がつらい？）&lt;/li&gt;
&lt;li&gt;Setが詰まるとつらい（ケースがまだわからない）&lt;/li&gt;
&lt;li&gt;対策１
&lt;ul&gt;
&lt;li&gt;memcached＋Riak&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;対応２（案）
&lt;ul&gt;
&lt;li&gt;hashからpartitionに直接取りに行くとか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;まとめ&#34;&gt;まとめ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;素のままRiakはちょっとつらい&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;qa&#34;&gt;QA&lt;/h4&gt;
&lt;p&gt;聞き取れたやつだけ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q：1台いくら位ですか？
&lt;ul&gt;
&lt;li&gt;A：10万から11万くらい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：どのくらいの性能ですか？
&lt;ul&gt;
&lt;li&gt;A：同時1000くらいをさばいてる？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：50ms以下を出すのに、ネットワーク周りで近さとかを考えることありますか？
&lt;ul&gt;
&lt;li&gt;A：国内だと10msあればなんとかなる。それよりもアプリ側のチューニングのほうがまだ重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：Cassandraとか候補に挙がらなかったんですか？
&lt;ul&gt;
&lt;li&gt;A：苦しんでる人が知人にいるので。。。あと、用途的に違うので。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：バックエンドとしてはなにを？
&lt;ul&gt;
&lt;li&gt;A：bitcaskにしてる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：サーバ構成、ネットワーク構成がどうなってる？
&lt;ul&gt;
&lt;li&gt;A：。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：Redirectとは？RiakがやってるRedirect？
&lt;ul&gt;
&lt;li&gt;A：はい。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：他に候補にあがったのは？
&lt;ul&gt;
&lt;li&gt;A：&lt;a href=&#34;http://www.aerospike.com&#34;&gt;商用のaerospike（これかな？）&lt;/a&gt;がスケールできそうだったけど、クライアントがいまいち。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;感想&#34;&gt;感想&lt;/h3&gt;
&lt;p&gt;広告業界のことをよくわかってないので、微妙にピンときてなかったりもするのですが、以下に素早く返すかって観点でどこに注力して、問題点を潰していくのかってのは面白そうだなぁと。
リクエスト処理の性能がクリアできたらつぎはスケールの観点（ノード追加時の挙動とか）で検証していくんだろうなと。次回の話も聞いてみたい感じです。&lt;/p&gt;
&lt;h2 id=&#34;iij-曽我部さん田中さん-yokozuna-日本語検索性能を評価しました&#34;&gt;IIJ 曽我部さん、田中さん 「Yokozuna 日本語検索性能を評価しました」&lt;/h2&gt;
&lt;h3 id=&#34;yokozunaって&#34;&gt;Yokozunaって？&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Riak＋Solrでいいとこ取り&lt;/li&gt;
&lt;li&gt;データの登録とかはRiakのAPIで。&lt;/li&gt;
&lt;li&gt;SolrのAPIが使える。&lt;/li&gt;
&lt;li&gt;YokozunaがSolrの分散検索の部分を隠してくれる。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;yokozunaのインストールとか&#34;&gt;Yokozunaのインストールとか。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SolrのAPIっぽい形で検索できるし、戻りもSolrのXMLっぽいのが出てくるよ。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wikipediaデータってstoreの性能とか&#34;&gt;Wikipediaデータってstoreの性能とか。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Riakのノード32台。（Xeon、メモリ24GB、HDD。。。）&lt;/li&gt;
&lt;li&gt;yz_extractor：Riakのコンテンツタイプを見てSolrにデータを入れる処理が書いてある。&lt;/li&gt;
&lt;li&gt;自分でschema.xmlを書いてYokozunaに指定することもできる。
&lt;ul&gt;
&lt;li&gt;スキーマの変更とか登録とか。
&lt;ul&gt;
&lt;li&gt;すでに指定済みスキーマを変更した場合の挙動ってどうなるの？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;デモではSolrからid取って、Riakからその他のデータを取り出していた。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rubyでの性能評価&#34;&gt;Rubyでの性能評価&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ベンチマークプログラム側の問題が先に影響が出てしまった。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qa-1&#34;&gt;QA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q：Riak単体とYokozunaつかった時でディスク容量がどのくらい増えた？
&lt;ul&gt;
&lt;li&gt;A：ちゃんと調べてないが、10%くらい増えた気がする。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：Solr側の設定でstored=trueだけど、falseにしてもいいんじゃないの？
&lt;ul&gt;
&lt;li&gt;A：デモはfalseにしてます。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：スキーマってあとから変更できるんですかね？
&lt;ul&gt;
&lt;li&gt;A：まだ良くわかってないです。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：ノードの追加、削除時の挙動とかも気になります。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;感想-1&#34;&gt;感想&lt;/h3&gt;
&lt;p&gt;今回はStore性能に関してでしたが、今後は検索性能やシナリオによる性能（KVSの処理メインで、時々全文検索とか、全文検索の処理も結構あるパターンとか）の測定とか、耐障害性とかの観点で調査を進めてもらってSolr勉強会で話をしてもらえると面白そうだなぁと勝手に思ってみたり。
Solr勉強会へのコンタクトお待ちしてます！ｗ&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>スキーマレスモード？（SOLR-4897）を調べて見ました。</title>
      <link>https://blog.johtani.info/blog/2013/07/04/schemaless-example/</link>
      <pubDate>Thu, 04 Jul 2013 01:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/07/04/schemaless-example/</guid>
      <description>Solr 4.4に取り込まれる予定のチケットで、気になるものを見つけたのでいつものごとく調べてみました。 元となるチケットはこちら。SOLR-4897</description>
      <content:encoded>&lt;p&gt;Solr 4.4に取り込まれる予定のチケットで、気になるものを見つけたのでいつものごとく調べてみました。&lt;/p&gt;
&lt;p&gt;元となるチケットは&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-4897&#34;&gt;こちら。SOLR-4897&lt;/a&gt;。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;スキーマレス&#34;&gt;スキーマレス？&lt;/h2&gt;
&lt;p&gt;Solrはschema.xmlにデータの定義（フィールドタイプやフィールドなど）を記述して、データを登録する全文検索システムです。
これまでのSolrではこの設定ファイルを元にデータを登録するフィールド名を決定しており、
変更を行う場合はSolrのコアを再起動するなどの手順が必要でした。（※ダイナミックフィールドはすこし特殊）&lt;/p&gt;
&lt;p&gt;それだと、Solrを管理するのがめんどくさいですね？という感じで現れたのが&lt;a href=&#34;http://wiki.apache.org/solr/SchemaRESTAPI&#34;&gt;SchemaREST API&lt;/a&gt;です。（たぶん。）&lt;/p&gt;
&lt;h2 id=&#34;schema-rest-api&#34;&gt;Schema REST API&lt;/h2&gt;
&lt;p&gt;Solr 4.2から導入されたSolrのスキーマに関する情報を提供するためのREST APIです。
4.2で導入されたのはあくまでもschema.xmlの情報を取得するためのAPIでした。
たとえば、Fieldの一覧を取得するとか。&lt;/p&gt;
&lt;p&gt;4.4から、フィールドの追加（変更、削除はできない）ができるようになりました。あくまでも、フィールドの追加で、フィールドタイプなどの追加はまだできません。（できるようになるのかもわからないですが。）
フィールドの追加方法などは&lt;a href=&#34;http://wiki.apache.org/solr/SchemaRESTAPI?highlight=%28managed%29#Adding_fields_to_a_schema&#34;&gt;Wiki&lt;/a&gt;に記載がありました。&lt;/p&gt;
&lt;p&gt;ということで、簡単に試してみることに。&lt;/p&gt;
&lt;h2 id=&#34;起動方法&#34;&gt;起動方法&lt;/h2&gt;
&lt;p&gt;exampleディレクトリの下にexample-schemalessというディレクトリが新設されています。
ここに、スキーマレスモード用の設定がされているファイルが入っているので、こちらを利用します。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd $SOLR/example
java -Dsolr.solr.home=example-schemaless/solr -jar start.jar
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ログにいくつかWARNが出ますが、影響の内パス設定ミスなので無視してOKです。&lt;/p&gt;
&lt;p&gt;最初に定義されているフィールドは「id」と「_version_」のみになります。（Schema Browserなどで確認できます。あ、REST APIでもいいですね。&lt;a href=&#34;http://localhost:8983/solr/schema/fields&#34;&gt;http://localhost:8983/solr/schema/fields&lt;/a&gt;）&lt;/p&gt;
&lt;h2 id=&#34;スキーマの更新&#34;&gt;スキーマの更新&lt;/h2&gt;
&lt;p&gt;さて、フィールドを追加してみます。
PUTを利用すると1フィールドの追加が可能です。
「fugatext」というフィールド名でフィールドを追加しています。今のところJSONのみ対応みたいです。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl -X PUT http://localhost:8983/solr/schema/fields/fugatext -H &amp;#39;Content-Type: application/json&amp;#39; -d &amp;#39;{&amp;#34;type&amp;#34;:&amp;#34;text_ja&amp;#34;,&amp;#34;stored&amp;#34;:false,&amp;#34;multiValued&amp;#34;:true}&amp;#39;
{
  &amp;#34;responseHeader&amp;#34;:{
    &amp;#34;status&amp;#34;:0,
    &amp;#34;QTime&amp;#34;:18}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;追加できたかどうかもREST APIで取得してみます。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl http://localhost:8983/solr/schema/fields
{
  &amp;#34;responseHeader&amp;#34;:{
    &amp;#34;status&amp;#34;:0,
    &amp;#34;QTime&amp;#34;:0},
  &amp;#34;fields&amp;#34;:[{
      &amp;#34;name&amp;#34;:&amp;#34;_version_&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;long&amp;#34;,
      &amp;#34;indexed&amp;#34;:true,
      &amp;#34;stored&amp;#34;:true},
    {
      &amp;#34;name&amp;#34;:&amp;#34;fugatext&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;text_ja&amp;#34;,
      &amp;#34;multiValued&amp;#34;:true,
      &amp;#34;stored&amp;#34;:false},
    {
      &amp;#34;name&amp;#34;:&amp;#34;id&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;string&amp;#34;,
      &amp;#34;multiValued&amp;#34;:false,
      &amp;#34;indexed&amp;#34;:true,
      &amp;#34;required&amp;#34;:true,
      &amp;#34;stored&amp;#34;:true,
      &amp;#34;uniqueKey&amp;#34;:true}]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;追加できました。
ちなみに、同じフィールド名を追加しようとするとエラーが帰ってきます。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl -X PUT http://localhost:8983/solr/schema/fields/fugatext -H &amp;#39;Content-Type: application/json&amp;#39; -d &amp;#39;{&amp;#34;type&amp;#34;:&amp;#34;text_ja&amp;#34;,&amp;#34;stored&amp;#34;:false,&amp;#34;multiValued&amp;#34;:true}&amp;#39;
{
  &amp;#34;responseHeader&amp;#34;:{
    &amp;#34;status&amp;#34;:400,
    &amp;#34;QTime&amp;#34;:1},
  &amp;#34;error&amp;#34;:{
    &amp;#34;msg&amp;#34;:&amp;#34;Field &amp;#39;fugatext&amp;#39; already exists.&amp;#34;,
    &amp;#34;code&amp;#34;:400}}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;設定の違い&#34;&gt;設定の違い&lt;/h2&gt;
&lt;p&gt;example-schemalessのsolrconfig.xmlは以下の設定が通常のexampleとは異なるようです。&lt;/p&gt;
&lt;h4 id=&#34;schemafactoryの設定&#34;&gt;schemaFactoryの設定&lt;/h4&gt;
&lt;p&gt;schemaをAPIから変更可能にする設定です。これまでの変更しない設定の場合は&lt;code&gt;ClassicIndexSchemaFactory&lt;/code&gt;を指定します。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;schemaFactory&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;class=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ManagedIndexSchemaFactory&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;bool&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mutable&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;true&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/bool&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;str&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;managedSchemaResourceName&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;managed-schema&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/str&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/schemaFactory&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;updatechainの設定&#34;&gt;update.chainの設定&lt;/h4&gt;
&lt;p&gt;更新処理（update関連のリクエストハンドラ「/update」とか）には次のような設定が追加されていました。（1006行目あたり）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  &amp;lt;requestHandler name=&amp;#34;/update&amp;#34; class=&amp;#34;solr.UpdateRequestHandler&amp;#34;&amp;gt;
    &amp;lt;!-- See below for information on defining 
         updateRequestProcessorChains that can be used by name 
         on each Update Request
      --&amp;gt;
    &amp;lt;lst name=&amp;#34;defaults&amp;#34;&amp;gt;
      &amp;lt;str name=&amp;#34;update.chain&amp;#34;&amp;gt;add-unknown-fields-to-the-schema&amp;lt;/str&amp;gt;
    &amp;lt;/lst&amp;gt;
  &amp;lt;/requestHandler&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;「add-unknown-fields-to-the-schema」というupdate.chainが指定されています。このchainの定義自体は1669行目くらいに存在します。
（長い。。。）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  &amp;lt;!-- Add unknown fields to the schema 
  
       An example field type guessing update processor that will
       attempt to parse string-typed field values as Booleans, Longs,
       Doubles, or Dates, and then add schema fields with the guessed
       field types.  
       
       This requires that the schema is both managed and mutable, by
       declaring schemaFactory as ManagedIndexSchemaFactory, with
       mutable specified as true. 
       
       See http://wiki.apache.org/solr/GuessingFieldTypes
    --&amp;gt;
  &amp;lt;updateRequestProcessorChain name=&amp;#34;add-unknown-fields-to-the-schema&amp;#34;&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.RemoveBlankFieldUpdateProcessorFactory&amp;#34;/&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.ParseBooleanFieldUpdateProcessorFactory&amp;#34;/&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.ParseLongFieldUpdateProcessorFactory&amp;#34;/&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.ParseDoubleFieldUpdateProcessorFactory&amp;#34;/&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.ParseDateFieldUpdateProcessorFactory&amp;#34;&amp;gt;
      &amp;lt;arr name=&amp;#34;format&amp;#34;&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ss.SSSZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ss,SSSZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ss.SSS&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ss,SSS&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ssZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ss&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mmZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm:ss.SSSZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm:ss,SSSZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm:ss.SSS&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm:ss,SSS&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm:ssZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm:ss&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mmZ&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd HH:mm&amp;lt;/str&amp;gt;
        &amp;lt;str&amp;gt;yyyy-MM-dd&amp;lt;/str&amp;gt;
      &amp;lt;/arr&amp;gt;
    &amp;lt;/processor&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.AddSchemaFieldsUpdateProcessorFactory&amp;#34;&amp;gt;
      &amp;lt;str name=&amp;#34;defaultFieldType&amp;#34;&amp;gt;text_general&amp;lt;/str&amp;gt;
      &amp;lt;lst name=&amp;#34;typeMapping&amp;#34;&amp;gt;
        &amp;lt;str name=&amp;#34;valueClass&amp;#34;&amp;gt;java.lang.Boolean&amp;lt;/str&amp;gt;
        &amp;lt;str name=&amp;#34;fieldType&amp;#34;&amp;gt;booleans&amp;lt;/str&amp;gt;
      &amp;lt;/lst&amp;gt;
      &amp;lt;lst name=&amp;#34;typeMapping&amp;#34;&amp;gt;
        &amp;lt;str name=&amp;#34;valueClass&amp;#34;&amp;gt;java.util.Date&amp;lt;/str&amp;gt;
        &amp;lt;str name=&amp;#34;fieldType&amp;#34;&amp;gt;tdates&amp;lt;/str&amp;gt;
      &amp;lt;/lst&amp;gt;
      &amp;lt;lst name=&amp;#34;typeMapping&amp;#34;&amp;gt;
        &amp;lt;str name=&amp;#34;valueClass&amp;#34;&amp;gt;java.lang.Long&amp;lt;/str&amp;gt;
        &amp;lt;str name=&amp;#34;valueClass&amp;#34;&amp;gt;java.lang.Integer&amp;lt;/str&amp;gt;
        &amp;lt;str name=&amp;#34;fieldType&amp;#34;&amp;gt;tlongs&amp;lt;/str&amp;gt;
      &amp;lt;/lst&amp;gt;
      &amp;lt;lst name=&amp;#34;typeMapping&amp;#34;&amp;gt;
        &amp;lt;str name=&amp;#34;valueClass&amp;#34;&amp;gt;java.lang.Number&amp;lt;/str&amp;gt;
        &amp;lt;str name=&amp;#34;fieldType&amp;#34;&amp;gt;tdoubles&amp;lt;/str&amp;gt;
      &amp;lt;/lst&amp;gt;
    &amp;lt;/processor&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.LogUpdateProcessorFactory&amp;#34;/&amp;gt;
    &amp;lt;processor class=&amp;#34;solr.RunUpdateProcessorFactory&amp;#34;/&amp;gt;
  &amp;lt;/updateRequestProcessorChain&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使ってるUpdateProcessorはこんな感じみたいです。最後の2つはこれ用じゃないので省略。&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;&lt;th&gt;プロセッサ名&lt;/th&gt;&lt;th&gt;説明&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;RemoveBlankFieldUpdateProcessorFactory&lt;/td&gt;&lt;td&gt;値がないフィールドは除去&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ParseBooleanFieldUpdateProcessorFactory&lt;/td&gt;&lt;td&gt;スキーマに定義されていないフィールドで、値がBooleanとしてパースできたら、Boolean型とする。&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ParseLongFieldUpdateProcessorFactory&lt;/td&gt;&lt;td&gt;スキーマに定義されていないフィールドで、値がLongとしてパースできたら、Long型とする。&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ParseDoubleFieldUpdateProcessorFactory&lt;/td&gt;&lt;td&gt;スキーマに定義されていないフィールドで、値がDoubleとしてパースできたら、Double型とする。&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ParseDateFieldUpdateProcessorFactory&lt;/td&gt;&lt;td&gt;スキーマに定義されていないフィールドで、値がDateとしてパースできたら、Date型とする。（パースの形式がformatで列挙されてる）&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;AddSchemaFieldsUpdateProcessorFactory&lt;/td&gt;&lt;td&gt;入力されたドキュメントの中でスキーマに定義されていないフィールド（静的、動的両方）を見つけた時に、フィールドの値の型を元にフィールド型をマッピングする。&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;とここまで見てきたところで、スキーマレスという名前の意図がちょっとわかったかも。&lt;/p&gt;
&lt;h2 id=&#34;定義されてないフィールドを持ったデータを登録&#34;&gt;定義されてないフィールドを持ったデータを登録&lt;/h2&gt;
&lt;p&gt;起動時には定義されてないフィールドをもったデータを登録してみます。
boolean型で試してみることに。以下のデータを管理画面のデータ登録画面から登録します。（http://localhost:8983/solr/#/collection1/documents）
（タイトルでbooleanってわかりにくいですが）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{&amp;#34;id&amp;#34;:&amp;#34;change.me&amp;#34;,&amp;#34;title&amp;#34;:true, &amp;#34;price&amp;#34;:1.25, &amp;#34;fuga&amp;#34;:&amp;#34;100,200&amp;#34;}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;エラーは出ません。で、またフィールド一覧を取得すると。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl http://localhost:8983/solr/schema/fields
{
  &amp;#34;responseHeader&amp;#34;:{
    &amp;#34;status&amp;#34;:0,
    &amp;#34;QTime&amp;#34;:1},
  &amp;#34;fields&amp;#34;:[{
      &amp;#34;name&amp;#34;:&amp;#34;_version_&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;long&amp;#34;,
      &amp;#34;indexed&amp;#34;:true,
      &amp;#34;stored&amp;#34;:true},
    {
      &amp;#34;name&amp;#34;:&amp;#34;fuga&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;tlongs&amp;#34;},
    {
      &amp;#34;name&amp;#34;:&amp;#34;fugatext&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;text_ja&amp;#34;,
      &amp;#34;multiValued&amp;#34;:true,
      &amp;#34;stored&amp;#34;:false},
    {
      &amp;#34;name&amp;#34;:&amp;#34;id&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;string&amp;#34;,
      &amp;#34;multiValued&amp;#34;:false,
      &amp;#34;indexed&amp;#34;:true,
      &amp;#34;required&amp;#34;:true,
      &amp;#34;stored&amp;#34;:true,
      &amp;#34;uniqueKey&amp;#34;:true},
    {
      &amp;#34;name&amp;#34;:&amp;#34;price&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;tdoubles&amp;#34;},
    {
      &amp;#34;name&amp;#34;:&amp;#34;title&amp;#34;,
      &amp;#34;type&amp;#34;:&amp;#34;booleans&amp;#34;}]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;おー、最後にtitleが追加されてます。他にもfugaやpriceも。（日付は手を抜きました。。。）&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;p&gt;詳細までは追いかけてないですが、こんなかんじです。
フィールド追加が可能になるのはいいんじゃないでしょうか。SolrCloudの機能との関連もあるのかもしれません。ZooKeeperへの出力も実装されてそうなので。&lt;/p&gt;
&lt;p&gt;ただ、機械的に出力されたschema.xml（exampleだとmanaged-schemaというファイル）には_「DO NOT EDIT」_との記述があるので、修正するとなにかおきてしまうかもしれないですねぇ。
現時点では、フィールドタイプの変更やフィールドの更新、削除に関してはSolrCoreの再起動などの手順が必要です。
あと、変なデータ（タイプミスとか）が登録されたりしないかってのは気になりますね。&lt;/p&gt;
&lt;p&gt;※ちなみに、別の人が気づいたんですが、ちょっとバグが有ったみたいで、代わりにチケットつくったらキリ番（&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-5000&#34;&gt;SOLR-5000&lt;/a&gt;）
ゲットしましたｗ&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Solrの管理画面でデータ登録</title>
      <link>https://blog.johtani.info/blog/2013/06/27/upload-docs-solr-admin/</link>
      <pubDate>Thu, 27 Jun 2013 16:28:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/27/upload-docs-solr-admin/</guid>
      <description>SolrのチケットをML経由で眺めてるんですが、便利そうなチケットが流れてきたのでブログを書いてみみようかと。 元になってるチケットはこちらで</description>
      <content:encoded>&lt;p&gt;SolrのチケットをML経由で眺めてるんですが、便利そうなチケットが流れてきたのでブログを書いてみみようかと。
元になってるチケットは&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-4921&#34;&gt;こちら&lt;/a&gt;です。昨日だか、今朝にtrunkとbranch_4xにコミットされたみたいです。試してみたい方は、branch_4xかtrunkをチェックアウトすると触ることができます。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;データ登録用の画面json&#34;&gt;データ登録用の画面（JSON）&lt;/h2&gt;
&lt;p&gt;branch_4xをチェックアウトしてexampleを起動し、Solrにアクセスします。&lt;br&gt;
管理画面に「Dcuments」という項目が追加されてます。開くとこんなかんじです。&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://blog.johtani.info/images/entries/20130626/add_default_page.jpg&#34; alt=&#34;デフォルトのデータ登録画面&#34;&gt;

&lt;/p&gt;
&lt;p&gt;なんと、デフォルトはJSONになってます。これも時代の流れでしょうかｗ&lt;br&gt;
Solrでは、これまで設定ファイルやデータ登録もXMLがメインになっていました。（&lt;a href=&#34;http://www.amazon.co.jp/gp/product/4774141755/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=4774141755&amp;amp;linkCode=as2&amp;amp;tag=johtani-22&#34;&gt;Apache Solr入門&lt;/a&gt;もXMLを基本に書いてます。このころはまだデフォルトでは対応してなかったので）&lt;/p&gt;
&lt;p&gt;登録するデータをテキストエリアに記述して、「Submit Document」をクリックすればデータは登録されます。基本的には単件登録の画面でしょうか。
（登録されたデータを確認するには「Query」画面を利用すればいいです。）
また、JSONのデータ形式は&lt;a href=&#34;http://wiki.apache.org/solr/UpdateJSON&#34;&gt;SolrのWiki&lt;/a&gt;を参照してください。&lt;/p&gt;
&lt;h3 id=&#34;csvやxmlも&#34;&gt;CSVやXMLも&lt;/h3&gt;
&lt;p&gt;この管理画面ではJSON以外の形式でもデータの登録が可能です。
「Document Type」の項目をクリックすると以下のように選択肢があられます。&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://blog.johtani.info/images/entries/20130626/select_document_type.jpg&#34; alt=&#34;Document Typeの選択&#34;&gt;

&lt;/p&gt;
&lt;p&gt;CSV、XMLについては、先ほどのJSONの画面の用に、テキストエリアが表示されます。
テキストエリアにCSV（データの形式は&lt;a href=&#34;http://wiki.apache.org/solr/UpdateCSV&#34;&gt;こちら&lt;/a&gt;）やXML（データの形式は&lt;a href=&#34;http://wiki.apache.org/solr/UpdateXmlMessages&#34;&gt;こちら&lt;/a&gt;）を入力してボタンを押せば登録できます。&lt;/p&gt;
&lt;h3 id=&#34;solr-command形式もjosnかxml&#34;&gt;Solr Command形式も（JOSNかXML）&lt;/h3&gt;
&lt;p&gt;Solr Command というのはXMLやJSONで登録、コミット、削除などを実行するための画面になります。
JSONのコマンドは&lt;a href=&#34;http://wiki.apache.org/solr/UpdateJSON#Update_Commands&#34;&gt;こちら&lt;/a&gt;、XMLのコマンドは&lt;a href=&#34;http://wiki.apache.org/solr/UpdateXmlMessages&#34;&gt;こちら&lt;/a&gt;をご覧ください。&lt;/p&gt;
&lt;p&gt;あと、便利なのがファイルアップロードです。
こんなかんじで、ファイルを選んでSubmitすればデータが登録出来ます。&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://blog.johtani.info/images/entries/20130626/file_upload.jpg&#34; alt=&#34;File Uploadの画面&#34;&gt;

&lt;/p&gt;
&lt;p&gt;ファイルのサイズが大きいとちょっと時間がかかりますが、コマンドを打つより簡単かもしれません。
post.jarツールと違って、デフォルトでコミットをしてくれるわけではないので、「Extracting Req. Handler Params」に「commit=true」をつけないと、データが登録されてない？と思ってしまうかもしれませんが。&lt;/p&gt;
&lt;h3 id=&#34;組立もできるみたいdocument-builder&#34;&gt;組立もできるみたい（Document Builder）&lt;/h3&gt;
&lt;p&gt;最後に紹介するのが「Document Builder」というタイプです。&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://blog.johtani.info/images/entries/20130626/document_builder_desc.jpg&#34; alt=&#34;Document Builderの画面説明&#34;&gt;

&lt;/p&gt;
&lt;p&gt;もっと簡易にデータを記述できるようにということで用意されているようです。
フィールドの情報はSolrに接続して利用できるフィールド？（ダイナミックはないのかな？）が表示されます。&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://blog.johtani.info/images/entries/20130626/document_builder_fields.jpg&#34; alt=&#34;Document Builderでフィールド表示&#34;&gt;

&lt;/p&gt;
&lt;p&gt;追記していくとこんなかんじになります。&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://blog.johtani.info/images/entries/20130626/document_builder_.jpg&#34; alt=&#34;出来上がったドキュメント&#34;&gt;

&lt;/p&gt;
&lt;p&gt;日本語のデータもちゃんと登録できました。
ただ、まだ、開発中なんでしょうがないかもしれませんが、以下の様な制約があるようです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multiValuedなフィールドに値を追加できない（上書きされる）&lt;/li&gt;
&lt;li&gt;改行が入ったデータをテキストエリアにいれると「Add Field」を押しても反応しない&lt;/li&gt;
&lt;li&gt;ダイナミックフィールドは自分で書きましょう&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ただ、これまでXMLでファイルを作ってコマンドで登録したり、curlコマンドでJSON書いたりして登録していたよりはお手軽にさわれるようになるかと思います。つぎの4x系のバージョンが出たときはこちらからデータを登録してみてください。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Basho Japanに遊びに行きました</title>
      <link>https://blog.johtani.info/blog/2013/06/19/visited-basho/</link>
      <pubDate>Wed, 19 Jun 2013 10:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/19/visited-basho/</guid>
      <description>ちょくちょく書こうと言いながら、前の記事が1週間以上前になってる。。。 昨日は、Basho Japanに遊びに行って来ました。 （Riak触ったこ</description>
      <content:encoded>&lt;p&gt;ちょくちょく書こうと言いながら、前の記事が1週間以上前になってる。。。&lt;/p&gt;
&lt;p&gt;昨日は、Basho Japanに遊びに行って来ました。
（Riak触ったことないのに。。。Erlangも。。。ゴメンナサイ）&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;RiakにSolrを組み合わせたYokozunaというものの名前を最近耳にしていたので、どんなものなのかなぁと興味がありまして。Solrがどんな使い方をされているのかってのが気になったので、
情報交換したいなぁと思っていたところ、Vの人が調整してくれたので色々と有意義な話ができたかなぁと。
（Yokozunaについての最新のスライドは&lt;a href=&#34;https://speakerdeck.com/rzezeski/yokozuna-scaling-solr-with-riak&#34;&gt;Berlin Buzzword 2013のものがここに&lt;/a&gt;）
Twitter上で見かけたことのある方々と話ができたり面白かったです。（やっぱ英語で会話できたりスラスラと読めるの必要だよなぁと痛感したりもしました。。。）&lt;/p&gt;
&lt;p&gt;ということで、遊びに行ったのに美味しいピザやこんなおみやげまでもらってしまいました。
（ピザの写真撮るの忘れてたw）&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20130619/riak-goods.jpg&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20130619/riak-goods.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption align=&#34;center&#34;&gt;&lt;h4&gt;Riak＆Bashoグッズ&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;ちなみに、Yokozunaですが、Riakに登録したデータを裏で起動しているSolにデータを流しこんでくれるものになります。
Solrの機能としては分散検索（Distributed Search）と呼ばれる仕組みを利用しているようです。
YokozunaのI/Fとしては、Solrのインデックスの分散構成は隠してくれていて、かつ、Solr（っぽい？）リクエストを投げれば裏の分散構成に問い合わせた結果をSolrのレスポンスの形で返してくれます。
KVSに全文検索の機能がついてくるお得感が満載な気がしますw。&lt;/p&gt;
&lt;p&gt;Riak自身のデータの取り扱いがどんなものかをまだちゃんと理解していないので（ゴメンナサイ。&lt;a href=&#34;http://littleriakbook.com&#34;&gt;Little Riak Book&lt;/a&gt;は開いてるんですが読んでなくて。。。）またおじゃましてもう少し情報交換したいかなぁとｗ。&lt;/p&gt;
&lt;p&gt;Cloudera Searchといい、Yokozunaといい、Solrを利用したものが少しずつ増えてきて嬉しい限りです。
Solrの作りがしっかりしている？活発？、だから取り込む形が多いんですかねぇ。
Solr本を書いてから数年たちますが、やっと検索のニーズが出てきたのかもしれないなぁと思ってみたり。
（流れのつながりはあまりないですが）ElasticSearchも少しずつ人気が出てきてるし、日本語の本とかのニーズあったりするかなぁ？&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>新しいsolr.xmlとCore探索ロジック</title>
      <link>https://blog.johtani.info/blog/2013/06/11/new-solr-xml/</link>
      <pubDate>Tue, 11 Jun 2013 19:11:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/11/new-solr-xml/</guid>
      <description>Lucene/Solr 4.3.1のRCのVoteが始まっていますが、そのMLできになったコトがあり、ちょっと調べたので メモを残しておきます。 マルチコアの設定ファ</description>
      <content:encoded>&lt;p&gt;Lucene/Solr 4.3.1のRCのVoteが始まっていますが、そのMLできになったコトがあり、ちょっと調べたので
メモを残しておきます。&lt;/p&gt;
&lt;p&gt;マルチコアの設定ファイルであるsolr.xmlの記述方法と、コアの探索ロジックが4.4（実装的には4.3から入っている）から変更されるようです。4.x系の最新版である、branch_4xのexampleディレクトリにあるsolr.xmlも新しい記述に変更されていました。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h3 id=&#34;参考url&#34;&gt;参考URL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/Core%20Discovery%20%284.4%20and%20beyond%29&#34;&gt;新しいCore探索（4.4以降）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/Solr.xml%204.4%20and%20beyond&#34;&gt;新しいsolr.xml（4.4以降）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/Solr.xml%20%28supported%20through%204.x%29&#34;&gt;4.3までのsolr.xml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ちなみに、最後のold styleと呼ばれる4.3までの記述方法はつぎの5.0ではDeprecatedになるようです。（5.0がいつ出るのかはわからないですが。）&lt;/p&gt;
&lt;h2 id=&#34;core探索ロジック&#34;&gt;Core探索ロジック&lt;/h2&gt;
&lt;p&gt;4.4から、$SOLR_HOMEディレクトリ以下の探索ロジックは次のようになるようです。
以下では、「新スタイル」（4.4以降の書式）、「旧スタイル」（4.3以下の書式）として記述します。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;solr.xmlファイルの存在チェック
&lt;ol&gt;
&lt;li&gt;solr.xmlが存在しない場合→旧スタイルとして処理→3へ（旧スタイル）&lt;/li&gt;
&lt;li&gt;solr.xmlが存在し&lt;code&gt;&amp;lt;cores&amp;gt;&lt;/code&gt;タグが存在しない場合→2へ（新スタイル）&lt;/li&gt;
&lt;li&gt;solr.xmlが存在し&lt;code&gt;&amp;lt;cores&amp;gt;&lt;/code&gt;タグが存在する場合→3へ（旧スタイル）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;新スタイルのロジック
&lt;ol&gt;
&lt;li&gt;SOLR_HOMEディレクトリに存在するディレクトリについて以下の処理を繰り返す&lt;/li&gt;
&lt;li&gt;SOLR_HOME/ディレクトリ/core.propertiesファイルが存在する→後続処理へ。存在しなければ終了&lt;/li&gt;
&lt;li&gt;SOLR_HOME/ディレクトリ/conf/solrconfig.xmlを読み込み、コアを起動&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;旧スタイルのロジック
&lt;ol&gt;
&lt;li&gt;これまで同様、solr.xmlの&lt;code&gt;&amp;lt;core&amp;gt;&lt;/code&gt;タグの記載内容を元にコアを起動（instanceDir以下のconf/solrconfig.xmlを使って）&lt;/li&gt;
&lt;li&gt;solr.xmlが存在しない場合はSOLR_HOME/collection1/conf/solrconfig.xmlが存在するものとしてコアを起動&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;このようなロジックになります。&lt;/p&gt;
&lt;p&gt;ちなみに、以下の場合はエラーとなりSolrは起動しますがログや管理画面にエラーである表示がされます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.3でsolrconfig.xmlが見つけられなかった場合&lt;/li&gt;
&lt;li&gt;3.1で&lt;code&gt;&amp;lt;core&amp;gt;&lt;/code&gt;タグが存在しなかった場合（この場合、ログにはエラーが出ません）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;propertiesに記述できる内容やsolr.xmlの記述内容については、Wikiを見てもらうということで。。。
CoreAdminHandlerでコアを生成したりした場合に、新スタイルの設定がどのように出力されるのかといった点が気になりますが、また今度にでも。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera Searchメモ（妄想版）</title>
      <link>https://blog.johtani.info/blog/2013/06/06/cloudera-search-memo2/</link>
      <pubDate>Thu, 06 Jun 2013 12:26:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/06/cloudera-search-memo2/</guid>
      <description>ざっとインストールガイドとかCloudera Searchのソース眺めて、テキトーにメモを書いてみました。 （ユーザガイドはまだ読んでないです。</description>
      <content:encoded>&lt;p&gt;ざっとインストールガイドとかCloudera Searchのソース眺めて、テキトーにメモを書いてみました。
（ユーザガイドはまだ読んでないです。）&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;ざっくりメモ&#34;&gt;ざっくりメモ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ストリーム処理でインデックス作るときはFlume経由でSolrに
&lt;ul&gt;
&lt;li&gt;SinkとEventの両方が用意されてる？（Flumeを良く知らないので、違いがわからない）&lt;/li&gt;
&lt;li&gt;FluemeからはリモートのSolrに対してインデックス登録するクラスがある。SolrServerDocumentLoaderがソレだと思う。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;バッチ処理でインデックス作るときはMapReduceIndexerToolsってのを使ってSolrに
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-1301&#34;&gt;SOLR-1301&lt;/a&gt;がベースになっている。色々と改良されてるようだけど、コアとなってる処理はSOLR-1301。&lt;/li&gt;
&lt;li&gt;GoLiveってクラスの処理の中で、現在動作してるSolrに配布したバッチで作成されたIndexをマージする処理が書いてある。&lt;/li&gt;
&lt;li&gt;HDFSへ出力されたインデックスはリモートのSolrからアクセスするとオーバヘッドとかどーなるのかなぁ？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;検索処理自体はHueでもできるけど、基本的にSolrCloud任せ&lt;/li&gt;
&lt;li&gt;インデキシングの処理のフローについてはCloudera Mrophlinesで定義&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ということで、
2つの流れがありそう。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HDFS→Flume→Solr&lt;/li&gt;
&lt;li&gt;HDFS→MapReduce→Solr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;で、まだ、わかってないですが、構成要素として&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop（HDFS）：データソース&lt;/li&gt;
&lt;li&gt;Hadoop（MapReduce）：データ変換処理、バッチインデキシング&lt;/li&gt;
&lt;li&gt;Zookeeper：SolrCloudのクラスタ管理&lt;/li&gt;
&lt;li&gt;Solr：インデキシング、検索エンジン&lt;/li&gt;
&lt;li&gt;Flume：データをストリーミングでSolrへ&lt;/li&gt;
&lt;li&gt;Coudera Morphlines：HDFSからSolrまでのETLデータ処理を定義実行する環境&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;って感じでしょうか。
SolrCloudのクラスタとHadoopのクラスタが同一マシン上なのか、別なのかとか。組み合わせがどんなものができるのかがまだわかってないです。
ユーザガイド読んでみたらなにか出てくるかなぁ。&lt;/p&gt;
&lt;p&gt;ちなみに、Cloudera SearchのgithubリポジトリにあるソースはCloudera Morphlinesのコードがメインで、SolrのHDFS対応版のソースがあるわけでは無かったです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SolrのHdfsDirectoryってのがClouderaのリポジトリにあるSolrには追加されていて、これが、HDFSのインデックスを読み込んだりする処理が出来る仕組みっぽい。&lt;/li&gt;
&lt;li&gt;一応、SolrCloud以外（分散検索）も考慮された形になってるっぽい。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ってとこでしょうか。&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;p&gt;読んでて思ったんですが、Cloudera Searchの肝はじつは、検索じゃなくて、Morphlinesにあるんじゃないかなぁと。今はSolrが出力先ですが、
その他のデータ変換処理とかが増えてくると、処理の流れがMorphlinesで定義できてデータ変換処理が簡便になりそうな気が。&lt;/p&gt;
&lt;h2 id=&#34;その他に気になる観点&#34;&gt;その他に気になる観点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CDH経由でSolrCloudのクラスタの管理するのかな？&lt;/li&gt;
&lt;li&gt;SolrCloud用のクラスタとCDHのクラスタって同一マシンに載るの？別マシンにもできるの？
&lt;ul&gt;
&lt;li&gt;併存したらIOがキツそうだけど&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hueで検索アプリとか組めるの？（そもそもHueがわかってないんだけど。。。）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ま、とりあえず、こんなとこで。
つぎは余力があれば、ユーザガイドかなぁ。
英語力。。。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera Searchのモジュールたち</title>
      <link>https://blog.johtani.info/blog/2013/06/05/cloudera-search-modules/</link>
      <pubDate>Wed, 05 Jun 2013 15:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/05/cloudera-search-modules/</guid>
      <description>Cloudera Searchは次のようなモジュールから構成されています。 これはCloudera Searchのモジュールで、さらにこれらがSolrとかを使っ</description>
      <content:encoded>&lt;p&gt;Cloudera Searchは次のようなモジュールから構成されています。
これはCloudera Searchのモジュールで、さらにこれらがSolrとかを使ってるみたいですね。pom.xmlを見たら何を使ってるかがわかるかな。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cdk-morphlines&lt;/li&gt;
&lt;li&gt;search-contrib&lt;/li&gt;
&lt;li&gt;search-core&lt;/li&gt;
&lt;li&gt;search-flume&lt;/li&gt;
&lt;li&gt;search-mr&lt;/li&gt;
&lt;li&gt;search-solrcell&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;てきとーに、README.mdみながらメモを残してみました。ソースとかはまだ読んでないです。
ざっと眺めたけど、インデキシング処理の話がメインで、検索側がどうやって動くかってのがわからなかったなぁ。
&lt;a href=&#34;http://www.cloudera.com/content/cloudera-content/cloudera-docs/Search/latest/PDF/Cloudera-Search-User-Guide.pdf&#34;&gt;ユーザガイド（注：PDF）&lt;/a&gt;ってのがあるから、これを読んでみるか。。。&lt;/p&gt;
&lt;p&gt;各モジュールについては、以下。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;cdk-morphlinescloudera-morphlines&#34;&gt;cdk-morphlines（Cloudera Morphlines）&lt;/h2&gt;
&lt;p&gt;Cloudera Morphlinesという名前みたい。
検インデキシングアプリの構築、変更をラクにするためのフレームワーク。
ETLの処理チェインを簡単にCloudera Searchにデータを入れる設定（Extract/Transform/Load処理）がかけると。
バッチ処理、Near Real Timeのために使えるみたい。検索結果をさらに入れるとかもできるんかなぁ。？&lt;/p&gt;
&lt;p&gt;Unixパイプラインのの進化版みたいなもので、一般的なレコードに対するStream処理から、Flueme、MapReduce、Pig、Hie、SqoopのようなHadoopコンポーネントも使えるみたい。&lt;/p&gt;
&lt;p&gt;Hadoop ETLアプリケーションのプロトタイピングにも使えて、リアルタイムで複雑なStreamやイベント処理やログファイル解析とかに使えるの？&lt;/p&gt;
&lt;p&gt;設定ファイルのフォーマットは&lt;a href=&#34;https://github.com/typesafehub/config/blob/master/HOCON.md&#34;&gt;HOCONフォーマット&lt;/a&gt;。AkkaやPlayで使われてる。&lt;/p&gt;
&lt;h3 id=&#34;cdk-morphlines-core&#34;&gt;cdk-morphlines-core&lt;/h3&gt;
&lt;p&gt;Cloudera Morphlinesのコンパイラ、実行環境、コマンドのライブラリを含んでる。
ログファイル解析やsingle-lineレコード、multi-lineレコード、CSVファイル、正規表現パターンマッチ、フィールドごとの比較とか条件分岐とか、文字列変換とか色々なコマンドを含んでる。&lt;/p&gt;
&lt;h3 id=&#34;cdk-morphlines-avro&#34;&gt;cdk-morphlines-avro&lt;/h3&gt;
&lt;p&gt;Avroファイルやオブジェクトの抽出、変換、読み込み処理コマンド&lt;/p&gt;
&lt;h3 id=&#34;cdk-morphlines-tika&#34;&gt;cdk-morphlines-tika&lt;/h3&gt;
&lt;p&gt;バイナリデータからMIMEタイプを検出して、解凍するコマンド。Tikaに依存&lt;/p&gt;
&lt;h4 id=&#34;雑感&#34;&gt;雑感&lt;/h4&gt;
&lt;p&gt;Cloudera Searchへのデータの流し込みを設定ファイルに記述して実行するとデータの変換処理とかが記述できるって感じかな？
Morphlinesのコマンドとして独自処理や使えそうな処理を作ることで、いろんな処理ができるって感じかなぁ。&lt;/p&gt;
&lt;h2 id=&#34;search-core&#34;&gt;search-core&lt;/h2&gt;
&lt;p&gt;Solrに対するMorphlineコマンドの上位モジュール&lt;/p&gt;
&lt;h3 id=&#34;search-solrcell&#34;&gt;search-solrcell&lt;/h3&gt;
&lt;p&gt;Tikaパーサを使ったSolrCellを使うためのMorphlineコマンド。
HTML、XML、PDF、Wordなど、Tikaがサポートしてるものがサポート対象。&lt;/p&gt;
&lt;h3 id=&#34;search-flume&#34;&gt;search-flume&lt;/h3&gt;
&lt;p&gt;Flueme Morphline Solr Sink。
Apache Flumeのイベントから検索ドキュメントを抽出、変換し、SolrにNearRealTimeで読み込むためのコマンド&lt;/p&gt;
&lt;h3 id=&#34;search-mr&#34;&gt;search-mr&lt;/h3&gt;
&lt;p&gt;HDFSに保存されたファイルに含まれる大量データをMapReduceで処理してHDFS上の検索インデックスに焼きこむモジュール。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;MapReduceIndexerTool&lt;/code&gt;は入力ファイルの集合からSolrのインデックスシャードの集合を作るためのmorphlineのタスクで、MapReduceのバッチジョブドライバー。
HDFSにインデックスを書き込む。
動作してるSolrサーバに対して出力されたデータをマージするのもサポートしてる。&lt;/p&gt;
&lt;p&gt;とりあえず、Near Real Time検索するにはFlueme使って、バッチ処理でインデックス焼くのはMapReduceIndexerToolみたいだなぁ。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Cloudera Searchってのが出たらしい（とりあえず、雑感？）</title>
      <link>https://blog.johtani.info/blog/2013/06/05/what-is-cloudera-search/</link>
      <pubDate>Wed, 05 Jun 2013 15:05:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/06/05/what-is-cloudera-search/</guid>
      <description>AWS Summitに来ていたのですが、TLでは、Cloudera Searchが賑わってました。 ということで、軽くどんなものか読んだり調べたりした</description>
      <content:encoded>&lt;p&gt;AWS Summitに来ていたのですが、TLでは、Cloudera Searchが賑わってました。
ということで、軽くどんなものか読んだり調べたりしたメモを残しとこうかと。
英語力はあやしいので、おかしいとこがあったらツッコミを。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;cloudera-searchとは&#34;&gt;Cloudera Searchとは？&lt;/h2&gt;
&lt;p&gt;CDH4.3に対応したCDHユーザ向けの検索システム（beta版）なのかな？
CDHに統合された検索フレームワークなのかな？&lt;/p&gt;
&lt;p&gt;基本はLucene/Solr 4.3でHadoopのペタバイトデータを検索することができるようになるみたいです。&lt;/p&gt;
&lt;h2 id=&#34;どんな仕組み&#34;&gt;どんな仕組み？&lt;/h2&gt;
&lt;p&gt;次のものを利用しているようです。（GithubのREADMEから。）&lt;/p&gt;
&lt;h4 id=&#34;使ってるもの&#34;&gt;使ってるもの&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Apache Solr(4.3.0＋α？)
&lt;ul&gt;
&lt;li&gt;Apache Lucene（Solrつかってるからね）&lt;/li&gt;
&lt;li&gt;Apache SolrCloud（うーん、Solrに含まれるのに別に出してるのなんで？）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apache Flume&lt;/li&gt;
&lt;li&gt;Apache Hadoop MapReduce &amp;amp; HDFS&lt;/li&gt;
&lt;li&gt;Apache Tika
&lt;ul&gt;
&lt;li&gt;SolrCellとしてSolrにも組み込まれてる、いろんな文書（WordとかHTMLなどなど）からメタデータと本文データとかを取り出せるライブラリラッパー。実際にはさらにpdfboxとかを使って各文書からのデータを取り出してる。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;何ができるの&#34;&gt;何ができるの？&lt;/h4&gt;
&lt;p&gt;HBaseやHDFSの用にZookeeperを使ってインデックスのシャーディングや高可用性ができる。（SolrCloudがZookeeperを使ってるからね。）
MapReduceのジョブの出力から自動的にSolrのインデックスにデータをマージできるらしい。
Cloudera Managerを使って、デプロイ、設定モニタリングなどが可能。&lt;/p&gt;
&lt;p&gt;Flumeのフィードをつかって、ストリーミングしてインデックスを作れる。FluemeがデータをSolrに流しこむのかな？
将来的にはHiveやHBaseのテーブルをインデックスすることも可能になるらしい。Impalaクエリの結果もフィードできるのか？&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://incubator.apache.org/blur/how_it_works.html&#34;&gt;Apache Blur&lt;/a&gt;ってキーワードも出てきた。HDFSのデータからLuceneのインデックス作るのかな？
NGDataのチームがSolr/HBaseの統合とかしてるみたい。&lt;/p&gt;
&lt;h3 id=&#34;参考url&#34;&gt;参考URL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.cloudera.com/blog/2013/06/cloudera-search-the-newest-hadoop-framework-for-cdh-users-and-developers/&#34;&gt;Cloudera社のブログ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cloudera.com/content/cloudera-content/cloudera-docs/Search/latest/PDF/Cloudera-Search-Frequently-Asked-Questions.pdf&#34;&gt;Cloudera SearchのFAQ（注：PDF）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudera/search&#34;&gt;Githubのリポジトリ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Solr4.3.0のChangesを訳してみた。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/25/solr4-3-0%E3%81%AEchanges%E3%82%92%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</link>
      <pubDate>Thu, 25 Apr 2013 11:14:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/25/solr4-3-0%E3%81%AEchanges%E3%82%92%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</guid>
      <description>まだ、Vote公開されていない、Solr 4.3（2013/04/25 11:00現在）ですが、 ひさびさに訳してみた。詳細まで追っていないので、</description>
      <content:encoded>&lt;p&gt;&lt;strong&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;まだ、Vote公開されていない、Solr 4.3（2013/04/25 11:00現在）ですが&lt;/span&gt;、&lt;/strong&gt;
ひさびさに訳してみた。詳細まで追っていないので、誤訳があるかもしれないですが。
おかしいとこあったらツッコミを。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;○Solr 4.3.0のChanges
　・Upgrading from Solr 4.2.0
　　1.schema REST APIのcopyFields、dynamicFieldsの出力パスをCamelCaseに。他も同様。（&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4623&#34;&gt;SOLR-4623&lt;/a&gt;）
　　2.Slf4j/logging jarをSolrのwarに含めないことに。すべてのlogging jarはexample/lib/extに。（&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-3706&#34;&gt;SOLR-3706&lt;/a&gt;、&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4651&#34;&gt;SOLR-4651&lt;/a&gt;）
　　3.SolrCloudでハードコードされてたhostContextとhostPortをdeprecatedに。Solr5.0で削除する。（&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4622&#34;&gt;SOLR-4622&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;　・New Features
　　1.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4648&#34;&gt;SOLR-4648&lt;/a&gt;　PreAnalyzedUpdateProcessorFactoryでPreAnalyzedFieldの機能をほかのフィールドタイプでも使えるようにした。詳しくはJavadocとexampleを見て。
　　2.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4623&#34;&gt;SOLR-4623&lt;/a&gt;　REST APIで現在のschemaのエレメントをすべて読めるように。REST APIの返却の形式として、XMLとJSONとschema.xmlの形式を追加REST APIのパッケージを変更。
　　　クラス名も変更しschemaにフォーカスした機能も除去。今後のschema以外のREST APIのために。
　　　copyFieldsとdynamicFieldsの出力パスをすべてlowercaseのものからCamelCaseに変更。他のREST APIも同様。
　　3.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4658&#34;&gt;SOLR-4658&lt;/a&gt;　REST APIリクエストでschemaを変更できるようにするために、「managed schema」を導入。solrconfig.xmlに「&amp;lt;schemaFactory class=&amp;ldquo;ManagedSchemaFactory&amp;rdquo; mutable=&amp;ldquo;true&amp;rdquo;/&amp;gt;」を追加。REST APIリクエストでスキーマ変更が可能にするために。
　　4.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4656&#34;&gt;SOLR-4656&lt;/a&gt;　2つのハイライトパラメータ（hl.maxMultiValuedToMatch、hl.maxMultiValuedToExamine）を追加。
　　　hl.maxMultiValuedToMatchは指定された数のsnippetが見つかったらそれ以降の探索を停止する設定。multiValuedフィールドがどんなに離れてても探索する。
　　　hl.maxMultiValuedToExamineは指定された数のmultiValuedのエントリ数を調査したら探索を停止する設定。
　　　両方を指定した場合、最初のlimitに達したら停止する。ドキュメント全体をハイライトするためにコピーされるのを削減する。これらの最適化はmultiValuedフィールドに大量のエントリが存在する時に効く。。。
　　5.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4675&#34;&gt;SOLR-4675&lt;/a&gt;　PostingsSolrHighlighterでper-field/クエリ次のパラメータ指定のサポート
　　6.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-3755&#34;&gt;SOLR-3755&lt;/a&gt;　既存のshardを動的にsplitしてshardを追加するための新コレクションAPI（shard splitting）
　　7.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4530&#34;&gt;SOLR-4530&lt;/a&gt;　DIH：TikaのIdentityHtmlMapperを使う設定の提供
　　8.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4662&#34;&gt;SOLR-4662&lt;/a&gt;　solr.xmlにあるSolrCoreの定義よりもディレクトリ構造で見つける。また、solr.xmlのフォーマットを変えて、solrconfig.xmlに近くする。Solrのこのバージョンは旧スタイルの例で提供するが、新しいスタイルも試すことができる。Solr 4.4では、新しいスタイルで提供し、Solr 5.0では旧スタイルは廃止する予定。
　　　&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4347&#34;&gt;SOLR-4347&lt;/a&gt;　Adminハンドラで新しく生成されたコアがsolr.xmlに永続化される
　　　&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-1905&#34;&gt;SOLR-1905&lt;/a&gt;　Adminリクエストハンドラで生成されたコアもsolr.xmlに永続化される。また、solr.solr.datadirのようなプロパティの用にsolr.xmlに永続化される問題のfix。
　　9.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4717&#34;&gt;SOLR-4717&lt;/a&gt;/SOLR-1351　SimpleFacetで同じフィールドに異なるファセットを適用出来るlocalParamsを追加
　　10.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4671&#34;&gt;SOLR-4671&lt;/a&gt;　CSVResponseWriterのpseudoフィールドのサポート
　　11.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4358&#34;&gt;SOLR-4358&lt;/a&gt;　HttpSolrServerでuseMultiPartPostでstream名を送信できる
　・Bug Fixes
　　1.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4543&#34;&gt;SOLR-4543&lt;/a&gt;：solr.xml/solr.propertiesでshardHandlerFactoryの設定が動作しない
　　2.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4634&#34;&gt;SOLR-4634&lt;/a&gt;：Java 8&amp;quot;Nashorn&amp;quot;JavaScript実装の動作に関するscripting engineのテストのfix
　　3.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4636&#34;&gt;SOLR-4636&lt;/a&gt;：SolrIndexSearcherをオープンする時に何かの理由でreaderがオープンできない時に、ディレクトリがリリースされない
　　4.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4405&#34;&gt;SOLR-4405&lt;/a&gt;：Admin UIのadmin-extraファイルでcore-menuが表示されない
　　5.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-3956&#34;&gt;SOLR-3956&lt;/a&gt;：group.facet=trueでfacet.limitがマイナスの時の動作
　　6.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4650&#34;&gt;SOLR-4650&lt;/a&gt;：copyFieldでダイナミックフィールドや暗黙的なフィールドがsourceでマッチしない。4.2で入ったバグ
　　7.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4641&#34;&gt;SOLR-4641&lt;/a&gt;：Schemaで、illegalなフィールドパラメータで例外が発生するようにする。
　　8.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-3758&#34;&gt;SOLR-3758&lt;/a&gt;：SpellCheckComponentが分散groupingで動作しない。
　　9.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4652&#34;&gt;SOLR-4652&lt;/a&gt;：solr.xmlプラグインのresource loaderで共有ライブラリの挙動がおかしい
　　10.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4664&#34;&gt;SOLR-4664&lt;/a&gt;：ZkStateReaderがaliasを更新しても見えない
　　11.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4682&#34;&gt;SOLR-4682&lt;/a&gt;：CoreAdminRequest.mergeIndexが複数コアやindexDirが複数の場合にマージできない
　　12.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4581&#34;&gt;SOLR-4581&lt;/a&gt;：Solr4.2で数値フィールドのファセットでマイナスの値があるとソートがおかしい
　　13.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4699&#34;&gt;SOLR-4699&lt;/a&gt;：Admin Handlerでデータディレクトリの場所がファイルシステムだと思い込んでる。（RAMの場合もある）
　　14.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4695&#34;&gt;SOLR-4695&lt;/a&gt;：non-cloudセットアップでもコア管理のSPLITが使えるように
　　15.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4680&#34;&gt;SOLR-4680&lt;/a&gt;：exampleのspellcheck設定のqueryAnalyzerFieldTypeの修正
　　16.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4702&#34;&gt;SOLR-4702&lt;/a&gt;：exampleの/browseの「Did you mean?」のサジェストをFix
　　17.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4710&#34;&gt;SOLR-4710&lt;/a&gt;：Zookeeperから全ノードをアップせずにコレクションを削除できないのを修正
　　18.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4487&#34;&gt;SOLR-4487&lt;/a&gt;：HttpSolrServerからのSolrExceptionがリモートのサーバから戻るHTTPステータスコードを含んでない
　　19.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4661&#34;&gt;SOLR-4661&lt;/a&gt;：Admin UIのレプリケーションで現在のレプリカ可能なマスタのバージョンを正確に表示
　　20.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4716&#34;&gt;SOLR-4716&lt;/a&gt;,&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4584&#34;&gt;SOLR-4584&lt;/a&gt;：SolrCloudリクエストプロキシがTomcatなどJetty出ないコンテナで動作していない
　　21.&lt;a href=&#34;http://issues.apache.org/jira/browse/SOLR-4746&#34;&gt;SOLR-4746&lt;/a&gt;：Distributed groupingのトップレベルグループコマンドでSimpleOrderedMapの代わりにNamedListを使う。non-distributed groupingと出力形式が異なるため。
　　&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Lucene 4.3.0のChangesにあるChanges in backwards compatibility policyが気になったので訳してみた。(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/04/24/lucene-4-3-0%E3%81%AEchanges%E3%81%AB%E3%81%82%E3%82%8Bchanges-in-backwards-compatibility-policy%E3%81%8C%E6%B0%97%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%AE%E3%81%A7%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</link>
      <pubDate>Wed, 24 Apr 2013 16:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/04/24/lucene-4-3-0%E3%81%AEchanges%E3%81%AB%E3%81%82%E3%82%8Bchanges-in-backwards-compatibility-policy%E3%81%8C%E6%B0%97%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%AE%E3%81%A7%E8%A8%B3%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/</guid>
      <description>現在、RC3のVoteをやっている最中（2013/04/24 16:00時点）で、まだリリースされていない、4.3.0についてです。 開発者ML</description>
      <content:encoded>&lt;p&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;現在、RC3のVoteをやっている最中（2013/04/24　16:00時点）で、まだリリースされていない、4.3.0についてです。&lt;/span&gt;
開発者MLでChangesの書き方を考えないとね、みたいなエントリーが流れてて気になっていたので、訳してみた。
lucene-gosenの実装を変更しないといけないっぽいなぁ。Lucene/Solr 4.2.1以前と4.3.0でI/Fとかが変わることになりそうです。（3.とか8.とか）
（ここで力尽きて、それより下はまだ読んでないです。。。）&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;○Changes in backwards compatibility policy
　　1.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4810&#34;&gt;LUCENE-4810&lt;/a&gt;：EdgeNGramTokenFilterが同じ入力tokenから複数のngramを生成した時にpositionを増加させていないのを修正
　　2.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4822&#34;&gt;LUCENE-4822&lt;/a&gt;：KeywordMarkerFilterがabstractクラスで、サブクラスがisKeyword()メソッドを実装する必要がある。新しく、SetKeywordTokenFilterというクラスにすでにある機能を分解した。
　　3.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4642&#34;&gt;LUCENE-4642&lt;/a&gt;：TokenizerとサブクラスのAttributeSourceのコンストラクタを削除。代わりにAttributeFactoryをもつコンストラクタを追加。
　　4.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4833&#34;&gt;LUCENE-4833&lt;/a&gt;：IndexWriterConfigがsetMergePolicy(null)の時にLogByteSizeMergePolicyを使っているのをデフォルトmerge policyをTieredMergePolicyに。また、nullが引数に渡されたらExceptionを返す。
　　5.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4849&#34;&gt;LUCENE-4849&lt;/a&gt;：ParallelTaxonomyArraysをDirectoryTaxonomyWriter/Readerのためのabstractとして作成。あと、o.a.l.facet.taxonomyに移動。
　　6.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4876&#34;&gt;LUCENE-4876&lt;/a&gt;：IndexDeletionPolicyをInterfaceではなく、abstractクラスに。IndexDeletionPolicy、MergeScheduler、InfoStreamでCloneableをimplement。
　　7.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4874&#34;&gt;LUCENE-4874&lt;/a&gt;：FilterAtomicReaderと関連するクラス（FilterTerms、FilterDocsEnumなど）でフィルタされたインスタンスをforwardしないように。メソッドが他のabstractメソッドを実装している場合に。（？）
　　8.&lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4642&#34;&gt;LUCENE-4642&lt;/a&gt;, &lt;a href=&#34;http://issues.apache.org/jira/browse/LUCENE-4877&#34;&gt;LUCENE-4877&lt;/a&gt;：TokenizerFactory、TokenFilterFactory、CharFilterFactoryの実装者は、少なくともMap&amp;lt;String,String&amp;gt;（SPIフレームワーク（Solrとか）によってロードされる）を引数にするコンストラクタを提供する必要がある。さらに、TokenizerFactoryはcreate(AttributeFactory,Reader)メソッドを実装する必要もある。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Partial UpdateとcopyFieldのバグ【Solr 4.0 ALPHA】(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/13/partial-update%E3%81%A8copyfield%E3%81%AE%E3%83%90%E3%82%B0solr-4-0-alpha/</link>
      <pubDate>Fri, 13 Jul 2012 20:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/13/partial-update%E3%81%A8copyfield%E3%81%AE%E3%83%90%E3%82%B0solr-4-0-alpha/</guid>
      <description>今日はSolr 4.0 ALPHAの興味深い機能があったので紹介です。 数日前に「Solr 4.0: Partial documents update」という記事を見つけました。 Solrには、</description>
      <content:encoded>&lt;p&gt;今日はSolr 4.0 ALPHAの興味深い機能があったので紹介です。
数日前に&lt;a href=&#34;http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/&#34;&gt;「Solr 4.0: Partial documents update」&lt;/a&gt;という記事を見つけました。&lt;/p&gt;
&lt;p&gt;Solrには、ドキュメント（RDBで言うレコード）のデータを更新したい場合には、特定のフィールドだけを更新するという機能がありませんでした。
ですので、特定の項目（例えば、priceなど）を更新したい場合、ドキュメントの全データをSolrに再度上書き登録するという処理をしなければなりませんでした。
RDBを触っていた方が、Solrを始めた場合に必ず使いづらいと思われる点だと思います。&lt;/p&gt;
&lt;p&gt;で、4.0でその機能がありますという、&lt;a href=&#34;http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/&#34;&gt;「Solr 4.0: Partial documents update」&lt;/a&gt;の記事を見つけました。
ただ、SolrのWikiや4.0 ALPHAの紹介のページには「partial update」という記述が見当たりません。
（あれ、これかな？&lt;a href=&#34;http://wiki.apache.org/solr/Per%20Steffensen/Update%20semantics&#34;&gt;Update semantics&lt;/a&gt;）
あと、まだ完成していないので、載っていないのかもしれないです。（この&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-139&#34;&gt;チケットSOLR-139&lt;/a&gt;が部分更新に関するもののはず。チケット番号をみても古くから望まれている機能だということがわかります。）&lt;/p&gt;
&lt;p&gt;ということで、調べてみました。&lt;/p&gt;
&lt;p&gt;###機能概要&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Solrの機能として、特定のフィールドのみを更新するという機能です。
あくまでも、Solrレベルでの機能となり、Luceneの機能を利用したものではありません。
つぎのような流れになっています。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solrに対して特定フィールドを更新したいという形のドキュメントを投げる&lt;/li&gt;
&lt;li&gt;Solrはドキュメントを受け取ると、内部のインデックスに保存してあるデータを取り出す&lt;/li&gt;
&lt;li&gt;取り出したドキュメントオブジェクトに対して、更新対象フィールドの値だけデータを更新する&lt;/li&gt;
&lt;li&gt;ドキュメントオブジェクトをインデックスに保存する&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;このような流れです。
まぁ、言われてみれば当たり前な処理です。
ただし、この機能を使う場合はいくつかの前提条件があります。&lt;/p&gt;
&lt;p&gt;###前提条件&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;前提条件はつぎのとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;すべてのフィールドをstored=&amp;ldquo;true&amp;quot;にする&lt;/li&gt;
&lt;li&gt;「&lt;em&gt;version&lt;/em&gt;」という特殊なフィールドを用意する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1点目は、データの保存方法についてです。
先ほど流れに書きましたが、Solrが内部に保存してあるデータを取り出して、更新対象以外のデータを保存しなおしてくれます。
このため、stored=&amp;ldquo;true&amp;quot;にしておかないと、元のデータがSolr内部で取得できません。&lt;/p&gt;
&lt;p&gt;2点目の「&lt;em&gt;version&lt;/em&gt;」というフィールドは4.0から導入されたフィールドです。
SolrCloudに必要な機能としてドキュメントのバージョン管理を行うために導入されたフィールドだと思います。（あまり詳しく調べていない。。。）
SolrCloud内でレプリカの更新などに使ってるのかなぁと（そのうち調べます。）
以上の2点が前提条件です。すべてのデータをstored=&amp;ldquo;true&amp;quot;としなければならない点は、インデックスのサイズや性能に関わってくるので考えて利用するほうがいいかと思います。&lt;/p&gt;
&lt;p&gt;###利用方法&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Solrのサンプルデータ（exampledocs/mem.xml）を例として利用します。
部分更新を行うにはつぎのような形のデータを投げると部分更新が可能です。
（JSONでの更新のサンプルについては、&lt;a href=&#34;http://solr.pl/en/2012/07/09/solr-4-0-partial-documents-update/&#34;&gt;こちらの記事&lt;/a&gt;を参考にしてください。）
####XMLのサンプル（partial_update.xmlというファイルで保存する）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;add&amp;amp;gt;
&amp;lt;doc&amp;amp;gt;
  &amp;lt;field name=&amp;#34;id&amp;#34;&amp;amp;gt;VS1GB400C3&amp;lt;/field&amp;amp;gt;
  &amp;lt;field name=&amp;#34;_version_&amp;#34;&amp;amp;gt;バージョン番号&amp;lt;/field&amp;amp;gt;
  &amp;lt;field name=&amp;#34;cat&amp;#34; update=&amp;#34;add&amp;#34;&amp;amp;gt;cats_and_dogs&amp;lt;/field&amp;amp;gt;
  &amp;lt;field name=&amp;#34;popularity&amp;#34; update=&amp;#34;inc&amp;#34;&amp;amp;gt;10&amp;lt;/field&amp;amp;gt;
  &amp;lt;!-- set empty for SOLR-3502 bug --&amp;amp;gt;
  &amp;lt;field name=&amp;#34;price_c&amp;#34; update=&amp;#34;set&amp;#34;&amp;amp;gt;0.0,USD&amp;lt;/field&amp;amp;gt;
&amp;lt;/doc&amp;amp;gt;
&amp;lt;/add&amp;amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上記サンプルのうち、&lt;b&gt;バージョン番号&lt;/b&gt;の部分は、現在Solrに登録してある値を指定します。（Solrの管理画面で検索すれば表示されます。）
上記ファイルを「SOLR_HOME/example/exampledocs」に保存し、同フォルダにてつぎのコマンドを実行すると、部分更新されるのがわかります。
Solrに更新であるというフィールドがわかるように、fieldタグにupdateという属性を指定してあります。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
java -Durl=http://localhost:8983/solr/update?versions=on -Dout=yes -jar post.jar partial_update.xml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ちなみに、上記post.jarのオプションで、「-Durl」「-Dout」を追加してあります。
「-Durl」はverions=onというパラメータを追加したいためです。
「-Dout」はPOSTした結果をターミナルに表示するために追加しています。
これらのオプションを指定すると、データ更新後のバージョンが取得できるようになります。&lt;/p&gt;
&lt;p&gt;####更新に利用できるコマンド？
部分更新にはつぎの3つのコマンド？（正式名は不明）が用意されています。fieldタグのupdate属性に指定します。&lt;/p&gt;
&lt;table class=&#34;list_view&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;コマンド？&lt;/th&gt;
      &lt;th&gt;説明&lt;/th&gt;
    &lt;/tr&gt; 
  &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
  &lt;td&gt;add&lt;/td&gt;
  &lt;td&gt;値を追加します。multiValuedのフィールドでない場合はエラーが出ます。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;set&lt;/td&gt;
  &lt;td&gt;値を新規に登録しなおします。現在入っているデータは無くなります&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inc&lt;/td&gt;
&lt;td&gt;指定された数値を加算（数値形式のみ）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
以上が、部分更新の機能になります。
ちなみに、登録されているバージョンと更新データに入っているバージョンが異なる場合はエラーが発生する仕組みになっているようです。
&lt;p&gt;それとは別に、この機能を調べていて、copyFieldのバグにぶつかってしまいました。。。
multiValuedでない、copyFieldを利用しているしている場合には注意が必要です。&lt;/p&gt;
&lt;p&gt;###copyFieldのバグ（SOLR-3502）&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;4.0-ALPHA（3.6.0でも再現しました。）のexampleのデータで部分更新の機能を確認できると言いました。
ただし、「price_c」というフィールドのせいで、2回部分更新を行うと2回目にエラーが発生します。
根本的な問題は、部分更新ではなく&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-3502&#34;&gt;copyFieldのバグ&lt;/a&gt;のようです。（部分更新の処理にも問題は有るような気がしますが。。。）&lt;/p&gt;
&lt;p&gt;バグの内容はつぎのとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multiValued=&amp;ldquo;false&amp;quot;のフィールドをdestに指定&lt;/li&gt;
&lt;li&gt;srcに指定されたフィールドに値を設定（exampleのpriceフィールドに「1」を指定）&lt;/li&gt;
&lt;li&gt;destに指定されたフィールドに値を設定（exampleのprice_cフィールドに「2,USD」を指定）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上記のように設定した場合、「price_c」フィールドに、指定された値＋「price」の値がcopyにより追加されます。
通常は「price_c」フィールドはmultiValued=&amp;ldquo;false&amp;quot;なのでエラーが出るはずなのですが、エラーが発生せず2つの値が登録されてしまいます。&lt;/p&gt;
&lt;p&gt;このバグのため、exampleのデータを利用して部分更新を行うとつぎのような状態が発生します。
更新を行う対象のデータはprice、price_cフィールド以外のフィールドとします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1回目の登録後：priceフィールド「&amp;ldquo;185.0&amp;rdquo;」、price_cフィールド「&amp;ldquo;185.0,USD&amp;rdquo;」&lt;/li&gt;
&lt;li&gt;2回目の登録後：priceフィールド「&amp;ldquo;185.0&amp;rdquo;」、price_cフィールド「[&amp;ldquo;185.0,USD&amp;rdquo;,&amp;ldquo;185.0,USD&amp;rdquo;]」&lt;/li&gt;
&lt;li&gt;3回目の登録：エラーが発生&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;部分更新の処理で、すでに登録済みのデータをSolrが自動で取り出すため、2回目の登録処理にて「price_c」の登録済みの値がSolrから取り出され、さらにcopyField設定により、「price」の値が追加されます。
本当は2回目の登録でエラーが発生すべきなのですが、バグのためエラーが発生せずに登録できてしまいます。
部分更新の処理としては、copyフィールドのdestに指定されているフィールドの値を取り出さないほうがいいような気もしますが、きちんと考えてないのでなんとも言えないです。（制約事項とする形のほうがいいかもしれません）&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>autoGeneratePhraseQueriesのデフォルト値について(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/14/autogeneratephrasequeries%E3%81%AE%E3%83%87%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E5%80%A4%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Thu, 14 Jun 2012 01:09:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/14/autogeneratephrasequeries%E3%81%AE%E3%83%87%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E5%80%A4%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>久々にSolrの話です。 といっても、結構前からの話でして。。。 schema.xmlのfieldTypeの設定に「autoGeneratePh</description>
      <content:encoded>&lt;p&gt;久々にSolrの話です。
といっても、結構前からの話でして。。。&lt;/p&gt;
&lt;p&gt;schema.xmlのfieldTypeの設定に「autoGeneratePhraseQueries」という属性があります。
Solr3.1で導入されました。動作に関しては&lt;a href=&#34;http://lucene.jugem.jp/?eid=403&#34;&gt;関口さんのブログ&lt;/a&gt;で説明されています。
Solr 1.4までは、Analyzerがトークンを複数返してくる場合（例：lucene-gosenで「Solr入門」という文字列を入れた場合など）にフレーズクエリとして処理していました。
Lucene 3.1.0から、この処理がデフォルトfalse（つまり、フレーズクエリにならない）という挙動になりました。（詳しくは&lt;a href=&#34;http://lucene.jugem.jp/?eid=403&#34;&gt;関口さんのブログ&lt;/a&gt;で。）
ただ、Solr 3.1.0では、下位互換性を考慮して、autoGeneratePhraseQueriesの設定値はデフォルトが「true」でした。&lt;/p&gt;
&lt;p&gt;このデフォルト値がSolr 3.3以降で提供されているschemaのバージョン（1.4以上）からデフォルト値が「false」に変更されています。
schemaのバージョンを1.3以前のものから1.4以上に移行する場合は注意が必要です。&lt;/p&gt;
&lt;p&gt;とまぁ、偉そうに書きましたが、私もちゃんと追えてませんでした。
Solr勉強会第６回で、&lt;a href=&#34;http://www.slideshare.net/KojiSekiguchi/lu-solr32-3420110912&#34;&gt;関口さんの発表&lt;/a&gt;できちんと説明されていて、参加してたのに聞けてなかったですし。（&lt;a href=&#34;http://johtani.jugem.jp/?eid=26&#34;&gt;メモ取ってるのに、書いてない。&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;ということで、Solr入門のサンプルschemaも少し修正しました。
&lt;a href=&#34;http://johtani.jugem.jp/?eid=76&#34;&gt;こちら&lt;/a&gt;と&lt;a href=&#34;http://johtani.jugem.jp/?eid=76&#34;&gt;こちら&lt;/a&gt;の記事に追記してありますので、参考にしてください。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Solr 3.6.0のCJKの設定とSynonymFilterFactoryの気になる点(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/17/solr-3-6-0%E3%81%AEcjk%E3%81%AE%E8%A8%AD%E5%AE%9A%E3%81%A8synonymfilterfactory%E3%81%AE%E6%B0%97%E3%81%AB%E3%81%AA%E3%82%8B%E7%82%B9/</link>
      <pubDate>Tue, 17 Apr 2012 01:16:11 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/17/solr-3-6-0%E3%81%AEcjk%E3%81%AE%E8%A8%AD%E5%AE%9A%E3%81%A8synonymfilterfactory%E3%81%AE%E6%B0%97%E3%81%AB%E3%81%AA%E3%82%8B%E7%82%B9/</guid>
      <description>先日、Solr入門のサンプルschema.xmlの3.6.0対応版の作成をしていて、気になったことがあったので、 メモとして残しておきます。 S</description>
      <content:encoded>&lt;p&gt;先日、Solr入門のサンプルschema.xmlの3.6.0対応版の作成をしていて、気になったことがあったので、
メモとして残しておきます。&lt;/p&gt;
&lt;p&gt;SynonymFilterFactoryの属性「&lt;span style=&#34;color:#FF0000&#34;&gt;tokenizerFactory&lt;/span&gt;」に関連する話です。
（&lt;a href=&#34;http://www.amazon.co.jp/exec/obidos/ASIN/4774141755/johtani-22/ref=nosim/&#34;&gt;「Apache Solr入門」&lt;/a&gt;の36-37ページに記載があります。）&lt;/p&gt;
&lt;p&gt;SynonymFilterFactoryでは、類義語設定ファイルを読み込む際に利用するTokenizerFactoryを「tokenizerFactory」という属性で指定できます。（以下は書籍の記述を抜粋）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
  &amp;lt;filter class=&amp;#34;sold.SynonymFilterFactory&amp;#34; synonyms=&amp;#34;synonyms.txt&amp;#34; ... tokenizerFactory=&amp;#34;solrbook.analysis.SenTokenizerFactory&amp;#34;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;このように、TokenizerFactoryが指定できます。&lt;/p&gt;
&lt;p&gt;ただ、&lt;a href=&#34;http://johtani.jugem.jp/?eid=76&#34;&gt;こちらの記事&lt;/a&gt;で書いたように、
Solr 3.6.0のexampleのschema.xmlではCJKのフィールドは次のように設定されています。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
    &amp;lt;!-- CJK bigram (see text_ja for a Japanese configuration using morphological analysis) --&amp;gt;
    &amp;lt;fieldType name=&amp;#34;text_cjk&amp;#34; class=&amp;#34;solr.TextField&amp;#34; positionIncrementGap=&amp;#34;100&amp;#34;&amp;gt;
      &amp;lt;analyzer&amp;gt;
        &amp;lt;tokenizer class=&amp;#34;solr.StandardTokenizerFactory&amp;#34;/&amp;gt;
        &amp;lt;!-- normalize width before bigram, as e.g. half-width dakuten combine  --&amp;gt;
        &amp;lt;filter class=&amp;#34;solr.CJKWidthFilterFactory&amp;#34;/&amp;gt;
        &amp;lt;!-- for any non-CJK --&amp;gt;
        &amp;lt;filter class=&amp;#34;solr.LowerCaseFilterFactory&amp;#34;/&amp;gt;
        &amp;lt;filter class=&amp;#34;solr.CJKBigramFilterFactory&amp;#34;/&amp;gt;
      &amp;lt;/analyzer&amp;gt;
    &amp;lt;/fieldType&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.6.0以前は、solr.CJKTokenizerFactoryを利用していましたが、3.6.0からはCJKTokenizerFactoryがdeprecatedになってしまい、代わりにStandardTokenizerFactory＋CJKBigramFilterFactoryの組み合わせになっています。
exampleのCJKのフィールドタイプ設定を利用して、かつ、そのフィールドにSynonymFilterを利用する場合に、
StandardTokenizerFactoryを指定してしまうと、類義語が展開できなくなってしまうので注意が必要です。&lt;/p&gt;
&lt;p&gt;CJKのフィールドでSynonymFilterを利用する場合は、類義語の設定ファイル内の記述を自力でCJKTokenizerが分割する形で記述する（まぁ、やらないでしょうが）か、deprecatedですが、CJKTokenizerFactoryを利用するのが現時点での対応でしょうか。&lt;/p&gt;
&lt;p&gt;なお、これに絡んで、&lt;a href=&#34;https://issues.apache.org/jira/browse/SOLR-3359&#34;&gt;このようなチケット&lt;/a&gt;もできています。&lt;/p&gt;
&lt;h5 id=&#34;syntaxhighlighterを導入してみました&#34;&gt;SyntaxHighlighterを導入してみました。&lt;/h5&gt;
&lt;h5 id=&#34;ちょっとはみやすくなってますかね&#34;&gt;ちょっとはみやすくなってますかね？&lt;/h5&gt;
&lt;h5 id=&#34;まだsyntaxhighlighterの設定を調べながら使っているのでコロコロ変わるかもしれないですが気にしないでください&#34;&gt;まだ、SyntaxHighlighterの設定を調べながら使っているので、コロコロ変わるかもしれないですが、気にしないでください。&lt;/h5&gt;
</content:encoded>
    </item>
    
    <item>
      <title>「Apache Solr入門」のサンプルのKuromojiとlucene-gosen対応（2章～4章）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/14/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C2%E7%AB%A04%E7%AB%A0/</link>
      <pubDate>Sat, 14 Apr 2012 02:58:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/14/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C2%E7%AB%A04%E7%AB%A0/</guid>
      <description>先日の続きです。「Apache Solr入門」の2章から4章の説明について、Solr3.6.0で動作させる時の変更点を以下に書いていきます。 な</description>
      <content:encoded>&lt;p&gt;先日の続きです。「Apache Solr入門」の2章から4章の説明について、Solr3.6.0で動作させる時の変更点を以下に書いていきます。
なお、前回も説明しましたが、3.6.0からKuromojiという形態素解析器がSolrに同梱されるようになりました。
これから説明する2章の変更点の手順ですが、Kuromojiとlucene-gosenそれぞれの利用方法について説明します。
添付のschema.xmlについては、基本的にKuromojiを利用する形に変更してあります。
それに加えて、lucene-gosen用のフィールドを別途追加で定義しました。
これらのフィールド名については、次の表の用になります。
適宜、書籍のフィールド名と置き換えながら読み進めたり、試したりしてください。&lt;/p&gt;
&lt;table class=&#34;list_view&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Kuromojiフィールド&lt;/th&gt;
      &lt;th&gt;lucene-gosenフィールド&lt;/th&gt;
    &lt;/tr&gt; 
  &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
  &lt;td&gt;title&lt;/td&gt;
  &lt;td&gt;title_gosen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;author&lt;/td&gt;
  &lt;td&gt;auther_gosen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;summary&lt;/td&gt;
  &lt;td&gt;summary_gosen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;intended_reader&lt;/td&gt;
  &lt;td&gt;intended_reader_gosen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;from_author&lt;/td&gt;
  &lt;td&gt;from_author_gosen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;toc&lt;/td&gt;
  &lt;td&gt;toc_gosen&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;2章&#34;&gt;2章&lt;/h2&gt;
&lt;h3 id=&#34;213-schemaxmlのバージョン27ページ&#34;&gt;2.1.3 schema.xmlのバージョン（27ページ）&lt;/h3&gt;
&lt;p&gt;Solr3.xではschema.xmlのファイルの最新バージョンは**1.&lt;span style=&#34;color:#FF0000&#34;&gt;5&lt;/span&gt;**になっています。&lt;/p&gt;
&lt;h3 id=&#34;223-代表的なトークナイザ35ページ&#34;&gt;2.2.3 代表的なトークナイザ（35ページ）&lt;/h3&gt;
&lt;p&gt;solrbook.analysis.SenTokenizerFactoryは必要ありません。
&lt;span style=&#34;color:#FF0000&#34;&gt;Solr 3.6.0からはKuromojiと呼ばれる形態素解析器が用意されています。
solr.JapaneseTokenizerFactoryがそれに該当します。
&lt;/span&gt;
これとは別に、lucene-gosenを利用する場合、Solr向けのトークナイザが用意されています。
solr.&lt;span style=&#34;color:#FF0000&#34;&gt;Gosen&lt;/span&gt;TokenizerFactoryがそれに該当します。&lt;/p&gt;
&lt;h3 id=&#34;224-代表的なトークンフィルタ37ページ&#34;&gt;2.2.4 代表的なトークンフィルタ（37ページ）&lt;/h3&gt;
&lt;p&gt;以下の2つについては&lt;span style=&#34;color:#FF0000&#34;&gt;Kuromojiが同等のトークンフィルタを提供しています。&lt;/span&gt;
また、lucene-gosenを利用する場合は、lucene-gosenに同等のトークンフィルタが存在します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;solrbook.analysis.KatakanaStemFilterFactory&lt;/li&gt;
&lt;li&gt;solrbook.analysis.POSFilterFactory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;次のものがSolr 3.6.0に用意されているので、こちらを利用します。&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;solr.JapaneseKatakanaStemFilterFactory&lt;/li&gt;
&lt;li&gt;solr.JapanesePartOfSpeechStopFilterFactory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;それぞれ、次のものがlucene-gosenにあるので、こちらを利用します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;solr.&lt;span style=&#34;color:#FF0000&#34;&gt;Gosen&lt;/span&gt;KatakanaStemFilterFactory&lt;/li&gt;
&lt;li&gt;solr.&lt;span style=&#34;color:#FF0000&#34;&gt;Gosen&lt;/span&gt;PartOfSpeechStopFilterFactory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2章向けの&lt;a href=&#34;https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/48834b2d0465/schema/schema.xml&#34;&gt;schema.xmlはこちら&lt;/a&gt;です。その他のtxtファイルについては、特に変更はありません。&lt;/p&gt;
&lt;p&gt;3,4章は特に変更はありません。Solrの起動の仕方にだけ注意してください。（-Dsen.homeは必要ありません）&lt;/p&gt;
&lt;p&gt;以上が4章までの修正点になります。&lt;/p&gt;
&lt;p&gt;昨日に引き続き、眠い目をこすりながら修正したので、おかしいかも。
動かない、意味がわからないなどあれば、コメントorツイートいただければと思います。&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;2012/06/14提供しているschema.xmlに関して修正を加えました。&lt;/span&gt;&lt;a href=&#34;http://johtani.jugem.jp/?eid=92&#34;&gt;こちらの記事&lt;/a&gt;で説明しているautoGeneratePhraseQueriesの値をtext_gosen、text_cjkのフィールドに対してtrueを設定する記述を追記しました。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 3.6.0リリース / 「Apache Solr入門」のサンプルのKuromojiとlucene-gosen対応（1章）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/14/lucene-solr-3-6-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9---apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0/</link>
      <pubDate>Sat, 14 Apr 2012 02:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/14/lucene-solr-3-6-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9---apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AEkuromoji%E3%81%A8lucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0/</guid>
      <description>以前より、アナウンスしていた、Kuromojiという日本語形態素解析が含まれるLucene/Solr 3.6.0がリリースされました。 以下、各</description>
      <content:encoded>&lt;p&gt;以前より、アナウンスしていた、Kuromojiという日本語形態素解析が含まれるLucene/Solr 3.6.0がリリースされました。&lt;/p&gt;
&lt;p&gt;以下、各リリース内容について簡単に説明されているページへのリンクです。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lucene.apache.org/solr/solrnews.html&#34;&gt;Solrリリースのお知らせ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lucene.apache.org/core/corenews.html&#34;&gt;Luceneリリースのお知らせ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Solr 3.6.0の変更の目玉は各言語のAnalyzer/Tokenizerの設定がexampleのschema.xmlに含まれるようになったことです。
Kuromojiという日本語用の形態素解析器もexampleを起動すればすぐに利用できる形になっています。
Kuromojiを利用する場合は、exampleのschema.xmlが参考になるでしょう。&lt;/p&gt;
&lt;p&gt;あと、大きな変更は、Ivyに対応した点です。ソースをダウンロードするとわかりますが、依存するjarファイルが含まれない形に変更されています。
SVNからチェックアウトした場合も同様です。ビルドにはネットワークに接続している環境が必要になりました。&lt;/p&gt;
&lt;p&gt;また、このリリースに合わせて、以前書いた「Apache Solr入門」のサンプルについての記事も変更が必要かと思い、
前回の記事をベースに以下に変更した記事を書いたので、参考にしてください。
今回は、Kuromojiという日本語形態素解析がデフォルトで含まれるようになったので、
Kuromojiの利用方法とあわせて、lucene-gosenの利用方法も記載します。
サンプルのschema.xmlについては、Kuromoji、lucene-gosenが同時に利用できる形のものを用意しました。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;サンプルのschema.xmlを最新版（Solr 3.6 + lucene-gosen-2.0.0-ipadic）のものを用意しました。
なお、あくまでも、3.xでlucene-gosenを利用する場合の「Apache Solr入門」のサンプルプログラムの変更点（とりあえず、4章まで）の違いについて記述します。
申し訳ございませんが、1.4と3.xの違いについての説明はここでは行いません。&lt;/p&gt;
&lt;p&gt;以下では、各章でschema.xmlに関連する記載のある部分を抜粋して、変更点と変更したschema.xmlのリンクを用意しました。参考にしてもらえればと思います。&lt;/p&gt;
&lt;h2 id=&#34;1章&#34;&gt;1章&lt;/h2&gt;
&lt;h3 id=&#34;161-n-gram17ページ&#34;&gt;1.6.1 N-gram（17ページ）&lt;/h3&gt;
&lt;p&gt;1.6.1の手順に変更はありません。
サンプルプログラムが入っているZip「solrbook.zip」のintroduction/ngram/schema.xmlファイルの代わりに
&lt;a href=&#34;https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/48834b2d0465/introduction/ngram/schema.xml&#34;&gt;こちらのschema.xml&lt;/a&gt;を利用してください。
&lt;span style=&#34;color:#FF0000&#34;&gt;※なお、Solr 3.6.0から、SOLR_HOME/example/solr/conf/schema.xmlにデフォルトでN-gramで利用しているCJKTokenizerの設定が入るようになっています。
（実際にはCJKTokenizerではなく、CJKBigramFilterとCJKWidthFilterに変更されています。）&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;162-形態素解析18ページ20ページ中盤まで&#34;&gt;1.6.2 形態素解析（18ページ～20ページ中盤まで）&lt;/h3&gt;
&lt;p&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;CJKと同様、exampleにKuromojiを利用した設定がすでに記述されています。text_jaというフィールドタイプになります。書籍の21ページ1行目に記載のある、
「Field」のテキストボックスに入力する文字列を「text_ja」とすると、Kuromojiを利用した形態素解析結果が表示されます。exampleですでに幾つかのフィルタも設定されているため、書籍の出力結果とは異なる表示となるはずです。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;lucene-gosenを利用する場合は手順が大きく変わります。
Senを利用する場合、Senの辞書のビルド、Senのjarファイルの配置、Senを利用するためのTokenizerクラスを含んだサンプルjarの配置という作業があります。
lucene-gosenではコンパイル済みの辞書がjarファイルに含まれています。
また、Solr向けのTokenizerもlucene-gosenのjarファイルに含まれています。
lucene-gosenを利用して形態素解析を体験するための手順は次の流れになります。
なお、schema.xmlについては上記N-gramでダウンロードしたschema.xmlに形態素解析の設定もあわせて記載してあります。&lt;/p&gt;
&lt;p&gt;jarファイル（&lt;a href=&#34;http://lucene-gosen.googlecode.com/files/lucene-gosen-2.0.0-ipadic.jar&#34;&gt;lucene-gosen-2.0.0-ipadic.jar&lt;/a&gt;）をダウンロードして、$SOLR/example/solr/lib（libディレクトリがない場合は作成）にコピーします。
コピーが終わりましたら、次のように$SOLR/exampleディレクトリでSolrを起動します。
（-Dsen.homeは必要なし）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
$ java -jar start.jar
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;あとは、書籍の記述にしたがって管理画面のAnalysis画面で動作を確認します。
ほぼ、図1-6と同じ結果になっていると思います。
（lucene-gosenで出力される情報には本書のサンプルよりも多くの情報が含まれています。また、サンプルでは、形態素解析の後の単語に基本形を採用しているため、「な」が「だ」として出力されています。基本形を出力する場合は後述するこちらで紹介したTokenFilterを利用すれば可能です。）&lt;/p&gt;
&lt;p&gt;2章については後日説明することにします（眠くなってきた。。。）&lt;/p&gt;
&lt;p&gt;動作しないなどあれば、コメントください。&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;2012/06/14追記提供しているschema.xmlに関して修正を加えました。&lt;/span&gt;&lt;a href=&#34;http://johtani.jugem.jp/?eid=92&#34;&gt;こちらの記事&lt;/a&gt;で説明しているautoGeneratePhraseQueriesの値をtext_gosen、text_cjkのフィールドに対してtrueを設定する記述を追記しました。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Lucene Eurocon 2011 Barcelona のスライド読みました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/08/lucene-eurocon-2011-barcelona-%E3%81%AE%E3%82%B9%E3%83%A9%E3%82%A4%E3%83%89%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Tue, 08 Nov 2011 13:02:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/08/lucene-eurocon-2011-barcelona-%E3%81%AE%E3%82%B9%E3%83%A9%E3%82%A4%E3%83%89%E8%AA%AD%E3%81%BF%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>最近忘れやすいので、記録しておこうかと。 読んだスライドの簡単な内容と感想です。 ちなみに、スライドの一覧はこちらです。 ※スライドへのリンクはす</description>
      <content:encoded>&lt;p&gt;最近忘れやすいので、記録しておこうかと。
読んだスライドの簡単な内容と感想です。
ちなみに、スライドの一覧はこちらです。
&lt;span style=&#34;color:#FF0000&#34;&gt;※スライドへのリンクはすべてPDFへのリンクになっていますので、注意が必要です。&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/miller_solr4highlights2_eurocon2011.pdf&#34;&gt;&lt;em&gt;&lt;strong&gt;Solr 4 Highlights（PDF）&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Solrの次期バージョン4.0で採用される機能の紹介でした。
紹介されているのは次の機能。各機能について、JIRAの番号も記載があるので便利ですね。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DirectSolrSpellChecker&lt;/li&gt;
&lt;li&gt;NRT (&lt;a href=&#34;http://wiki.apache.org/solr/NearRealtimeSearch&#34;&gt;Near RealTime search&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Realtime Get&lt;/li&gt;
&lt;li&gt;SolrCloud - search side&lt;/li&gt;
&lt;li&gt;SolrCloud - indexing side (WIP)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これまでと異なるSpellChecker、Commit前のデータが検索できるNRT（なんでNRSじゃないんだろう？）、Commit前の登録済みデータを取得することが出来るRealtime Getなどの簡単な紹介です。
あと、個人的に興味のあるSolrCloud周りが絵付きで紹介されてます。ZooKeeperもちょっと出てきます。
まだ、ちゃんとまとめてないですが、NewSolrCloudDesignの翻訳したものも参考までに。（&lt;a href=&#34;http://johtani.jugem.jp/?eid=31&#34;&gt;その１&lt;/a&gt;、&lt;a href=&#34;http://johtani.jugem.jp/?eid=32&#34;&gt;その２&lt;/a&gt;）&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;a href=&#34;http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/Binns_archiveit_eurocon2011.pdf&#34;&gt;Archive-It: Scaling Beyond a Billion Archival Web-pages&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;InternetArchiveの事例紹介。1996年からWebページのアーカイブを行なっているサイトですね。
その一部でSolrが利用されています。
「1,375,473,187 unique documents」との記述もあり、データ量が巨大です。
データ量が多いのに、ここでFieldCollapsing/Groupingも利用しているようで、インデックス作成、検索両方に対してカスタマイズしたものをgithubで公開している模様です。&lt;/p&gt;
&lt;hr&gt;
[&lt;em&gt;**Scaling search at Trovit with Solr and Hadoop**&lt;/em&gt;](http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/MarcSturlese_scalingsearchTrovit_eurocon2011.pdf)
&lt;p&gt;次は、Trovitという会社のSolr＋Hadoopの事例紹介です。
最初はLuceneをベースに検索サーバ作ってたけど、Solrが出てきたので、Solrを使うようになったようで。
データ保存先として最初はMySQLを利用してDataImportHandlerでSolrにデータ登録してたけど、
データ量が増加するが、MySQLのShardingが面倒なので、Hadoop（Hive）でデータをパイプライン処理してSolrのインデックスを作成しましょうという流れになったようです。
私が以前、&lt;a href=&#34;http://www.slideshare.net/nabeta/ss-8118052&#34;&gt;Solr勉強会で紹介したSOLR-1301&lt;/a&gt;のパッチをベースにMap/Reduceの処理を2段階にして性能をアップさせたという話が記載されてました。
ただ、これで早くなるのかはよくわからないんですが。。。
一応、資料では、いきなり大きなSolrのインデックスを作らずに、最初のM/Rで小さなインデックスを作成し（TaskTrackerの数＞＞Solrのshardサーバ数だから小さくしたほうが速い？）、
2段目のM/Rでインデックスをマージしてshardサーバ数のインデックスに集約する？という形みたいです。
（英語力のなさが。。。）
あとは、テキスト処理を幾つかHadoopでやってますよという紹介でした。
SOLR-1301の利用者が他にもいて、違うアプローチをとっていたのが印象的。
毎回全データインデックス生成するときは、SOLR-1301を利用してshard数が増えてもすぐに対応が可能になるので、
かなり便利ですよ。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/Gio_Kincade-Solr_Etsy_eurocon2011.pdf&#34;&gt;&lt;em&gt;&lt;strong&gt;Solr @ Etsy&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Etsyは個人の作家（編み物とかシールとか）の方が出店するためのショッピングモールのようなサイトです。
実は、最近、MacBookAirのステッカーを購入したのがここでした。
で、検索にSolrを使っています。
面白いのが、検索サーバとWebアプリ（PHPで書かれている）の間のデータのやり取りにThriftを利用していること。
Solrの前にThriftを話すサーバを別途用意しているようです。ネットワークのデータ量を減らすことが目的らしいです。
そのあとは、少しThriftのサーバでのLoadBalancingの話が続きます。
次にレプリケーションの性能問題のはなし。定期的にレプリケーションに異様に時間がかかるのが問題になったようで、
Multicast-Rsyncを試してみたけどダメでしたというはなし。
Bit Torrent + Solrという組み合わせで回避したらしいのですが、いまいち仕組みがわからなかったです。。。
こちらもgithubに公開されている模様。
あとは、QParser、Stemmerをカスタマイズしたものの話です。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/Baldeschwieler_HortonWorks_LuceneEurocon20111018.pdf&#34;&gt;&lt;strong&gt;&lt;em&gt;Architecting the Future of Big Data and Search&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LuceneのカンファレンスにHortonworksが出てきてびっくりしました。
まぁ、Luceneの生みの親＝Hadoopの生みの親ですから、問題ないのかもしれないですが。
大半が予想通り、Hadoopに関する話でした。
知らないApacheのプロジェクト「&lt;a href=&#34;http://incubator.apache.org/ambari/&#34;&gt;Ambari&lt;/a&gt;」というのが出てきました。これは、HadoopConferenceJapan2011 Fallでの発表にもチラッと出てきたようです。
「Ambari is a monitoring, administration and lifecycle management project for Apache Hadoop clusters.」ということで、Hadoopクラスタの統合管理のツールになるんでしょうか？
最後の2枚くらいにLuceneが出てきます。絡めてみたって感じですかね。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lucidimagination.com/sites/default/files/file/Eurocon2011/scholten_Configuring_Mahout_Clustering_Jobs_Eurocon2011.pdf&#34;&gt;&lt;em&gt;&lt;strong&gt;Configuring mahout Clustering Jobs&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;今度はMahoutが出てきました。はやりのものが満載です。
まぁ、MahoutもLuceneのインデックスを利用するという話もありますので。
スライドはクラスタリングとはどういうものか、Mahoutの説明とテキストクラスタリング処理のお話、最後はstuckoverflowでのMahoutとSolrの活用の仕方について。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ということで、英語力がない中、かなり流し読みな感じですが、あとで思い出すために書きだして見ました。
何かの役に立てれば幸いです。&lt;/p&gt;
&lt;p&gt;他に、こんなスライドが面白かったとか、このスライドについても書いてほしいなどあれば、コメントください。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Solrの新しい管理画面（Solr4.x trunk系）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/05/solr%E3%81%AE%E6%96%B0%E3%81%97%E3%81%84%E7%AE%A1%E7%90%86%E7%94%BB%E9%9D%A2solr4-x-trunk%E7%B3%BB/</link>
      <pubDate>Wed, 05 Oct 2011 19:43:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/05/solr%E3%81%AE%E6%96%B0%E3%81%97%E3%81%84%E7%AE%A1%E7%90%86%E7%94%BB%E9%9D%A2solr4-x-trunk%E7%B3%BB/</guid>
      <description>Lucene/SolrのMLでSolrの管理画面を新しくするというチケットが流れていたのでちょっと触って見ました。 ほんとにちょっと触っただけ</description>
      <content:encoded>&lt;p&gt;Lucene/SolrのMLでSolrの管理画面を新しくするというチケットが流れていたのでちょっと触って見ました。
ほんとにちょっと触っただけですが、いくつかキャプチャ撮ってみたので、アップしときます。
※以下ではサムネイル画像に元画像（100Kくらいの画像）へのリンクが設定されています。携帯などでは見づらいかもしれませんが、ご容赦を。&lt;/p&gt;
&lt;p&gt;URLは旧管理画面とことなり、http://localhost:8983/solr/になります。&lt;/p&gt;
&lt;hr&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075207.jpg&#34; alt=&#34;新管理画面：トップ画面&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075207.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
まずはトップ画面
ダッシュボードと呼ばれるトップ画面。メモリの利用率や起動してからの時間、Luceneなどのバージョンが表示されます。&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075141.png&#34; alt=&#34;新管理画面：クエリ実行結果&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075141.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
次は検索画面すっきりしてます。facetが指定できるようになったのは大きいかな。ただし、facet.fieldを複数指定などができないが。結果についてはとくに指定がなければXMLで帰ってきます。ただ、パラメータの追加ができなくなってる気がするなぁ&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075230.png&#34; alt=&#34;新管理画面：クエリ接続エラー&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075230.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
ちなみに、Solrを止めて検索したらこんな感じの画面になりました。クエリの実行ならこのようにエラーがわかったのですが、停止後に左のメニューにあるSchemaなどをクリックしても白い画面が出るだけで、エラーかどうかがわかりにくいです。&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075122.png&#34; alt=&#34;新管理画面：Analysisのサンプル（lucene-gosen）&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075122.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
Analysis画面。入力画面がシンプルになりました。フィールド名はリストで表示されるので選択するだけです。あとは、これまでどおり。サンプルはlucene-gosenの解析結果です。ハイライトもきちんと表示されます。ただし、長い文章の場合は結果部分だけがスクロールできる形になり、ちょっとわかりにくかったです。&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075142.png&#34; alt=&#34;新管理画面：Analysis失敗&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075142.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
Analysisの入力画面を表示したあとにSolrを停止して解析してみたらこんなエラー画面が出ました。ちなみに、その後、画面を切り替えずにSolrを起動して解析したら、赤い帯のエラーは出たままでした。一度別画面にすれば、元に戻りますが。&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075140.png&#34; alt=&#34;新管理画面：キャッシュの状態確認&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075140.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
Pluginsの画面（旧管理画面のstatisticsに相当）。
キャッシュの状態が確認できます。今まであった画面と情報的には一緒かと。一段カテゴリ（CACHEとかCOREとか）の選択ができるようになり、見やすくなりました。
&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075143.png&#34; alt=&#34;新管理画面：updatehandlerの状態。ドキュメント数とか&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075143.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
同じくPluginsの画面。
こちらはupdateHandlerについての情報です。commit数やoptimizeの回数、updateして、commitする前の状態のドキュメント数などが表示されます。前より表示される項目が多くなってるかな？&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075208.jpg&#34; alt=&#34;新管理画面：スキーマブラウザ&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075208.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
最後はスキーマブラウザこの画面が一番良くなっています。旧管理画面では、フィールド名がすべて大文字で表示され、しかもソートがされていない状態だったため、ダイナミックフィールドを利用しているとフィールドを探すのが一苦労でした。
今回は、プルダウンでフィールドやフィールドタイプのリストが表示され、辞書順で並んでいます。Filterなどもわかりやすい表示になっているかと。
&lt;br style=&#34;clear:both&#34; /&gt;
&lt;br style=&#34;clear&#34;/&gt;
&lt;hr&gt;
おまけ


&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075181.png&#34; alt=&#34;Solritasと呼ばれるサンプル画面&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20111005/20111005_2075181.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;
Solritasと呼ばれるVelocityを使った、3.x系で入ってきた新しいサンプル画面です。URLはhttp://localhost:8983/solr/browseです。ファセットなどを使った簡単なサンプル画面なので、検索結果画面でこんなことができるというデモにも使えるかと。ただ、これも旧管理画面よりはましですが、デザインが。。。&lt;br style=&#34;clear:both&#34; /&gt;
&lt;p&gt;とまぁ、簡単ですが、4.x系の管理画面をいくつか触ってみて、キャプチャをとって見ました。
デザインは前よりもすっきりしています。ただ、クエリについてはパラメータの追加ができなくなっているので、もう少し改良されるといいかなぁ（自分でやれよと言われそうですが。。。）&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>New SolrCloud Designの翻訳（その2）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/04/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE2/</link>
      <pubDate>Tue, 04 Oct 2011 18:32:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/04/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE2/</guid>
      <description>遅くなりましたが、続きです。 さらに英語力のなさを痛感して凹んでいるところですが、何かの役に立てばと恥を晒すところです。。。 一応、訳してみたの</description>
      <content:encoded>&lt;p&gt;遅くなりましたが、続きです。
さらに英語力のなさを痛感して凹んでいるところですが、何かの役に立てばと恥を晒すところです。。。&lt;/p&gt;
&lt;p&gt;一応、訳してみたのですが、訳すのに必死になってしまい、つながりがわかっていない点もちらほら。
このあと一旦見直しつつ、再度理解する「理解編」をアップしようかと思います。
できれば、シーケンス図とかも交えつつ。（そうしないと理解ができない可能性が。。。）
前回同様、原文は最後に付加しておきます。&lt;/p&gt;
&lt;h3 id=&#34;boot-strapping&#34;&gt;Boot Strapping&lt;/h3&gt;
&lt;h4 id=&#34;cluster-startupクラスタの起動&#34;&gt;Cluster Startup（クラスタの起動）&lt;/h4&gt;
&lt;p&gt;ノードはZookeeperのホストとポートを指定することから始めます。
クラスタの最初のノードはクラスタのschema/configとクラスタの設定を指定するとこから開始します。
最初のノードはZookeeperに設定をアップロードしてクラスタをブートします。
クラスタは「ブートストラップ」状態です。
この状態ではノード-&amp;gt;パーティションマッピングは計算されず、クラスタはクラスタ管理コマンド以外のどんなread/writeリクエストも受け付けません。&lt;/p&gt;
&lt;p&gt;クラスタの最初のノード集合が起動した後、クラスタ管理コマンド（TBD記述？？？）が管理者によって発行されます。このコマンドは整数「partitions」パラメータを受け取り、次のステップを実行します。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cluster Lockを取得&lt;/li&gt;
&lt;li&gt;「partitions」をパーティション数として割り当て&lt;/li&gt;
&lt;li&gt;各パーティションのためのノードを取得&lt;/li&gt;
&lt;li&gt;ZooKeeperのノード-&amp;gt;パーティションマッピングを更新&lt;/li&gt;
&lt;li&gt;Cluster Lockをリリース&lt;/li&gt;
&lt;li&gt;全ノードに対して最新版のノード-&amp;gt;パーティションマッピングをZooKeeper経由で更新させる&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;node-startup&#34;&gt;&lt;strong&gt;Node Startup&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;ノードが起動すると、自分がすでに存在するシャードの一部かどうかZooKeeperでチェックします。
もし、ZooKeeperがノードのレコードを持っていない、またはどのシャードの一部でもないと判断したら、
ノードは後述の「New Node」のステップを実行します。すでに存在するノードの場合は後述の「Node Restart」のステップを実行します。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New Node&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;新しいノードはクラスタの一部ではなく、クラスタのキャパシティを増強するためのものです。&lt;/p&gt;
&lt;p&gt;「auto_add_new_nodes」クラスタプロパティが「false」の場合、新しいノードはZooKeeperに「idle」として登録され、他のノードが参加してくれと言うまで待機します。
そうでない場合（auto_add_new_nods=true）は次のステップを実行します。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cluster Lockを取得します。&lt;/li&gt;
&lt;li&gt;適切なnode-&amp;gt;partitionエントリを選び出します。&lt;/li&gt;
&lt;li&gt;利用可能なパーティションのリストをスキャンして「replication_factor」のノード数以下のパーティションのエントリを探します。複数ある場合はノード数が最小のエントリを選びます。それも一緒ならランダムに選びます。&lt;/li&gt;
&lt;li&gt;全パーティションが「replication_factor」以上のノードを持っている場合、ノードはパーティションが最も多いものをスキャンします。複数ある場合はパーティション内のドキュメント数が最大のエントリを選びます。ドキュメント数が同一なら任意のエントリを選びます。&lt;/li&gt;
&lt;li&gt;もし、選んだノード-&amp;gt;パーティションエントリを現在のノードに移動させることでがクラスタのパーティション：ノード比率の最大値を小さくするなら、現在のエントリを返します。。それ以外の場合選ばれたエントリがないので、アルゴリズムは終了です。。&lt;/li&gt;
&lt;li&gt;ZooKeeper内のノード-&amp;gt;パーティションマッピングを更新します&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;ZooKeeper内のノードステータスを「リカバリ」状態にします&lt;/li&gt;
&lt;li&gt;Cluster Lockをリリースします&lt;/li&gt;
&lt;li&gt;「リカバリ」はパーティションのリーダーから開始します。&lt;/li&gt;
&lt;li&gt;リカバリが終了したら、再度、Cluster Lockを取得します。&lt;/li&gt;
&lt;li&gt;元のエントリはZooKeeperのノード-&amp;gt;パーティションマッピングから削除されます。&lt;/li&gt;
&lt;li&gt;Cluster Lockをリリースします&lt;/li&gt;
&lt;li&gt;元のノードはZooKeeperからノード-&amp;gt;パーティションマッピングを更新させられます&lt;/li&gt;
&lt;li&gt;ステップ1に戻ります。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;node-restart&#34;&gt;&lt;strong&gt;Node Restart&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;ノードの再起動とは次のいずれかを意味しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JVMがクラッシュし、手動または自動でのリスタート&lt;/li&gt;
&lt;li&gt;ノードが一時的にネットワークから切り離された。もしくは、ZooKeeperに接続できなかった（死んでいると思われた）。または、ある一定期間、リーダーからの更新を受信できなかった。&lt;/li&gt;
&lt;li&gt;このシナリオが表す書き込み処理のライフサイクルの間にネットワークから分断された&lt;/li&gt;
&lt;li&gt;ハード故障もしくはメンテナンスウインドウによりクラスタからノードが分断され、ノードをクラスタにrejoinさせるために起動した。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ノードが各パーティションに対してメンバーであるパーティションのリストを読み、パーティションのリーダーがリカバリプロセスを実行する。その時、ノードは「auto_add_new_nods」プロパティをチェックして、「New Node」処理のステップを実行する。
これはクラスタが。。。（元の文章が切れてて意味が不明）&lt;/p&gt;
&lt;p&gt;クライアントは標準的なSolrの更新形式を利用して書き込みできます。
書き込み処理はクラスタの任意のノードに送信されます。
ノードはハッシュ関数を利用して、どのパーティションに所属するか決めるためにrange-パーティションマッピングを使います。
ZooKeeperはシャードのリーダーを識別して、書き込み処理をそこに送ります。
SolrJはリーダーに対して書き込みを直接送信するための拡張がされています。&lt;/p&gt;
&lt;p&gt;リーダーはPartitionバージョンの操作を割り当て、そのトランザクションログの操作を書き込み、シャードに属する他のノードにドキュメントバージョンハッシュを転送します。
ノードはインデックスにドキュメントハッシュを書き込み、トランザクションログに操作を記録します。
リーダーは、min_writesの最小数のノード以上のノードが「OK」とレスポンスを返したら「OK」とレスポンスを返します。
クラスタプロパティのmin_writesは書き込みリクエスト時に指定することで、異なる値を指定できます。&lt;/p&gt;
&lt;p&gt;クラウドモードはコミット/ロールバック操作を明示的には行いません。
コミットは特定の間隔で（commit_within）リーダーによりオートコミットにより管理されます。
また、シャードの全メンバーのコミットはトリガーにより管理されます。
ノードが利用可能な最新バージョンはコミットの時点で記録されます。&lt;/p&gt;
&lt;h3 id=&#34;transaction-log&#34;&gt;&lt;strong&gt;Transaction Log&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;トランザクションログは2つのコミットの間にインデックスに対して実行された操作全てを記録したもの&lt;/li&gt;
&lt;li&gt;コミットはそれ以前に実行された操作の耐久性を保証するために、新しいトランザクションログを開始します。&lt;/li&gt;
&lt;li&gt;同期は調整が可能です。例えば、flush vs fsynです。fsyncがデフォルトで、JVMクラッシュに対して保証できるが、電源異常の場合には保証できないが、速度的には早いです。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recovery&#34;&gt;Recovery&lt;/h3&gt;
&lt;p&gt;次のトリガーにより復旧が可能です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bootstrap&lt;/li&gt;
&lt;li&gt;パーティション分割&lt;/li&gt;
&lt;li&gt;クラスタの再構築&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ノードは自身に「recovering」というステータスを設定して復旧を開始します。
このフェーズの間、ノードは読み込みリクエストを受けることができませんが、トランザくkションログに書きこまれるすべての新しい書き込みリクエストを受け取ります。
ノードは自身が持つインデックスのバージョンを調べて、パーティションの最新バージョンのリーダーに問い合わせます。
リーダーはシャード内の残りのノードと同期する前に実行されるべき操作の集合を返します（？？？）。&lt;/p&gt;
&lt;p&gt;最初にインデックスをコピーし、最新のノードにあるトランザクションログをリプレイします。
もし、インデックスのコピーが必要ならば、インデックスファイルをローカルにまずコピーし、その後トランザクションログをリプレイします。
トランザクションログのリプレイは通常の書き込みリクエストの流れと同じです。
この時、ノードは新しい書き込みを受け付けるかもしれません。その書き込みはインデックスに再生されるべきです。
ある時点でノードは最新のコミットポイントに追いつき、自身のステータスを「ready」にします。
この時点で、このノードは読み込みリクエストを処理できます。&lt;/p&gt;
&lt;h4 id=&#34;handling-node-failures&#34;&gt;Handling Node Failures&lt;/h4&gt;
&lt;p&gt;一時的にネットワークが分断され、幾つかのノードとZooKeeperの間の通信が遮断されるかもしれません。
クラスタはデータの再構築（リバランシング）の前にしばらく待ちが発生します。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leader failure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ノードが故障し、もしそれがシャードのリーダだった場合、他のメンバーがリーダー選出のプロセスを開始します。
新しいリーダーが選出されるまで、このパーティションへの書き込みは受け付けられません。
この時、これはリーダー以外の故障ステップを処理します。（？？？）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leader failure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;シャードの一部に新しいノードが割り当てられる前にリーダーはmin_reaction_timeの間待ちます。
リーダーはCluster Lockを取得し、シャードの新規メンバーとしてノードを割り当てるためのノード-シャード割り当てアルゴリズムを使用します。
ZooKeeperのノード-&amp;gt;パーティションマッピングが更新され、Cluster Lockがリリースされます。
新しいノードはZooKeeperからノード-&amp;gt;パーティションマッピングを強制的にリロードされます。&lt;/p&gt;
&lt;h3 id=&#34;splitting-partitions&#34;&gt;Splitting partitions&lt;/h3&gt;
&lt;p&gt;明示的なクラスタ管理コマンドもしくはSolrによる自動的な分割戦略（ストラテジ）はパーティションを分割することができます。
明示的な分割コマンド（split command）は対象となるパーティションを分割するために実行されます。&lt;/p&gt;
&lt;p&gt;パーティションXが100から199のハッシュの範囲を持つものとし、X（100から149）、Y（150～199）に分割するとします。
Xのリーダーは、XとYの新しい値の範囲をZooKeeperに分割アクションを記録します。
ノードはこの分割アクションもしくは新しいパーティションの存在については通知を受けません。（？？？）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;XのリーダはCluster Lockを取得し、パーティションY（アルゴリズムはto be determined）を割り当てるノードを決定し、新しいパーティションを知らせ、パーティション-&amp;gt;ノードマッピングを更新します。Xのリーダはノードのレスポンスを街、新しいパーティションがコマンドを受付可能な状態になったら次の処理を実行します。&lt;/li&gt;
&lt;li&gt;Xのリーダーは分割が完了するまですべてのコミットを停止します。&lt;/li&gt;
&lt;li&gt;Xのリーダーは最新のコミットポイント（バージョンVとする）のIndexReaderをオープンし、同じバージョンのIndexReaderもオープンするように命じます&lt;/li&gt;
&lt;li&gt;XのリーダーはYのリーダーに対してバージョンV以降のトランザクションログのうちハッシュ値の範囲が150から199のものを流します。&lt;/li&gt;
&lt;li&gt;Yのリーダーはトランザクションログの#2（#3の間違い？）で送られたリクエストだけを記録します？？？&lt;/li&gt;
&lt;li&gt;Xのリーダーはステップ#2で開いたIndexReaderに対してインデックスの分割を開始します。&lt;/li&gt;
&lt;li&gt;#5で作成されたインデックスはYのリーダーに送られ、登録されます。&lt;/li&gt;
&lt;li&gt;Yのリーダーは「recovery」プロセスを開始するように（シャードの）他のノード命令し、インデックスのトランザクションログを再生し始めます。&lt;/li&gt;
&lt;li&gt;パーティションYのすべてのノードがバージョンVに到達したならば&lt;/li&gt;
&lt;li&gt;YのリーダーはXのリーダーに#2で作成されたReaderの上に、ハッシュの範囲が100から149だけに属しているドキュメントを抽出するようにするFilteredIndexReaderを準備するように頼みます。&lt;/li&gt;
&lt;li&gt;Xのリーダーは#8aのリクエストが完了したのを検知したら、YのリーダーがCluster Lockを取得し、クラスタ全体の検索/登録リクエストの受信を開始するためにレンジ-&amp;gt;パーティションマッピングを変更します。&lt;/li&gt;
&lt;li&gt;YのリーダーはXのリーダーに検索リクエストのために#8aで作成されたFilteredIndexReaderの利用開始を頼みます&lt;/li&gt;
&lt;li&gt;YのリーダーはXのリーダーに、ZooKeeperからレンジ-&amp;gt;パーティションマッピングを矯正リフレッシュするように頼みます。この時点で#3で開始されたトランザクションログの流しこみが停止されるのが保証されます。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Xのリーダーは自身のパーティションに存在するべきでないハッシュ値をもつドキュメントを削除し、最新のコミットポイントのsearcherを再度開きます。&lt;/li&gt;
&lt;li&gt;この時点で分割は完全に終了し、Xのリーダーはcommit_withinパラメータによるコミットをレジュームします（？？？）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分割操作が完了するまで、commit_withinパラメータによるパーティションの分割は実行されない&lt;/li&gt;
&lt;li&gt;#8b開始から#8c終了までの間の分散検索は一貫しない検索結果を帰す場合がある（例えば：検索結果が異なる）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cluster-re-balancing&#34;&gt;Cluster Re-balancing&lt;/h3&gt;
&lt;p&gt;クラスタは明示的なクラスタ管理コマンドにより再構築（リバランシング）できる。&lt;/p&gt;
&lt;p&gt;TBD
（to be determined）&lt;/p&gt;
&lt;h3 id=&#34;cluster-re-balancing-1&#34;&gt;Cluster Re-balancing&lt;/h3&gt;
&lt;p&gt;TBD
（to be determined）&lt;/p&gt;
&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;h4 id=&#34;solr_clusterproperties&#34;&gt;&lt;strong&gt;solr_cluster.properties&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;これはクラスタ内の全ノードにわたって適用される一般的なSolr設定ファイルとは別のプロパティファイルの集合である。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;replication_factor：クラスタによって管理されるドキュメントのレプリカの数&lt;/li&gt;
&lt;li&gt;min_writes：書き込み操作が成功になる前の最小の書き込み？？？？。これは書き込みごとに上書き設定可能&lt;/li&gt;
&lt;li&gt;commit_within：検索に現れるまでの書き込み操作の最大回数&lt;/li&gt;
&lt;li&gt;hash_function：ドキュメントのハッシュ値を計算するための関数の実装&lt;/li&gt;
&lt;li&gt;max_hash_value：ハッシュ関数が出力することができる最大値。理論的には、この値はクラスタが保持できるパーティションの最大数でもある&lt;/li&gt;
&lt;li&gt;min_reaction_time：起動、停止の後に再配分/分割にかかる時間（？？）&lt;/li&gt;
&lt;li&gt;min_replica_for_reaction：レプリカノード数がこの値以下になったら、min_reaction_timeにならなくても分割が実行される。&lt;/li&gt;
&lt;li&gt;auto_add_new_nodes：booleanフラグ。もしtrueなら新しいノードは自動的にパーティションからレプリカを読み込む。そうでない場合は新しいノードはクラスタに「idle」状態で登録される&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cluster-admin-commands&#34;&gt;&lt;strong&gt;Cluster Admin Commands&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;すべてのクラスタ管理コマンドはすべてのノードでパス（/cluster_admin）を与えることで実行できます。
全ノードは同じコマンドを受け付けることができ、振る舞いも同じものになるでしょう。
以下のコマンドはユーザが利用できるパブリックなコマンドです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;init_cluster：（パラメータ：パーティション）このコマンドはノードの集合の初期化後に実施されます。このコマンドが実行されるまで、クラスタは読み込み/書き込みコマンドを受け付けません。&lt;/li&gt;
&lt;li&gt;split_partition：（パラメータ：パーティション（任意））パーティションを2つに分割します。もしパーティションパラメータが指定されない場合は、ドキュメント数が最大の&lt;/li&gt;
&lt;li&gt;add_idle_nodes：このコマンドはauto_add_new_nodes=falseの場合に利用できます。このコマンドはクラスタに対して「idle」状態のすべてのノードを追加するトリガーとなります。&lt;/li&gt;
&lt;li&gt;move_partition：（パラメータ：パーティション、from、to）fromのノードからtoの別のノードに引数で指定されたパーティションを移動します。&lt;/li&gt;
&lt;li&gt;command_status：（パラメータ：completion_id（任意））上記コマンドはすべて非同期で実行され、completion_idを返します。このコマンドは特定の実行中のコマンドもしくは全ての実行中のコマンドの状態を表示するために利用できます。&lt;/li&gt;
&lt;li&gt;status：（パラメータ：パーティション（任意））パーティションのリストを表示し各パーティションの次の情報を表示します。&lt;/li&gt;
&lt;li&gt;リーダーノード&lt;/li&gt;
&lt;li&gt;ノードのリスト&lt;/li&gt;
&lt;li&gt;ドキュメント数&lt;/li&gt;
&lt;li&gt;平均読み込み回数（reads/sec）&lt;/li&gt;
&lt;li&gt;平均書き込み回数（writes/sec）&lt;/li&gt;
&lt;li&gt;平均読み込み時間（time/read）&lt;/li&gt;
&lt;li&gt;平均書き込み時間（time/write）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;migrating-from-solr-to-solrcloud&#34;&gt;Migrating from Solr to SolrCloud&lt;/h3&gt;
&lt;p&gt;クラウドに移行するときに幾つかの特徴は不要かもしれないし、サポートされないかもしれません。
既存の（クラウドでない）バージョンでのすべての特徴をSolrCloudでサポートし続けなければなりません。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;レプリケーション：これは必要ありません。&lt;/li&gt;
&lt;li&gt;CoreAdminコマンド：明示的なコアの操作は許可されません。内部にコアがあるかもしれないが、暗黙的に管理されるでしょう&lt;/li&gt;
&lt;li&gt;複数スキーマのサポート？：単純化のため、ver1.0ではサポートしないかもしれない&lt;/li&gt;
&lt;li&gt;solr.xml：SolrCloudでほんとに必要？&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;alternative-to-a-cluster-lock&#34;&gt;Alternative to a Cluster Lock&lt;/h3&gt;
&lt;p&gt;リーダーを選出する常設の調停ノード（masterはインデックスレプリケーションで利用している用語なので、「調停」とする）を持つほうが単純かもしれません。
「truth」状態をZookeeperの状態としてみなすような次のパターンでは、将来の柔軟性（クラスタを制御するためのZookeeperの状態を直接変更するような外部管理ツールのような）を考慮に入れることができます。
（毎回ロックを取得するよりも）調停ノードを持つことにより、よりスケーラブルになるかもしれません。
特定条件下でのみCluster Lockを利用するハイブリッドも意味があるでしょう。&lt;/p&gt;
&lt;h3 id=&#34;single-node-simplest-use-case&#34;&gt;Single Node Simplest Use Case&lt;/h3&gt;
&lt;p&gt;単一ノードでスタートして、ドキュメントをインデックス登録できないといけません。
また、あとで、クラスタに2番目のノードを追加できないと行けません。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1つのノードから開始し、最初にZookeeperに設定ファイルをアップロードし、shard1にノードを作成＋登録します。&lt;/li&gt;
&lt;li&gt;他の情報がない状態で設定が作成され、1つのシャードのシステムとなります。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;いくつかのドキュメントをインデックスします&lt;/li&gt;
&lt;li&gt;他のノードが起動し、「まだ割り当てられていない場合、レプリカの最小の数をもつshardに割り当てられ、「recovery」プロセスを開始します」というパラメータを受け取ります。
* 出来れば、同一ホスト上に同じシャードはコピーしない
* この時点の後で、ノードが停止したら、再起動し、同じ役割が再開されるべきです。（Zookeeperでそれ自身であると判別されれば）
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;原文はこちらからです。&lt;/p&gt;
&lt;h3 id=&#34;boot-strapping-1&#34;&gt;Boot Strapping&lt;/h3&gt;
&lt;h4 id=&#34;cluster-startup&#34;&gt;Cluster Startup&lt;/h4&gt;
&lt;p&gt;A node is started pointing to a Zookeeper host and port. The first node in the cluster may be started with cluster configuration properties and the schema/config files for the cluster. The first node would upload the configuration into zookeeper and bootstrap the cluster. The cluster is deemed to be in the “bootstrap” state. In this state, the node -&amp;gt; partition mapping is not computed and the cluster does not accept any read/write requests except for clusteradmin commands.&lt;/p&gt;
&lt;p&gt;After the initial set of nodes in the cluster have started up, a clusteradmin command (TBD description) is issued by the administrator. This command accepts an integer “partitions” parameter and it performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Acquire the Cluster Lock&lt;/li&gt;
&lt;li&gt;Allocate the “partitions” number of partitions&lt;/li&gt;
&lt;li&gt;Acquires nodes for each partition&lt;/li&gt;
&lt;li&gt;Updates the node -&amp;gt; partition mapping in ZooKeeper&lt;/li&gt;
&lt;li&gt;Release the Cluster Lock&lt;/li&gt;
&lt;li&gt;Informs all nodes to force update their own node -&amp;gt; partition mapping from ZooKeeper&lt;/li&gt;
&lt;li&gt;The Cluster Lock is acquired&lt;/li&gt;
&lt;li&gt;A suitable source (node, partition) tuple is chosen:&lt;/li&gt;
&lt;li&gt;The list of available partitions are scanned to find partitions which has less then “replication_factor” number of nodes. In case of tie, the partition with the least number of nodes is selected. In case of another tie, a random partition is chosen.&lt;/li&gt;
&lt;li&gt;If all partitions have enough replicas, the nodes are scanned to find one which has most number of partitions. In case of tie, of all the partitions in such nodes, the one which has the most number of documents is chosen. In case of tie, a random partition on a random node is chosen.&lt;/li&gt;
&lt;li&gt;If moving the chosen (node, partition) tuple to the current node will decrease the maximum number of partition:node ratio of the cluster, the chosen tuple is returned.Otherwise, no (node, partition) is chosen and the algorithm terminates&lt;/li&gt;
&lt;li&gt;The node -&amp;gt; partition mapping is updated in ZooKeeper&lt;/li&gt;
&lt;/ol&gt;
&lt;li&gt;The node status in ZooKeeper is updated to “recovery” state&lt;/li&gt;
&lt;li&gt;The Cluster Lock is released&lt;/li&gt;
&lt;li&gt;A “recovery” is initiated against the leader of the chosen partition&lt;/li&gt;
&lt;li&gt;After the recovery is complete, the Cluster Lock is acquired again&lt;/li&gt;
&lt;li&gt;The source (node, partition) is removed from the node -&amp;gt; partition map in ZooKeeper&lt;/li&gt;
&lt;li&gt;The Cluster Lock is released&lt;/li&gt;
&lt;li&gt;The source node is instructed to force refresh the node -&amp;gt; partition map from ZooKeeper&lt;/li&gt;
&lt;li&gt;Goto step #1&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;node-restart-1&#34;&gt;&lt;strong&gt;Node Restart&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;A node restart can mean one of the following things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The JVM crashed and was manually or automatically restarted&lt;/li&gt;
&lt;li&gt;The node was in a temporary network partition and either could not reach ZooKeeper (and was supposed to be dead) or could not receive updates from the leader for a period of time. A node restart ine node failure.&lt;/li&gt;
&lt;li&gt;Lifecycle of a Write Operation this scenario signifies the removal of the network partition.&lt;/li&gt;
&lt;li&gt;A hardware failure or maintenance window caused the removal of the node from the cluster and the node has been started again to rejoin the cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node reads the list of partitions for which it is a member and for each partition, starts a recovery process from each partition’s leader respectively. Then, the node follows the steps in the New Node section without checking for the auto_add_new_nodes property. This ensures that the cluster recovers from the imbalance created by th&lt;/p&gt;
&lt;p&gt;Writes are performed by clients using the standard Solr update formats. A write operation can be sent to any node in the cluster. The node uses the hash_function , and the Range-Partition mapping to identify the partition where the doc belongs to. A zookeeper lookup is performed to identify the leader of the shard and the operation is forwarded there. A SolrJ enhancement may enable it to send the write directly to the leader&lt;/p&gt;
&lt;p&gt;The leader assigns the operation a Partition Version and writes the operation to its transaction log and forwards the document + version + hash to other nodes belonging to the shard. The nodes write the document + hash to the index and record the operation in the transaction log. The leader responds with an ‘OK’ if at least min_writes number of nodes respond with ‘OK’. The min_writes in the cluster properties can be overridden by specifying it in the write request.&lt;/p&gt;
&lt;p&gt;The cloud mode would not offer any explicit commit/rollback operations. The commits are managed by auto-commits at intervals (commit_within) by the leader and triggers a commit on all members on the shard. The latest version available to a node is recorded with the commit point.&lt;/p&gt;
&lt;h3 id=&#34;transaction-log-1&#34;&gt;&lt;strong&gt;Transaction Log&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A transaction log records all operations performed on an Index between two commits&lt;/li&gt;
&lt;li&gt;Each commit starts a new transaction log because a commit guarantees durability of operations performed before it&lt;/li&gt;
&lt;li&gt;The sync can be tunable e.g. flush vs fsync by default can protect against JVM crashes but not against power failure and can be much faster&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recovery-1&#34;&gt;Recovery&lt;/h3&gt;
&lt;p&gt;A recovery can be triggered during:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bootstrap&lt;/li&gt;
&lt;li&gt;Partition splits&lt;/li&gt;
&lt;li&gt;Cluster re-balancing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node starts by setting its status as ‘recovering’. During this phase, the node will not receive any read requests but it will receive all new write requests which shall be written to a separate transaction log. The node looks up the version of index it has and queries the ‘leader’ for the latest version of the partition. The leader responds with the set of operations to be performed before the node can be in sync with the rest of the nodes in the shard.&lt;/p&gt;
&lt;p&gt;This may involve copying the index first and replaying the transaction log depending on where the node is w.r.t the state of the art. If an index copy is required, the index files are replicated first to the local index and then the transaction logs are replayed. The replay of transaction log is nothing but a stream of regular write requests. During this time, the node may have accumulated new writes, which should then be played back on the index. The moment the node catches up with the latest commit point, it marks itself as “ready”. At this point, read requests can be handled by the node.&lt;/p&gt;
&lt;h4 id=&#34;handling-node-failures-1&#34;&gt;Handling Node Failures&lt;/h4&gt;
&lt;p&gt;There may be temporary network partitions between some nodes or between a node and ZooKeeper. The cluster should wait for some time before re-balancing data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leader failure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If node fails and if it is a leader of any of the shards, the other members will initiate a leader election process. Writes to this partition are not accepted until the new leader is elected. Then it follows the steps in non-leader failure&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-Leader failure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The leader would wait for the min_reaction_time before identifying a new node to be a part of the shard. The leader acquires the Cluster Lock and uses the node-shard assignment algorithm to identify a node as the new member of the shard. The node -&amp;gt; partition mapping is updated in ZooKeeper and the cluster lock is released. The new node is then instructed to force reload the node -&amp;gt; partition mapping from ZooKeeper.&lt;/p&gt;
&lt;h3 id=&#34;splitting-partitions-1&#34;&gt;Splitting partitions&lt;/h3&gt;
&lt;p&gt;A partition can be split either by an explicit cluster admin command or automatically by splitting strategies provided by Solr. An explicit split command may give specify target partition(s) for split.&lt;/p&gt;
&lt;p&gt;Assume the partition ‘X’ with hash range 100 - 199 is identified to be split into X (100 - 149) and a new partition Y (150 - 199). The leader of X records the split action in ZooKeeper with the new desired range values of X as well as Y. No nodes are notified of this split action or the existence of the new partition.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The leader of X, acquires the Cluster Lock and identifies nodes which can be assigned to partition Y (algorithm TBD) and informs them of the new partition and updates the partition -&amp;gt; node mapping. The leader of X waits for the nodes to respond and once it determines that the new partition is ready to accept commands, it proceeds as follows:&lt;/li&gt;
&lt;li&gt;The leader of X suspends all commits until the split is complete.&lt;/li&gt;
&lt;li&gt;The leader of X opens an IndexReader on the latest commit point (say version V) and instructs its peers to do the same.&lt;/li&gt;
&lt;li&gt;The leader of X starts streaming the transaction log after version V for the hash range 150 - 199 to the leader of Y.&lt;/li&gt;
&lt;li&gt;The leader of Y records the requests sent in #2 in its transaction log only i.e. it is not played on the index.&lt;/li&gt;
&lt;li&gt;The leader of X initiates an index split on the IndexReader opened in step #2.&lt;/li&gt;
&lt;li&gt;The index created in #5 is sent to the leader of Y and is installed.&lt;/li&gt;
&lt;li&gt;The leader of Y instructs its peers to start recovery process. At the same time, it starts playing its transaction log on the index.&lt;/li&gt;
&lt;li&gt;Once all peers of partition Y have reached at least version V:&lt;/li&gt;
&lt;li&gt;The leader of Y asks the leader of X to prepare a FilteredIndexReader on top of the reader created in step #2 which will have documents belonging to hash range 100 - 149 only.&lt;/li&gt;
&lt;li&gt;Once the leader of X acknowledges the completion of request in #8a, the leader of Y acquires the Cluster Lock and modifies the range -&amp;gt; partition mapping to start receiving regular search/write requests from the whole cluster.&lt;/li&gt;
&lt;li&gt;The leader of Y asks leader of X to start using the FilteredIndexReader created in #8a for search requests.&lt;/li&gt;
&lt;li&gt;The leader of Y asks leader of X to force refresh the range -&amp;gt; partition mapping from ZooKeeper. At this point, it is guaranteed that the transaction log streaming which started in #3 will be stopped.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;The leader of X will delete all documents with hash values not belonging to its partitions, commits and re-opens the searcher on the latest commit point.&lt;/li&gt;
&lt;li&gt;At this point, the split is considered complete and leader of X resumes commits according to the commit_within parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The partition being split does not honor commit_within parameter until the split operation completes&lt;/li&gt;
&lt;li&gt;Any distributed search operation performed starting at the time of #8b and till the end of #8c can return inconsistent results i.e. the number of search results may be wrong.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cluster-re-balancing-2&#34;&gt;Cluster Re-balancing&lt;/h3&gt;
&lt;p&gt;The cluster can be rebalanced by an explicit cluster admin command.&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;monitoring&#34;&gt;Monitoring&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h3 id=&#34;configuration-1&#34;&gt;Configuration&lt;/h3&gt;
&lt;h4 id=&#34;solr_clusterproperties-1&#34;&gt;&lt;strong&gt;solr_cluster.properties&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This are the set of properties which are outside of the regular Solr configuration and is applicable across all nodes in the cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;replication_factor&lt;/strong&gt; : The number of replicas of a doc maintained by the cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;min_writes&lt;/strong&gt; : Minimum no:of successful writes before the write operation is signaled as successful . This may me overridden on a per write basis&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;commit_within&lt;/strong&gt; : This is the max time within which write operation is visible in a search&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hash_function&lt;/strong&gt; : The implementation which computes the hash of a given doc&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;max_hash_value&lt;/strong&gt; : The maximum value that a hash_function can output. Theoretically, this is also the maximum number of partitions the cluster can ever have&lt;/li&gt;
&lt;li&gt;min_reaction_time : The time before any reallocation/splitting is done after a node comes up or goes down (in secs)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;min_replica_for_reaction&lt;/strong&gt; : If the number of replica nodes go below this threshold the splitting is triggered even if the min_reaction_time is not met&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;auto_add_new_nodes&lt;/strong&gt; : A Boolean flag. If true, new nodes are automatically used as read replicas to existing partitions, otherwise, new nodes sit idle until the cluster needs them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cluster-admin-commands-1&#34;&gt;&lt;strong&gt;Cluster Admin Commands&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;All cluster admin commands run on all nodes at a given path (say /cluster_admin). All nodes are capable of accepting the same commands and the behavior would be same. These are the public commands which a user can use to manage a cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;init_cluster&lt;/strong&gt; : (params : partition) This command is issued after the initial set of nodes are started. Till this command is issued, the cluster would not accept any read/write commands&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;split_partition&lt;/strong&gt; : (params : partitionoptional). The partition is split into two halves. If the partition parameter is not supplied, the partition with the largest number of documents is identified as the candidate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;add_idle_nodes&lt;/strong&gt; : This can be used if auto_add_new_nodes=false. This command triggers the addition of all ‘idle’ nodes to the cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;move_partition&lt;/strong&gt; : (params : partition, from, to). Move the given partition from a given node from to another node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;command_status&lt;/strong&gt; :(params : completion_idoptional) . All the above commands are asynchronous and returns with a completion_id . This command can be used to know the status of a particular running command or all the current running commands&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;status&lt;/strong&gt; : (params : partitionoptional) Shows the list of partitions and for each partition, the following info is provided&lt;/li&gt;
&lt;li&gt;leader node&lt;/li&gt;
&lt;li&gt;nodes list&lt;/li&gt;
&lt;li&gt;doc count&lt;/li&gt;
&lt;li&gt;average reads/sec&lt;/li&gt;
&lt;li&gt;average writes/sec&lt;/li&gt;
&lt;li&gt;average time/read&lt;/li&gt;
&lt;li&gt;average time/write&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;migrating-from-solr-to-solrcloud-1&#34;&gt;Migrating from Solr to SolrCloud&lt;/h3&gt;
&lt;p&gt;A few features may be redundant or not supported when we move to cloud such as. We should continue to support the non cloud version which supports all the existing features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replication. This feature is not required anymore&lt;/li&gt;
&lt;li&gt;CoreAdmin commands. Explicit manipulation of cores will not be allowed. Though cores may exist internally and they meay be managed implicitly&lt;/li&gt;
&lt;li&gt;Multiple schema support ? Should we just remove it from ver 1.0 for simplicity?&lt;/li&gt;
&lt;li&gt;solr.xml . Is there a need at all for this in the cloud mode?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;alternative-to-a-cluster-lock-1&#34;&gt;Alternative to a Cluster Lock&lt;/h3&gt;
&lt;p&gt;It may be simpler to have a coordinator node (we avoid the term master since that is associated with traditional index replication) that is established via leader election. Following a pattern of treating the zookeeper state as the &amp;ldquo;truth&amp;rdquo; and having nodes react to changes in that state allow for more future flexibility (such as allowing an external management tool directly change the zookeeper state to control the cluster). Having a coordinator (as opposed to grabbing a lock every time) can be more scalable too. A hybrid model where a cluster lock is used only in certain circumstances can also make sense.&lt;/p&gt;
&lt;h3 id=&#34;single-node-simplest-use-case-1&#34;&gt;Single Node Simplest Use Case&lt;/h3&gt;
&lt;p&gt;We should be able to easily start up a single node and start indexing documents. At a later point in time, we should be able to start up a second node and have it join the cluster.&lt;/p&gt;
&lt;p&gt;start up a single node, upload it&amp;rsquo;s configuration (the first time) to zookeeper, and create+assign the node to shard1.
in the absence of other information when the config is created, a single shard system is assumed
index some documents
start up another node and pass it a parameter that says &amp;ldquo;if you are not already assigned, assign yourself to any shard that has the lowest number of replicas and start recovery process&amp;rdquo;
avoid replicating a shard on the same host if possible
after this point, one should be able to kill the node and start it up again and have it resume the same role (since it should see itself in zookeeper)&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>New SolrCloud Designの翻訳（その１）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/28/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE%EF%BC%91/</link>
      <pubDate>Wed, 28 Sep 2011 20:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/28/new-solrcloud-design%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%81%9D%E3%81%AE%EF%BC%91/</guid>
      <description>ちょっと興味があるので、訳してみました。（Wikiのページはこちら） 更新されているようなので、もとの文章も残しておきます。（ページ下部の続き</description>
      <content:encoded>&lt;p&gt;ちょっと興味があるので、訳してみました。（Wikiのページは&lt;a href=&#34;http://wiki.apache.org/solr/NewSolrCloudDesign&#34;&gt;こちら&lt;/a&gt;）
更新されているようなので、もとの文章も残しておきます。（ページ下部の続きはこちら部分以降）
全部訳そうと思ったのですが、終わらなかったので、まずは前半部分です。まだ、訳しただけで理解できてない。。。
（英語力のなさをさらけ出してしまうのですが、これも修行です。。。おかしいところはツッコミを。）&lt;/p&gt;
&lt;h3 id=&#34;what-is-solrcloud&#34;&gt;What is SolrCloud?&lt;/h3&gt;
&lt;p&gt;Solrクラウドはクラウドでの検索サービスとしてのSolrを管理、運用するための既存のSolrを拡張するものです。&lt;/p&gt;
&lt;h3 id=&#34;用語集&#34;&gt;用語集&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cluster：クラスタは1単位として管理されるSolrノードの集合です。クラスタ全体で単一のschema、solrconfigをもたないといけません。&lt;/li&gt;
&lt;li&gt;Node：ひとつのJVMインスタンスで起動しているSolrのこと&lt;/li&gt;
&lt;li&gt;Partition：パーティションはドキュメント集合全体のサブセット（部分集合）のことです。パーティションは部分集合のドキュメントが単一のインデックスに含まれるような形で作られます。&lt;/li&gt;
&lt;li&gt;Shard：パーティションはn（＝replication factor）個のノードに保存される必要があります。このn個のノードすべてでひとつのshardです。1つのノードはいくつかのshardの一部にで有る場合があります。&lt;/li&gt;
&lt;li&gt;Leader：各Shardは1つのリーダとなるノードを持っています。パーティションに登録されたドキュメントリーダーからコピーされます&lt;/li&gt;
&lt;li&gt;Replication Factor：クラスタによって保持されるレプリカの最小限の数&lt;/li&gt;
&lt;li&gt;Transaction Log：各ノードによって保持される書き込み処理の追記ログ&lt;/li&gt;
&lt;li&gt;Partition version：これは各shardのリーダーが持っているカウンターで、書き込み処理ごとに増加し、レプリカに送られます。&lt;/li&gt;
&lt;li&gt;Cluster Lock：これはrange（※後述されているハッシュ値の範囲のことか？）-&amp;gt;パーティションもしくはパーティション-&amp;gt;ノードのマッピングを変更するために取得しなければいけないグローバルなロックのことです。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;※用語だけだと関係がわかりづらかったので、図を書いてみました。&lt;/p&gt;
&lt;p&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.johtani.info/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20110928/20110928_2063185.png&#34; alt=&#34;SolrCloudのパーティションについて&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20110928/20110928_2063185.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

ドキュメントの集合とパーティションについての考え方&lt;/p&gt;
&lt;p&gt;

&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://blog.johtani.info/images/entries/20110928/20110928_2063186.png&#34; alt=&#34;SolrCloudのクラスターについて&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://blog.johtani.info/images/entries/20110928/20110928_2063186.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

クラスタ、ノード、シャードの考え方。&lt;/p&gt;
&lt;h3 id=&#34;処理原則&#34;&gt;&lt;strong&gt;処理原則&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;任意の処理はクラスタにある任意のノードに実行可能です。&lt;/li&gt;
&lt;li&gt;リカバリできないSPOFはありません。&lt;/li&gt;
&lt;li&gt;クラスタは伸縮自在（elastic）でなければならない&lt;/li&gt;
&lt;li&gt;書き込みが失われないこと（耐久性）を保証する&lt;/li&gt;
&lt;li&gt;書き込み順序が保証されなければならない&lt;/li&gt;
&lt;li&gt;2つのクライアントが2つの「A」というドキュメントを同時に送信してきた場合、すべてのレプリカで一貫してどちらか一方が保存されなければならない。&lt;/li&gt;
&lt;li&gt;クラスタの設定は中央管理されなければならない。また、クラスタのどのノードからもクラスタ設定が更新できます。&lt;/li&gt;
&lt;li&gt;読み込み（検索）の自動的なフェイルオーバー&lt;/li&gt;
&lt;li&gt;書き込み（インデクシング）の自動的なフェイルオーバー&lt;/li&gt;
&lt;li&gt;ノードの故障が発生しても自動的にrepcation factorの数は守られます。（故障したら動的にレプリカを再配置？）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;zookeeper&#34;&gt;&lt;strong&gt;Zookeeper&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;ZooKeeperクラスタは次のために使用されます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;クラスタ設定の集中管理&lt;/li&gt;
&lt;li&gt;分散同期に必要な操作のコーディネータ&lt;/li&gt;
&lt;li&gt;クラスタ構成を保存するためのシステム&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;partitioning&#34;&gt;&lt;strong&gt;Partitioning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;クラスタは固定されたmax_hash_value＝「N」が設定されます。
max_hash_valueは1000のような大きな値が設定されます。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
hash = hash_function(doc.getKey()) % N
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ハッシュ値の範囲がパーティションに割り当てられ、ZooKeeperに保存されます。
次の例のような形で、パーティションに対して範囲が設定されます。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
range  : partition
------  ----------
0 - 99 : 1
100-199: 2
200-299: 3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ハッシュはドキュメントにインデックスフィールドとして追加され、変更されない値です。
これは、インデックスを分割するときにも利用します。&lt;/p&gt;
&lt;p&gt;ハッシュ関数はプラガブルです。これはドキュメントを受け取り、一貫した正整数ハッシュ値を返します。デフォルトのハッシュ関数として、必須でかつ変更されないフィールド（デフォルトはユニークキーフィールド）からハッシュ値を計算する関数が提供されます。&lt;/p&gt;
&lt;h4 id=&#34;using-full-hash-range&#34;&gt;&lt;strong&gt;Using full hash range&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;max_hash_valueは必ずしも必要ではありません。各shardはいずれにしろハッシュ値の範囲持っているので、完全な32 bitsハッシュを使うこともできます。
設定可能なmax_hash_valueを利用しないで、クライアントからの値をもとにハッシュ値を作ることができます。
例えば、電子メールの検索アプリでは次のようにハッシュ関数を作ることができます。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
(hash(user_id)&amp;lt;&amp;lt;24) | (hash(message_id)&amp;gt;&amp;gt;&amp;gt;8)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ユーザIDから8bitのハッシュコードの先頭8ビットを利用することで、任意のユーザのメールがクラスタの同じ256番目（のノード？）にあるのを保証します。検索時はこの情報をもとにクラスタのその部分への問い合わせだけで情報が得られます。&lt;/p&gt;
&lt;p&gt;おそらく、私たちは最大値から最小値をカバーする範囲を表現するのにハッシュ空間を輪（固定のハッシュではなく）とみなしたいです。（？？？円状のハッシュ空間とすることで、クラスタ内のノード数の増減に耐えられるようにするよということかな？）&lt;/p&gt;
&lt;h4 id=&#34;shard-naming&#34;&gt;&lt;strong&gt;shard naming&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;シャードからハッシュ値の範囲へのマッピングを別々に管理するよりも、ハッシュコードによりパーティションを構成するときに実際にはハッシュの範囲をシャード名にします。
（シャード「1-1000」は1から1000の間のハッシュコードを持つドキュメントが含まれるという形）&lt;/p&gt;
&lt;p&gt;現時点では（コレクション1つに対してシングルコアの1Solrサーバと仮定して）solrコア名はコレクション名をつけるようになっています。
同一コレクションのためのマルチコアに対してのいい命名規則をつけるという課題が残っています。
（※コレクションに対する説明がここまでないかな？）&lt;/p&gt;
&lt;h3 id=&#34;shard-assignment&#34;&gt;Shard Assignment&lt;/h3&gt;
&lt;p&gt;ノード-&amp;gt;パーティションのマッピングはZooKeeperにあるCluster Lockを取得したノードによってのみ変更が可能です。
ノードの追加時に、まず、Cluster Lockを取得し、次にそれがどのパーティションを取得できるかを識別します。&lt;/p&gt;
&lt;h4 id=&#34;node-to-a-shard-assignment&#34;&gt;Node to a shard assignment&lt;/h4&gt;
&lt;p&gt;新しいノードを探しているノードはまずCluster Lockを取得しないといけません。
第一に、リーダーはシャードを決めます。
シャードが持つ、すべての利用可能なノード数で最小の値を持つノードが選び出されます。
もし、同値ならランダムにノードを選びます。&lt;/p&gt;
&lt;p&gt;原文はこちらからです。&lt;/p&gt;
&lt;h3 id=&#34;new-solrcloud-design&#34;&gt;&lt;strong&gt;New SolrCloud Design&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;(Work in progress)&lt;/p&gt;
&lt;h3 id=&#34;what-is-solrcloud-1&#34;&gt;What is SolrCloud?&lt;/h3&gt;
&lt;p&gt;SolrCloud is an enhancement to the existing Solr to manage and operate Solr as a search service in a cloud.&lt;/p&gt;
&lt;h3 id=&#34;glossary&#34;&gt;Glossary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cluster : Cluster is a set of Solr nodes managed as a single unit. The entire cluster must have a single schema and solrconfig&lt;/li&gt;
&lt;li&gt;Node : A JVM instance running Solr&lt;/li&gt;
&lt;li&gt;Partition : A partition is a subset of the entire document collection. A partition is created in such a way that all its documents can be contained in a single index.&lt;/li&gt;
&lt;li&gt;Shard : A Partition needs to be stored in multiple nodes as specified by the replication factor. All these nodes collectively form a shard. A node may be a part of multiple shards&lt;/li&gt;
&lt;li&gt;Leader : Each Shard has one node identified as its leader. All the writes for documents belonging to a partition should be routed through the leader.&lt;/li&gt;
&lt;li&gt;Replication Factor : Minimum number of copies of a document maintained by the cluster&lt;/li&gt;
&lt;li&gt;Transaction Log : An append-only log of write operations maintained by each node&lt;/li&gt;
&lt;li&gt;Partition version : This is a counter maintained with the leader of each shard and incremented on each write operation and sent to the peers&lt;/li&gt;
&lt;li&gt;Cluster Lock : This is a global lock which must be acquired in order to change the range -&amp;gt; partition or the partition -&amp;gt; node mappings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;guiding-principles&#34;&gt;&lt;strong&gt;Guiding Principles&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Any operation can be invoked on any node in the cluster.&lt;/li&gt;
&lt;li&gt;No non-recoverable single point of failures&lt;/li&gt;
&lt;li&gt;Cluster should be elastic&lt;/li&gt;
&lt;li&gt;Writes must never be lost i.e. durability is guaranteed&lt;/li&gt;
&lt;li&gt;Order of writes should be preserved&lt;/li&gt;
&lt;li&gt;If two clients send document &amp;ldquo;A&amp;rdquo; to two different replicas at the same time, one should consistently &amp;ldquo;win&amp;rdquo; on all replicas.&lt;/li&gt;
&lt;li&gt;Cluster configuration should be managed centrally and can be updated through any node in the cluster. No per node configuration other than local values such as the port, index/logs storage locations should be required&lt;/li&gt;
&lt;li&gt;Automatic failover of reads&lt;/li&gt;
&lt;li&gt;Automatic failover of writes&lt;/li&gt;
&lt;li&gt;Automatically honour the replication factor in the event of a node failure&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;zookeeper-1&#34;&gt;&lt;strong&gt;Zookeeper&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A ZooKeeper cluster is used as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The central configuration store for the cluster&lt;/li&gt;
&lt;li&gt;A co-ordinator for operations requiring distributed synchronization&lt;/li&gt;
&lt;li&gt;The system-of-record for cluster topology&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;partitioning-1&#34;&gt;&lt;strong&gt;Partitioning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The cluster is configured with a fixed max_hash_value (which is set to a fairly large value, say 1000) ‘N’. Each document’s hash is calculated as:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
hash = hash_function(doc.getKey()) % N
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ranges of hash values are assigned to partitions and stored in Zookeeper. For example we may have a
range to partition mapping as follows&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
range  : partition
------  ----------
0 - 99 : 1
100-199: 2
200-299: 3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The hash is added as an indexed field in the doc and it is immutable. This may also be used during an index split&lt;/p&gt;
&lt;p&gt;The hash function is pluggable. It can accept a document and return a consistent &amp;amp; positive integer hash value. The system provides a default hash function which uses the content of a configured, required &amp;amp; immutable field (default is unique_key field) to calculate hash values.&lt;/p&gt;
&lt;h4 id=&#34;using-full-hash-range-1&#34;&gt;&lt;strong&gt;Using full hash range&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Alternatively, there need not be any max_hash_value - the full 32 bits of the hash can be used since each shard will have a range of hash values anyway. Avoiding a configurable max_hash_value makes things easier on clients wanting related hash values next to each other. For example, in an email search application, one could construct a hashcode as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
(hash(user_id)&amp;lt;&amp;lt;24) | (hash(message_id)&amp;gt;&amp;gt;&amp;gt;8)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By deriving the top 8 bits of the hashcode from the user_id, it guarantees that any users emails are in the same 256th portion of the cluster. At search time, this information can be used to only query that portion of the cluster.&lt;/p&gt;
&lt;p&gt;We probably also want to view the hash space as a ring (as is done with consistent hashing) in order to express ranges that wrap (cross from the maximum value to the minimum value).&lt;/p&gt;
&lt;h4 id=&#34;shard-naming-1&#34;&gt;&lt;strong&gt;shard naming&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;When partitioning is by hash code, rather than maintaining a separate mapping from shard to hash range, the shard name could actually be the hash range (i.e. shard &amp;ldquo;1-1000&amp;rdquo; would contain docs with a hashcode between 1 and 1000).&lt;/p&gt;
&lt;p&gt;The current convention for solr-core naming is that it match the collection name (assuming a single core in a solr server for the collection). We still need a good naming scheme for when there are multiple cores for the same collection.&lt;/p&gt;
&lt;h3 id=&#34;shard-assignment-1&#34;&gt;Shard Assignment&lt;/h3&gt;
&lt;p&gt;The node -&amp;gt; partition mapping can only be changed by a node which has acquired the Cluster Lock in ZooKeeper. So when a node comes up, it first attempts to acquire the cluster lock, waits for it to be acquired and then identifies the partition to which it can subscribe to.&lt;/p&gt;
&lt;h4 id=&#34;node-to-a-shard-assignment-1&#34;&gt;Node to a shard assignment&lt;/h4&gt;
&lt;p&gt;The node which is trying to find a new node should acquire the cluster lock first. First of all the leader is identified for the shard. Out of the all the available nodes, the node with the least number of shards is selected. If there is a tie, the node which is a leader to the least number of shard is chosen. If there is a tie, a random node is chosen.&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 3.4リリース（速報）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/15/lucene-solr-3-4%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</link>
      <pubDate>Thu, 15 Sep 2011 09:31:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/15/lucene-solr-3-4%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</guid>
      <description>Solr/Lucene 3.4がリリースされました。（速報） 以下、各サイトへのリンクです。 Solrリリースのお知らせ Luceneリリースのお知らせ ちなみに、先日の</description>
      <content:encoded>&lt;p&gt;Solr/Lucene 3.4がリリースされました。（速報）&lt;/p&gt;
&lt;p&gt;以下、各サイトへのリンクです。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lucene.apache.org/solr/#14+September+2011+-+Solr+3.4.0+Released&#34;&gt;Solrリリースのお知らせ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lucene.apache.org/#14+September+2011+-+Lucene+Core+3.4.0+and+Solr+3.4.0+Available&#34;&gt;Luceneリリースのお知らせ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ちなみに、先日のSolr勉強会で関口さんが話されていた&lt;a href=&#34;https://issues.apache.org/jira/browse/LUCENE-3418&#34;&gt;インデックスが壊れるバグ&lt;/a&gt;ですが、
先日のアメリカのハリケーン（Irene）で実際に電源が落ちて見つかったみたいです。&lt;/p&gt;
&lt;p&gt;ということで、3.4がリリースされたので、3.1~3.3は利用しないほうがいいようです。&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#FF0000&#34;&gt;追記：&lt;/span&gt;
&lt;a href=&#34;http://www.lucidimagination.jp/2011/09/lucene-3-4solr-3-4-%E3%81%8C%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/&#34;&gt;lucidimagination.jpのサイトに日本語のリリースノートが公開されていた&lt;/a&gt;ので、リンクを記載しておきます。&lt;/p&gt;
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 3.3リリース（速報）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/01/lucene-solr-3-3%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</link>
      <pubDate>Fri, 01 Jul 2011 15:29:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/01/lucene-solr-3-3%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E9%80%9F%E5%A0%B1/</guid>
      <description>Solr/Lucene 3.3がリリースされました。（速報） 以下、各サイトへのリンクです。 Solrリリースのお知らせ Luceneリリースのお知らせ リリースのタイミ</description>
      <content:encoded>&lt;p&gt;Solr/Lucene 3.3がリリースされました。（速報）&lt;br /&gt;
&lt;br /&gt;
以下、各サイトへのリンクです。&lt;br /&gt;
&lt;br /&gt;
&lt;a href=&#34;http://lucene.apache.org/solr/#July+2011+-+Solr+3.3+Released&#34;&gt;Solrリリースのお知らせ&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;a href=&#34;http://lucene.apache.org/#1+July+2011+-+Lucene+Core+3.3+and+Solr+3.3+Available&#34;&gt;Luceneリリースのお知らせ&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;
リリースのタイミングがどんどん早くなってる。。。&lt;/p&gt;
</content:encoded>
    </item>
    
  </channel>
</rss>
