<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>lucene-gosen on @johtaniの日記 3rd</title>
    <link>https://blog.johtani.info/tags/lucene-gosen/</link>
    <description>Recent content in lucene-gosen on @johtaniの日記 3rd</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Tue, 28 Jan 2014 12:34:00 +0900</lastBuildDate><atom:link href="https://blog.johtani.info/tags/lucene-gosen/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>lucene-gosen 4.6.1のリリースに関する注意点</title>
      <link>https://blog.johtani.info/blog/2014/01/28/release-lucene-gosen-4-dot-6-1/</link>
      <pubDate>Tue, 28 Jan 2014 12:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2014/01/28/release-lucene-gosen-4-dot-6-1/</guid>
      <description>Lucene/Solr 4.6.1がリリースされそう(バイナリ配布待ち)lucene-gosenの4.6.1対応版をリリースしました。 ライブラリのインタフェースな</description>
      <content:encoded><p>Lucene/Solr 4.6.1がリリースされそう(バイナリ配布待ち)<a href="https://code.google.com/p/lucene-gosen/">lucene-gosen</a>の4.6.1対応版をリリースしました。</p>
<p>ライブラリのインタフェースなどは特に変更はないのですが、ライブラリのダウンロード先が変更になっているため、注意喚起です。</p>
<!-- more -->
<p>Google Project Hostingの仕様変更により、Downloadsに新規ファイルがアップロードできなくなっています。（2014年から）</p>
<p>このため、プロジェクトの選択肢としては以下の3点となっています。</p>
<ol>
<li>Google Driveにファイルをアップロードしてダウンロードしてもらう</li>
<li>他のソースコード管理サイトなどを利用する。</li>
<li>他のダウンロードサイトを利用する</li>
</ol>
<p>1.と3.は場所が違うだけで、方法は一緒です。
今回は、暫定的に1.を利用してダウンロードするように対応しました。</p>
<p>ダウンロード先はプロジェクトのページにリンクが有りますが、わかりにくいのでキャプチャを撮ってみました。</p>

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20140128/project_home.jpg" />
    </div>
    <a href="/images/entries/20140128/project_home.jpg" itemprop="contentUrl"></a>
      <figcaption align="center"><h4>ダウンロード先</h4>
      </figcaption>
  </figure>
</div>

<p>これまでの<code>Featured - Downloads</code>とは異なり、<code>Links - External links</code>の下に
<a href="https://drive.google.com/folderview?id=0B0xz3tf1TTPnYTlSNExkTzBhWnc&amp;usp=sharing">Downloads lucene-gosen 4.6.1</a>というリンクを用意してあります。</p>
<p>フォルダとなっており、各種jarファイルがリストされていますので、こちらからダウンロードをお願いします。
今後は、この下にダウンロードリンクを追加していく予定です。</p>
<p>ただし、2.で述べたように「別のソースコード管理サイト」も検討中です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen4.3.0をリリースしました。（Lucene/Solr4.3.0以上での利用が可能です）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2013/05/06/lucene-gosen4-3-0%E3%82%92%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9Flucene-solr4-3-0%E4%BB%A5%E4%B8%8A%E3%81%A7%E3%81%AE%E5%88%A9%E7%94%A8%E3%81%8C%E5%8F%AF%E8%83%BD%E3%81%A7%E3%81%99/</link>
      <pubDate>Mon, 06 May 2013 23:25:24 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/05/06/lucene-gosen4-3-0%E3%82%92%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9Flucene-solr4-3-0%E4%BB%A5%E4%B8%8A%E3%81%A7%E3%81%AE%E5%88%A9%E7%94%A8%E3%81%8C%E5%8F%AF%E8%83%BD%E3%81%A7%E3%81%99/</guid>
      <description>Lucene/Solr 4.3.0がリリースされた（LuceneのChanges、SolrのChanges）ので、lucene-gosen 4.3.0をリリースしま</description>
      <content:encoded><p>Lucene/Solr 4.3.0がリリースされた（<a href="http://lucene.apache.org/core/4_3_0/changes/Changes.html">LuceneのChanges</a>、<a href="http://lucene.apache.org/solr/4_3_0/changes/Changes.html">SolrのChanges</a>）ので、lucene-gosen 4.3.0をリリースしました。（<a href="http://code.google.com/p/lucene-gosen/downloads/list?PHPSESSID=ab5edaac2154a82b90c0d9865454c0c9">ダウンロードはこちら</a>）
なお、lucene-gosen 4.3.0ですが、<span style="color:#FF0000">Lucene/Solr 4.2.1以下</span>のバージョンのLucene/Solrでは利用<span style="color:#FF0000">できません。</span>
注意してください。
また、lucene-gosen 4.2.1もLucene/Solr 4.3.0では動作しませんので注意が必要です。</p>
<p>現時点（2013/05/06）では、lucene-gosen 4.3.0はLucene/Solr 4.3.0でのみ利用できます。
これは、<a href="http://johtani.jugem.jp/?eid=129">先日のエントリ</a>にも書きましたが、LuceneにてAPIの変更が行われたためとなります。
いくつかのクラスおよびメソッドが廃止されたため、下位互換が保てない変更が入っているためです。</p>
<p>独自のTokenizerやTokenizerFactory、TokenFilterFactory、CharFilterFactoryを作成されている方は、Lucene/Solrのバージョンアップを行う際は注意が必要です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Lucene/Solr 4.0.0リリース＆lucene-gosenの4.0対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/10/12/lucene-solr-4-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9lucene-gosen%E3%81%AE4-0%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Fri, 12 Oct 2012 17:45:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/10/12/lucene-solr-4-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9lucene-gosen%E3%81%AE4-0%E5%AF%BE%E5%BF%9C/</guid>
      <description>##Lucene/Solr 4.0.0リリース ついに、Lucene/Solr4.0.0がリリースされました。 MLで流れていましたが、3年越しのリリースだったようです。</description>
      <content:encoded><p>##Lucene/Solr 4.0.0リリース
ついに、Lucene/Solr4.0.0がリリースされました。
MLで流れていましたが、3年越しのリリースだったようです。
コミッターの方々、JIRAにバグ報告をした方々、お疲れ様でした。</p>
<p>ということで、ちょっと忙しくなりそうです。。。
4.0の機能を調べたりもしたいですし、すこしずつ紹介もしたいです。</p>
<p>本家サイトのニュースは<a href="http://lucene.apache.org/solr/solrnews.html">こちら</a></p>
<hr>
<p>##lucene-gosenの4.0対応版について
lucene-gosenも4.0正式版のjarを利用したバージョンを公開する予定です。
branches/4xでは、すでに作業を行なっており、jarファイルの差し替えは終了しています。
お急ぎの方は、branches/4xをエクスポートして、ビルドしていただくと利用可能となっています。
なお、Lucene/Solrのバージョンが上がっているため、lucene-gosenのメジャーバージョンも変更し、lucene-gosen-4.0.0としてリリース<del>する予定です</del>しました。。（順当に行けば、3.0.0ですが、Luceneのメジャーバージョンに合わせたほうがわかりやすいかと思いまして。）
また、現在、trunkが3.6.x対応のソースになっていますが、このあと、現在のbranches/4xのソースをtrunkにする予定です。
3.6.x対応のlucene-gosenについては、branches/lucene-gosen-rel2.0にて作業を行うこととなります。
今後は注意してチェックアウトするようにお願いいたします。</p>
<p>ということで、<a href="http://code.google.com/p/lucene-gosen/downloads/list">ダウンロードできるようにしました。</a>
lcuene-gosen-4.0.0*.jarとついているものがLucene/Solr 4.0.0に対応したライブラリになります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのLucene/Solr4.0-ALPHA対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/07/05/lucene-gosen%E3%81%AElucene-solr4-0-alpha%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Thu, 05 Jul 2012 12:11:49 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/07/05/lucene-gosen%E3%81%AElucene-solr4-0-alpha%E5%AF%BE%E5%BF%9C/</guid>
      <description>Lucene/Solrの4.0.0-ALPHAが7/3にリリースされました。 これに伴い、lucene-goenの4xブランチのjarファイル</description>
      <content:encoded><p>Lucene/Solrの4.0.0-ALPHAが7/3にリリースされました。</p>
<p>これに伴い、lucene-goenの4xブランチのjarファイルも4.0-ALPHAのものに置き換え、現在のtrunkの修正もマージしました。
<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2F4x">こちらに</a>あります。チェックアウトしてビルドしてから利用してください。</p>
<p>※さすがに、jarをダウンロードできるようにすべきかもなぁ。
あと、Maven登録も。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのUniDic対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/18/lucene-gosen%E3%81%AEunidic%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Mon, 18 Jun 2012 23:03:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/18/lucene-gosen%E3%81%AEunidic%E5%AF%BE%E5%BF%9C/</guid>
      <description>Issue 32で上がってきたlucee-gosenのUniDic対応の最初のパッチを書いたので、ブログに残しておきます。 ###UniDicとは___</description>
      <content:encoded><p>Issue 32で上がってきたlucee-gosenのUniDic対応の最初のパッチを書いたので、ブログに残しておきます。</p>
<p>###UniDicとは___
<a href="http://www.tokuteicorpus.jp/dist/">UniDic</a>とは、日本語テキストを単語に分割し，形態論情報を付与するための電子化辞書です。
<a href="http://www.tokuteicorpus.jp/dist/">UniDicの詳細や特長についてはHP</a>を御覧ください。</p>
<p>残念ながら、UniDicは利用者登録をして、利用規約に従うと利用が可能となります。
ですので、lucene-gosenでは、Ipadicやnaist-chasenの辞書とは異なり自動で辞書をダウンロードする機能はありません。</p>
<p>###利用手順___
以下が、Unidicの辞書を利用したjarファイルの作成方法となります。</p>
<p><b>1. lucene-gosenをダウンロードし、パッチを当てる</b>
lucene-gosenのリポジトリからソースをエクスポートし、パッチをダウンロードし、パッチを適用します。
コマンドは以下のとおりです。</p>
<pre><code>
svn co . lucene-gosen-trunk-readonly
cd lucene-gosen-trunk-readonly
patch -p0 &amp;gt; ...patch
</code></pre><p>（パッチに関しては今後正式版をリリースされたら手順からは必要なくなります。）</p>
<p><b>2. Unidic辞書生成のためのディレクトリを作成「$GOSEN_HOME/dictionary/unidic」</b></p>
<pre><code>
mkdir dictionary/unidic
</code></pre><p><b>3. 対象となるUnidicの辞書のソースファイルをダウンロード</b>
利用者登録をし、利用規約に同意の上、以下のファイルをダウンロードします。
「/」に添ってダウンロードページから遷移してダウンロードしてください。</p>
<pre><code>
1.3.12個別ファイル/unidic-chasen/unidic-chasen1312src.tar.gz
</code></pre><p><b>4. ダウンロードしたtar.gzファイルを「dictionary/unidic/」にコピー</b></p>
<pre><code>
cp .. lucene-gosen-trunk-readonly/dictionary/unidic/
</code></pre><p><b>5. Antを実行してjarファイルの作成</b></p>
<pre><code>
ant -Ddictype=unidic
</code></pre><p>成功すれば、lucene-gosen-trunk-readonly/dist/lucene-gosen-2.1-dev-unidic.jarファイルが生成されます。
あとは、通常通り、SolrやLuceneで利用することが可能です。</p>
<p>以上がjarファイルの作成手順となります。</p>
<p>###制約事項（2012/06/18現在）___
現在（2012/06/18）時点で公開しているパッチは、以下の制約が存在します。</p>
<ul>
<li>COMPOUNDエントリー未対応</li>
<li>品詞情報（発音）の内容の制限</li>
</ul>
<p><b>COMPOUNDエントリー未対応</b>
Unidicの辞書のエントリーの中に1件だけ、COMPOUNDと呼ばれるエントリーが1件だけ存在します。
別々の単語を組み合わせて1単語として扱うことができるようになっているようです。
lucene-gosenでは、残念ながら、このような辞書の形式には対応していません。
1件しか存在しないデータでもあることを鑑みて、今回の辞書構築処理では、スキップするようにしました。</p>
<p><b>品詞情報（発音）の内容の制限</b>
lucene-gosenの実装上、単語の読みのバリエーション数と発音のバリエーション数には以下の制限が存在します。</p>
<pre><code>
「読み」バリエーション数  ＜ 「発音」バリエーション数
</code></pre><p>「読み」に対応する形で、「発音」がlucene-gosenでは品詞情報としてデータ登録されています。
UniDicのデータには上記制約を満たさないデータが5件ほど存在します。
現在、これら5件のデータについて、「読み」に対応した「発音」データには空文字が表示されるようになっています。</p>
<p>まだ、簡単に動作確認をしただけです。UniDicを利用していて問題など有りましたら連絡、Issueへのアップをしていただけると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 2.0.2リリース（リソース周りの改善など）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/07/lucene-gosen-2-0-2%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E5%91%A8%E3%82%8A%E3%81%AE%E6%94%B9%E5%96%84%E3%81%AA%E3%81%A9/</link>
      <pubDate>Thu, 07 Jun 2012 17:53:54 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/07/lucene-gosen-2-0-2%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E5%91%A8%E3%82%8A%E3%81%AE%E6%94%B9%E5%96%84%E3%81%AA%E3%81%A9/</guid>
      <description>lucene-gosenの最新版（2.0.1）をリリースしました。 プロジェクトページよりダウンロードが可能です。 今回の修正では、@haruy</description>
      <content:encoded><p>lucene-gosenの最新版（2.0.1）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>今回の修正では、@haruyama さんからいただいていたパッチの取り込み（<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=29">リソース周りの改善など</a>）が主な対応となっています。
また、コンパイルに利用するjarファイルがLucene/Solr3.6.0に変更になっています。（<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=31">Issueはこちら</a>）
3.6.0から追加されたテストケースにて、<a href="http://johtani.jugem.jp/?eid=88">発生する問題への対処</a>も施したものとなっています。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Issue32について（4096の壁）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/06/issue32%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A64096%E3%81%AE%E5%A3%81/</link>
      <pubDate>Wed, 06 Jun 2012 01:21:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/06/issue32%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A64096%E3%81%AE%E5%A3%81/</guid>
      <description>昨晩に引き続き、情けない内容のブログになってしまいますが。。。 昨晩、書いた記事の調査をしていた時に気づいた、問題になるケースがあったので調査</description>
      <content:encoded><p>昨晩に引き続き、情けない内容のブログになってしまいますが。。。</p>
<p>昨晩、<a href="http://johtani.jugem.jp/?eid=88">書いた記事</a>の調査をしていた時に気づいた、問題になるケースがあったので調査をしていました。
<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=32">Issue32</a>に登録した内容になります。
拙い英語を振り絞って書いた英語なので、伝わらないかもしれないのでブログに残しておきます。
昨晩の問題点となったクラス（StreamTagger2.java）の内部処理についてです。
lucene-gosenのLucene向けのTokenizerの内部処理では入力文字列の処理を行うのに、「char buffer[]」を用いて
入力文字列をReaderから読み込むときにバッファリングしています。
このバッファリングにて、特定のケースにて、想定していない場所を単語の切れ目と認識してしまう問題が実装上存在しました。
<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=32">Issue32</a>に記載した内容は次のようになります。</p>
<p>上記バッファは4096文字というサイズで固定になっています。
StreamTagger2クラスでは、この4096文字をオーバーするような文字列の場合に、つぎの処理により、4096文字以下の場所に文章の区切りを探そうとします。</p>
<ol>
<li>4096文字以上の文字列が入力される</li>
<li>バッファに入れられるだけの文字（4096文字）をバッファにコピーする。</li>
<li>バッファの後ろから、つぎの5種類の文字を探索し、見つかった場合（場所をkとし、ｋ＞０の場合）は、その場所を文章の切れ目と判定して、0からk+1文字目までの文字をStringTaggerを利用して形態素解析する。</li>
<li>あとは、繰り返し</li>
<li>上記条件に合致しない場合は、4096文字を文章とみなして形態素解析を行います。</li>
</ol>
<p>ここで、5種類の文字は以下のとおり。</p>
<pre><code>
0x000D:CARRIAGE RETURN(CR)
0x000A:LINE FEED(LF)
0x0085:NEXT LINE (NEL)
0x2028:LINE SEPARATOR
0x2029:PARAGRAPH SEPARATOR
</code></pre><p>見ての通り、制御文字ばかりです。
この5文字以外は切れ目と判断してくれません。
ということで、4097文字の文字列が存在し、上記5種類の文字が一度も出てこない場合、バッファのサイズで文字列が途切れてしまい、想定しない区切り位置で区切られた文章に対して形態素解析が実行されてしまいます.
HTMLから文字列を抜き出して解析したり、長い文書を解析する場合に、改行文字を削除して処理するといったことも考えられます。
上記5種類の文字列のみで文章の区切り位置を判断してしまうのは問題ではないかと。
まずは、4096文字内に「。」句点が存在した場合に、その部分を区切り位置として認識するようなパッチを記載して、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=32">Issue32</a>に添付しました。</p>
<p>「。」句点を採用した理由はつぎのとおりです。
StreamTagger2では、上記バッファリングした文字列を更に、BreakIterator.getSentenceIterator(Locale.JAPANESE)にて取得したBreakIteratorにて文節単位に区切ってから、各文節ごとに形態素解析を実施しています。
ということは、BraekIteratorにて分節の区切りとして判断される文字については、上記の文字種に追加しても問題無いという判断からです。
ただし、この修正でも、純粋に4096文字以上、句点が出てこない場合には区切り位置がおかしなことになってしまいますが。。。
もう少し、BreakIteratorの挙動を調べて、他にも利用可能な区切り文字が存在しないかを調査していく予定です。。。</p>
<p>1年以上コミッターをやっているのに、こんなことも理解していないのかよいうツッコミを受けてしまいそうでなんとも情けない話です。。。
まずは、現状報告でした。</p>
</content:encoded>
    </item>
    
    <item>
      <title>trunkのライブラリ差し替え（Lucene/Solr3.6.0）とランダムテストの失敗について(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/06/05/trunk%E3%81%AE%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E5%B7%AE%E3%81%97%E6%9B%BF%E3%81%88lucene-solr3-6-0%E3%81%A8%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%83%86%E3%82%B9%E3%83%88%E3%81%AE%E5%A4%B1%E6%95%97%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Tue, 05 Jun 2012 01:29:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/06/05/trunk%E3%81%AE%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E5%B7%AE%E3%81%97%E6%9B%BF%E3%81%88lucene-solr3-6-0%E3%81%A8%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%83%86%E3%82%B9%E3%83%88%E3%81%AE%E5%A4%B1%E6%95%97%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>久々にlucene-gosenを触っています。 trunkのlibにある、jarファイルが3.5ベースだったので、3.6ベースにしてテストをし</description>
      <content:encoded><p>久々にlucene-gosenを触っています。
trunkのlibにある、jarファイルが3.5ベースだったので、3.6ベースにしてテストをしたところ、
いくつかある、ランダムテストで結果の不整合が検出されたので、調査していました。
先程、trunkに対応版をコミットしました。もう少しテストケースを追加してからリリースします。
おそらく、通常の使い方では問題無いと思います。</p>
<p>Luceneでは、ランダムな文字列を利用したテストが実装されています。
lucene-gosenでもこのテストを利用してランダムなテストをしています。
実際にはtest/以下のorg.apache.luceneパッケージにテストケースがあります。
今回、jarファイルを差し替えた時に、このランダムなテスト実施にて、assertが失敗するケースが発生しました。</p>
<p>原因究明までに、いくつかフェーズがあったので、忘れないように書いておきます。</p>
<p>1.ランダムテストのテストケースにてエラーがassertが失敗するケースが発生
※ただし、成功する場合もあり。
2.該当のテストを再現しつつデバッグ＋該当のテストがどんなものかを解読（勉強不足。。。）
※テストは、失敗した場合につぎのようなメッセージが表示され、同じテストが再現可能です。
以下、エラーの出力例。「NOTE: reproduce with: 」のあとにあるantコマンドを実行すれば、同じテストが再現可能です。</p>
<pre><code>
    [junit] ------------- Standard Error -----------------
    [junit] TEST FAIL: useCharFilter=false text='wgxznwk'
    [junit] 
    [junit] ===&gt;
    [junit] Uncaught exception by thread: Thread[Thread-3,5,main]
    [junit] org.junit.ComparisonFailure: term 0 expected:&lt;w[gxznwk]&gt; but was:&lt;w[]&gt;
    [junit] 	at org.junit.Assert.assertEquals(Assert.java:125)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:146)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:565)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:396)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.access$000(BaseTokenStreamTestCase.java:51)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:337)
    [junit] &lt;===
    [junit] 
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGosenAnalyzer -Dtestmethod=testReliability -Dtests.seed=4ad9618caecb9fb2:d5476c03b8172df:-9c569f70013ffbb -Dargs=&quot;-Dfile.encoding=SJIS&quot;
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testReliability(org.apache.lucene.analysis.gosen.TestGosenAnalyzer):	Caused an ERROR

</code></pre><p>3.デバッグの結果、Luceneのtest-frameworkにある「MockReaderWrapper」というクラスの影響を確認
LuceneのJIRAの<a href="https://issues.apache.org/jira/browse/LUCENE-3894">LUCENE-3894</a>にて追加されたクラスであるとわかる。
このクラス、Readerのreadメソッド内部で、ランダムな値を元に長さを途中で返すという実装のReaderになっている。
（全部で18文字の文字列なのだが、ランダムな値を元に、12文字として結果を一旦返すという仕組みが実装されている）
4.lucene-gosenの問題箇所を特定。
Readerが途切れた部分を分節として扱ってしまう実装になっていた。
<a href="http://code.google.com/p/lucene-gosen/source/detail?r=204">該当の部分に修正をいれてコミット。</a>
という具合です。
一応、テストを数回走らせてランダムテストが問題なく終了するのを確認はしてあります。
今回の問題に対する、個別のテストケースを追加してから近日中リリースする予定です。
対応が遅くなって申し訳ないです。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 2.0.1リリース（Java7対応）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/05/09/lucene-gosen-2-0-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9java7%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Wed, 09 May 2012 01:35:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/05/09/lucene-gosen-2-0-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9java7%E5%AF%BE%E5%BF%9C/</guid>
      <description>lucene-gosenの最新版（2.0.1）をリリースしました。 プロジェクトページよりダウンロードが可能です。 今回の修正では、Java7で</description>
      <content:encoded><p>lucene-gosenの最新版（2.0.1）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>今回の修正では、Java7でUnicodeのバージョン変更に伴う対応（<a href="http://johtani.jugem.jp/?eid=73">詳細はこちらを参照</a>）を行なっています。</p>
<p>リソース周りの対応はまた後日。。。すみません。2012/05/16
遅くなりましたが、昨晩、JavaDocをアップしました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのJava7でのテスト失敗問題の対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/05/lucene-gosen%E3%81%AEjava7%E3%81%A7%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E5%A4%B1%E6%95%97%E5%95%8F%E9%A1%8C%E3%81%AE%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Thu, 05 Apr 2012 00:20:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/05/lucene-gosen%E3%81%AEjava7%E3%81%A7%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E5%A4%B1%E6%95%97%E5%95%8F%E9%A1%8C%E3%81%AE%E5%AF%BE%E5%BF%9C/</guid>
      <description>先日、2.0.0リリースの記事にも記載しましたが、Java7でテストケースが失敗する問題がありました。 @haruyamaさんと@hideak</description>
      <content:encoded><p>先日、<a href="http://johtani.jugem.jp/?eid=72">2.0.0リリースの記事</a>にも記載しましたが、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=28">Java7でテストケースが失敗する問題</a>がありました。</p>
<p>@haruyamaさんと@hideaki_tさんの協力により問題を解消し、trunkと4xブランチにコミットしました。</p>
<p>issueにも記載しましたが、Java6からJava7にバージョンアップするタイミングで変更されたUnicodeのバージョンが原因でした。
Java6ではUnicodeのバージョンが4.0です。Java7ではUnicodeのバージョンが6.0に変更されています。
今回の問題は「・」（0x30FB）の文字列のCharacter.getType()がCONNECTOR_PUNCTUATIONからOTHER_PUNCTUATIONに変更されたのが原因です。（この変更自体はUnicode 4.1で変更されたみたい）
カタカナ文字種の判別をlucene-gosenのnet.java.sen.tokenizers.ja.JapaneseTokenizerのgetCharClass(char c)メソッドで行なっています。
修正前は、ここで、charの範囲が0x30A0～0x30FFにある文字でかつ、Character.getType()がCONNECTOR_PUNCTUATIONでないものがカタカナとして判別されていました。
issueの添付ファイルにJava6とJava7で上記範囲の文字のCharacter.getType()のリストを生成して、該当する文字を探した所、「・」（0x30FB）のみであることがわかりました。
ということで、このコードの意図としては、「・」はカタカナではないと判別したかったのだと。
上記の確認を行えたので、ソースコードを修正してコミットしました。
2.0.1としてリリースするかは、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=29">Issue29</a>のボリュームを見て考えますので、もう少しお待ちください。</p>
<hr>
<p>参考にしたサイト：
<a href="http://www.hos.co.jp/blog/20111004/">JavaSE 7でメソッド名に使えなくなった文字</a>
<a href="http://www.unicode.org/reports/tr44/tr44-4.html#Change_History">UNICODE CHARACTER DATABASE<a/>のHistory</p>
</content:encoded>
    </item>
    
    <item>
      <title>【重要】lucene-gosen 2.0.0リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/04/02/%E9%87%8D%E8%A6%81lucene-gosen-2-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 02 Apr 2012 19:22:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/04/02/%E9%87%8D%E8%A6%81lucene-gosen-2-0-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>先日、宣言したとおり、lucene-gosenのパッケージ名＋クラス名の変更を行ったlucene-gosen 2.0.0をリリースしました。 Lucene/Solr</description>
      <content:encoded><p>先日、宣言したとおり、lucene-gosenのパッケージ名＋クラス名の変更を行ったlucene-gosen 2.0.0をリリースしました。
Lucene/Solr 3.6.0のリリースを待つつもりだったのですが、なかなか出ないので先にリリースを行いました。
現時点では、branches/4xについては、パッケージ名、クラス名の修正が追いついていません。
明日までに4xブランチについても修正を反映する予定です。</p>
<p>参考までに、1.2.1から2.0.0への変更点について以下にまとめました。
また、変更に伴い、Solrのschema.xmlに記述するクラス名も変更になります。
<a href="http://lucene-gosen.googlecode.com/svn/trunk/example/schema.xml.snippet">schema.xmlのサンプルについてはこちらをご覧下さい。</a></p>
<h3 id="変更点"><strong>変更点</strong></h3>
<hr>
<p>まずは、パッケージ名の変更点です。
左が旧パッケージ名、右が新パッケージ名となります。</p>
<table class="list_view">
  <thead>
    <tr>
      <th>旧パッケージ名</th>
      <th>新パッケージ名</th>
    </tr>
  </thead>
  <tbody>
    <tr class="spec">
      <td>org.apache.lucene.analysis.ja</td>
      <td>org.apache.lucene.analysis.gosen</td>
    </tr>
    <tr class="specalt">
      <td class="alt">org.apache.lucene.analysis.ja.tokenAttributes</td>
      <td class="alt">org.apache.lucene.analysis.gosen.tokenAttributes</td>
    </tr>
  </tbody>
</table>
<p>また、パッケージ名とは別に、以下のクラス名も変更になっています。
まずは、<b>org.apache.lucene.analysis.gosen</b>のクラス名の変更点です。</p>
<table class="list_view">
  <thead>
    <tr>
      <th>旧クラス名</th>
      <th>新クラス名</th>
    </tr>
  </thead>
  <tbody>
    <tr class="spec">
      <td>JapaneseAnalyzer.java</td>
      <td>GosenAnalyzer.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseBasicFormFilter.java</td>
      <td>GosenBasicFormFilter.java</td>
    </tr>
    <tr class="spec">
      <td>JapaneseKatakanaStemFilter.java</td>
      <td>GosenKatakanaStemFilter.java</td>
    </tr>
    <tr class="specalt">
      <td>JapanesePartOfSpeechKeepFilter.java</td>
      <td>GosenPartOfSpeechKeepFilter.java</td>
    </tr>
    <tr class="spec">
      <td>JapanesePartOfSpeechStopFilter.java</td>
      <td>GosenPartOfSpeechStopFilter.java</td>
    </tr>
    <tr class="specalt">
      <td>JapanesePunctuationFilter.java</td>
      <td>GosenPunctuationFilter.java</td>
    </tr>
    <tr class="spec">
      <td>なし</td>
      <td>GosenReadingsFormFilter.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseTokenizer.java</td>
      <td>GosenTokenizer.java</td>
    </tr>
    <tr class="spec">
      <td>JapaneseWidthFilter.java</td>
      <td>GosenWidthFilter.java</td>
    </tr>
  </tbody>
</table>
次は<b>**org.apache.solr.analysis**</b>です。
<table class="list_view">
  <thead>
    <tr>
      <th>旧クラス名</th>
      <th>新クラス名</th>
    </tr>
  </thead>
  <tbody>
    <tr class="spec">
      <td>JapaneseBasicFormFilterFactory.java</td>
      <td>GosenBasicFormFilterFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseKatakanaStemFilterFactory.java</td>
      <td>GosenKatakanaStemFilterFactory.java</td>
    </tr>
    <tr class="spec">
      <td>JapanesePartOfSpeechKeepFilterFactory.java</td>
      <td>GosenPartOfSpeechKeepFilterFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>JapanesePartOfSpeechStopFilterFactory.java</td>
      <td>GosenPartOfSpeechStopFilterFactory.java</td>
    </tr>
    <tr class="spec">
      <td>JapanesePunctuationFilterFactory.java</td>
      <td>GosenPunctuationFilterFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>なし</td>
      <td>GosenReadingsFormFilterFactory.java</td>
    </tr>
    <tr class="spec">
      <td>JapaneseTokenizerFactory.java</td>
      <td>GosenTokenizerFactory.java</td>
    </tr>
    <tr class="specalt">
      <td>JapaneseWidthFilterFactory.java</td>
      <td>GosenWidthFilterFactory.java</td>
    </tr>
  </tbody>
</table>
<p>また、上記クラスに関連するテストクラスの名前も変更になっています。</p>
<p>以上がクラス名、パッケージ名の対応に関する修正ついてでした。</p>
<hr>
<p>また、現在、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=28">Java7にてテストケースが失敗する問題</a>が見つかっています。
こちらの問題の対応版についても近日中にリリースを行う予定です。</p>
<p>問題点、質問などありましたら、コメントしていただくと回答いたします。</p>
<p><b>2012-04-03追記</b>
忘れてました、すみません。今回のリリースで、以下の機能が追加されています。</p>
<ul>
<li>Antのパラメータにproxy.user、proxy.passwordの追加</li>
<li>GosenReadingsFormFilterの追加</li>
<li>TokenAttributeの修正（PronunciationsAttributeImpl、ReadingsAttributeImpl）</li>
</ul>
<p>Antは認証が必要なプロキシ環境で辞書のビルドを実施するときにユーザ名、パスワードを指定できるようにしました。</p>
<p>GosenReadingsFormFilterは単語を読みに変換するTokenFilterになります。
よみは、辞書に登録してある読みになります。オプションとして、romanizedが指定可能です。指定をすると、よみをローマ字に変換します。</p>
<p>TokenAttributeの修正は、バグフィックスになります。<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=26&amp;can=1">Issueはこちら</a>です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>【重要】lucene-gosenの次期リリースについて(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/03/27/%E9%87%8D%E8%A6%81lucene-gosen%E3%81%AE%E6%AC%A1%E6%9C%9F%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Tue, 27 Mar 2012 02:39:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/03/27/%E9%87%8D%E8%A6%81lucene-gosen%E3%81%AE%E6%AC%A1%E6%9C%9F%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>lucene-gosenを利用して頂いてる皆様に連絡があります。 連絡事項 次期lucene-gosenのリリース（2.0を予定）にて、org.</description>
      <content:encoded><p>lucene-gosenを利用して頂いてる皆様に連絡があります。</p>
<h3 id="連絡事項">連絡事項</h3>
<hr>
<p><strong>次期lucene-gosenのリリース（2.0を予定）にて、org.apache系のパッケージ名および、クラス名の変更を行います。</strong>
Lucene/Solrの次期リリース版である3.6.0以降では、lucene-gosen 2.0（予定）を利用するようにしてください。</p>
<h3 id="経緯">経緯</h3>
<hr>
<p>Twitterでは少しずつツイートしていますが、Lucene/Solr 3.6から日本語の形態素解析器がLucene/Solrにて用意されることになりました。
ベースとなっているのは、Atilika社が開発した<a href="http://atilika.org/">Kuromoji</a>という形態素解析器です。
Lucene/SolrにコントリビュートされたタイミングではKuromojiAnalyzerなど、Kuromojiという名称が残った形で取り込みが行われました。
その後、<a href="https://issues.apache.org/jira/browse/LUCENE-3909">LUCENE-3909</a>にて、Kuromojiではなく、一般的な名称（Japanese*）に変更する提案が行われました。
この提案で、luene-gosenが利用しているクラス名、パッケージ名と大半のクラスが衝突してしまうこととなりましす。
今後も、lucene-gosenを利用していただけるように、lucene-gosenの<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=27">Issue</a>を発行し、
現在、lucene-gosenのtrunkにてパッケージ名の変更及びクラス名の変更作業を行なっています。
正式にリリースするタイミングになりましたら、再度連絡いたします。</p>
<h3 id="ブランチなどについて">ブランチなどについて</h3>
<hr>
<p>現時点ではパッケージ名、クラス名の変更はリポジトリの以下のものについてのみ作業を行う予定です。</p>
<ul>
<li>trunk</li>
<li>branches/4x</li>
</ul>
<p>現時点でのリリース版（1.2系）のソースについてはbranches/rel-1.2にて、これまで同様のクラス名、パッケージ名のまま変更を行いません。
1.2系についてはこちらをご覧下さい。</p>
<p>また、当ブログにて、提供しているSolr入門のサンプルに利用可能なschema.xmlなどの<a href="http://johtani.jugem.jp/?eid=44">記事</a>についてもLucene/Solr3.6がリリースされた際には再度修正して掲載いたします。
Kuromojiの利用方法もあわせて記載したいです。</p>
<hr>
<p>参考：
lucene-gosenとKuromojiの機能などの比較については<a href="http://www.rondhuit.com/solr%E3%81%AE%E6%97%A5%E6%9C%AC%E8%AA%9E%E5%AF%BE%E5%BF%9C.html">こちら</a>を参考にしてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenとSynonymFilterを利用するときの注意点（問題点編）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/02/21/lucene-gosen%E3%81%A8synonymfilter%E3%82%92%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AE%E6%B3%A8%E6%84%8F%E7%82%B9%E5%95%8F%E9%A1%8C%E7%82%B9%E7%B7%A8/</link>
      <pubDate>Tue, 21 Feb 2012 01:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/02/21/lucene-gosen%E3%81%A8synonymfilter%E3%82%92%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AE%E6%B3%A8%E6%84%8F%E7%82%B9%E5%95%8F%E9%A1%8C%E7%82%B9%E7%B7%A8/</guid>
      <description>久々にlucene-gosenの話です。 しかも、あんまり嬉しくない話しです。 すでにissueをアップしていますが、lucene-gosenと</description>
      <content:encoded><p>久々にlucene-gosenの話です。
しかも、あんまり嬉しくない話しです。</p>
<p>すでに<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=23">issueをアップ</a>していますが、lucene-gosenとSynonymFilterを併用する場合に、特定の条件下でNullPointerExceptionが発生してしまいます。</p>
<p>条件は以下の組み合わせになります。</p>
<ul>
<li>Solr 3.5.0以前</li>
<li>lucene-gosen1.2.0 - 1.2.1の辞書なしjar</li>
<li>SynonymFilterFactoryにてtokenizerFactoryを指定</li>
</ul>
<p>根本的にはSolr側の問題のようです。<a href="https://issues.apache.org/jira/browse/SOLR-2909">SOLR-2909</a>としてissueが上がっています。</p>
<p>SynonymFilterFactoryでは、類義語の設定ファイルの単語を読み込むときにtokenizerFactoryを指定できます。
このとき、SynonymFilterFactory内部でtokenizerFactoryに指定されたFactoryのクラスが読み込まれ、
インスタンス化されて、Tokenizerが作成されます。
この、Tokenizerのインスタンス化の処理シーケンスに問題があります。
schema.xmlの&lt;tokenizer&gt;タグで指定されたTokenizerFactoryでは、ResourceLoaderAwareインタフェースのinform(ResourceLoader loader)メソッドが実行されます。
このinform()メソッドがSynonymFilterFactoryのToeknizerのインスタンス化の場合に実行されません。
lucene-gosenのJapaneseTokenizerFactoryではこのinform()メソッドでdictionaryDirのパスの読み込みを行なっています。（<a href="http://code.google.com/p/lucene-gosen/source/browse/branches/rel-1.2/src/java/org/apache/solr/analysis/JapaneseTokenizerFactory.java#73">このへん</a>）</p>
<p>上記の条件では、NullPointerExceptionが発生すると書きました。
辞書を内包したjarファイルを利用している場合、NullPointerExceptionが発生しなくても次のような問題点があります。こちらの問題は見た目は動いているように見えてしまうので注意が必要です。
すべて、SynonymFilterを利用する時点でも問題点になります。</p>
<ul>
<li>compositePOS設定が類義語辞書読み込み時に無効</li>
<li>dictionaryDir設定が類義語辞書読み込み時に無効（＝jarに内包されている辞書で動作する）</li>
</ul>
<p>一見動いているように見えるかもしれませんが、望んでいてる動作になっていない可能性があるので注意が必要です。</p>
<hr>
<h3 id="解決策まだ途中">解決策（まだ途中）</h3>
<p>先程書きましたが、基本的にはSolr側の修正をするのが妥当です。
<a href="https://issues.apache.org/jira/browse/SOLR-2909?PHPSESSID=15f554bea5726faaad9185880c7e6a15">SolrのJIRAにパッチもアップされました。</a>
こちらのパッチをSolrに適用し、SynonymFilterFactoryを次のように指定することで問題を回避することが可能になります。</p>
<pre><code>

　&lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; compositePOS=&quot;compositePOS.txt&quot;
　 dictionaryDir=&quot;dictionary/naist-chasen&quot;/&amp;gt;
　...
　　&lt;filter class=&quot;solr.SynonymFilterFactory&quot; synonyms=&quot;synonyms.txt&quot; ignoreCase=&quot;true&quot; 
　　  expand=&quot;true&quot; tokenizerFactory=&quot;solr.JapaneseTokenizerFactory&quot; 
　　  **compositePOS=&quot;compositePOS.txt&quot; dictionaryDir=&quot;dictionary/naist-chasen&quot;**/&amp;gt;
　...
</code></pre><p>SynonymFilterFactoryの設定にcompositePOS、dictionaryDirを追加します。
ここの設定は&lt;tokenizer&gt;タグで指定された設定と同じ物を指定します。以上で問題なく動作することになります。</p>
<p>ただし、この方法はSolrにパッチを当てなければいけません。
Solrにパッチを当てるのもなかなかな作業だと思います。
ということで、どうにかlucene-gosen側だけでも対応出来る形にしたいなぁと考えているところです。
残念ながら、まだ考えているだけですので、もう少し提供できるのは先になってしまいますが。。。
現時点では、次の方法を考え中です。</p>
<ol>
<li>informメソッドを呼ぶフラグを追加して、どうにかしてinformメソッドを呼び出す</li>
<li>SynonymFilterの修正版をlucene-gosenに内包して提供する</li>
</ol>
<p>できれば、a.にて対応できればと思っています。
最悪、b.の方法かと。
悩んでいる間にSolrの次のバージョンが出てしまわないように出来るだけ早く対応しようと思っています。
他にも問題点や気になる点があれば、日本語、英語を問わないので、気兼ねなくissueに上げてもらうか、Twitterで私宛にメンションしてもらえればと。
（あ、issue23へのパッチでもいいですよ！）</p>
<p>追記：
まだ、SOLR-2909のパッチを適用してからの確認はできていません。（ソース見て大丈夫だと思ってるレベル）
あと、現時点での対応方法としては、「lucene-gosenとは別のjarにSynonymFilterFactoryなどを入れて提供」が妥当かなぁと考えているところです。（無理やりinformメソッド呼び出すのは骨が折れそう＋パッチが思いの外早く出て、導入されたのでlucene-gosen本体に特殊処理を入れるのはあまりメリットを感じない。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのLucene/Solr4.0対応ブランチ更新(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2012/01/08/lucene-gosen%E3%81%AElucene-solr4-0%E5%AF%BE%E5%BF%9C%E3%83%96%E3%83%A9%E3%83%B3%E3%83%81%E6%9B%B4%E6%96%B0/</link>
      <pubDate>Sun, 08 Jan 2012 23:41:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2012/01/08/lucene-gosen%E3%81%AElucene-solr4-0%E5%AF%BE%E5%BF%9C%E3%83%96%E3%83%A9%E3%83%B3%E3%83%81%E6%9B%B4%E6%96%B0/</guid>
      <description>先日のSolr勉強会でLucene/Solr4.x系のlucene-gosenについて質問を受けていたのを忘れないように（年越しちゃいました</description>
      <content:encoded><p>先日のSolr勉強会でLucene/Solr4.x系のlucene-gosenについて質問を受けていたのを忘れないように（年越しちゃいました、すみません。）先週金曜日（1/6）に<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=22">issueに登録</a>しました。
まずは忘れないようにと思って、登録だけして3連休に突入したのですが、Robertさんが1/7に対応してくれました。
Lucene/Solr 4.x系では3.x系とはパッケージやメソッドが変更されるなど少し異なる部分があります。
lucene-gosenでは、プロジェクトのページにもあるとおり、4.x系にも対応しています。
ただ、<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2F4x">この4x系に対応したブランチ</a>が、2011年5月から放置されていました。</p>
<p>ということで、Lucene/Solr 4.0系でlucene-gosenを利用されている方、これから利用される方は、<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2F4x">この4x系に対応したブランチ</a>を利用してください。
なお、このブランチにはLucene/Solrのtrunk (r1228509)のSNAPSHOT版のjarファイルが利用されています。</p>
<p>今後はlucene-gosen側でバグ修正や機能追加を行った場合にも4xブランチを更新していく予定です。
※ただし、Lucene/Solr 4.0が正式リリースされていないため、頻繁にSNAPSHOTのjarファイルを入れ替えることはこなわないと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>1.2.1リリースしました(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/12/21/1-2-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</link>
      <pubDate>Wed, 21 Dec 2011 12:15:39 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/12/21/1-2-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F/</guid>
      <description>lucene-gosenの最新版（1.2.1）をリリースしました。 プロジェクトページよりダウンロードが可能です。 今回の修正では、特定文字列で</description>
      <content:encoded><p>lucene-gosenの最新版（1.2.1）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>今回の修正では、特定文字列でメモリの使用量が爆発してしまうバグへの対処となっています。
1.2.1以前のバージョンを利用している場合は最新版を利用するようにしてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>「Apache Solr入門」のサンプルのlucene-gosen対応（1章から4章）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/11/26/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AElucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0%E3%81%8B%E3%82%894%E7%AB%A0/</link>
      <pubDate>Sat, 26 Nov 2011 03:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/11/26/apache-solr%E5%85%A5%E9%96%80%E3%81%AE%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AElucene-gosen%E5%AF%BE%E5%BF%9C1%E7%AB%A0%E3%81%8B%E3%82%894%E7%AB%A0/</guid>
      <description>先週末から勤労感謝の日まで風邪で寝こんでました。。。 みなさん、朝晩、冷え込みが激しいので風邪には気をつけてください。 季節の言葉も入れたので本</description>
      <content:encoded><p>先週末から勤労感謝の日まで風邪で寝こんでました。。。
みなさん、朝晩、冷え込みが激しいので風邪には気をつけてください。</p>
<p>季節の言葉も入れたので本題です。
つい最近、「Apache Solr入門」のサンプルをlucene-gosenでどうやって動かすんですかー？という質問を受けました。
確かに、「Apache Solr入門」を書いたのはSolrのバージョンが1.4が出る直前でしたし、lucene-gosenは存在せず、
当時はSenを元にした日本語の形態素解析のサンプルとなっていました。
そのSenも入手しづらくなってきており、私もlucene-gosenのプロジェクトに携わるようになってきてある程度時間が
経ちました。
せっかくなので、サンプルのschema.xmlだけでも最新版（Solr 3.4 + lucene-gosen-1.2.0-ipadic）のものを用意しました。
なお、あくまでも、3.xでlucene-gosenを利用する場合の「Apache Solr入門」のサンプルプログラムの変更点（とりあえず、4章まで）の違いについて記述します。
申し訳ございませんが、1.4と3.xの違いについての説明はここでは行いません。</p>
<p>以下では、各章でschema.xmlに関連する記載のある部分を抜粋して、変更点と変更したschema.xmlのリンクを用意しました。参考にしてもらえればと思います。</p>
<h2 id="1章">1章</h2>
<h3 id="161-n-gram17ページ">1.6.1 N-gram（17ページ）</h3>
<p>1.6.1の手順に変更はありません。
サンプルプログラムが入っているZip「solrbook.zip」のintroduction/ngram/schema.xmlファイルの代わりに
<a href="https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/b51b74e8c573/introduction/ngram/schema.xml">こちらのschema.xml</a>を利用してください。</p>
<h3 id="162-形態素解析18ページ20ページ中盤まで">1.6.2 形態素解析（18ページ～20ページ中盤まで）</h3>
<p>手順が大きく変わります。
Senを利用する場合、Senの辞書のビルド、Senのjarファイルの配置、Senを利用するためのTokenizerクラスを含んだサンプルjarの配置という作業があります。
lucene-gosenではコンパイル済みの辞書がjarファイルに含まれています。
また、Solr向けのTokenizerもlucene-gosenのjarファイルに含まれています。
lucene-gosenを利用して形態素解析を体験するための手順は次の流れになります。
なお、schema.xmlについては上記N-gramでダウンロードしたschema.xmlに形態素解析の設定もあわせて記載してあります。</p>
<p>jarファイル（<a href="http://lucene-gosen.googlecode.com/files/lucene-gosen-1.2.0-ipadic.jar">lucene-gosen-1.2.0-ipadic.jar</a>）をダウンロードして、$SOLR/example/solr/lib（libディレクトリがない場合は作成）にコピーします。
コピーが終わりましたら、次のように$SOLR/exampleディレクトリでSolrを起動します。
（-Dsen.homeは必要なし）</p>
<pre><code>
$ java -jar start.jar
</code></pre><p>あとは、書籍の記述にしたがって管理画面のAnalysis画面で動作を確認します。
ほぼ、図1-6と同じ結果になっていると思います。
（lucene-gosenで出力される情報には本書のサンプルよりも多くの情報が含まれています。また、サンプルでは、形態素解析の後の単語に基本形を採用しているため、「な」が「だ」として出力されています。基本形を出力する場合は後述するこちらで紹介したTokenFilterを利用すれば可能です。）</p>
<h2 id="2章">2章</h2>
<h3 id="213-schemaxmlのバージョン27ページ">2.1.3 schema.xmlのバージョン（27ページ）</h3>
<p>Solr3.xではschema.xmlのファイルの最新バージョンは<strong>1.4</strong>になっています。</p>
<h3 id="223-代表的なトークナイザ35ページ">2.2.3 代表的なトークナイザ（35ページ）</h3>
<p>solrbook.analysis.SenTokenizerFactoryは必要ありません。
先ほども説明しましたが、lucene-gosenにはSolr向けのトークナイザが用意されています。
solr.JapaneseTokenizerFactoryがそれに該当します。</p>
<h3 id="224-代表的なトークンフィルタ37ページ">2.2.4 代表的なトークンフィルタ（37ページ）</h3>
<p>以下の2つについてはlucene-gosenに同等のトークンフィルタが存在します。</p>
<ul>
<li>solrbook.analysis.KatakanaStemFilterFactory</li>
<li>solrbook.analysis.POSFilterFactory</li>
</ul>
<p>それぞれ、次のものがlucene-gosenにあるので、こちらを利用します。</p>
<ul>
<li>solr.JapaneseKatakanaStemFilterFactory</li>
<li>solr.JapanesePartOfSpeechStopFilterFactory</li>
</ul>
<p>2章向けの<a href="https://bitbucket.org/johtani/solrbook-lucene-gosen-3.x/raw/b51b74e8c573/schema/schema.xml">schema.xmlはこちら</a>です。その他のtxtファイルについては、特に変更はありません。</p>
<p>3,4章は特に変更はありません。Solrの起動の仕方にだけ注意してください。（-Dsen.homeは必要ありません）</p>
<p>以上が4章までの修正点になります。
動作しないなどあれば、コメントください。
サンプルアプリについてはまた後日余裕があれば。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>1.2.0リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/31/1-2-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 31 Oct 2011 15:46:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/31/1-2-0%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>lucene-gosenの最新版（1.2.0）をリリースしました。 プロジェクトページよりダウンロードが可能です。 新規追加機能についてはこちら</description>
      <content:encoded><p>lucene-gosenの最新版（1.2.0）をリリースしました。</p>
<p><a href="http://code.google.com/p/lucene-gosen/">プロジェクトページ</a>よりダウンロードが可能です。</p>
<p>新規追加機能については<a href="http://johtani.jugem.jp/?eid=38">こちらのエントリ</a>を御覧ください。</p>
<p>バグなどありましたら、容赦なく報告をいただけると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書の外部化とLucene/Solr3.4対応(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/26/%E8%BE%9E%E6%9B%B8%E3%81%AE%E5%A4%96%E9%83%A8%E5%8C%96%E3%81%A8lucene-solr3-4%E5%AF%BE%E5%BF%9C/</link>
      <pubDate>Wed, 26 Oct 2011 01:12:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/26/%E8%BE%9E%E6%9B%B8%E3%81%AE%E5%A4%96%E9%83%A8%E5%8C%96%E3%81%A8lucene-solr3-4%E5%AF%BE%E5%BF%9C/</guid>
      <description>すぐやりますと言いつつ、はや1ヶ月。。。 腰が重い、ダメエンジニアですね。。。 すみませんでした。。。 ようやくtrunkにコミットしました。 すぐ</description>
      <content:encoded><p><a href="http://johtani.jugem.jp/?eid=22?PHPSESSID=744a73e2820c6e94289eb9ba777c1ff2">すぐやりますと言いつつ</a>、はや1ヶ月。。。
腰が重い、ダメエンジニアですね。。。</p>
<p>すみませんでした。。。
ようやくtrunkにコミットしました。
すぐにリリース版を用意すると思います。</p>
<p>1ヶ月もあいてしまったので、追加した機能に関するまとめと、
用途別の利用方法を記載しておきます。
（lucene-gosenのWikiにもそろそろ書かないとなぁ。日本語でもいいから。）</p>
<p><strong>追加した機能</strong></p>
<hr>
これまでのlucene-gosenはjarに辞書を含む形でライブラリを提供していました。
ただ、この場合、カスタム辞書を利用している環境ではカスタム辞書を修正し、ビルドしなおすたびに、
jarファイルを作成しなければなりません。
また、jarファイルをSolrなどに配布する必要も出てきます。
この手間を考慮して、辞書を外部ディレクトリで指定することができるようにしたものが
今回の修正になります。
また、修正の過程で同一VM内で異なる辞書を使えるようにする機能も副産物として生まれました。
今回追加した機能は次のようなものになります。
<ul>
<li>辞書を含まないjarのビルドおよび提供</li>
<li>ディレクトリ指定による辞書の指定</li>
<li>同一VM内での複数辞書の利用</li>
<li>辞書リビルド用のAntターゲットの追加</li>
<li>Lucene/Solr jarファイルの最新化（3.4.0対応）</li>
</ul>
<p>ディレクトリ指定による辞書の指定ですが、以下のような形になります。
まずは、LuceneのTokenizerでの指定方法です。
「辞書のディレクトリ」という引数が追加になっています。
ここに、辞書ディレクトリ（*.senファイルが存在するディレクトリ）を相対/絶対パスで指定します。</p>
<pre><code>
...
Tokenizer tokenizer = new JapaneseTokenizer(reader, null, &quot;辞書のディレクトリ&quot;);
...
</code></pre><p>つぎは、Solrでの設定の方法です。
schema.xmlにて次のような設定を行います。</p>
<pre><code>
...
&lt;fieldType name=&quot;text_ja&quot; ...&gt;
  &lt;analyzer&gt;
    ...
    &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; compositePOS=&quot;compositePOS.txt&quot; dictionaryDir=&quot;辞書のディレクトリ&quot;/&amp;gt;
    ...
  &lt;/analyzer&gt;
&lt;/fieldType&gt;
...  
</code></pre><p>schema.xmlの設定については、<a href="https://code.google.com/p/lucene-gosen/source/browse/trunk/example/schema.xml.snippet">example/schema.xml.snippet</a>にも説明がありますので、こちらもあわせて参考に。
なお、Solrの設定については、先ほどのLuceneでの辞書のディレクトリの指定方法（絶対/相対パス）に加えて、
<em>$SOLR_HOME/conf</em> からの相対パスでの指定も可能になっています。</p>
<p><strong>Antのターゲットについて</strong></p>
<hr>
辞書なしjarファイルを作成するターゲットなどを追加しています。
実際に追加したターゲットは以下のとおりです。
<table class="list_view">
<thead>
<tr>
<th>ターゲット名</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>nodic-jar</td>
<td>辞書なしのjarファイルを生成するためのターゲット。辞書のダウンロード、コンパイルは行いません。</td>
</tr>
<tr class="specalt">
<td class="alt">rebuild-dic</td>
<td class="alt">lucene-gosenのビルド済み辞書（.senファイル）を削除してから辞書のコンパイル（ビルド）を行います。-Ddictypeにより辞書のタイプ（ipadic|naist-chasen）の指定が必要です。また、-Dcustom.dicsによりカスタム辞書の指定もあわせて可能です。</td>
</tr>
<tr class="spec">
<td>build-dic-ipadic</td>
<td>テスト用に追加。-Ddictype=ipadicを指定してbuild-dicを実行。</td>
</tr>
<tr class="specalt">
<td class="alt">build-dic-naist-chasen</td>
<td class="alt">ついでに追加。-Ddictype=naist-chasenを指定してbuild-dicを実行。</td>
</tr>
</tbody>
</table>
<p>最後の２つはあまり関係ありません。内部的に有ると便利だったため、作りました。
重要なのは最初の２つです。
ひとつめの「nodic-jar」は辞書を含まないjarファイルをビルドします。
このjarファイル＋辞書の入ったディレクトリを利用することで、辞書の外部化が可能となります。</p>
<p>そして、「rebuild-dic」です。こちらは、<a href="http://johtani.jugem.jp/?eid=4">以前記事に書きました</a>が、カスタム辞書のコンパイルが思いの外面倒だったので、ターゲットを追加しました。
次のように指定することで、辞書のリビルドが可能です。</p>
<pre><code>
$ ant -Ddictype=naist-chasen -Dcustom.dcs=&quot;custom1.csv custom2.csv&quot; rebuild-dic
</code></pre><p><strong>提供されるjarファイルについて</strong><hr></p>
<p>提供されるjarファイルは次のようになる予定です。
1番目のjarファイルが今回追加になる、辞書なしのjarファイルになります。</p>
<ul>
<li>lucene-gosen-1.x.x.jar</li>
<li>lucene-gosen-1.x.x-ipadic.jar</li>
<li>lucene-gosen-1.x.x-naist-chasen.jar</li>
</ul>
<p><strong>用途別の利用方法</strong><hr>
利用用途別に利用するjarファイルやantのターゲットを利用シーンを交えて想定を書いてみます。
Solrでの利用シーンを想定します。</p>
<p><em>**お手軽に使う。辞書ありjarファイルで一発インストール。**</em>
これまでどおりの使い方になります。
辞書込みのjarファイルを利用すれば、すぐに利用可能になります。</p>
<p><em>**カスタム辞書を使い倒す。定期的に辞書をメンテナンス。**</em>
定期的にシステム固有の単語が増える（例：製品名、新語など）場合です。</p>
<ul>
<li>利用するjar：lucene-gosen-1.x.x.jar</li>
<li>辞書のコンパイル＋配置：ant -Ddictype=naist-chasen -Dcustom.dics=&quot;custom1.csv&rdquo; rebuild-dir</li>
<li>Solrの該当コアのRELOAD</li>
</ul>
<p>Solrのマルチコア環境を利用します。なお、sharedLib設定にlucene-gosen-1.x.x.jarを配置すると、辞書の再読み込みができないので注意してください。
設定は、上記のようにTokenizerFactoryの設定でdictionaryDirにて辞書のディレクトリを設定しておきます。
カスタム辞書に単語を追加後、antにて、辞書のリビルドを行います。
リビルドした辞書ファイルを必要に応じて対象の辞書ディレクトリにコピーします。（ビルド後のディレクトリをそのまま利用している場合はコピーの必要はないです。）
最後に、Solrの該当コアのリロードを行います。（リロードの仕方は<a href="http://wiki.apache.org/solr/CoreAdmin?PHPSESSID=d9edccf00e1c13655f96b005f36819c4">こちらを参考に</a>。）
コアのリロードにより、辞書の再読み込みが行われるので、リロード後から新しい辞書が適用されます。</p>
<p><em><strong>異なる辞書を使い倒す。TokenizerごとにdictionaryDir設定するぞ</strong></em>
1つのSolrで異なる辞書を使ったフィールドを使いたい場合です。
ipadicとnaist-chasenといった異なる場合はあまり想定できないですが、カスタム辞書の部分が異なるという形が想定できるでしょうか。（例：製品名のフィールド、企業名のフィールド。。。など）</p>
<ul>
<li>利用するjar：lucene-gosen-1.x.x.jar</li>
<li>設定：schema.xmlに異なるdictionaryDirを設定したTokenizerFactoryを設定</li>
</ul>
<p>上記、カスタム辞書の定期更新も一緒に行うことも可能です。コアをリロードすれば、リロードしたコアで利用している
辞書がすべてリロードされます。</p>
<p><strong>最後に</strong><hr>
遅くなってしまいましたが、ようやく、trunkにコミットしました。
できるだけ速く、リリースしますので、もう少々お待ちを。</p>
<p>Solrのconfディレクトリからの指定については、<a href="http://twitter.com/shinobu_aoki">＠shinobu_aoki</a>さんにパッチを提供してもらいました。
また、trunkにコミットしていないパッチを適用して記事を書いてくれた方もいらっしゃいました。こちらもあわせて参考に。私より説明が上手です。
<a href="http://www.mwsoft.jp/programming/munou/lucene_gosen.html">Java製形態素解析ライブラリ「lucene-gosen」を試してみる</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenで文章からキーワード抽出（イレギュラー？）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/10/12/lucene-gosen%E3%81%A7%E6%96%87%E7%AB%A0%E3%81%8B%E3%82%89%E3%82%AD%E3%83%BC%E3%83%AF%E3%83%BC%E3%83%89%E6%8A%BD%E5%87%BA%E3%82%A4%E3%83%AC%E3%82%AE%E3%83%A5%E3%83%A9%E3%83%BC/</link>
      <pubDate>Wed, 12 Oct 2011 12:17:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/10/12/lucene-gosen%E3%81%A7%E6%96%87%E7%AB%A0%E3%81%8B%E3%82%89%E3%82%AD%E3%83%BC%E3%83%AF%E3%83%BC%E3%83%89%E6%8A%BD%E5%87%BA%E3%82%A4%E3%83%AC%E3%82%AE%E3%83%A5%E3%83%A9%E3%83%BC/</guid>
      <description>昨日、文章から特定の単語（リストあり）を探したいという話を聞き、lucene-gosenでもできるねぇという話になりました。 まぁ、考えてみれ</description>
      <content:encoded><p>昨日、文章から特定の単語（リストあり）を探したいという話を聞き、lucene-gosenでもできるねぇという話になりました。
まぁ、考えてみればごくごく当たり前なのですが。。。（その筋の方たちにしてみれば常識なのかもしれないですが。。。）
一応やってみたので、こんなこともできるなという一例ですということで、記録を残しておきます。</p>
<p>今回の例文として<a href="http://www.kantei.go.jp/jp/noda/statement/201109/13syosin.html">野田首相の所信表明演説の一部</a>を活用させてもらいます。
単語のリストは次のようにします。</p>
<ul>
<li>内閣総理大臣</li>
<li>正心誠意</li>
<li>東日本</li>
<li>日本</li>
</ul>
<p>今回も結果をわかりやすくするためにSolrのanalysis画面を利用します。
作業手順は以下のとおり。</p>
<ol>
<li>dictionary.csvの編集</li>
<li>辞書のコンパイル</li>
<li>fieldTypeの定義（Solrのschema.xmlの設定）</li>
<li>文章からキーワード抽出（Solrのanalysis画面）</li>
</ol>
<div>
### **1.dictionary.csvの編集**
今回はnaist-chasenディレクトリで作業します。
なお、今回利用するlucene-gosenは[ここ](http://johtani.jugem.jp/?eid=21)で紹介した辞書分離バージョンです。（はやくtrunkにコミットせねば。。。）
dictionary.csvを先ほど上げた単語だけのエントリに変更します。
キーワードだけを抽出したいので、他の単語は必要ないからです。
```
<p>&ldquo;内閣総理大臣&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;内閣総理大臣&rdquo;,&ldquo;ナイカクソウリダイジン&rdquo;,&ldquo;ナイカクソウリダイジン&rdquo;
&ldquo;正心誠意&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;正心誠意&rdquo;,&ldquo;セイシンセイイ&rdquo;,&ldquo;セイシンセイイ&rdquo;
&ldquo;東日本&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;東日本&rdquo;,&ldquo;ヒガシニホン&rdquo;,&ldquo;ヒガシニホン&rdquo;
&ldquo;日本&rdquo;,1,名詞,一般,<em>,</em>,<em>,</em>,&ldquo;日本&rdquo;,&ldquo;ニホン&rdquo;,&ldquo;ニホン&rdquo;</p>
<pre><code>&lt;/div&gt;

&lt;div&gt;
### **2.辞書のコンパイル**
先ほど作成した辞書をコンパイルし、lucene-gosen用バイナリ辞書を作成します。
</code></pre><p>$ cd $LUCENE_GOSEN_HOME¥dictionary
$ ant -Ddictype=naist-chasen clean-sen compile</p>
<pre><code>&lt;/div&gt;

&lt;div&gt;
### **3.fieldTypeの定義（Solrのschema.xmlの設定）**
Solrのschema.xmlにlucene-gosenを利用するフィールドタイプを定義します。
追加するのは次の通り
</code></pre><pre><code>&lt;fieldType name=&quot;text_ja&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot; autoGeneratePhraseQueries=&quot;false&quot;&amp;gt;
  &lt;analyzer&amp;gt;
    &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; compositePOS=&quot;compositePOS.txt&quot; dictionaryDir=&quot;keyword-dic&quot;/&amp;gt;
    &lt;filter class=&quot;solr.JapanesePartOfSpeechKeepFilterFactory&quot; tags=&quot;keeptags_ja.txt&quot; enablePositionIncrements=&quot;true&quot;/&amp;gt;
  &lt;/analyzer&amp;gt;
&lt;/fieldType&amp;gt;
</code></pre>
<pre><code>
また、ここで定義しているcompositePOS.txt、keeptags_ja.txtは次のようになります。


compositePOS.txt
</code></pre><p>未知語</p>
<pre><code>
keeptags_ja.txt
</code></pre><p>名詞-一般</p>
<pre><code>
未知語がバラバラに出現しないようにして見やすくするためと、必要な単語（今回は「名詞-一般」しか利用しないため。）だけを抽出したいための設定です。

&lt;/div&gt;

&lt;div&gt;
**### 4.文章からキーワード抽出（Solrのanalysis画面）**
あとは、analysis画面で解析して見るだけになります。

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20111012/20111012_2085981.png" alt="キーワード抽出結果"/>
    </div>
    <a href="/images/entries/20111012/20111012_2085981.png" itemprop="contentUrl"></a>
  </figure>
</div>


ということで、辞書に登録された単語だけが抽出されてますね。
この例ではインデックスに登録となりますが。
ただし、「東日本」「日本」のような一部を含む単語の場合、「東日本」が見つかった場合は「日本」は抽出されません。
あくまでも、ベストな解が見つかるのみという形です。
すべての単語を出したい場合はもう少しやり方を考えたほうがいいかもしれません。
（まぁ、このやり方でキーワードを抽出するかも考えたほうがいいかもしれませんが。。。）

&lt;/div&gt;
最近、頭が硬くなってきてるなぁと実感してしまいました。まぁ、こんな使い方もあるかなぁと。
もっと頭を柔らかくして問題を解けるけるようになりたいなぁと。


</code></pre></content:encoded>
    </item>
    
    <item>
      <title>辞書分離のテストケース追加と残タスク(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/09/07/%E8%BE%9E%E6%9B%B8%E5%88%86%E9%9B%A2%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E3%82%B1%E3%83%BC%E3%82%B9%E8%BF%BD%E5%8A%A0%E3%81%A8%E6%AE%8B%E3%82%BF%E3%82%B9%E3%82%AF/</link>
      <pubDate>Wed, 07 Sep 2011 01:56:28 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/09/07/%E8%BE%9E%E6%9B%B8%E5%88%86%E9%9B%A2%E3%81%AE%E3%83%86%E3%82%B9%E3%83%88%E3%82%B1%E3%83%BC%E3%82%B9%E8%BF%BD%E5%8A%A0%E3%81%A8%E6%AE%8B%E3%82%BF%E3%82%B9%E3%82%AF/</guid>
      <description>すぐにテストケース追加しますといいつつ、はや一週間。 ようやく仕事が落ち着いたので、テストケースを追記しました。まだパッチの段階です。 一応、異</description>
      <content:encoded><p>すぐに<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16#c5?PHPSESSID=e73e7c99982a0b86f03ccbef7738d416">テストケース追加します</a>といいつつ、はや一週間。</p>
<p>ようやく仕事が落ち着いたので、テストケースを追記しました。まだパッチの段階です。
一応、異なる辞書の読み込みのテストケースなどを追加し、テストケース追加時点で
いくつか気になったところもあったので、ついでに修正を加えました。
一応、辞書の分離＋複数辞書対応については現時点ではこんなところかと。</p>
<p>あと、もう１項目対応してからtrunkに取り込む予定です。
対応する項目とは以下の項目です。</p>
<ul>
<li>カスタム辞書ビルドの簡易化</li>
</ul>
<p>以前の記事にも書いていますが、カスタム辞書のビルドが思いのほか手間がかかります。
辞書の外部化は対応したのですが、せっかく辞書が外部化できたのですから、コマンド（ant？）で一発で
カスタム辞書を含んだビルドをしたくなるのが人情です。
辞書なしのjarファイルもあることなので、すぐに対応可能かと。
ということで、今週中には対応する予定です（宣言しないとまた先延ばしになりそう）</p>
<p>忘れてそうだったらTwitterやコメントでツッコミを入れてもらえると助かります。</p>
</content:encoded>
    </item>
    
    <item>
      <title>複数辞書の読み込み機能追加（仮）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/30/%E8%A4%87%E6%95%B0%E8%BE%9E%E6%9B%B8%E3%81%AE%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E6%A9%9F%E8%83%BD%E8%BF%BD%E5%8A%A0%E4%BB%AE/</link>
      <pubDate>Tue, 30 Aug 2011 10:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/30/%E8%A4%87%E6%95%B0%E8%BE%9E%E6%9B%B8%E3%81%AE%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E6%A9%9F%E8%83%BD%E8%BF%BD%E5%8A%A0%E4%BB%AE/</guid>
      <description>先日、辞書のjarファイルからの分離についてパッチと記事を書きました。 IssueにあげていたパッチをRobertさんが見ていたらしく、次のよ</description>
      <content:encoded><p>先日、<a href="http://johtani.jugem.jp/?eid=20">辞書のjarファイルからの分離</a>についてパッチと記事を書きました。</p>
<p>IssueにあげていたパッチをRobertさんが見ていたらしく、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16#c4">次のようなコメント</a>をもらいました。</p>
<p><em>Maybe if we change SenFactory.getInstance to use a ConcurrentHashMap then you can easily use multiple dictionaries at the same time?
</em></p>
<p>「SenFactory.getInstanceメソッドでConcurrentHashMap使ったら複数辞書対応できるんじゃない？」（訳）
たしかに。。。なんで思いつかなかったのだろう。。。</p>
<p>ということで、実装してみました。
<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16#c5">パッチはこちら</a>。</p>
<p>使い方ですが、先日の記事と代わりはありません。
ただし、あった制限事項が次のようになります。</p>
<ul>
<li>マルチコアの設定でsharedLibにlucene-gosenのjarを含まない</li>
<li><del>同一コア内で異なるdictinaryDirの指定はできない</del></li>
</ul>
<p>ソースの変更点ですが、ものすごく単純です。
dictionaryDirに指定された文字列をキー、その辞書ディレクトリを利用したSenFactoryのインスタンスを値に持つmapをSenFactory内に保持します。あとは、SenFactoryのgetInstance(String dictionaryDir)メソッドで取得する際にmapに対応するインスタンスがあれば、そのインスタンスをなければ、dictionaryDirから辞書を読み込んでインスタンス生成してmapにキャッシュしつつ返すという実装に変えただけです。
ということで、次のようなIPADICとNAIST-JDIC for ChaSenを同時に使う設定も可能となります。</p>
<pre><code>
    &lt;fieldType name=&quot;text_ja_ipadic&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
      &lt;analyzer&amp;gt;
        &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; dictionaryDir=&quot;/tmp/lucene-gosen/dictionary/ipadic&quot;/&amp;gt;
      &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
    &lt;fieldType name=&quot;text_ja_naist_chasen&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
      &lt;analyzer&amp;gt;
        &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; dictionaryDir=&quot;/tmp/lucene-gosen/dictionary/naist-chasen&quot;/&amp;gt;
      &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
</code></pre><p>あと、注意事項です。
普通に考えるとわかることですが、辞書を複数読み込めるようになったことで、読み込んだ複数の辞書をメモリに保持することになります。ですので、今までよりも多くのメモリを利用するので、Heapのサイズには注意が必要です。
例のごとく（ほんとよくない。。。）テストコードを書いていない状態のパッチをまずはアップしました。
テスト書かないと。。。次回はテストコードかきましたと言う報告をしたいな</p>
<p>あと、Robertさんのコメントの前に<a href="http://twitter.com/#!/shinobu_aoki">@shinobu_aoki</a>さんからJapaneseTokenizerFactoryの設定では辞書のディレクトリを$SOLR_HOME/confからの相対パスで記述できるというパッチもいただいています。
この部分については先日と使い方が異なります。
（すみません、まだきちんとソースを見れてないです。。。）</p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書のjarファイルからの分離(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/08/23/%E8%BE%9E%E6%9B%B8%E3%81%AEjar%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%8B%E3%82%89%E3%81%AE%E5%88%86%E9%9B%A2/</link>
      <pubDate>Tue, 23 Aug 2011 10:26:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/08/23/%E8%BE%9E%E6%9B%B8%E3%81%AEjar%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%8B%E3%82%89%E3%81%AE%E5%88%86%E9%9B%A2/</guid>
      <description>ひさびさに、lucene-gosenの話題です。 lucene-gosenはjarファイルに辞書も同梱されており、jarファイルをクラスパスに</description>
      <content:encoded><p>ひさびさに、lucene-gosenの話題です。</p>
<p>lucene-gosenはjarファイルに辞書も同梱されており、jarファイルをクラスパスに取り込むだけで、
簡単に形態素解析器が利用できるといお手軽さがあり、便利です。</p>
<p>ですが、以前<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16">カスタム辞書の登録</a>について記事を書いたように、カスタム辞書の登録は思いのほか手間がかかります。
lucene-gosenのソースをダウンロードし、lucene-gosenを一度コンパイルし、カスタム辞書のcsvファイルを作成し、カスタム辞書を取り込んだ辞書のバイナリを生成し、最後にjarファイルにするという作業です。（書くだけでいやになってきました。。。）さらに作成したjarファイルをSolrや各プログラムに再度配布するという具合です。</p>
<p>そこで、<a href="http://code.google.com/p/lucene-gosen/issues/detail?id=16">辞書ファイルの外部化ができないかという話</a>があがっていました。
すこし時間ができたので、山積みになっているissueを横目に軽く実装をしてpatchをissueにアップしました。</p>
<p>機能としてはごく簡単で、JapaneseTokenizerのコンストラクタに辞書のディレクトリ（*.senファイルのあるディレクトリ）を指定可能にしただけです。
また、JapaneseTokenizerFactoryでもdictionaryDir属性で指定可能になっています。
まずは、コンパイルの方法から。
trunkをSVNでcheckoutし、issueにあるpatchをダウンロードして適用します。（svnのチェックアウトについては<a href="http://johtani.jugem.jp/?eid=3">こちら</a>を参考にしてください。）</p>
<pre><code>
$ cd lucene-gosen-trunk
$ patch -p0 --dry-run &lt; lucene-gosen-separate-dictionary.patch
$ patch -p0 &lt; lucene-gosen-separate-dictionary.patch
</code></pre><p>次に、antを実行し辞書なし版のjarファイルをビルドします。</p>
<pre><code>
$ ant nodic-jar
</code></pre><p>これで、dictディレクトリに「lucene-gosen-1.2-dev.jar」というjarファイルが出来上がります。
（※ただし、これだけでは動作しないので、別途辞書のコンパイルは必要です。）</p>
<p>次に、指定の仕方です。JapaneseTokenizerのコンストラクタは第3引数に辞書のディレクトリ（フルパスor実行ディレクトリからの相対パス）を渡すだけです。</p>
<pre><code>
  Tokenizer tokenizer = new JapaneseTokenizer(reader, compositeTokenFilter, dictionaryDir);
</code></pre><p>最後に、Solrのtokenizerタグでの指定方法です。</p>
<pre><code>
    &lt;fieldType name=&quot;text_ja&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&amp;gt;
    &lt;analyzer&amp;gt;
        &lt;tokenizer class=&quot;solr.JapaneseTokenizerFactory&quot; dictionaryDir=&quot;/hoge/dictionarydir&quot;/&amp;gt;
    &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
</code></pre><p>以上が、簡単な設定の仕方です。なお、辞書を内包したjarファイルでもdictionaryDirは利用可能です。優先度としては、dictionaryDirが指定されている場合はdictionaryDirを探索しファイルがなければRuntimeExceptionです。指定がnullもしくは空文字の場合はjarファイルの辞書の読み込みを行います。</p>
<p>次に利用シーン、制限事項についてです。
利用シーンとしてはカスタム辞書を定期的にメンテナンス（追加更新）しながらSolrを運用するというのが想定されます。定期的に辞書の再読み込みをしたい場合です。
利用方法は次のようになります。</p>
<ul>
<li>Solrのマルチコア構成を利用する</li>
<li>各コアごとにlib/lucene-gosen-1.2-dev.jarを用意</li>
<li>辞書の更新が終わったらコアのRELOADを実施</li>
</ul>
<p>コアをリロードすることで、lucene-gosenが辞書を再読み込みようになります。（現状でも再読み込みするが、jarファイルを再配置しないといけない。）あとは、定期的に辞書ファイルを更新、再構築しコアをリロードすれば、
リロード後に新しい辞書が利用できるという具合です。
（もちろん、辞書更新後に入った単語は辞書更新以前に作成したインデックスにはでてこないですが。。。）
また、コアごとにdictinaryDirを別々に指定することも可能です。</p>
<p>制限事項は次のようになります。</p>
<ul>
<li>マルチコアの設定でsharedLibにlucene-gosenのjarを含まない</li>
<li>同一コア内で異なるdictinaryDirの指定はできない</li>
</ul>
<p>以上が、辞書の外部ファイル化のパッチについてでした。
少しテストケースを追加したら、trunkにコミットする予定です。興味があれば、パッチを利用してみてください。</p>
<p><a href="http://code.google.com/p/syntaxhighlighter/">SyntaxHighlighter</a>の導入をしないとソースコードが見にくいですね。。。導入を検討しないと。。。どこかにWebサーバ用意しないとダメかも</p>
</content:encoded>
    </item>
    
    <item>
      <title>NAIST-JDic for MeCabのPreprocessorの実装に関する備忘録(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/12/naist-jdic-for-mecab%E3%81%AEpreprocessor%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E5%82%99%E5%BF%98%E9%8C%B2/</link>
      <pubDate>Tue, 12 Jul 2011 10:06:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/12/naist-jdic-for-mecab%E3%81%AEpreprocessor%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E5%82%99%E5%BF%98%E9%8C%B2/</guid>
      <description>忘れてしまうので、備忘録を残しておきます。 一応、ソースには少しずつコメントをいれてはいるのですが。 私は残念ながら、自然言語処理は初心者に毛が</description>
      <content:encoded><p>忘れてしまうので、備忘録を残しておきます。
一応、ソースには少しずつコメントをいれてはいるのですが。
私は残念ながら、自然言語処理は初心者に毛が生えた程度（現在、鋭意勉強中）で、対応方法に問題があるかもしれません。気づいた方はコメントをいただけると助かります。</p>
<h3 id="辞書ファイルについて">辞書ファイルについて</h3>
<p>NAIST-JDic for MeCabの辞書ファイルは以下の構成になっています。</p>
<table class="list_view">
<thead>
<tr>
<th>ファイル名</th>
<th>メモ</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>char.def</td>
<td>文字種の設定</td>
</tr>
<tr class="specalt">
<td class="alt">feature.def</td>
<td class="alt">辞書学習用の設定？</td>
</tr>
<tr class="spec">
<td>left-id.def</td><td>左文脈IDのマスタ（左文脈ID、品詞情報）</td>
</tr>
<tr class="specalt">
<td class="alt">matrix.def</td>
<td class="alt">連接コスト表（前件文脈ID,後件文脈ID,連接コスト）</td>
</tr>
<tr class="spec">
<td>pos-id.def</td>
<td>品詞IDのマスタ（品詞情報、ID）</td>
</tr>
<tr class="specalt">
<td class="alt">rewrite.def</td>
<td class="alt">rewrite情報（左右文脈に出現した場合のそれぞれの品詞情報のrewriteルール。辞書学習で主に利用）</td>
</tr>
<tr class="spec">
<td>right-id.def</td>
<td>右文脈IDのマスタ（右文脈ID、品詞情報）</td>
</tr>
<tr class="specalt">
<td class="alt">unk.def</td>
<td class="alt">未知語の品詞情報（文字種ごとに未知語のコスト、左右文脈ID、品詞情報が記載されている）</td>
</tr>
<tr class="spec">
<td>naist-jdic.csv</td>
<td>単語辞書（単語、左右文脈ID、単語コスト、品詞情報、読みなど記載）</td>
</tr>
</tbody>
</table>
<p>現時点では、MeCabDicPreprocessorでは以下のファイルを利用しています。</p>
<ul>
<li>left-id.def</li>
<li>matrix.def</li>
<li>right-id.def</li>
<li>naist-jdic.csv</li>
</ul>
<p>上記以外のファイルは現時点では利用しない実装になっています。
ただし、rewrite.def、unk.def、char.defについては利用したほうがよりMeCabに近い結果が得られるような気がしています。（特に文字種ごとのコストを利用することは有効と思われます。）</p>
<h3 id="preprocessorでの処理について">Preprocessorでの処理について</h3>
<p>lucene-gosenはSenの後継であり、MeCabの昔のバージョンを移植したものがベースとなっています。
lucene-gosenとMeCabの現時点での実装の大きな違いとして、連接コスト表の違いがあります。
ここからは憶測になってしまいますので、注意してください。（論文を探せばどこかにこの実装の変化の過程が記載してあるかもしれないですが、まだ探していません、すみません。）
過去のMeCabではChaSen向けの辞書を利用していました。
ChaSenでは連接コスト表が3つの項（前の前、前、後）から構成されていました。（n項まで定義可能らしい）
ですので、lucene-gosenのViterbiアルゴリズムの引数も3つのノードが引数となっています。
lucene-gosen向けの連接コスト辞書も同様の作りになっています。
一方、現在のMeCabは先ほど書いたとおり、matrix.defでは2項の連接コスト表（前、後）となっています。この違いを保管するために、Preprocessorでは、matrix.defを3項にするために一番左（前の前）については任意の品詞を採用できるように「<em>,</em>,<em>,</em>,<em>,</em>,*」のみを設定しています。</p>
<p>現時点では、Preprocessorの出力である中間ファイルを共通の形式に出力することで、DictinaryBuilder以降の処理に変更を加えることなくNAIST-JDic for MeCabへの対応を行う形を取りました。まずは使えるようにするのが先かと思いまして。
ただ、MeCabの辞書の構成から考えると中間ファイルに落とし込む処理に無駄があると感じています。
matrix.defでせっかく、IDによる連接コスト表を構成しているのに、IDを品詞情報の文字列に戻したconnection.csvを生成していますので。</p>
<p>ということで、備忘録でした。
あとは、テストをどうするか（正解をどう考えるか）なども考える必要があります。現時点での悩みの種です。。。アイデア募集中です。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 1.1.1リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/07/04/lucene-gosen-1-1-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 04 Jul 2011 20:00:40 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/07/04/lucene-gosen-1-1-1%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>lucene-gosen 1.1.1をリリースしました。 先日お知らせしたバグ修正を取り込んだjarを用意いしました。 ダウンロードはこちらから</description>
      <content:encoded><p>lucene-gosen 1.1.1をリリースしました。<br />
先日お知らせした<a href="http://johtani.jugem.jp/?eid=10">バグ修正</a>を取り込んだjarを用意いしました。<br />
<br />
<a href="http://code.google.com/p/lucene-gosen/downloads/list">ダウンロードはこちらから</a></p>
</content:encoded>
    </item>
    
    <item>
      <title>compositePOS（CompositeTokenFilter）のバグ修正(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/28/compositeposcompositetokenfilter%E3%81%AE%E3%83%90%E3%82%B0%E4%BF%AE%E6%AD%A3/</link>
      <pubDate>Tue, 28 Jun 2011 12:36:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/28/compositeposcompositetokenfilter%E3%81%AE%E3%83%90%E3%82%B0%E4%BF%AE%E6%AD%A3/</guid>
      <description>以前、こちらで話題に上がっていた「未知語」に関するcompositePOSのエラーの件を調査しました。（Twitterでも流れてました。） 次</description>
      <content:encoded><p>以前、<a href="http://blog.livedoor.jp/haruyama_seigo/archives/51801386.html#comments">こちら</a>で話題に上がっていた「未知語」に関するcompositePOSのエラーの件を調査しました。（Twitterでも流れてました。）
次のような条件の場合にエラーが発生するようです。</p>
<ul>
<li>compositePOSの設定に構成品詞として「未知語」が指定されたエントリが存在する。</li>
<li>未知語が連続して出現する文字列をanalyzeする。（例：ニンテンドーDSi）</li>
</ul>
<p>ということで、trunkに修正版をコミットしました。
Issueは<a href="https://code.google.com/p/lucene-gosen/issues/detail?id=11">こちら。</a></p>
<p>※お茶をにごす感じの日記になってしまいました。次回はマシな記事を書く予定です。。。</p>
<p><span style="color:#FF0000">6/29追記</span>：恥ずかしいバグをいれこんでしましました。。。
ということで、trunkに再度修正版をコミットしました。</p>
</content:encoded>
    </item>
    
    <item>
      <title>NAIST-JDic for MeCab対応版（仮実装）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/21/naist-jdic-for-mecab%E5%AF%BE%E5%BF%9C%E7%89%88%E4%BB%AE%E5%AE%9F%E8%A3%85/</link>
      <pubDate>Tue, 21 Jun 2011 07:38:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/21/naist-jdic-for-mecab%E5%AF%BE%E5%BF%9C%E7%89%88%E4%BB%AE%E5%AE%9F%E8%A3%85/</guid>
      <description>lucene-gosenのtrunkbranches/impl-mecab-dicにNAIST-JDic for MeCabの辞書を利用出来るPre</description>
      <content:encoded><p>lucene-gosenの<del>trunk</del>branches/impl-mecab-dicにNAIST-JDic for MeCabの辞書を利用出来るPreprocessorをコミットしました。<br />
<br />
ビルド方法は次のとおりです。<br /></p>
<pre><code>
$ cd lucene-gosen-trunk
$ ant -Ddictype=naist-mecab
</code></pre><p><br />
現在のstable版で利用できる辞書は「ipadic」「naist-chasen」の2種類でした。<br />
<a href="http://johtani.jugem.jp/?eid=4">以前の記事</a>に書きましたが、naist-chasenの辞書でも2008年の更新となっています。<br />
今回コミットしたPreprocessorでは<a href="http://sourceforge.jp/projects/naist-jdic/">NAIST-JDicのサイト</a>で公開されているMeCab向けの辞書である「mecab-naist-jdic-0.6.3-20100801」を利用出来るようになります。<br />
<br />
ただし、lucene-gosenは昔のMeCabから派生したSenをもとにしていますので、最新のMeCabが持っている機能は<br />
利用できません。<br />
MeCab向けの辞書のうち一部のもの（matrix.def、naist-jdic.csvなど）を利用してlucene-gosen向けの辞書の中間ファイルを生成する仕組みになっています。<br />
<br />
まだ、仮実装版ということで、とりあえず動作するバージョンとなっています。<br />
まだテストが不十分ですが。。。<br />
利用してみて問題などあれば、lucene-gosenの<a href="http://code.google.com/p/lucene-gosen/issues/list">issue</a>に登録していただくか、コメントを頂ければと思います。<br />
<br />
※更新が週1回に落ちてきてるので、もう少し頑張らねば。<br />
<br />
<span style="color:#FF0000">※2011/07/04追記</span>
trunkにコミットしていましたが、branchに一旦移動しました。
仮実装として一旦コミットしたので、trunkとは別でテストする必要があるかと思った次第です。
ということで、試してみたい方は、<a href="http://code.google.com/p/lucene-gosen/source/browse/#svn%2Fbranches%2Fimpl-mecab-dic">branches/impl-mecab-dic</a>にありますので、触ってみてください。</p>
</content:encoded>
    </item>
    
    <item>
      <title>compositePOSの利用例（naist-chasenでの英単語の出力方法例）(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/14/compositepos%E3%81%AE%E5%88%A9%E7%94%A8%E4%BE%8Bnaist-chasen%E3%81%A7%E3%81%AE%E8%8B%B1%E5%8D%98%E8%AA%9E%E3%81%AE%E5%87%BA%E5%8A%9B%E6%96%B9%E6%B3%95%E4%BE%8B/</link>
      <pubDate>Tue, 14 Jun 2011 00:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/14/compositepos%E3%81%AE%E5%88%A9%E7%94%A8%E4%BE%8Bnaist-chasen%E3%81%A7%E3%81%AE%E8%8B%B1%E5%8D%98%E8%AA%9E%E3%81%AE%E5%87%BA%E5%8A%9B%E6%96%B9%E6%B3%95%E4%BE%8B/</guid>
      <description>前回、naist-chasenではアルファベットが別々の単語としてanalyzeされてしまうという話をしました。 ただ、これだと、英単語が含ま</description>
      <content:encoded><p>前回、naist-chasenではアルファベットが別々の単語としてanalyzeされてしまうという話をしました。</p>
<p>ただ、これだと、英単語が含まれた文章を形態素解析すると、英単語がアルファベット単位に区切られてしまい、
単語の意味をなさなくなってしまいます。</p>
<p>lucene-gosenでは、この問題に対応するための方法が提供されています。
CompositeTokenFilter（compositePOS）という機能です。</p>
<p>文字通り「トークン」を「合成」するための機能になります。</p>
<p>利用するためには以下の作業が必要です。（※Solrでのの利用方法を説明します。）</p>
<ol>
<li>compositePOS設定ファイル（composite_pos_ja_naist-chasen.txt）の用意</li>
<li>schema.xmlのtokenizerにcompositePOS設定を追加</li>
</ol>
<p>まずは、compositePOS設定ファイルの記述方法について説明します。
compositePOS設定ファイルには１行につき１つのcompositeの設定を記述していきます。
記述方法は次のようになります。品詞名を半角スペース区切りで記述します。</p>
<pre><code>
連結品詞名 構成品詞名1 構成品詞名2 ... 構成品詞名n
</code></pre><p>それぞれは次のような意味を持ちます。</p>
<ul>
<li>連結品詞名：合成したあとのトークンの品詞として出力する品詞名</li>
<li>構成品詞名：合成したい品詞名（スペース区切りで複数指定可能）</li>
</ul>
<p>TokenizerのcompositePOS機能は、構成品詞に定義されたトークンが連続して出力された場合に、
結合（合成）して１つのトークン（連結品詞名）として出力します。
また、以下のように構成品詞名が１種類で連続品詞名としても利用する場合は次のように省略した記述も可能です。</p>
<p>以下にcompositePOSファイルの設定例を上げます。
※なお、現時点では#によるコメント機能はありません。ので、記述した内容がそのまま利用されます。</p>
<pre><code>
名詞-数 
未知語 記号-アルファベット
</code></pre><p>１行目は連続した数字を1つのトークン（名詞-数）として出力する設定です。（連続品詞名＝構成品詞名として省略して記述した例になります。）
2行目は連続したアルファベットを１つのトークン（未知語）として出力する設定です。</p>
<p>次にSolrのschema.xmlにlucene-gosenのtokenizerを利用するフィールドタイプの設定を記述します。
$SOLR_HOME/conf/schema.xmlに以下を追加します。&lt;types&gt;～&lt;/types&gt;タグの間に記載します。</p>
<pre><code>
...
 &lt;types&amp;gt;
 ...

    &lt;fieldType name=&amp;quot;text_ja&amp;quot; class=&amp;quot;solr.TextField&amp;quot; positionIncrementGap=&amp;quot;100&amp;quot;&amp;gt;
    &lt;analyzer&amp;gt;
        &lt;tokenizer class=&amp;quot;solr.JapaneseTokenizerFactory&amp;quot; compositePOS=&amp;quot;composite_pos_ja_naist-chasen.txt&amp;quot;/&amp;gt;
    &lt;/analyzer&amp;gt;
    &lt;/fieldType&amp;gt;
 &lt;/types&amp;gt;
...

</code></pre><p>重要なのはtokenizerタグのcompositePOS属性になります。ここに1.で記載したファイルを指定します。指定したファイルはschema.xmlと同じディレクトリに配置します。
以上が利用するための設定です。</p>
<p>前回同様、「このComputerは、10回に1回の割合で起動中に青い画面が表示されます。」という文章をanalyze画面で解析した結果を示します。</p>
<p>
<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110614/20110614_1890705.png" alt="compositePOS設定済み"/>
    </div>
    <a href="/images/entries/20110614/20110614_1890705.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
とまぁ、記事を書きましたが、すでに<a href="http://wiki.livedoor.jp/haruyama_seigo/d/Solr/Tokenizer%C9%BE%B2%C1201105/JapaneseTokenizer">こちら</a>で出ている話ですね。。。</p>
<p>みなさん手が早くて困ってますｗ</p>
<p>ちなみに、上記の設定の場合、「100,000」や「3.14」といった文字列は「100」「,」「000」という形で出力されてしまいます。これらも数字とみなしたい場合は「名詞-数 名詞-数 記号-句点 記号-読点」という設定で１つのトークンとして出力されます。ただし、「。」も「記号-句点」なので注意が必要です。</p>
<h4 id="なお今回はlucene-gosen-110solr32を利用した例になっています">※なお、今回はlucene-gosen-1.1.0、Solr3.2を利用した例になっています。</h4>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosen 1.1.0 リリース(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/13/lucene-gosen-1-1-0-%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</link>
      <pubDate>Mon, 13 Jun 2011 12:08:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/13/lucene-gosen-1-1-0-%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9/</guid>
      <description>lucene-gosenの1.1.0がリリースされました。 大きな目玉はJapaneseTokenizerが出力する形態素に関するデータを遅延</description>
      <content:encoded><p>lucene-gosenの1.1.0がリリースされました。</p>
<p>大きな目玉はJapaneseTokenizerが出力する形態素に関するデータを遅延ロードすることで、パフォーマンスの改善を行ったことです。</p>
<p>詳しくは関口さんの<a href="http://lucene.jugem.jp/?eid=444">ブログ</a>で実測されてます。さすが、早い。。。
あと、先日リリースされたLucene/Solr 3.2への対応も行われています。</p>
<p>lucene-gosen-1.1.0のダウンロードは<a href="http://code.google.com/p/lucene-gosen/downloads/list">こちらから。</a></p>
<p>うーん、中身がない記事だ。。。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenのTokenFilterたち(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/06/lucene-gosen%E3%81%AEtokenfilter%E3%81%9F%E3%81%A1/</link>
      <pubDate>Mon, 06 Jun 2011 16:23:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/06/lucene-gosen%E3%81%AEtokenfilter%E3%81%9F%E3%81%A1/</guid>
      <description>lucene-gosenをSolr/Luceneで利用する場合、TokenFilterを利用してTokenizerが出力したToken対して</description>
      <content:encoded><p>lucene-gosenをSolr/Luceneで利用する場合、TokenFilterを利用してTokenizerが出力したToken対してさまざまな処理（Tokenに対する正規化や展開など）を追加することが可能です。</p>
<p>今回は現在（ver. 1.0.1）用意されているTokenFilterについて説明します。
以下はTokenFilterの一覧です。
「フィルタ名」にはSolrのschema.xmlで記述するクラス名を書いてあります。</p>
<table class="list_view">
<thead>
<tr>
<th>フィルタ名(Factory名)</th>
<th>概要</th>
</tr>
</thead>
<tbody>
<tr class="spec">
<td>solr.JapaneseWidthFilterFactory</td>
<td>全角のASCII文字を半角に、半角カタカナを全角にするフィルタ。例：「Ｃｏｍｐｕｔｅｒ」-&gt;「Computer」</td>
</tr>
<tr class="specalt">
<td class="alt">solr.JapanesePunctuationFilterFactory</td>
<td class="alt">区切り文字、記号などを除外するフィルタ。[※1](#token_filter_kome_1)</td>
</tr>
<tr class="spec">
<td>solr.JapanesePartOfSpeechStopFilterFactory</td>
<td>設定ファイルに記載した品詞に該当するTokenを除外するフィルタ。ファイルは「tags="ファイル名"」とfilterに記載。なお、ここで記述する品詞とはanalysis画面に表示される「partOfSpeech」の完全一致となります。</td>
</tr>
<tr class="specalt">
<td class="alt">solr.JapanesePartOfSpeechKeepFilterFactory</td>
<td class="alt">設定ファイルに記載した品詞に該当するToken<span style="color:#ff0000">"以外"</span>を除去フィルタ。ファイルは「tags="ファイル名"」とfilterに記載。なお、ここで記述する品詞とはanalysis画面に表示される「partOfSpeech」の完全一致となります。</td>
</tr>
<tr class="spec">
<td>solr.JapaneseBasicFormFilterFactory</td>
<td>Tokenを基本形に変換するフィルタ。例：「悲しき」-&gt;「悲しい」</td>
</tr>
<tr class="specalt">
<td class="alt">solr.JapaneseKatakanaStemFilterFactory</td>
<td class="alt">カタカナの長音（ー）の正規化フィルタ。4文字以上のカタカナのみの文字列の最後の長音（ー）を除去した文字列に変換します。例：「コンピューター」-&gt;「コンピュータ」、「コピー」-&gt;「コピー」</td>
</tr>
</tbody>
</table>
<p>上記のTokenFilterをJapanizeTokenizerを利用するフィールドタイプに設定することで
各フィルタによる機能が有効になります。
schema.xmlの記載に関する詳細については<a href="http://lucene-gosen.googlecode.com/svn/trunk/example/schema.xml.snippet">こちら</a>を参考にしてください。</p>
<p><span style="font-size:x-small;" id="token_filter_kome_1">※1　Characterクラスの以下の定数に相当する文字が。SPACE_SEPARATOR、LINE_SEPARATOR、PARAGRAPH_SEPARATOR、CONTROL、FORMAT、DASH_PUNCTUATION、START_PUNCTUATION、END_PUNCTUATION、CONNECTOR_PUNCTUATION、OTHER_PUNCTUATION、MATH_SYMBOL、CURRENCY_SYMBOL、MODIFIER_SYMBOL、OTHER_SYMBOL、INITIAL_QUOTE_PUNCTUATION、FINAL_QUOTE_PUNCTUATION</span></p>
</content:encoded>
    </item>
    
    <item>
      <title>辞書とカスタム辞書について(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/06/02/%E8%BE%9E%E6%9B%B8%E3%81%A8%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E8%BE%9E%E6%9B%B8%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</link>
      <pubDate>Thu, 02 Jun 2011 17:16:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/06/02/%E8%BE%9E%E6%9B%B8%E3%81%A8%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E8%BE%9E%E6%9B%B8%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/</guid>
      <description>辞書の特性について 現在lucene-gosenでは以下の2つの辞書が利用可能です。 簡単に違いについて説明します。 IPAdicの辞書について バ</description>
      <content:encoded><h3 id="辞書の特性について">辞書の特性について</h3>
<p>現在lucene-gosenでは以下の2つの辞書が利用可能です。
簡単に違いについて説明します。</p>
<p>IPAdicの辞書について</p>
<ul>
<li>バージョン：2.6.0（※IPAdicとして公開されている最新は2.7.0）</li>
<li>最終更新日：2003/06/19</li>
<li>登録単語数：約24万語</li>
<li>NAIST-Jdicができたためか、更新されていない</li>
</ul>
<p>NAIST-Jdic-for-ChaSenの辞書について</p>
<ul>
<li>バージョン：0.4.3（※NAISTとして公開されている最新はMeCab用の辞書0.6.3）</li>
<li>最終更新日：2008/07/07</li>
<li>登録単語数：約28万語</li>
<li>IPAdicの後継として整備。品詞の定義など大まかな仕様は共通。</li>
<li>IPAdicと異なり、アルファベットや数字が1文字ずつ単語として登録されている。</li>
</ul>
<p>IPAdicとNAIST-Jdicで大きな違いはアルファベットと数字の扱いについてです。
次のような文章をそれぞれの辞書で解析した結果は次のようになります。(SolrのField Analysisの画面です。思いの外大きいのでサムネイルのみですが。)
「このComputerは、10回に1回の割合で起動中に青い画面が表示されます。」
○IPAdicの場合

<link rel="stylesheet" href="/css/hugo-easy-gallery.css" />
<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1866934.png" alt="IPAdicの解析結果"/>
    </div>
    <a href="/images/entries/20110602/20110602_1866934.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
○NAIST-Jdicの場合


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1866935.png" alt="NAIST-Jdicの解析結果"/>
    </div>
    <a href="/images/entries/20110602/20110602_1866935.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
「Computer」と「10」という単語の区切り方が異なることがわかります。
この違いは、辞書のエントリが異なるために発生します。
NAIST-Jdicでは、数字（例：「1」）やアルファベット（例：「a」）が個々のエントリで登録されているため、分割された単語として認識されます。</p>
<p>※この問題への対応方法はまた後日。</p>
<h3 id="カスタム辞書について">カスタム辞書について</h3>
<p>実際のデータを形態素解析したい場合、辞書に存在しない単語を登録して、1単語として認識させたい場合があります。（固有名詞など）
このような場合にカスタム辞書を利用することで、新しい単語を辞書に登録することが可能になります。
カスタム辞書を利用する手順としては次のようになります。</p>
<ol>
<li>カスタム辞書ファイルの作成</li>
<li>作成した辞書ファイルを利用したjarファイルの生成</li>
</ol>
<p>まずは辞書ファイルの作成についてです。
以下では、naist-chasen(NAIST-Jdic)の辞書を例として説明します。（ディレクトリの違いだけで、IPAdicでも同じ方法でOKです。）</p>
<p>lucene-gosenでは辞書のコンパイルに2つのフェーズが存在します。</p>
<ol>
<li>gosen用辞書を生成する前処理（中間csvファイルの生成）</li>
<li>gosen用バイナリ辞書の生成</li>
</ol>
<p>カスタム辞書は1の出力の形式(=中間csvファイル=dictionary.csv)にあわせたCSVファイルとして作成します。
CSVの各カラムは次のような意味を持っています。</p>
<table border="1">
<tr>
<td>単語</td>
<td>単語の生起コスト</td>
<td>品詞</td>
<td>品詞細分類1</td>
<td>品詞細分類2</td>
<td>品詞細分類3</td>
<td>活用型</td>
<td>活用形</td>
<td>基本形</td>
<td>読み</td>
<td>発音</td>
</tr>
</table>
3カラム目以降は「素性（そせい？）」と呼ばれる項目です。ipadic、naist-jdicでは「品詞」「品詞細分類1」「品詞細分類2」「品詞細分類3」「活用型」「活用形」「基本形」「読み」「発音」となります。
※「見出し語」「形態素生起コスト」「素性」と呼ばれる項目を表形式にする。
厳密な品詞の体系に関してはIPAdicやNAIST-Jdicのサイトをご覧ください（説明できるレベルにはまだまだなっていないので。。。）
今回は、固有名詞（人名、地名など）を追加するという例でカスタム辞書について説明します。
固有名詞として「達川」という人名を追加してみましょう。
まずは、次のようなエントリをもつ「custom-dic.csv」ファイルを作成します。ファイルはUTF-8で保存してください。
コストはすでにあるエントリで似たようなエントリのコストを真似します。（今回は固有名詞,人名で似ているものを採用）。ちなみに、コストは小さいほど単語として出てきやすくなります。
※カスタム辞書にはSenで利用していたものが利用できます。
<hr>
<span style="background-color:#FFFFFF; color:#FF0000">**<em>"達川",2245,名詞,固有名詞,人名,名,*,*,"達川","タツカワ","タツカワ"</em>**</span>
<hr>
<p>上記ファイルを、先日紹介した$LUCENE-GOSEN/dictionary/ディレクトリにコピーします。
では、カスタム辞書を含んだlucene-gosenのjarを作成しましょう。
カスタム辞書のビルドは$LUCENE-GOSEN/dictionary/で行います。
また、カスタム辞書の指定はCSVファイル名をantの引数で指定します。次がコマンドの例になります。</p>
<pre><code>
$ cd lucene-gosen-trunk
$ cd dictionary
$ ant -Ddictype=naist-chasen clean-sen
$ ant -Ddictype=naist-chasen -Dcustom.dics=&quot;../custom-dic.csv&quot; compile
$ cd ..
$ ant -Ddictype=naist-chasen
</code></pre><p>上記コマンドの例で&quot;clean-sen&quot;というタスクを実行しています。これは、すでに出来上がっているgosen用のバイナリ辞書を削除するタスクになります。すでにgosen用の辞書が作成されている場合には辞書の再生成が行われないためです。
また、複数のファイルを利用したい場合は-Dcustom.dics=&quot;custom-dic.csv custom-dic2.csv&quot;という形でスペース区切りでファイル名を記述すればOKです。</p>
<p>カスタム辞書を適用する前と適用後の違いは次のとおりです。
適用前


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1867257.png" alt="カスタム辞書適用前"/>
    </div>
    <a href="/images/entries/20110602/20110602_1867257.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all">
適用後


<div class="box" >
  <figure  itemprop="associatedMedia" itemscope itemtype="http://schema.org/ImageObject">
    <div class="img">
      <img itemprop="thumbnail" src="/images/entries/20110602/20110602_1867258.png" alt="カスタム辞書適用後"/>
    </div>
    <a href="/images/entries/20110602/20110602_1867258.png" itemprop="contentUrl"></a>
  </figure>
</div>

<br clear="all"></p>
<p>簡単ですが、以上がカスタマイズ辞書を利用する方法でした。
ちなみに、この記事を書く前にすでにカスタム辞書の件を書いている<a href="http://d.hatena.ne.jp/shinobu_aoki/20110525/1306342970">ブログ</a>がありました。。。こちらも参考にしてください。
今回の例でいくつかSolrのanalysis画面を利用して説明してきました。Solrでのlucene-gosenの利用方法についてはまた後日記載したいと思います。
※参考までに。Solrでの利用方法は<a href="http://lucene.jugem.jp/">こちら</a>にも記載してあります。</p>
<p>また、IPAdicなどの辞書について記載のある書籍を見つけましたので、参考になれば。</p>


<div class="amazon-shortcode-box">
  <div class="amazon-shortcode-image">
    <a style="display: inline-block;" target="_blank" href="https://www.amazon.co.jp/gp/product/4625434394/?tag=johtani-22">
      <img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=JP&ASIN=4625434394&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL160_&tag=johtani-22" >
    </a>
  </div>
  <div class="amazon-shortcode-info">
    <p class="amazon-shortcode-title">
      <a target="_blank" href="https://www.amazon.co.jp/gp/product/4625434394/?tag=johtani-22">
      アプリケーションソフトの基礎 (講座ITと日本語研究)
      </a>
    </p>
  </div>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>ソースからのビルドと構成(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/05/26/%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%8B%E3%82%89%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89%E3%81%A8%E6%A7%8B%E6%88%90/</link>
      <pubDate>Thu, 26 May 2011 17:34:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/05/26/%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%8B%E3%82%89%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89%E3%81%A8%E6%A7%8B%E6%88%90/</guid>
      <description>今回はソースのダウンロードとビルドについてです。 最新版のソースを利用したり、JavaDocを見たい場合はソースをダウンロードしてからビルドす</description>
      <content:encoded><p>今回はソースのダウンロードとビルドについてです。</p>
<p>最新版のソースを利用したり、JavaDocを見たい場合はソースをダウンロードしてからビルドすることになります。
ソースのダウンロードからビルドまでの手順について説明します。</p>
<p>まずはソースのダウンロードです。</p>
<pre><code>$ mkdir ~/work
$ cd work
$ svn co http://lucene-gosen.googlecode.com/svn/trunk/ lucene-gosen-trunk
$ cd lucene-gosen-trunk
</code></pre><p>ダウンロードしたソースは次のようなディレクトリ構成です。</p>
<table class="simple_table">
<tbody>
<tr><th>.classpath</th><td>Eclipse用ファイル</td></tr>
<tr><th>.project</th><td>Eclipse用ファイル</td></tr>
<tr><th>.settings</th><td>Eclipse用ファイル</td></tr>
<tr><th>AUTHORS</th><td>作者のリスト（Sen、GoSen）</td></tr>
<tr><th>CHANGES.txt</th><td>lucene-gosenにおける更新履歴</td></tr>
<tr><th>COPYING.LGPL</th><td>ライセンス</td></tr>
<tr><th>README.txt</th><td>Readme</td></tr>
<tr><th>build.xml</th><td>Antのビルドファイル</td></tr>
<tr><th>dictionary</th><td>辞書コンパイル用ディレクトリ</td></tr>
<tr><th>docs</th><td>APIドキュメント用ディレクトリ</td></tr>
<tr><th>lib</th><td>ライブラリ</td></tr>
<tr><th>prettify</th><td>Google Code Prettify用ディレクトリ（APIドキュメントでの色づけ用）</td></tr>
<tr><th>src</th><td>ソースコード</td></tr>
</tbody></table>
<p>また、辞書やソースのコンパイルにはAntを利用します。
通常利用するAntのタスクには次のようなものがあります。</p>
<table class="simple_table">
<tbody>
<tr><th>clean</th><td>プロジェクトのクリーンアップ</td></tr>
<tr><th>build-dic</th><td>辞書のコンパイル（辞書のダウンロードも行う）</td></tr>
<tr><th>jar</th><td>jarファイル生成</td></tr>
<tr><th>dist</th><td>配布パッケージの生成（2つのjarファイル生成）</td></tr>
<tr><th>javadoc</th><td>JavaDocの生成</td></tr>
</tbody></table>
<p>jarファイルの生成までの大まかな流れは「javaソースのコンパイル」～「辞書のダウンロード」～「辞書のプレコンパイル」～「辞書のコンパイル」～「jarファイルの生成」となります。
Antのタスク以外にjarファイルを生成する場合に利用するオプションは以下の通りです。</p>
<table class="simple_table">
<tbody>
<tr><th>-Dproxy.host</th><td>プロキシのホスト</td></tr>
<tr><th>-Dproxy.port</th><td>プロキシのポート</td></tr>
<tr><th>-Ddictype</th><td>辞書の指定（指定可能なものは次の通り。naist-chasen、ipadic）</td></tr>
</tbody></table>
<p>以下はNaist-Jdicのjarファイルを生成するコマンドの実行例になります。プロキシサーバを利用する環境の場合は-Dproxy.hostと-Dproxy.portも指定してください。（※認証が必要なプロキシの場合はAntのビルドファイルを修正する必要が出てきます。）</p>
<pre><code>
$ ant -Ddictype=naist-chasen
</code></pre><p>jarファイルはdistディレクトリに生成されます。
これで、jarファイルが利用できるようになります。</p>
<p>次回は、ipadicとNaist-chasenの辞書の違いとカスタム辞書を利用する方法について書こうと思います。</p>
</content:encoded>
    </item>
    
    <item>
      <title>lucene-gosenとは(Jugemより移植)</title>
      <link>https://blog.johtani.info/blog/2011/05/23/lucene-gosen%E3%81%A8%E3%81%AF/</link>
      <pubDate>Mon, 23 May 2011 15:00:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2011/05/23/lucene-gosen%E3%81%A8%E3%81%AF/</guid>
      <description>概要： Lucene/SolrのコミッターであるRobert Muirさんが始めたプロジェクト 歴史： MeCabのJava移植版としてスタートした</description>
      <content:encoded><p>概要：
　Lucene/SolrのコミッターであるRobert Muirさんが始めたプロジェクト
　
</p>
<p>
歴史：
　MeCabのJava移植版としてスタートしたSenがベースになります。
　その後、辞書の構築部分をPerlからJavaに置き換えたGoSenが登場しました。
　が、どちらもメンテナンスされなくなってきたので、Robertさんが引き継いでメンテナンスとLucene/Solr対応をはじめました。そして、現在にいたります。
</p>
<p>
ライセンス：
　LGPLライセンス（ベースになったMeCabのライセンスにならって）
</p>
<p>
特徴：
　以下のような特徴があります。
　・Lucene/Solrですぐに利用可能（3.1、4.0に対応済み）
　・jarファイル1つで利用可能（辞書をjarファイルに内包）
　・LuceneのAttributeをベースにしたTokenの解析
　・その他（パフォーマンス改善、テスト改善など）
</p>
<p>
プロジェクトのサイト：
　http://code.google.com/p/lucene-gosen/
</p>
<p>
ダウンロード：
　http://code.google.com/p/lucene-gosen/downloads/list
　現時点では2つの辞書を内包したjarファイルが用意されています。
<p>　Naist-jdic 0.4.3（for ChaSen） 参考サイト：http://sourceforge.jp/projects/naist-jdic/
　IpaDic 2.6.0　参考サイト：http://sourceforge.jp/projects/ipadic/
　</p>
</p>
</content:encoded>
    </item>
    
  </channel>
</rss>
