<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kuromoji on @johtaniの日記 3rd</title>
    <link>https://blog.johtani.info/tags/kuromoji/</link>
    <description>Recent content in kuromoji on @johtaniの日記 3rd</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Tue, 09 Jun 2020 17:44:00 +0900</lastBuildDate><atom:link href="https://blog.johtani.info/tags/kuromoji/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Azure Cognitive Searchでの日本語向けAnalyzerの違い</title>
      <link>https://blog.johtani.info/blog/2020/06/09/difference-analyzers-in-azure-search/</link>
      <pubDate>Tue, 09 Jun 2020 17:44:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2020/06/09/difference-analyzers-in-azure-search/</guid>
      <description>Azure Cognitive Searchで日本語を扱うときに、形態素解析器を使いたい場合、2種類のAnalyzerが用意されています。今回はこれらの違いがどんなもの</description>
      <content:encoded><p>Azure Cognitive Searchで日本語を扱うときに、形態素解析器を使いたい場合、2種類のAnalyzerが用意されています。今回はこれらの違いがどんなものかを見ていくことにします。</p>
<h2 id="analyzerとは">Analyzerとは?</h2>
<p>まずは、その前にAnalyzerとは何者か?というのを少しだけ。
Azure Cognitive Searchは<a href="https://ja.wikipedia.org/wiki/%E8%BB%A2%E7%BD%AE%E3%82%A4%E3%83%B3%E3%83%87%E3%83%83%E3%82%AF%E3%82%B9">転置インデックス</a>を内部で作成して、検索を行っています。
この、転置インデックスは、「単語」がどのドキュメントに入っているか?を素早く見つけることができるデータ構造となっています。</p>
<p>Azure Cognitive Searchは、この「単語」を入力された文章から生成するときに、Analyzerというものを利用します。
Analyzerは入力された文章をある規則に則って単語に分割する機能です。
この「ある規則」が、各種言語や用途によって様々に用意されています。
今回はこの中の<code>ja.lucene</code>と<code>ja.microsoft</code>という2種類のAnalyzerについて違いを見ていきます。</p>
<h2 id="2種類のanalyzerの違いはどんなもの">2種類のAnalyzerの違いはどんなもの?</h2>
<p>このAnalyzerの挙動を見るためのエンドポイントとして<code>analyze</code>というAPIがあります(<a href="https://blog.johtani.info/blog/2020/03/19/azure-search-analyze-plugin/">詳細は昔のブログを参照</a>)。</p>
<p>このAPIを利用して、Wikipediaのいくつかの文章を単語に区切って見て、
<code>ja.microsoft</code>がどんな動きをしているのか想像してみます(残念ながら<code>ja.microsoft</code>の仕様?や挙動についてはページが見つからないため)。</p>
<h3 id="もとの文章と解析結果一部抜粋">もとの文章と解析結果(一部抜粋)</h3>
<p>文章は、手元のElasticsearchに登録したjawikiのデータからランダムに抽出しています。また、自前のツールで生成したWikipediaのデータなので、まだ一部、見苦しい文字列になっています(そっちもなおさないと)。</p>
<h4 id="1-砂川熊本県">1. 砂川（熊本県）</h4>
<blockquote>
<p>thumb|250px|right|上砂川橋より上流方砂川（すながわ）は、熊本県宇城市・八代郡氷川町を流れる二級河川。</p>
</blockquote>
<p>この文字列から抽出された単語で特徴的なものを一部抜粋しました。</p>
<table>
<thead>
<tr>
<th><code>ja.microsoft</code></th>
<th><code>ja.lucene</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>250px</td>
<td>250</td>
</tr>
<tr>
<td></td>
<td>px</td>
</tr>
<tr>
<td>上砂</td>
<td>上, 上砂川</td>
</tr>
<tr>
<td>川橋</td>
<td>砂川</td>
</tr>
<tr>
<td></td>
<td>橋</td>
</tr>
<tr>
<td>宇城</td>
<td>宇</td>
</tr>
<tr>
<td>市</td>
<td>城市</td>
</tr>
<tr>
<td>二級</td>
<td>二</td>
</tr>
<tr>
<td>^</td>
<td>級</td>
</tr>
</tbody>
</table>
<p>まず、最初の<code>250px</code>ですが、<code>ja.microsoft</code>では、<code>px</code>が単位であると判定しているのか、数値と合わせた単語として抽出されています。この場合、<code>250</code>で検索しても、この文字列はヒットしない形になるので、ノイズが減ることが考えられるかと。</p>
<p><code>上砂川橋</code>という文字は、分割の仕方が別れました。
<code>ja.lucene</code>では、<code>上砂川</code>という単語が地名として辞書に存在するために、このような分割になっています。<code>ja.microsoft</code>のデータは品詞の情報が取れないのですが、<code>上砂</code>、<code>川橋</code>ともに、名詞として辞書に存在しているのではないかなと。<code>ja.lucene</code>には<code>川橋</code>という単語は存在していないようでした。</p>
<p><code>宇城市</code>(うきし)については、2005年に合併でできた市のようで、<code>ja.lucene</code>が利用している辞書には存在しない可能性があり、<code>宇城</code>という文字が抽出できてないと思われます。</p>
<p>最後は<code>二級</code>です。<code>ja.lucene</code>では、数字と助数詞として分割されています。こちらも何かしらのロジックにより、<code>二級</code>という1単語でヒットできるように数字と単位?が合わせた単語で出てくる仕組みが<code>ja.microsoft</code>なのかなと。</p>
<h4 id="2-uefa-u-18女子選手権2000">2. UEFA U-18女子選手権2000</h4>
<blockquote>
<p>UEFA U-18女子選手権2000は第3回目のUEFA U-18女子選手権である。決勝トーナメントは2000年7月27日から8月4日までフランスで行われ、ドイツが初優勝を果たした。</p>
</blockquote>
<p>この文字列から抽出された単語で特徴的なものを一部抜粋しました。</p>
<table>
<thead>
<tr>
<th><code>ja.microsoft</code></th>
<th><code>ja.lucene</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>u-18, u</td>
<td>u</td>
</tr>
<tr>
<td>18, nn18</td>
<td>18</td>
</tr>
<tr>
<td>第3回目</td>
<td>第</td>
</tr>
<tr>
<td></td>
<td>3</td>
</tr>
<tr>
<td></td>
<td>回</td>
</tr>
<tr>
<td></td>
<td>目</td>
</tr>
<tr>
<td>トーナメント, トナメント</td>
<td>トーナメント</td>
</tr>
<tr>
<td>2000年</td>
<td>2000</td>
</tr>
<tr>
<td></td>
<td>年</td>
</tr>
<tr>
<td>7月</td>
<td>7</td>
</tr>
<tr>
<td></td>
<td>月</td>
</tr>
<tr>
<td>27日</td>
<td>27</td>
</tr>
<tr>
<td></td>
<td>日</td>
</tr>
</tbody>
</table>
<p>数字を含む単語<code>第3回目</code>や<code>2000年</code>、<code>7月</code>などは、<code>ja.microsoft</code>は先程と同様、数字と単位の組み合わせを1単語として出力しています。</p>
<p>また、<code>トーナメント</code>という単語を<code>トナメント</code>という形で、長音を除去した形で出力しています。
今回の文字列ではないですが、この他に、<code>センター</code>を<code>センター</code>と<code>センタ</code>の2パターンの単語で出力するといったことを行っています。
<code>ja.lucene</code>の場合、単語の最後に長音がある場合だけ<code>センタ</code>として、長音を除去した単語が出力されます。
これは、長音の表記ゆれに対応するためではないかなと。たとえば、<code>インターフェース</code>と<code>インタフェース</code>、<code>インターフェイス</code>のように、人や文章によって、間にでてくる長音を使ったり使わなかったりという表記ゆれに対応するためだと思われます。
その他にも、<code>イプロゥヴェト</code>を<code>イプロゥベト</code>に、<code>ネクスト</code>を<code>ネキスト</code>に、<code>バラエティ</code>を<code>バラエチ</code>にも変換するなどといった処理をしてくれるようです。カタカナの表記ゆれには強そうですね(これどうやってるんだろう?)。</p>
<p><code>ja.microsoft</code>では、<code>nn18</code>というちょっと変わった単語も出てきていました。純粋な数字の場合は<code>nn</code>と入力してくれるようで、数字だけで検索したい場合に利用できるのかな?これはドキュメントに書いておいてほしいかも?</p>
<h3 id="共通点">共通点</h3>
<p><code>ja.lucene</code>、<code>ja.microsoft</code>ともに、共通している動作として、「てにをは」といった単語は除去されていました。違いがあるものとしては、「より」(助詞-格助詞-一般)、「されている」(動詞-自立、動詞-接尾、助詞-接続助詞、動詞-非自立)、「ある」(助動詞)といったものは<code>ja.microsoft</code>では除去されずに出てきていました。
ストップワード的に「てにをは」あたりを除去をしている感じでしょうか?</p>
<p>アルファベットで構成されている単語についても、基本はそのまま出力される挙動のようでした。</p>
<h2 id="じゃあどっちがいいの">じゃあどっちがいいの?</h2>
<p>残念ながらどちらがいいかは、一長一短かなぁと。
<code>ja.lucene</code>に関しては、Luceneの仕組みを利用しているので、Elasticsearchなどを使えば、個別の単語についてより詳細の情報を取得することが可能です(品詞、読みなど)。<code>ja.microsoft</code>については、残念ながら手の入れようがないので、そういう動きのものだという割り切った使い方になるでしょうか?
ただ、長音の除去による表記ゆれなどについては、便利な機能なので、そのあたりの問題に対応したい場合は、<code>ja.microsoft</code>を活用するのも良いかと思います。</p>
<p>個人的には、より細かい単語としてインデックスに登録できるもののほうが、柔軟な検索には対応できるのではないかなぁと考えています(Kuromojiの辞書をUniDicにするとか?も考えますが、これはAzure Searchではできないですが)。</p>
<h2 id="まとめ">まとめ</h2>
<p>Wikipediaのデータをいくつか使って、<code>ja.microsoft</code>と<code>ja.lucene</code>の違いについて、考察してみました。何かの役に立てばと。
他に、これはどんな感じになるの?などありましたら、コメントいただければと。</p>
</content:encoded>
    </item>
    
    <item>
      <title>Elasticsearch 0.90.8がリリースされました＆注意点（2013/12/20追記）</title>
      <link>https://blog.johtani.info/blog/2013/12/20/release-elasticsearch-0-90-8/</link>
      <pubDate>Fri, 20 Dec 2013 16:24:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/12/20/release-elasticsearch-0-90-8/</guid>
      <description>昨夜、Elasticsearchの0.90.8がリリースされました。 リリースされた内容などについては、本家のブログ「0.90.8 releas</description>
      <content:encoded><p>昨夜、Elasticsearchの0.90.8がリリースされました。</p>
<p>リリースされた内容などについては、本家のブログ「<a href="http://www.elasticsearch.org/blog/0-90-8-released/">0.90.8 released</a>」をご覧いただくこととして。
1点注意したほうが良い点があります。</p>
<!-- more -->
<p><a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">elasticsearch-analysis-kuromoji</a>を利用している場合は、0.90.8に対応したバージョンがリリースされるのを待つ必要があります。</p>
<p>elasticsearch 0.90.8はLuceneのバージョンが4.6.0に変更されています。
Lucene 4.6.0では、TokenStreamというTokenizerのI/Fに変更があり、Tokenizerの実装を変更する必要があります。</p>
<p>現時点（2013年12月19日現在）のelasticsearch-analysis-kuromojiの1.6.0にはlucene-analyzers-kuromoji-4.5.1.jarが含まれており、この部分でI/Fが異なるためエラーが発生してしまいます。
プラグインをインストールする時点ではエラーは発生せず、実際にKuromojiのTokenizerやAnalyzerを利用するタイミングでエラーが出ます。
以下、0.90.8にanalysis-kuromojiの1.6.0をインストールした状態で<code>_analyze</code>を実行した時のエラー。</p>
<pre><code>curl -XPOST 'localhost:9200/_analyze?tokenizer=kuromoji_tokenizer&amp;filters=kuromoji_baseform&amp;pretty' -d '寿司が美味しかった'
{
  &quot;error&quot; : &quot;IllegalStateException[TokenStream contract violation: reset()/close() call missing, reset() called multiple times, or subclass does not call super.reset(). Please see Javadocs of TokenStream class for more information about the correct consuming workflow.]&quot;,
  &quot;status&quot; : 500
}
</code></pre><p>ということで、1.7.0がリリースされるのを待つか、自分で<code>mvn package</code>してビルドする必要があります。
他にも独自でTokenizerなどを造られている方は注意が必要かと。</p>
<p>たぶん、すぐにリリースされるんじゃないかなぁと。</p>
<p><strong>2013/12/20追記</strong></p>
<p>とりあえず、masterブランチが0.90.8に変更されたみたいです。(と書いてるそばから、1.7.0がリリースされました)
ということで、0.90.8では1.7.0を使うとエラーが出ないです。
（あと、踊り字対応のcharfilterも追加されたみたいです）</p>
</content:encoded>
    </item>
    
    <item>
      <title>日本語Wikipediaをインデクシング（Kuromojiバージョン）</title>
      <link>https://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji/</link>
      <pubDate>Tue, 03 Sep 2013 01:15:00 +0900</pubDate>
      
      <guid>https://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji/</guid>
      <description>前々回紹介した、日本語Wikipediaのデータをインデックス登録する記事の続きです。 今回は、Kuromojiのアナライザを利用してインデッ</description>
      <content:encoded><p><a href="/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch">前々回紹介した、日本語Wikipediaのデータをインデックス登録する記事</a>の続きです。</p>
<!-- more -->
<p>今回は、Kuromojiのアナライザを利用してインデックス登録してみます。</p>
<h2 id="余談proxy環境でのプラグインインストール">余談（Proxy環境でのプラグインインストール）</h2>
<p>ElasticSearchのpluginコマンドはJavaで実装されています。（<a href="https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/plugins/PluginManager.java#L315">org.elasticsearch.plugins.PluginManager</a>）
プラグインのダウンロードには、java.net.URL.openConnection()から取得URLConnectionを使用しています。</p>
<p>ですので、pluginのインストールを行う際に、Proxy環境にある場合は以下のようにコマンドを実行します。</p>
<pre><code>./bin/plugin -DproxyPort=ポート番号 -DproxyHost=ホスト名 -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre><h2 id="elasticsearch-analysis-kuromojiのインストール">elasticsearch-analysis-kuromojiのインストール</h2>
<p>WikipediaのデータをKuromojiを使って、形態素解析ベースの転置インデックスを作成していきます。
まずは、Kuromojiを利用するために、<a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">Analysisプラグイン</a>のインストールです。
ElasticSearchのバージョンに対応したプラグインのバージョンがあります。（プラグインのページに対応したバージョンの記載あり）
今回はElasticSearchの0.90.3を利用しているため、1.5.0をインストールします。</p>
<pre><code>./bin/plugin -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre><p>インストール後は再起動しておきます。
なお、Kuromojiを利用して、Wikipediaのデータを登録するばあい、デフォルトの設定では、ヒープが足りなくなるおそれがあります。
ElasticSearchの起動時に以下のオプションを指定して、最大ヒープサイズを2Gとしておきます。</p>
<pre><code>export ES_HEAP_SIZE=2g;./bin/elasticsearch
</code></pre><h2 id="indexの作成デフォルトでkuromojiのanalyzerを利用する">Indexの作成（デフォルトでKuromojiのAnalyzerを利用する）</h2>
<p>Wikipediaのデータを登録する際に、Kuromojiのアナライザを利用したいのが今回の趣旨でした。
一番ラクな方法として、Wikipediaデータのインデックスの設定として、デフォルトのアナライザをKuromojiにしてしまいます。
（きちんと設計する場合は、必要に応じてフィールドごとに指定しましょう）</p>
<pre><code>curl -XPUT 'localhost:9200/ja-wikipedia-kuromoji' -d '{
    &quot;settings&quot;: {
        &quot;analysis&quot;: {
            &quot;analyzer&quot;: {
                &quot;default&quot; : {
                    &quot;type&quot; : &quot;kuromoji&quot;
                }
            }
        }
    }
}'
</code></pre><p>これでkuromojiのアナライザがデフォルトで利用される形となります。
あとは、Riverを起動して登録するだけです。</p>
<h2 id="riverの実行">Riverの実行</h2>
<p>前回と一緒です。
インデックス名（<strong>_river/&lt;インデックス名&gt;/_meta</strong>）だけは、先ほど作成した「<code>ja-wikipedia-kuromoji</code>」に変更してください。</p>
<pre><code>curl -XPUT localhost:9200/_river/ja-wikipedia-kuromoji/_meta -d '
{
    &quot;type&quot; : &quot;wikipedia&quot;,
    &quot;wikipedia&quot; : {
        &quot;url&quot; : &quot;file:/home/johtani/src/jawiki-latest-pages-articles.xml&quot;
    },
    &quot;index&quot; : {
        &quot;bulk_size&quot; : 10000
    }
}'
</code></pre><p>あとは、インデックスされるのを待つだけです。</p>
<h2 id="データ量とか">データ量とか</h2>
<p>5.8gbになりました。Kuromojiを利用したため、形態素解析により単語にきちんとトークないずされた結果でしょう。
Uni-gramだと、転置インデックスのボキャブラリも単語に対してヒットするドキュメント数も大きくなるため、
インデックスサイズも大きくなっているのかと。</p>
<p>検索クエリのサンプルなどはまた後日。（夜遅いので。。。）</p>
</content:encoded>
    </item>
    
  </channel>
</rss>
