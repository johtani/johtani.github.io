---
layout: post
title: "辞書のjarファイルからの分離(Jugemより移植)"
slug: 辞書のjarファイルからの分離
author: johtani
date: 2011-08-23T10:26:00+09:00
comments: true
tags: [lucene-gosen]
---
ひさびさに、lucene-gosenの話題です。

lucene-gosenはjarファイルに辞書も同梱されており、jarファイルをクラスパスに取り込むだけで、
簡単に形態素解析器が利用できるといお手軽さがあり、便利です。

ですが、以前[カスタム辞書の登録](http://code.google.com/p/lucene-gosen/issues/detail?id=16)について記事を書いたように、カスタム辞書の登録は思いのほか手間がかかります。
lucene-gosenのソースをダウンロードし、lucene-gosenを一度コンパイルし、カスタム辞書のcsvファイルを作成し、カスタム辞書を取り込んだ辞書のバイナリを生成し、最後にjarファイルにするという作業です。（書くだけでいやになってきました。。。）さらに作成したjarファイルをSolrや各プログラムに再度配布するという具合です。

そこで、[辞書ファイルの外部化ができないかという話](http://code.google.com/p/lucene-gosen/issues/detail?id=16)があがっていました。
すこし時間ができたので、山積みになっているissueを横目に軽く実装をしてpatchをissueにアップしました。

機能としてはごく簡単で、JapaneseTokenizerのコンストラクタに辞書のディレクトリ（*.senファイルのあるディレクトリ）を指定可能にしただけです。
また、JapaneseTokenizerFactoryでもdictionaryDir属性で指定可能になっています。
まずは、コンパイルの方法から。
trunkをSVNでcheckoutし、issueにあるpatchをダウンロードして適用します。（svnのチェックアウトについては[こちら](http://johtani.jugem.jp/?eid=3)を参考にしてください。）
<div class="run_sample">
```

$ cd lucene-gosen-trunk
$ patch -p0 --dry-run &lt; lucene-gosen-separate-dictionary.patch
$ patch -p0 &lt; lucene-gosen-separate-dictionary.patch
```
</div>

次に、antを実行し辞書なし版のjarファイルをビルドします。
<div class="run_sample">
```

$ ant nodic-jar
```
</div>
これで、dictディレクトリに「lucene-gosen-1.2-dev.jar」というjarファイルが出来上がります。
（※ただし、これだけでは動作しないので、別途辞書のコンパイルは必要です。）

次に、指定の仕方です。JapaneseTokenizerのコンストラクタは第3引数に辞書のディレクトリ（フルパスor実行ディレクトリからの相対パス）を渡すだけです。
```

  Tokenizer tokenizer = new JapaneseTokenizer(reader, compositeTokenFilter, dictionaryDir);
```

最後に、Solrのtokenizerタグでの指定方法です。
```

    &lt;fieldType name="text_ja" class="solr.TextField" positionIncrementGap="100"&gt;
    &lt;analyzer&gt;
        &lt;tokenizer class="solr.JapaneseTokenizerFactory" dictionaryDir="/hoge/dictionarydir"/&gt;
    &lt;/analyzer&gt;
    &lt;/fieldType&gt;
```


以上が、簡単な設定の仕方です。なお、辞書を内包したjarファイルでもdictionaryDirは利用可能です。優先度としては、dictionaryDirが指定されている場合はdictionaryDirを探索しファイルがなければRuntimeExceptionです。指定がnullもしくは空文字の場合はjarファイルの辞書の読み込みを行います。


次に利用シーン、制限事項についてです。
利用シーンとしてはカスタム辞書を定期的にメンテナンス（追加更新）しながらSolrを運用するというのが想定されます。定期的に辞書の再読み込みをしたい場合です。
利用方法は次のようになります。
* Solrのマルチコア構成を利用する
* 各コアごとにlib/lucene-gosen-1.2-dev.jarを用意
* 辞書の更新が終わったらコアのRELOADを実施


コアをリロードすることで、lucene-gosenが辞書を再読み込みようになります。（現状でも再読み込みするが、jarファイルを再配置しないといけない。）あとは、定期的に辞書ファイルを更新、再構築しコアをリロードすれば、
リロード後に新しい辞書が利用できるという具合です。
（もちろん、辞書更新後に入った単語は辞書更新以前に作成したインデックスにはでてこないですが。。。）
また、コアごとにdictinaryDirを別々に指定することも可能です。

制限事項は次のようになります。
* マルチコアの設定でsharedLibにlucene-gosenのjarを含まない
* 同一コア内で異なるdictinaryDirの指定はできない


以上が、辞書の外部ファイル化のパッチについてでした。
少しテストケースを追加したら、trunkにコミットする予定です。興味があれば、パッチを利用してみてください。



[SyntaxHighlighter](http://code.google.com/p/syntaxhighlighter/)の導入をしないとソースコードが見にくいですね。。。導入を検討しないと。。。どこかにWebサーバ用意しないとダメかも

