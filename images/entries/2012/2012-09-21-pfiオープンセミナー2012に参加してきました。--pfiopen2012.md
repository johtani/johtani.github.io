---
layout: post
title: "PFIオープンセミナー2012に参加してきました。 #pfiopen2012(Jugemより移植)"
slug: pfiオープンセミナー2012に参加してきました。--pfiopen2012
author: johtani
date: 2012-09-21T17:05:00+09:00
comments: true
tags: [勉強会]
---
PFIオープンセミナー2012に参加してきました。
対象から微妙に外れてたり、話の内容についていけるか自信がありませんでしたが、参加してきました。
PFIさんは前から面白そうなことやってる会社だなぁと思っていたので。

面白い話がいっぱい聞けました。
~~電池が切れそうなので、とりあえず、まずはメモをアップしときます。~~
かろうじてついていけたという感じですが。
丸山先生の話はアーキテクチャの話に入る前のビッグデータの光と影の話がよかったです。
ビッグデータと言っても、まずは、サンプリングなどで小さなデータで処理できるかもしれないと考えるのも必要なのでは？という話や、相関があるからといって、因果が有るわけではないとか、おそらく、統計やってる人や、数学やってる人にしてみれば、当たり前の事なんでしょうが、その部分に警鐘を鳴らす話が聞けたのは良かったです。
もちろん、ビッグデータでなければ意味が無い解析などもありますという話もきちんと出ていました。

伊藤さんの話は、Screwと呼ばれる、多言語解析基盤のお話です。その前にSedueの紹介で、SolrとSedueの比較の話も出ていました。（若干、強引な感じもしましたが。。。）
多言語解析基盤は、Solrでも少し入ってきています。ただ、それよりも汎用的な作りになるようなので、今後Solrと組み合わせて使うといったことも可能になるかもしれません。
まだ、対応言語などが少ないので今後に期待という感じでしょうか。
複数の言語が混ざった時の挙動がどうなるのかや、身近な文章での言語判定の正確さは少し気になります。

サイバーセキュリティの話も面白かったのですが、話が多岐にわたるのと、スライドの情報量の多さに少しついていけませんでした。
資料が公開されたら、もう一度見直したいかなぁと。

比戸さんの話は、Jubatusと関連のある話でした。機械学習の実際の利用の話が特に興味深かったです。
最近になって、ようやく、実際のデータを利用した話が出てきているみたいで、もっと事例が出てくると機械学習も身近になりそうだなぁと。

最後は、日経BPの中田さんの話でした。これが、一番想像していたものと内容が違って、驚きつつ、楽しめた話でした。
ビッグデータというバズワードがいかにして生まれたのかがよく分かりました。
私は、バズワードだなぁと思う程度だったのですが、出てきた背景にある程度意味があるという考察に感心して聞き入ってしまいました。

ということで、思っていたよりも話の内容についていけたので、講演された方々の話しのされ方が良かっただと思います。

少し無理をして参加してよかったと。

残念だったのは、会場が地下だったため、携帯が入らなかったことでしょうか。
私はe-mobileで接続していたので大丈夫でしたが、docomoの携帯は圏外でした。
ツイートがもう少し盛り上がれば、もっと質問も出たのかもしれないです。

[http://preferred.jp/news/seminar/](http://preferred.jp/news/seminar/)

資料が公開されたので、リンクを貼っておきます。
PFIの方たちの資料へのリンク：
[http://preferred.jp/news/?id=1139](http://preferred.jp/news/?id=1139)


ゲスト講師の資料へのリンク：
[http://preferred.jp/news/?id=1159](http://preferred.jp/news/?id=1159)

___

```

◯「多様化する情報を支える技術」　講師：西川徹（株式会社プリファードインフラストラクチャー　代表取締役）
　・PFIの説明
　　VCに頼らない。製品につながるビジネスにこだわる（受託開発しない）、技術の多様性を重視
　・PFIの技術領域、ビジネス
　　製品開発（Sedue/Bazil/Jubatus）、自然言語処理、機械学習、分散システムなど
　・”人”が生み出すデータと"機械"が生み出すデータ
　　ビッグデータの発端はGoogleが元じゃないか？→最後の公演で解説があるよ
　　人：質が高いけど、量が少ない
　　機械：質は低いけど、量が多い
　・検索システムについてのお話
　　社内の資料とか情報が、人によって、まちまちなデータの保存（形式、場所など）が実施されてしまう。
　　情報検索技術と大規模データ
　・人のデータへ必要なアプローチ
　　より検索システムを活用してもらうために、楽に整理できる仕組みなどをどう提供するか
　　質の高いデータなのに、形式的な共有しかできていないのはもったいない
　・機械のデータへ必要なアプローチ

　　大量データと高度な解析が重要（CEPとか）
　　デバイスが性能向上→流れてくるデータが大量に→蓄積するだけでも問題になってくる
　　　→蓄積したデータを扱うだけでも処理コストが高くなる
　　分析をオンライン化、ストリーム化すること→Jubatusで貯めずに高度な解析をしましょう。
　　Edge-Heavyになりつつある。　　

◯「ITアーキテクチャはどこへ向かうのか」
　講師：丸山宏氏（統計数理研究所　副所長　モデリング研究系教授　工学博士）
　・ビッグデータの光と影
　　「その数学が戦略を決める」という本がオススメ
　　・大量データでも、ランダムサンプリングでとければ、ビッグデータじゃなくてもいいよね。
　　　もちろん、ランダムサンプリングだけじゃダメな場合もある。
　　・Hadoopが解ける問題領域って少ないのでは。
　　・TVを見る時間が長い人ほど、方言の使用率が高い
　　　因果関係と相関関係の違いをきちんと理解しましょう。
　　・データをきちんと理解して意思決定などをしたほうがいいよと。
　・つぎのアーキテクチャは何か？
　　・コンピュータ・アーキテクチャの歴史
　　　ConnetionMachine CM-1（1985）
　　　SPARC
　　　Transputer（CSPによる並列性、Occam）
　　　SymbolicsLispMachine
　　　Intelアーキテクチャの台頭により、アーキテクチャの研究が廃れてくる
　　・クラサバ、スマホ・クラウドなどのアーキテクチャの話
　　・じゃあつぎは？
　　　Edge-Heay Data＝スマホなどデータが保存される場所がEdgeになりつつある
　　　ビッグデータのほとんどが廃棄されるデータ
　　・Edge-Heavy Dataに特化したアーキテクチャとは？
　　　分散マッチング・プロトコル→サマリ情報を交換することで、絞り込みが可能
　　　X=3とした場合、センサーとかなら、ピンポイントな値ではなく、範囲では。
　　　分布表現を1stクラスオブジェクトとするプログラミング言語が必要では？
　・アーキテクチャの変節点を見極めよう
　QA：
　　Q：スパースネス問題がランダムサンプリングやフィルタリングじゃ解けないんでは？
　　A：はい。ただ、その前にやることがあるはずですよねという注意喚起の意味での発表です。

　　価値に応じて、EdgeにあるデータをCenterに持ってくるという考え方が必要。
　　今は価値が見いだせないのなら、Centerにまで持ってこなくてもいいのでは。

◯「グローバル化する情報処理」
　講師：伊藤敬彦（株式会社プリファードインフラストラクチャー 研究開発部門　リサーチャー）
　・Sedueの説明
　　NHKニュースなどで
　・提供する機能
　　・検索補助
　　　レコメンド、サジェストなど
　・レコメンド機能の紹介
　・Sedue/Solrの比較
　　サポート体制：開発チームがサポートしてくれる
　　安定性：GCがないのがいい
　　付加機能：
　　検索の完全性：接尾辞配列による検索

　・多言語処理の話
　　・翻訳ではなく、任意の自然言語言語で動作・精度を向上させる処理の話。
　　・背景
　　　サービスのグローバル化、会社組織のグローバル化
　　・複数言語を扱う場合の難しさ
　　　多言語解析基盤Screwの開発。
　　　1.必要な処理を順番に適用する
　　　　処理の順序は設定で。出力はJSONで。
　　　　例：言語同定、単語分割、単語正規化
　　　　　→言語同定処理で
　　　2.言語ごとに必要な処理を適用
　・疑問
　　ScrewはSolrとの組み合わせもできる？
　　複数言語が混ざった文章の場合にどういう形で動作する？
　　言語判定は独自実装？
　
◯「BigData処理技術とサイバーセキュリティ」→題名変更されてた
　講師：桑名栄二氏（NTTセキュアプラットフォーム研究所　所長）
　・経歴
　　Jubatusプロジェクト立ち上げに参画
　・攻撃に関する話
　　原因のわかっていないケースが多い。
　・端末の初期設定のパスワードとかが狙われるケースも多い
　・変化する攻撃、変化するシステム・サービス、変化するデータ
　・マルウェアの分類にJubatus
　・不正IPアドレスを機械学習して
　・ABC
　　「あたりまえ」のことを「ばかみたいに」「ちゃんとやる」

◯「先進ビッグデータ応用を支える機械学習に求められる新技術」
　講師：比戸将平（株式会社プリファードインフラストラクチャー　研究開発部門　リサーチャー）
　・ビッグデータ分析はより深い地検を得られるビッグデータ「解析」へ
　　・ビッグデータ分析プロセス
　　　　Volume、Variety、Velocity
　　　　蓄積（NoSQL系）、分析（CEP）、両方やるのがHadoop
　　・分析から深い解析へ
　　　予測、カテゴリ分類、レコメンド、異常検知　
　　　これを機械学習で解決する方向で
　　・機械学習を応用している例
　　　クレジットカードの不正利用検知：FICO
　　　ネットワーク攻撃/侵入検出
　　　Jeopardy!でクイズ王に勝利
　　　医療診断支援

　・データ解析技術への過度な期待と現実とのギャップ
　　いろいろできるみたいだけど、何が必要？
　　・ビッグデータ処理系を使える人
　　・データサイエンティスト
　　・機械学習ツール

　・ビッグデータ処理系での機械学習への対応状況
　　Hadoop本体（YARN）
　　MapReduce系（Mahout、AllReduce or Vowpal Wabbit、SystemML）
　　非MapReduce系（Spark）
　　・機械学習からビッグデータへの歩み寄り
　　　ベンチマーク性能への固執とか、応用との乖離を批判する論文もあるらしい。
　　・機械学習の応用例
　　　Machine Lerning for the New York City Power Grid[Rudin et al., TPAMI, 2012]
　　　電力配電設備の障害予測・検知
　　　実データを用いた例が今後増えていくのでは。
　・今後重要になる技術とPFIの取り組み
　　・データ解析の敷居を下げるためのトレーサビリティ
　　　機械学習向けスクリプト言語は敷居が高い
　　　WekaやSPSSのようなアイコンベースのデータ処理プロセスの記述は前処理には強力だけど、機械学習とは相性が良くない
　　　結果が見える化部分との統合が不十分。
　　・Bazil Farm学習結果分析例
　　　Tweet年齢推定、Tweet性別推定

◯「“ビッグデータ”が話題になった理由」
　講師：中田敦氏（株式会社日経BP社　記者）
　・自己紹介
　・バズワードができるまで
　　まずは、「クラウド」のバズワードの歴史
　　「バズワードはIT企業やThe Economist誌の煽りでなく一般企業の経営陣が納得すると生まれる」
　・なぜ経営者がビッグデータに興味を？
　　「ザ・クオンツ」という書籍に金融業界のルールの変化が書かれてる。面白いよ。
　　Google/Amazonに対する警戒心から。
　　破壊的な新規産業者へ対抗して行かないといけない思うところからビッグデータが流行ってるのでは。
　　「買ってきたIT」は差別化要因にならないのでは？→自分で作ったITなら差別化できる。
　・競争力は自分で作るしか無い
　　日本のとある特殊事情
　　ITエンジニアの所属先が日米で割合がぜんぜん違う。米国はユーザ企業が75%、日本は25%くらい
　・ビッグデータの次はなに？
　　3次元プリンタがあれば、好きなモノが作れちゃう。＝消費地の近くで作成しちゃえば良くなるのでは。
```
