<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: logstash | @johtaniの日記 2nd]]></title>
  <link href="http://blog.johtani.info/blog/categories/logstash/atom.xml" rel="self"/>
  <link href="http://blog.johtani.info/"/>
  <updated>2015-06-10T18:03:47+09:00</updated>
  <id>http://blog.johtani.info/</id>
  <author>
    <name><![CDATA[johtani]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Logstashを使ったElasticsearchの再インデックス（日本語訳）]]></title>
    <link href="http://blog.johtani.info/blog/2015/05/26/reindex-elasticsearch-with-logstash-ja/"/>
    <updated>2015-05-26T16:08:10+09:00</updated>
    <id>http://blog.johtani.info/blog/2015/05/26/reindex-elasticsearch-with-logstash-ja</id>
    <content type="html"><![CDATA[<p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>

<p>原文：<a href="http://david.pilato.fr/blog/2015/05/20/reindex-elasticsearch-with-logstash/">Reindex Elasticsearch With Logstash</a></p>

<p>Thanks David!</p>

<!-- more -->


<p>マッピングを変更したり、インデックスの設定を変更したり、あるサーバから他のサーバや、
あるクラスタから他のクラスタ（例えば複数のデータセンターのような場合）にデータを再インデックスしたくなることがあるでしょう。</p>

<p>後者のような場合は<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html">SnapshotやRestoreの機能</a>を利用することもできますが、インデックスの設定を変更をしたい場合は
その他の方法が必要になります。</p>

<p><a href="https://www.elastic.co/blog/logstash-1-5-0-ga-released">Logstash 1.5.0</a>で、
<a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-elasticsearch.html">elasticsearch input</a>と<a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html">elasticsearch output</a>を使うことで、とても簡単に再インデックスができます。</p>

<p>ではやってみましょう。</p>

<h2>古いクラスタ</h2>

<p>elasticsearch 1.5.2 はすでにダウンロード済みとして、<code>localhost:9200</code>で<code>old</code>という名前のクラスタを起動します。</p>

<pre><code class="bash">bin/elasticsearch --cluster.name=old
</code></pre>

<p>クラスタに<code>person</code>という名前のインデックスが存在します。
これは、5シャードで、100万件のドキュメントを持っています。</p>

<p><img class="<a" src="href="http://david.pilato.fr/blog/images/reindex-es01/sense01.png">http://david.pilato.fr/blog/images/reindex-es01/sense01.png</a>&#8221;></p>

<h2>新しいクラスタ</h2>

<p>次に新しいクラスタを起動します。
<code>localhost:9201</code>で<code>new</code>という名前のクラスタを起動します。</p>

<pre><code class="bash">bin/elasticsearch --cluster.name=new
</code></pre>

<p>こちらは、空です。</p>

<pre><code class="bash">curl -XGET "http://localhost:9201/person"
</code></pre>

<pre><code class="json">{
   "error": "IndexMissingException[[person] missing]",
   "status": 404
}
</code></pre>

<h2>Logstashのインストール</h2>

<p>次に、Logstash 1.5.0をダウンロードして、インストールします。</p>

<pre><code class="bash">wget http://download.elastic.co/logstash/logstash/logstash-1.5.0.tar.gz
tar xzf logstash-1.5.0.tar.gz
cd logstash-1.5.0
</code></pre>

<p>logstashの設定ファイル<code>logstash.conf</code>を次のように設定します。</p>

<pre><code>input {
  # We read from the "old" cluster
  elasticsearch {
    hosts =&gt; [ "localhost" ]
    port =&gt; "9200"
    index =&gt; "person"
    size =&gt; 500
    scroll =&gt; "5m"
    docinfo =&gt; true
  }
}

output {
  # We write to the "new" cluster
  elasticsearch {
    host =&gt; "localhost"
    port =&gt; "9201"
    protocol =&gt; "http"
    index =&gt; "%{[@metadata][_index]}"
    index_type =&gt; "%{[@metadata][_type]}"
    document_id =&gt; "%{[@metadata][_id]}"
  }
  # We print dots to see it in action
  stdout {
    codec =&gt; "dots"
  }
}
</code></pre>

<h2>実行と修正</h2>

<p>実行します。</p>

<pre><code class="bash">bin/logstash -f logstash.conf
</code></pre>

<h3>ドキュメントのチェックと修正</h3>

<p>何が起きたでしょう？</p>

<pre><code class="bash">curl -XGET "http://localhost:9200/person/person/AU1wqyQWZJKU8OibfxgH"
</code></pre>

<pre><code class="json">{
   "_index": "person",
   "_type": "person",
   "_id": "AU1wqyQWZJKU8OibfxgH",
   "_version": 1,
   "found": true,
   "_source": {
      "name": "Tali Elyne",
      "dateOfBirth": "1955-05-03",
      "gender": "female",
      "children": 2,
      "marketing": {
         "cars": null,
         "shoes": null,
         "toys": null,
         "fashion": null,
         "music": null,
         "garden": null,
         "electronic": null,
         "hifi": null,
         "food": 846
      },
      "address": {
         "country": "Germany",
         "zipcode": "0099",
         "city": "Bonn",
         "countrycode": "DE",
         "location": [
            7.075943707068682,
            50.72883500730124
         ]
      }
   }
}
</code></pre>

<p>もう一方のクラスタと比較してみましょう。</p>

<pre><code class="bash">curl -XGET "http://localhost:9201/person/person/AU1wqyQWZJKU8OibfxgH"
</code></pre>

<pre><code class="json">{
   "_index": "person",
   "_type": "person",
   "_id": "AU1wqyQWZJKU8OibfxgH",
   "_version": 1,
   "found": true,
   "_source": {
      "name": "Tali Elyne",
      "dateOfBirth": "1955-05-03",
      "gender": "female",
      "children": 2,
      "marketing": {
         "cars": null,
         "shoes": null,
         "toys": null,
         "fashion": null,
         "music": null,
         "garden": null,
         "electronic": null,
         "hifi": null,
         "food": 846
      },
      "address": {
         "country": "Germany",
         "zipcode": "0099",
         "city": "Bonn",
         "countrycode": "DE",
         "location": [
            7.075943707068682,
            50.72883500730124
         ]
      },
      "@version": "1",
      "@timestamp": "2015-05-20T09:53:44.089Z"
   }
}
</code></pre>

<p>Logstashは<code>@version</code>と<code>@timestamp</code>フィールドを追加してしました。
これらを除去したいので、<a href="http://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html">Mutate filter plugin</a>の<a href="http://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-remove_field"><code>remove_field</code></a>を使います。</p>

<pre><code>filter {
  mutate {
    remove_field =&gt; [ "@timestamp", "@version" ]
  }
}
</code></pre>

<h3>マッピングのチェックと修正</h3>

<p>実際に、logstashは<code>_source</code>フィールドを既存のドキュメントから読み込み、
それらを新しいクラスタに直接投入しています。
しかし、logstashはマッピングについてはケアしていません。</p>

<p>古いマッピングと新しいマッピングを比較するために、マッピングを取得してみましょう。</p>

<pre><code class="bash">curl -XGET "http://localhost:9200/person/person/_mapping"
</code></pre>

<pre><code class="json">{
   "person": {
      "mappings": {
         "person": {
            "properties": {
               "address": {
                  "properties": {
                     "city": {
                        "type": "string",
                        "index": "not_analyzed"
                     },
                     "country": {
                        "type": "string",
                        "index": "not_analyzed"
                     },
                     "countrycode": {
                        "type": "string",
                        "index": "not_analyzed"
                     },
                     "location": {
                        "type": "geo_point"
                     },
                     "zipcode": {
                        "type": "string"
                     }
                  }
               },
               "children": {
                  "type": "long"
               },
               "dateOfBirth": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "gender": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "marketing": {
                  "properties": {
                     "cars": {
                        "type": "long"
                     },
                     "electronic": {
                        "type": "long"
                     },
                     "fashion": {
                        "type": "long"
                     },
                     "food": {
                        "type": "long"
                     },
                     "garden": {
                        "type": "long"
                     },
                     "hifi": {
                        "type": "long"
                     },
                     "music": {
                        "type": "long"
                     },
                     "shoes": {
                        "type": "long"
                     },
                     "toys": {
                        "type": "long"
                     }
                  }
               },
               "name": {
                  "type": "string"
               }
            }
         }
      }
   }
}
</code></pre>

<pre><code class="bash">curl -XGET "http://localhost:9201/person/person/_mapping"
</code></pre>

<pre><code class="json">{
   "person": {
      "mappings": {
         "person": {
            "properties": {
               "address": {
                  "properties": {
                     "city": {
                        "type": "string"
                     },
                     "country": {
                        "type": "string"
                     },
                     "countrycode": {
                        "type": "string"
                     },
                     "location": {
                        "type": "double"
                     },
                     "zipcode": {
                        "type": "string"
                     }
                  }
               },
               "children": {
                  "type": "long"
               },
               "dateOfBirth": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "gender": {
                  "type": "string"
               },
               "marketing": {
                  "properties": {
                     "cars": {
                        "type": "long"
                     },
                     "electronic": {
                        "type": "long"
                     },
                     "fashion": {
                        "type": "long"
                     },
                     "food": {
                        "type": "long"
                     },
                     "garden": {
                        "type": "long"
                     },
                     "hifi": {
                        "type": "long"
                     },
                     "music": {
                        "type": "long"
                     },
                     "shoes": {
                        "type": "long"
                     },
                     "toys": {
                        "type": "long"
                     }
                  }
               },
               "name": {
                  "type": "string"
               }
            }
         }
      }
   }
}
</code></pre>

<p>これにより、いくつかの相違を発見できます。</p>

<pre><code class="json"> "location": {
    "type": "geo_point"
 }
</code></pre>

<pre><code class="json"> "location": {
    "type": "double"
 }
</code></pre>

<p>データをインデックスする「前」に、実際に利用したいマッピングでインデックスを作成しておくことで、
この問題に対処できます。
この時点で、オリジナルのマッピングを望んだ形に変更することができます。例えば、アナライザを変更したりです。
また、インデックスの設定を新しく定義することもできます。
デフォルトでは、Elasticsearchは5つのシャードと各シャードに対して1つのレプリカを作成します。
しかし、この時点でもう一度変更することが可能です。</p>

<pre><code class="bash">curl -XDELETE "http://localhost:9201/person"
curl -XPUT "http://localhost:9201/person" -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}'
curl -XPUT "http://localhost:9201/person/person/_mapping" -d'
{
  "person": {
    "properties": {
      "address": {
        "properties": {
          "city": {
            "type": "string",
            "index": "not_analyzed"
          },
          "country": {
            "type": "string",
            "index": "not_analyzed"
          },
          "countrycode": {
            "type": "string",
            "index": "not_analyzed"
          },
          "location": {
            "type": "geo_point"
          },
          "zipcode": {
            "type": "string"
          }
        }
      },
      "children": {
        "type": "long"
      },
      "dateOfBirth": {
        "type": "date",
        "format": "dateOptionalTime"
      },
      "gender": {
        "type": "string",
        "index": "not_analyzed"
      },
      "marketing": {
        "properties": {
          "cars": {
            "type": "long"
          },
          "electronic": {
            "type": "long"
          },
          "fashion": {
            "type": "long"
          },
          "food": {
            "type": "long"
          },
          "garden": {
            "type": "long"
          },
          "hifi": {
            "type": "long"
          },
          "music": {
            "type": "long"
          },
          "shoes": {
            "type": "long"
          },
          "toys": {
            "type": "long"
          }
        }
      },
      "name": {
        "type": "string"
      }
    }
  }
}'
</code></pre>

<p>さて、もう一度再インデックスしましょう！</p>

<pre><code class="bash">bin/logstash -f logstash.conf
</code></pre>

<p><img class="<a" src="href="http://david.pilato.fr/blog/images/reindex-es01/sense02.png">http://david.pilato.fr/blog/images/reindex-es01/sense02.png</a>&#8221;></p>

<h2>インデックスやタイプ名の変更</h2>

<p>もちろん、インデックス名やタイプ名、IDを変更したい場合も変更が可能です！:)</p>

<pre><code class="json">  elasticsearch {
    host =&gt; "localhost"
    port =&gt; "9201"
    protocol =&gt; "http"
    index =&gt; "europe_people"
    index_type =&gt; "someone"
    document_id =&gt; "%{[@metadata][_id]}"
  }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logstashプラグインのエコシステムの変更（日本語訳）]]></title>
    <link href="http://blog.johtani.info/blog/2014/12/14/plugin-ecosystem-changes/"/>
    <updated>2014-12-14T01:00:40+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/12/14/plugin-ecosystem-changes</id>
    <content type="html"><![CDATA[<p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>

<p>原文：<a href="http://www.elasticsearch.org/blog/plugin-ecosystem-changes/">exciting logstash plugin ecosystem changes</a></p>

<p>Logstash 1.5.0 Beta 1(<a href="http://www.elasticsearch.org/overview/logstash/download/">お試しはこちら</a>)のリリースで、
プラグインのインストール、管理、公開の方法を変更しています。
ユーザやコミュニティからフィードバックをもらいました。
その目的は、プラグインの利用や開発をより簡単にすることです。
このプロジェクトは始まったばかりです。プラグインのコミュニティを探し、
共有するためのワンストップソリューションを提供するこのアイデアを改善していく予定です。
このブログで、この決定を行った理由を説明し、新しいワークフローをと今後のロードマップを説明します。</p>

<!-- more -->


<h2>プラグインがあります！</h2>

<p>Logstashは、プラグイン（input、filter、output、codec）が豊富にあります。
これらは、Elasticsearchにより開発されたものと、コミュニティからコントリビュートされたものです。
Logstashの主な特長の1つは、これらのプラグインの有効性と動作を拡張するプラグインを追加するのが簡単なことです。
現在、165以上のプラグインがエコシステムにあり、これらは、2つのプロジェクトに分かれています。</p>

<ul>
<li><code>logstash-core</code>は最もよく使われるプラグインで、Logstashにデフォルトで含まれます</li>
<li><code>logstash-contrib</code>はコミュニティにより開発されたプラグインを含み、別途ダウンロードできます</li>
</ul>


<h2>新プラグインエコシステムの変更</h2>

<p>1.5.0では、全てのプラグインは、Logstashコアから分離され、rubygemsを使って個別にパッケージングされます。
rubygemsを選択したのは、依存関係のあるライブラリの配布とパッケージングがパワフルで一般的なものだからです。
さらに、<a href="http://rubygems.org/">rubygems.org</a>プラットフォームは配布や探索に影響があります。
また、Logstashにプラグインをインストール、アップデート、削除するのが簡単な基盤も追加しました。
<code>contrib</code>プロジェクトは徐々に終了します。全てのプラグインは個別のプロジェクトになります。</p>

<h2>プラグインエコシステム変更の理由</h2>

<p>多数のプラグインをもっていると、配布と公開に関して難題が出てきます。
私たちが変更するに至った理由は次のようなものです。</p>

<ul>
<li>現在は、プラグインの更新に伴い、Logstashの新バージョンのリリースが必要</li>
<li>開発者は、Logstashのリリース間隔とは別に、新バージョンをリリースをしたい</li>
<li>プラグイン開発者は、外部依存を記述できるようにしたい</li>
<li>Logstashコアの配布パッケージのダウンロードサイズを小さくし、ユーザは必要なプラグインのみインストール</li>
<li><code>logstash-contrib</code>を1つのリポジトリとして管理するのは難しい</li>
</ul>


<h2>詳細：</h2>

<h3>ソースコードの場所</h3>

<p>Logstashのソースコードは、今後も<a href="https://github.com/elasticsearch/logstash">現在のGitHubのリポジトリ</a>のままです。
しかし、プラグインに関するコードやテストコードは含まなくなります。
この分離により、個別のプラグインの改善と同様にコアの改善に集中できます。
これにより、Logstashプロジェクトの全体の品質も向上します。</p>

<p>全プラグインのソースコードは、新しいGitHub organization、<a href="https://github.com/logstash-plugins">logstash-plugins</a>にて管理します。
各プラグインは個別のリポジトリとして、ここに配置されます。
一見すると、これはメンテナンスが難しくなるように思えます。しかし、テスト、Issue、依存関係を明確にすることができます。
私たちの目的は、テスト、ドキュメント、gemの公開の自動化であり、これを簡単にするためのツールを追加します。</p>

<p>しかし、プラグインの開発者はプラグインのソースコードソースコードをlogstash-pluginsに置く必要はありません。
 ー コミュニティで利用可能にするために、<a href="http://rubygems.org/">rubygems.org</a>でそれを公開するだけで良いです。</p>

<h2>ワークフロー</h2>

<p>ここで、新プラグインエコシステムのやりとり/ワークフローについて、いくつかの観点から説明します。</p>

<h3>logstashユーザ:</h3>

<p>ユーザは、これまでのリリース同様にLogstashのバイナリをダウンロードします。
Logstash 1.5.0は、1.4.2でパッケージされていたプラグインと同等のものが含まれています。
新しいシステムに簡単に移行できるようにです。
そして、ユーザは、最初のデプロイの後に、Logstashプラグインのをインストール、アップグレードできるようになります。</p>

<p><code>$LS_HOME/bin/plugin</code>スクリプトがプラグイン操作に関連するコマンドになります。</p>

<h5>プラグインのインストール</h5>

<p>プラグインのほとんどはgemとして<a href="http://rubygems.org/">rubygems.org</a>にアップロードされます。
例えば、もしユーザが<a href="https://github.com/logstash-plugins/logstash-output-kafka">Apache Kafka outputプラグイン</a>をインストールする場合、次のコマンドを実行します。</p>

<pre><code>bin/plugin install logstash-output-kafka
</code></pre>

<p>または、ファイルをダウンロード済みの場合は次のコマンドとなります。</p>

<pre><code>bin/plugin install /path/to/logstash-output-kafka-1.0.0.gem
</code></pre>

<h5>プラグインの削除</h5>

<pre><code>bin/plugin uninstall logstash-output-kafka
</code></pre>

<h5>1つまた全プラグインのアップデート</h5>

<pre><code>bin/plugin update
</code></pre>

<pre><code>bin/plugin update logstash-output-kafka
</code></pre>

<h5>プラグインのリストアップ</h5>

<pre><code>bin/plugin list
</code></pre>

<pre><code>bin/plugin list elasticsearch ( List all plugins containing a name )
</code></pre>

<pre><code>bin/plugin list --group output ( list all outputs )
</code></pre>

<h4>ドキュメント</h4>

<p>プラグインが個別に管理されても、<a href="http://www.elasticsearch.org/guide/en/logstash/current/index.html">全プラグインのドキュメントは1カ所</a>です。</p>

<h3>logstash plugin開発者:</h3>

<p>プラグイン開発者と作者は、Logstashエコシステムのためにプラグインを公開することができます。
プラグインは、gemやJavaライブラリの依存関係を宣言できます。
より重要なのは、Logstashのリリース間隔に関係なく、プラグインの改善版をリリースできます。</p>

<p>Rubygemsテクノロジはパッケージングシステム、依存関係管理、ホスティングのために選択されてきました。
Rubyのgemを公開することに慣れている開発者は、Logstashプラグインを簡単に公開することができます。
Elasticsearchはこれらの機能に関して開発者を支援するために、ツールを提供、メンテナンスします。</p>

<h4>開発およびローカルでのテスト</h4>

<p>JRuby <code>1.7.16</code>がプラグインを開発するための唯一の前提条件です。
プラグインにパッチを提供するのは以前と同様です。
例えば、<code>logstash-output-kafka</code>にパッチを送るのは次のようになります。</p>

<ol>
<li><code>git clone https://github.com/logstash-plugins/logstash-output-kafka.git</code></li>
<li>変更</li>
<li>プラグインをローカルでテスト

<ul>
<li><code>bundle install</code></li>
<li><code>bundle exec rspec</code></li>
<li>Logstashの他のバージョンもしくはローカルでテストする場合、Gemfileを編集し、    次のように別のロケーションを加えます。<code>gem "logstash", :github =&gt; "elasticsearch/logstash", :ref =&gt; "master"</code></li>
</ul>
</li>
<li>新しいPull Requestを<code>logstash-output-kafka</code>に対して作成</li>
<li>コミュニティでコードレビューを受け、Elasticsearchがパッチを受け入れ</li>
</ol>


<h4>バージョン</h4>

<p>バージョン情報は、それぞれのプラグインの<code>.gemspec</code>で管理します。
例えば、Apache Kafka outputのgemspecは<a href="https://github.com/logstash-plugins/logstash-output-kafka/blob/master/logstash-output-kafka.gemspec">こちら</a>です。
バージョニングは<a href="http://semver.org/">semantic versioning</a>のルールに従い、
Logstashのバージョニングとは別に、プラグインの開発者によって管理されます。
Logstash 1.5.0がリリースされると、マイルストーン1のプラグインはバージョン1.0.0となり、マイルストーン2のプラグインはバージョン2.0.0となるでしょう。</p>

<h4>公開</h4>

<p>開発者が変更を加えプラグインを公開したいと思った時、<code>.gemspec</code>のバージョン番号を変更します。
全テストが成功した時、Elasticsearchはrubygems.orgにプラグインを手動で公開します。
もし、テストが失敗した場合、プラグインは公開されません。
長期的には、プラグインの公開の自動化を行いたいと思っています。
この変更は新しいため、公開の自動化を提供する前に、自動化についてより理解し、プラグインのテスト基盤を改良したいと思っています。</p>

<h4>Issue</h4>

<p>Issueは、各プラグインのGitHubリポジトリに対してオープンなければなりません。
Logstashコアのリポジトリは、コアのパイプラインや共通的な機能に関連するIssueについて扱います。</p>

<h4>ドキュメント</h4>

<p>プラグインのドキュメントはソースコード自体から生成されます。
それぞれのプラグインのドキュメントは、そのプラグインのリポジトリに含まれます。
Elasticsearchは
<a href="http://www.elasticsearch.org/guide">elasticsearch.org/guide</a>に全てのプラグインのドキュメントを集め生成できる基盤を提供します。</p>

<h4>移行</h4>

<p>全ての新しいpull requestとissueは<a href="https://github.com/logstash-plugins">logstash-plugin</a> organisation配下にある各プラグインのリポジトリに対してオープンする必要があります。</p>

<h5>すでにあるPRはどうすれば良いですか？</h5>

<p>気にしないでください。すでにあるpull requestは開発者によって移行する必要はありません。
LogstashチームがLogstashコアリポジトリに対してのPRを、個別の関連するプラグインのリポジトリに対してマージします。</p>

<pre><code>git clone … # clone the specific plugin repo
# now apply the patch
curl -s https://github.com/elasticsearch/logstash/pull/XXXX | git am --3way
git push
</code></pre>

<p><strong>Note:このプロセスはすでにあるPRに対してgit historyを管理します</strong></p>

<h5>GitHub Issue</h5>

<p>現在、LogstashリポジトリにオープンされているIssueは、それぞれのプラグインのリポジトリに移行します。
Logstashチームがgithub.com APIを利用してこの処理を自動的に行います。
安心してください。私たちが個別のプラグインに対する既存のIssueを移行します。</p>

<h2>今後のロードマップ</h2>

<p>これは、最初のステップであり、これらの変更は、ユーザや開発者に対してエコシステムをよりよくするために、
しっかりとした基盤を提供します。</p>

<p>短期的には、開発者のためにpull requestのフィードバックでテスト自動化を提供する基盤を追加していきます。
プラグインリポジトリのブートストラップや管理のためのツールも提供していきます。</p>

<p>長期的には、すべてのLogstashプラグインを探し、公開するためのコミュニティポータルを提供したいと思っています。
このアイデアは、Puppet ForgeやAWS marketplaceのようなものです。</p>

<p><a href="http://www.elasticsearch.org/blog/logstash-1-5-0-beta1-released/">Logstash 1.5.0 Beta 1</a>をリリースし、これは新しいエコシステムを提供します。
ぜひ、試していただき、これらの変更に関して感じたことを教えてください。
あなたのフィードバック(<a href="http://twitter.com/elasticsearch">Twitter</a>もしくは<a href="https://github.com/elasticsearch/logstash/issues/new">GitHub</a>)はとても貴重です！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logstash 1.5.0 Beta1リリース(日本語訳)]]></title>
    <link href="http://blog.johtani.info/blog/2014/12/12/logstash-1-5-0-beta1-released-ja/"/>
    <updated>2014-12-12T17:17:26+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/12/12/logstash-1-5-0-beta1-released-ja</id>
    <content type="html"><![CDATA[<p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>

<p>原文：<a href="http://www.elasticsearch.org/blog/logstash-1-5-0-beta1-released/">logstash 1.5.0.beta1 released</a></p>

<p>Logstash 1.5.0 Beta1をリリースしました。<a href="http://www.elasticsearch.org/overview/logstash/download/">こちらのページ</a>からダウンロードできます。</p>

<p><strong>Note: ベータリリースです。本番環境では使用しないでください。</strong></p>

<!-- more -->


<h2>1.5.0の主な変更点は？</h2>

<p>1.5.0の主なテーマはプラグイン管理、パフォーマンス改善、<a href="http://kafka.apache.org/">Apache Kafka</a>インテグレーションです。Logstashの主な特徴の1つは
プラグインを利用できることであり、パイプラインの動作を拡張するためにプラグインを追加するのが簡単なことです。
このリリースで、プラグインの開発、管理、公開がより簡単になります。
また、Logstashの速度をより良くしたため、より多くのデータを短時間に処理することができます。
興味ありませんか？では、詳細を見ていきましょう。</p>

<h2>plugin ecosystemの変更</h2>

<p>Logstashは165ものプラグイン(inputs、filters、outputs、codecs)を持っており、
これらはElasticsearchとコミュニティからのコントリビュートで開発されています。
多くのプラグインを管理することは、使いやすさと素早さの間のトレードオフがあります。
Logstashの全てのプラグインをまとめることは使いやすさがある一方、プラグインの更新を取り込むために
Logstashの新しいリリースを待ってもらうことになります。
Logstashからプラグインを分離して個別に配布する場合、更新は簡単になりますが、使いやすさ（特に新しいユーザに）に影響が出ます。</p>

<p>私たちは、プロジェクトを前進させるために、これらのバランスをとることを考えました。
これまで、全ての利用可能なプラグインは’core’と&#8217;contrib&#8217;の2つに分割していました。
&lsquo;core&#8217;にあるよく使われるプラグインは、Logstashに含めていました。
コミュニティによりコントリビュートされたプラグインは&#8217;contrib&#8217;パッケージとして分離して配布していました。
1.5.0のリリースで、ユーザに対してより良いプラグイン管理をできるように変更しました。
全てのプラグインは、それ自身によるパッケージに移行しました。
パッケージングフレームワークとしてrubygemsを使い、<a href="http://rubygems.org/">rubygem.org</a>経由でこれらのプラグインを配布、公開します。
また、Logstashにプラグインのインストール、更新、削除を簡単にするための構造も追加しました。</p>

<p>例えば、S3 output pluginをインストールするには、以下のコマンドを実行します。</p>

<pre><code>$LS_HOME/bin/plugin install logstash-output-s3
</code></pre>

<p>それだけです！Logstashがgemと依存するgemをrubygems.orgからダウンロードし、インストールします。
あなたは、S3にデータを送ることができるようになります。</p>

<p>ダウンロード可能なLogstashリリースはプラグインをまだ多く含んでいますが、
いつでも、個別にプラグインをアップグレードし、インストールすることができます。
プラグインエコシステムの変更に関する詳細のブログ記事をお待ち下さい。</p>

<h2>パフォーマンス改善</h2>

<p>Logstash 1.5.0はより高速になっています。パフォーマンスが改善された2カ所について説明します。</p>

<h3>grok filter</h3>

<p>Grok filterはLogstashで、構造化データを抽出するためにパターンを記述するのに使われます。
本リリースで、人気のある幾つかのパターンのgrok filterのスループットを100%に改善しました。
言い換えると、grok filterを使うときに、Logstashを通してより多くのデータを処理することができます。</p>

<p>私たちのベンチマークテストで、1.5.0と1.4.2のスループットの比較をしました。
利用したデータは690万件のApache Webアクセスlogで、<code>COMBINEDAPACHELOG</code>のgrok patternです。
1.5.0で、スループットは34,000 event per sec(eps)から50,000 epsに増加しました。
両方のテストを8コアのマシンでLogstashで8つのワーカーを実行しました。
これらのテストで、一つのgrok filterを実行し、
<code>stdin</code>と<code>stdout</code>を使ったパイプラインでイベントのスループットを計測しました。
全体的なパフォーマンスは、様々なハードウェアやLogstashのコンフィグによって変化することに注意してください。</p>

<h3>json serialization / deserialization</h3>

<p>JSONのシリアライズ/でシリアライズを<a href="https://github.com/guyboertje/jrjackson">JrJackson</a>ライブラリを利用して実装しました。
これにより、100%以上のスループットの改善がありました。
先ほど説明したパフォーマンステストにおいて、1.3KBのサイズの500,00 JSONイベントを送信し、
16,000 epsから30,000 epsにスループットが改善しました。
45,000サイズのイベントで、850 epsから3500 epsにスループットが増加しました。
すばらしいです。</p>

<h2>apache kafka integration</h2>

<p>いまでは、Apache Kafkaが大規模スケールデータ処理システムでよく利用されます。
Logstashの配備のスケーリングにおいて、Kafkaもまた、shippingインスタンスとindexingインスタンス間の
データを保存するための中間メッセージバッファとして使うことができます。</p>

<p>1.5.0で、Logstash Kafkaのinputとoutputのプラグインのビルトインサポートを追加しました。
これは、<a href="https://github.com/joekiller/logstash-kafka">Joseph Lawson</a>によって最初に開発されました。
私たちは、これらのプラグインにインテグレーションテストとドキュメントを追加することにより改良し、
新しいKafkaの機能を開発し続けます。
また、<a href="http://avro.apache.org/">Apache Avro</a> codecを追加することで、Kafkaに保存されたイベントを
簡単に取得でき、ELKスタックを使ってそれらを解析できるようにしました。</p>

<p>Kafka inputを追加するのは次のコマンドです。</p>

<pre><code>$LS_HOME/bin/plugin install logstash-input-kafka
</code></pre>

<p>Kafka outputは次のコマンドです。</p>

<pre><code>$LS_HOME/bin/plugin install logstash-output-kafka
</code></pre>

<h2>セキュリティに関する改善</h2>

<p>認証と経路暗号化のサポートを追加し、Elasticsearchのoutput、input、filterのセキュリティを改良しました。
例えば、HTTPプロトコルでSSL/TLSにより暗号化を有効にでき、
HTTPベーシック認証をユーザ名とパスワードをリクエストに与えることで設定できます。
これらの機能は、時期にリリースされる<a href="http://www.elasticsearch.org/overview/shield/">Elasticsearch Shield</a>セキュリティプロダクトとLogstashを統合できます。</p>

<h2>ドキュメント</h2>

<p>これまで、Logstashのドキュメントは[logstash.net])(<a href="http://logstash.net/">http://logstash.net/</a>)に置いてあり、
他のELKスタックと一緒に動かす時に、情報を探すのが厄介でした。
1.5.0および、今後のバージョンのドキュメントはelasticsearch.orgの<a href="http://www.elasticsearch.org/guide/en/logstash/current/index.html">Logstash Guide</a>に移行します。
この移行で<a href="http://elasticsearch.org/guide">elasticsearch.org/guide</a>にELKスタックを利用、
学習するためにドキュメントが1つになりました。
このベータリリースのイテレーションで、私たちはプレゼンテーションとドキュメントの品質を改善することに活発に取り組んでいきます。
(過去のLogstashのドキュメントの全てはいままでの<a href="http://logstash.net/docs/1.4.2/">logstash.net</a>で引き続き公開していく予定です。)</p>

<h2>バグフィックスと改善</h2>

<p>ここまでの新しい機能に加えて、Logstash 1.5.0では、多くのバグフィックスと多くの機能改善があります。
ここで、これらのいくつかを紹介します。</p>

<ul>
<li>出力しない&#8217;metadata&#8217;をイベントに格納可能に。これは、例えば、date filterに使う中間フィールドのために必要。(<a href="https://github.com/elasticsearch/logstash/issues/1834">#1834</a>,<a href="https://logstash.jira.com/browse/LOGSTASH-1798"> #LOGSTASH-1798</a>)</li>
<li>HTTPを利用しているときのファイルデスクリプタリークの修正。Logstashがストールするのを防ぎ、OOMエラーからクラッシュするケースも防ぎます。(<a href="https://github.com/elasticsearch/logstash/issues/1604">#1604</a>)</li>
<li>Twitter input:<code>full_tweet</code>オプションの追加、Twitter rate limitingエラーのハンドリング(<a href="https://github.com/elasticsearch/logstash/issues/1471">#1471</a>)</li>
<li>イベントを生成するfilter(multiline、clone、split、metrics)により、
後続の条件文にこれらのイベントを正しく伝搬(<a href="https://github.com/elasticsearch/logstash/issues/1431">#1431</a>)</li>
<li>Elasticsearch output:Logstashはデフォルトで<code>message.raw</code>フィールドを作成しない。messageフィールドはElasticsearch
により<code>not_analyzed</code>でマルチフィールドとして追加される。マルチフィールドはディスクスペースが2倍必要だが、利点がない。</li>
<li>bin/logstashの複数のサブコマンドを除去(<a href="https://github.com/elasticsearch/logstash/issues/1797">#1797</a>)</li>
</ul>


<p>これらの機能、改善、バグフィックスについては、Logstash 1.5.0.Beta1 の<a href="https://github.com/elasticsearch/logstash/blob/master/CHANGELOG">changelog</a>をごらんください。</p>

<h2>試してみてください！</h2>

<p>ぜひ、Logstash 1.5.0 Beta 1をダウンロードして試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/logstash/issues">GitHub issues page</a>で報告をお願いします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[インデックステンプレートとLogstash]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/25/import-apache-accesslog-using-logstash-2/"/>
    <updated>2014-11-25T16:25:46+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/25/import-apache-accesslog-using-logstash-2</id>
    <content type="html"><![CDATA[<p>前回の「<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/">Logstashを利用したApacheアクセスログのインポート</a>」の続きです。
前回の記事では、Logstashの設定ファイルについて説明しました。
今回は「Elasticsearchに設定するインデックステンプレート」について説明します。</p>

<!-- more -->


<h2>テンプレートの設定</h2>

<p>Elasticsearchでは、登録するデータの特性に合わせてMappingを定義する方がデータを効率良く扱うことができる場合があります。
この場合、通常ですと、インデックス作成時にMappingを指定します。</p>

<p>ただ、今回は、<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash#output-elasticsearch">インデックス名に「年」を含める形</a>で指定してあります。
「年」はLogstashで処理したデータによって決まります。このため、あらかじめMappingを指定してインデックスを作成するのは難しいです。</p>

<p>このような場合に便利な機能として、「<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#indices-templates">インデックステンプレート</a>」があります。</p>

<h3>インデックステンプレートとは</h3>

<p>実際のテンプレートの説明に入る前に、少しだけ説明を。
インデックステンプレートとは、インデックスが作成されるタイミングで自動的に適用される設定をテンプレートとして登録できる機能のことです。
実際にテンプレートが適用されるかどうかは、インデックス名で判断されます。</p>

<p>例えば、大して重要でもなく、データ量も少ないインデックス用のテンプレートとして、シャード数が1、レプリカ数が0、&#8221;_source&#8221;を保存しない設定のテンプレートを登録する場合、
次のようになります。</p>

<pre><code class="yaml">curl -XPUT localhost:9200/_template/template_1 -d '
{
  "template" : "te*",
  "settings" : {
    "number_of_shards" : 1,
    "number_of_replicas" : 0
  },
  "mappings" : {
    "type1" : {
      "_source" : { "enabled" : false }
    }
  }
}
'
</code></pre>

<p><code>_template</code>がインデックステンプレートを登録するためのエンドポイントです。
<code>template_1</code>がこのテンプレートのIDです。削除などについては、このIDを利用します。</p>

<p>そして、重要なのは、&#8221;<code>template</code>&ldquo;の設定です。
&rdquo;<code>template</code>&ldquo;には、このテンプレートが適用されるべきインデックス名を記載します。
上記サンプルでは<code>te*</code>となっているため、<code>te</code>で始まる名前のインデックスを作成した場合にテンプレートにある設定が適用されます。</p>

<h3>今回利用するテンプレート</h3>

<p>私がJJUG CCCや第7回Elasticsearch勉強会のKibana4のデモで利用したインデックスのテンプレートは次のものになります。
&ldquo;<code>template</code>&#8220;には、<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/#output-elasticsearch">前回の記事で紹介したoutput/elasticsearchの設定</a> に合致する<code>new_demo_access_log-*</code>を指定しています。</p>

<pre><code class="yaml">curl -XPUT localhost:9200/_template/new_access_log_for_demo -d '
{
  "template": "new_demo_access_log-*",
  "settings": {
    "number_of_shards": "2",
    "number_of_replicas": "0"
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "string_template": {
            "mapping": {
              "index": "not_analyzed",
              "type": "string"
            },
            "match_mapping_type": "string",
            "match": "*"
          }
        }
      ],
      "properties": {
        "path": {
          "type": "multi_field",
          "fields": {
            "no_analyzed": {
              "index": "not_analyzed",
              "type": "string"
            },
            "analyzed": {
              "index": "analyzed",
              "type": "string"
            }
          }
        },
        "referer": {
          "type": "multi_field",
          "fields": {
            "no_analyzed": {
              "index": "not_analyzed",
              "type": "string"
            },
            "analyzed": {
              "index": "analyzed",
              "type": "string"
            }
          }
        },
        "agent": {
          "type": "multi_field",
          "fields": {
            "no_analyzed": {
              "index": "not_analyzed",
              "type": "string"
            },
            "analyzed": {
              "index": "analyzed",
              "type": "string"
            }
          }
        },
        "geoip": {
          "type": "object",
          "properties": {
            "location": {
              "geohash": true,
              "geohash_precision": 10,
              "type": "geo_point",
              "lat_lon": true,
              "geohash_prefix": true
            }
          }
        },
        "response": {
          "copy_to": "response_int",
          "type": "string"
        },
        "bytes": {
          "type": "long"
        },
        "response_int": {
          "type": "integer"
        }
      }
    }
  }
}
'
</code></pre>

<h4>settings設定</h4>

<p>デモ用であり、手元で2台のノードを起動するということもあり、<code>number_of_shards</code>に<code>2</code>を、<code>number_of_replicas</code>に<code>0</code>を指定してあります。</p>

<h4>mappings設定</h4>

<h5>インデックスのタイプ</h5>

<p>Mappingsの指定は通常、特定の<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type">タイプ</a>を指定します。
今回のデモでは、1種類しかないのですが、タイプ名を特に意識しないために、<code>_default_</code>を使用しました。
この場合、任意のタイプに適用されることとなります。
タイプを指定してMappingの設定を行う場合は<code>_default_</code>の部分に特定のタイプ名を記入します。</p>

<pre><code class="yaml">"mappings": {
  "_default_": {
    ...
</code></pre>

<h5>ダイナミックテンプレート</h5>

<p>次は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates">ダイナミックテンプレート</a>です。
インデックステンプレートはインデックスの設定をテンプレート化しました。ダイナミックテンプレートはフィールドに対してテンプレートを設定できます。</p>

<p>以下のダイナミックテンプレートでは、<code>string</code>タイプのフィールドのデフォルト設定を変更しています。
通常、<code>string</code>タイプのフィールドは<code>analyzed</code>となりますが、<code>not_analyzed</code>に変更してあります。
詳しく検索したいフィールドの方が少ないためです。</p>

<pre><code class="yaml">...
"dynamic_templates": [
  {
    "string_template": {
      "mapping": {
        "index": "not_analyzed",
        "type": "string"
      },
      "match_mapping_type": "string",
      "match": "*"
    }
  }
],
...  
</code></pre>

<h5>multi_field指定</h5>

<p>検索もしたいし、Terms Aggregationでも利用したいフィールドについては、<code>multi_field</code>を利用して、
<code>analyzed</code>と<code>not_analyzed</code>の2種類のフィールドを用意しています。
<code>multi_field</code>設定を用いることで、1つのJSONのデータから、異なる形のフィールドを用意することが可能です。</p>

<p>今回のテンプレートでは、<code>path</code>、<code>referer</code>、<code>agent</code>に<code>multi_field</code>を指定しました。</p>

<pre><code class="yaml">...
"path": {
  "type": "multi_field",
  "fields": {
    "no_analyzed": {
      "index": "not_analyzed",
      "type": "string"
    },
    "analyzed": {
      "index": "analyzed",
      "type": "string"
    }
  }
},
...
</code></pre>

<p>例えば、上記の設定の場合、入力のJSONは<code>path</code>というデータのみですが、インデックス上には<code>path.no_analyzed</code>と
<code>path.analyzed</code>というフィールドができあがります。
実際に検索する場合は、<code>path.analyzed:検索したい文字列</code>という形で検索をすることで、いわゆる部分一致のような検索が可能です。
また、完全一致をしたい場合は<code>path.no_analyzed:検索したい文字列</code>という指定になります。
用途を考えると、<code>request</code>も指定したほうが良いかもしれません。</p>

<h5>geoip</h5>

<p><a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash#filter-geoip">Logstashでgeoipデータ</a>を付与していました。
このgeoipのデータをKibana4で利用するために、geoデータとして登録する必要があります。</p>

<pre><code class="yaml">"geoip": {
  "type": "object",
  "properties": {
    "location": {
      "geohash": true,
      "geohash_precision": 10,
      "type": "geo_point",
      "lat_lon": true,
      "geohash_prefix": true
    }
  }
},
</code></pre>

<p>上記の設定がgeoデータの指定です。
<code>type</code>に<code>object</code>が指定してありますが、これは、geoipのデータがネストしているためです。
geoipオブジェクトのうち、緯度経度のデータは<code>location</code>に入っているため、こちらに緯度経度関係の設定を指定します。</p>

<ul>
<li><code>"type": "geo_point"</code>：<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html#mapping-geo-point-type"><code>geo_point</code></a>タイプであることを指定</li>
<li><code>"geohash": true</code>：緯度経度のデータをもとに、geohashの値もインデックス</li>
<li><code>"geohash_precision": 10</code>：geohashの精度の指定</li>
<li><code>"lat_lon": true</code>：緯度経度を個別の<code>.lat</code>、<code>.lon</code>というフィールドにもインデックス</li>
<li><code>"geohash_prefix": true</code>：該当するgeohashのみでなく、その親にあたるgeohashについてもインデックスする</li>
</ul>


<h5>response、response_int、bytes</h5>

<p>最後は、response、response_int、bytesです。</p>

<p>responseには、HTTPステータスコードが入ります。
文字列としても扱いたいですが、integerとして、Renge Aggregationなどを行いたいので、
response_intというフィールドにも値を入れています。
<code>multi_field</code>でも可能ですが、ここでは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#copy-to"><code>copy_to</code></a>を利用しました。
<code>copy_to</code>を用いることで、異なるフィールドに値をコピーすることができます。</p>

<p>bytesについては、longで扱いたいとういう理由だけです。</p>

<pre><code class="yaml">
"response": {
  "copy_to": "response_int",
  "type": "string"
},
"bytes": {
  "type": "long"
},
"response_int": {
  "type": "integer"
}
</code></pre>

<h2>まとめ</h2>

<p>今回はデモに利用したインデックスてプレートについて説明しました。
前回の、Logstashの設定とこのインデックステンプレートを用いることで、Kibanaで解析するデータの準備ができます。
実際の操作などについては、また次回の記事で説明しようかと思います。</p>

<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logstashを利用したApacheアクセスログのインポート]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/"/>
    <updated>2014-11-21T17:30:39+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash</id>
    <content type="html"><![CDATA[<p>JJUG CCCや第7回Elasticsearch勉強会のKibana4のデモにアクセスログを利用しました。</p>

<p>ただ、セッションでは、どうやってElasticsearchに投入したのかという詳しい話をしていませんでした。
本記事では、データ取り込み時に利用したLogstashの設定ファイルについて説明します。</p>

<!-- more -->


<p>Logstashの設定の説明に入る前に、全体の流れを。
「ApacheアクセスログをKibana4により可視化」です。</p>

<h2>材料の準備</h2>

<p>「ApacheアクセスログをKibana4により可視化」に必要な材料は次の通りです。
（今回は起動するところまでいかないので、実際に必要なのは次回以降になります。）</p>

<ul>
<li>Java 7（u55以上を1つ）</li>
<li>Logstash 1.4.2（1つ）</li>
<li>Elasticsearch 1.4.0（1つ）</li>
<li>Kibana4 Beta2（1つ）</li>
<li>Apacheのアクセスログ（適量）</li>
</ul>


<p>Apacheのアクセスログ以外は、公式サイトからダウンロードできます。
それぞれをダウンロードして、起動できるようにしておきましょう。</p>

<p>※1台のマシン上で行う場合は、アクセスログの量を少なめにするなどの対策をとりましょう。
※今回は、1台のマシン（Mac）上で、VMなどを利用せず、それぞれ直接起動するものとします。</p>

<h2>可視化の手順と流れ</h2>

<p>可視化の流れとしては、</p>

<ol>
<li>Logstashでファイルを読み込み、各種処理（パースしたり、情報を追加したり、切り出したり）</li>
<li>Elasticsearchに保存</li>
<li>Kibanaでグラフを作ったり、検索してみたり</li>
</ol>


<p>です。</p>

<p>今回は、1のLogstashでファイルを読み込んだりする設定ファイルの説明です。</p>

<h3>Logstashの設定</h3>

<h4>Logstashの基本</h4>

<p>まずは、Logstashの設定ですが、簡単にLogstashの説明を。
Logstashは大きく3つのパーツに分かれています。</p>

<ol>
<li>input：データの入力処理</li>
<li>filter：inputで読み込んだデータに対する操作など</li>
<li>output：データの出力処理</li>
</ol>


<p>inputでデータを読み込み（複数可）、filterでデータに対して各種処理を行い、outputでデータを指定されたところに出力（複数可）します。</p>

<h4>アクセスログの読み込み設定</h4>

<p>アクセスログの読み込み処理は大まかに次のようなものとなります。</p>

<ol>
<li>アクセスログを読み込む（input/file）</li>
<li>読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</li>
<li>日付のパース（filter/date）</li>
<li>クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</li>
<li>リクエストのパスの第1階層の抽出（filter/grok）</li>
<li>ユーザエージェントのパース（filter/useragent）</li>
<li>Elasticsearchへの出力（output/elasticsearch）</li>
</ol>


<p>設定ファイルは次のようなものになります。</p>

<pre><code class="ruby">input {
  file {
    path =&gt; "/Users/johtani/demo_access_log/*/*.log"
    start_position =&gt; "beginning"
  }
}

filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    break_on_match =&gt; false
    tag_on_failure =&gt; ["_message_parse_failure"]
  }
  date {
    match =&gt; ["timestamp", "dd/MMM/YYYY:HH:mm:ss Z"]
    locale =&gt; en
  }
  geoip {
    source =&gt; ["clientip"]
  }
  grok {
    match =&gt; { "request" =&gt; "^/%{WORD:first_path}/%{GREEDYDATA}$" }
    tag_on_failure =&gt; ["_request_parse_failure"]
  }
  useragent {
    source =&gt; "agent"
    target =&gt; "useragent"
  }
}

output {
  elasticsearch {
    host =&gt; "localhost"
    index =&gt; "new_demo_access_log-%{year}"
    cluster =&gt; "demo_cluster"
    protocol =&gt; "http"
  }
}
</code></pre>

<h5>1. アクセスログを読み込む（input/file）</h5>

<p>inputの<a href="http://logstash.net/docs/1.4.2/inputs/file">fileモジュール(a)</a>を使用してアクセスログのファイルを読み込みます。
<code>path</code>でアクセスログのファイルのパスを指定します。
今回利用したアクセスログは<code>demo_access_log/2010/access20100201.log</code>といった日毎のファイルに分割されていたため、
<code>*</code>を利用してファイルのパスを指定しました。
また、今回は既存のファイルの読み込みだけのため、<code>start_position</code>に<code>beginning</code>を指定してあります。
デフォルトでは<code>end</code>が指定されるため、Logstashを起動後に追記されたログから対象になってしまうためです。
その他の設定については、公式ガイドをご覧ください。</p>

<pre><code class="ruby">input {
  file { # a
    path =&gt; "/Users/johtani/demo_access_log/*/*.log" # b
    start_position =&gt; "beginning" # c
  }
}
</code></pre>

<blockquote><p>Logstashでは、ファイルをどこまで読み込んだかという情報を保持するために、<a href="http://logstash.net/docs/1.4.2/inputs/file#sincedb_path">sincedb</a>を利用しています。
設定変更後に同じファイルを最初から読み込みたい場合などは、こちらのファイルを一旦削除するなどの対応が必要です。</p></blockquote>

<p>ちなみに、読み込んだデータは次のようなJSONになっています。</p>

<pre><code class="json">{
  "message": "読み込んだアクセスログ",
  "@version": "1",
  "@timestamp":"2014-11-21T06:16:21.644Z",
  "host":"jupiter.local",
  "path":"/Users/johtani/demo_access_log/2010/access20100201.log"}
}
</code></pre>

<p>特に指定がない場合は、<code>message</code>に読み込んだデータが入ってきます。
<code>@timestamp</code>がLogstashが読み込んだ時刻、<code>host</code>はLogstashが動作しているホスト名です。
<code>path</code>はfileモジュールが読み込んだファイルのパスを設定しています。
この後の処理で、どこの項目に対して処理を行うかといったことが重要になるので、</p>

<h5>2. 読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</h5>

<p>2.〜6.の処理は、inputで読み込んだ1アクセスログに対する処理となります。</p>

<p>ここでは、<a href="http://logstash.net/docs/1.4.2/filters/grok">grokフィルタ</a>を使用して
Apacheのアクセスログを各フィールドに分割します。
Logastashでは、簡単に使えるようにいくつかの<a href="https://github.com/elasticsearch/logstash/tree/v1.4.2/patterns">パターン</a>が用意されています。
Apacheのログのために、<a href="https://github.com/elasticsearch/logstash/blob/v1.4.2/patterns/grok-patterns#L91"><code>COMBINEDAPACHELOG</code></a>というのが用意されています。
今回はこちらを使用しています。その他にも日付などパターンが用意されているので、試してみてください。</p>

<p><code>message</code>にアクセスログが入っているので、こちらの項目に対して<code>COMBINEDAPACHELOG</code>のパターンを
<code>match</code>で適用してフィールドに抜き出します。
<code>tag_on_failure</code>は、<code>match</code>でパースに失敗した場合に、<code>tag</code>というフィールドに指定した文字列を出力する機能になります。
デフォルトだと<code>_grokparsefailure</code>が付与されますが、ここでは、どの処理で失敗したがを判別するために文字列を変更しています。</p>

<pre><code class="ruby">filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    break_on_match =&gt; false
    tag_on_failure =&gt; ["_message_parse_failure"]
  }
  ...
</code></pre>

<p><code>clientip</code>、<code>ident</code>、<code>auth</code>、<code>timestamp</code>、<code>verb</code>、<code>request</code>、<code>httpversion</code>、<code>response</code>、<code>bytes</code>、<code>referrer</code>、<code>agent</code>がgrokフィルタにより抜き出された項目です。</p>

<pre><code class="json">{
  "message":"アクセスログ",
  "@version":"1",
  "@timestamp":"2014-11-21T07:20:54.387Z",
  "host":"jupiter.local",
  "path":"/Users/johtani/demo_access_log/2010/access20100201.log",
  "clientip":"クライアントのIPアドレス",
  "ident":"-",
  "auth":"-",
  "timestamp":"01/Feb/2010:00:00:26 +0900",
  "verb":"GET",
  "request":"/images/favicon.ico",
  "httpversion":"1.1",
  "response":"200",
  "bytes":"318",
  "referrer":"\"-\"",
  "agent":"\"Mozilla/5.0 (Windows; U; Windows NT 5.1; ja; rv:1.9.1.7) Gecko/20091221 Firefox/3.5.7 (.NET CLR 3.5.30729)\""
}
</code></pre>

<h5>3. 日付のパース（filter/date）</h5>

<p>Logstashは特に指定がない場合、inputでデータを取り出した日付が<code>@timestamp</code>となります。
そして、このフィールドが特に指定がない場合は、Elasticsearchのデータの日付となり、Kibanaで利用する日付となります。</p>

<p>リアルタイムにアクセスログを読み込む場合は、読み込んだ日時でもほぼ問題はありませんが、過去データの場合はそうもいきません。
そこで、<a href="http://logstash.net/docs/1.4.2/filters/date"><code>dateフィルタ</code></a>を使用して、<code>@timestamp</code>の値を書き換えます。</p>

<pre><code class="ruby">date {
  match =&gt; ["timestamp", "dd/MMM/YYYY:HH:mm:ss Z"]
  locale =&gt; en
}
</code></pre>

<p>上記では、<code>timestamp</code>という項目に対して<code>dd/MMM/YYYY:HH:mm:ss Z</code>という日付パターンの場合に値を書き換える設定となります。
なお、日付の月の部分が<code>Feb</code>となっているため、<code>locale</code>に<code>en</code>を指定しています。Logstashが動作するマシンの<code>locale</code>が<code>ja</code>などの場合にパースに失敗するためです。</p>

<h5><a name="filter-geoip">4. クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</a></h5>

<p>どの国からのアクセスかなどを判別したいので、IPアドレスを元にgeoipを利用してより詳細な情報を付与します。
Logstashでもこの機能が用意されており、簡単に利用ができます。</p>

<pre><code class="ruby">geoip {
  source =&gt; ["clientip"]
}
</code></pre>

<p>これだけです。対象とするIPアドレスのフィールドを指定しているだけです。
<code>geoip</code>というフィールドが追加され、次のような情報が付与されます。
国名、緯度経度、タイムゾーンなどです。</p>

<pre><code class="json">{
  ...  
  "geoip": {
    "ip": "IPアドレス",
    "country_code2": "JP",
    "country_code3": "JPN",
    "country_name": "Japan",
    "continent_code": "AS",
    "latitude": 36,
    "longitude": 138,
    "timezone": "Asia/Tokyo",
    "location": [
      138,
      36
    ]
  }
  ...
}
</code></pre>

<h5>5. リクエストのパスの第1階層の抽出（filter/grok）</h5>

<p>リクエストされたURLは<code>request</code>フィールドにありますが、個別のURLだと、大まかな集計が大変です。
もちろん、クエリで処理することもできますが、Logstashで処理するついでに、第1階層のディレクトリ名を抽出しておくことで、
検索や集計を行いやすくしておきます。</p>

<pre><code class="ruby">grok {
  match =&gt; { "request" =&gt; "^/%{WORD:first_path}/%{GREEDYDATA}$" }
  tag_on_failure =&gt; ["_request_parse_failure"]
}
</code></pre>

<p>また、grokフィルタの登場です。
今回は、<code>WORD:first_path</code>という記述方法で、<code>WORD</code>パターンにマッチした文字列を<code>first_path</code>というフィールドに展開する指定をしています。</p>

<p>例えば、サイトのスクリプトなどが<code>scripts</code>というディレクトリにある場合は、<code>first_path</code>の値を利用して、
後続のフィルタでログデータを出力しないといった処理にも使えます。</p>

<h5>6. ユーザエージェントのパース（filter/useragent）</h5>

<p>Logstashではユーザエージェントの文字列から、いくつかの情報を付与するフィルタも用意されています。
<a href="http://logstash.net/docs/1.4.2/filters/useragent"><code>useragent</code>フィルタです。</a></p>

<pre><code>useragent {
  source =&gt; "agent"
  target =&gt; "useragent"
}
</code></pre>

<p><code>agent</code>というフィールドにユーザエージェントの文字列があるので、このフィールドに対してフィルタを適用します。
元の文字列も取っておきたいので、<code>useragent</code>という別のフィールドに出力するように指定してあります。</p>

<pre><code class="json">"useragent": {
  "name": "Firefox",
  "os": "Windows XP",
  "os_name": "Windows XP",
  "device": "Other",
  "major": "17",
  "minor": "0"
},
</code></pre>

<p>このように、OS名やバージョン名などが抽出できます。</p>

<h5><a name="output-elasticsearch">7. Elasticsearchへの出力（output/elasticsearch）</a></h5>

<p>最後は、<a href="http://logstash.net/docs/1.4.2/outputs/elasticsearch">Elasticsearchへのデータの出力設定</a>です。</p>

<p><code>index</code>にて、出力するindex名を指定してあります。
また、年毎のインデックス名にするために<code>%{year}</code>を利用しています。
<a href="http://logstash.net/docs/1.4.2/configuration#sprintf">sprintf format</a>です。</p>

<pre><code class="ruby">elasticsearch {
  host =&gt; "localhost"
  index =&gt; "new_demo_access_log-%{year}"
  cluster =&gt; "demo_cluster"
  protocol =&gt; "http"
}
</code></pre>

<h2>まとめ</h2>

<p>ということで、今回はアクセスログをLogstashにて読み込む時の設定について説明してきました。
次回は、実際にLogstashを起動してElasticsearchにデータを登録するところまでを説明します。</p>

<p>JJUG CCCや勉強会のデモに用いたデータは、
Elasticsearchにデータを登録する前にテンプレートも設定してありました。こちらについても、次回説明しようと思います。</p>

<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
]]></content>
  </entry>
  
</feed>
