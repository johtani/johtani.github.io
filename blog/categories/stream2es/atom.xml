<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: stream2es | @johtaniの日記 2nd]]></title>
  <link href="http://blog.johtani.info/blog/categories/stream2es/atom.xml" rel="self"/>
  <link href="http://blog.johtani.info/"/>
  <updated>2018-03-24T15:23:01+09:00</updated>
  <id>http://blog.johtani.info/</id>
  <author>
    <name><![CDATA[johtani]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[stream2esと複数データの登録]]></title>
    <link href="http://blog.johtani.info/blog/2014/04/24/usage-stream2es/"/>
    <updated>2014-04-24T21:11:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/04/24/usage-stream2es</id>
    <content type="html"><![CDATA[<p>kopfの記事の続きも書く必要があるんだけど、こんなツイートを見つけてしまったので。。。</p>

<blockquote class="twitter-tweet" lang="ja"><p>ElasticsearchのBulk APIの仕様、JSONファイルをいい感じに加工して置かなければならないしハマりどころ多い。 <a href="http://t.co/hmfycqZlqk">http://t.co/hmfycqZlqk</a></p>&mdash; Kenta Suzuki (@suzu_v) <a href="https://twitter.com/suzu_v/statuses/459216999592124416">2014, 4月 24</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>前に思いついたけど、放ったらかしにしてた疑問が再浮上してきたので、せっかくだから調べてみようかなと。</p>

<!-- more -->


<p>複数JSONデータがある場合にもっと楽にデータを入れる方法ないかなぁと思って、これかな？というのがあったのですが、
そのまま手を動かさずに放置してたので、一念発起してブログ書いてます。</p>

<h2>Bulk APIって？</h2>

<p>ElasticsearchはURLにアクセスしてデータを登録できます。
基本的には次のように1件毎の登録になります。</p>

<pre><code class="json">$ curl -XPUT http://localhost:9200/bookshop/books/1 -d
'
{
  "book_id" : 1,
  "title" : "ElasticSearch Server Japanese Edition",
  "price" : 3024,
  "publisher" : "KADOKAWA"
}'
</code></pre>

<p>これでもいいのですが、大量のデータを登録するときは、Elasticsearch側での効率が悪いです。
そこで、Elasticsearchは大量データを登録するために<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html#docs-bulk">Bulk API</a>というものを用意しています。</p>

<p>これは、次のような形式のJSONを作ってデータを登録します。</p>

<pre><code class="json">{ "index" : { "_index" : "bookshop", "_type" : "books", "_id" : "1" } }
{ "book_id" : 1, "title" : "ElasticSearch Server Japanese Edition", "price" : 3024, "publisher" : "KADOKAWA"}
{ "index" : { "_index" : "bookshop", "_type" : "books", "_id" : "2" } }
{ "book_id" : 2, "title" : "Introduction of Apache Solr", "price" : 3888, "publisher" : "gihyo"}
</code></pre>

<p>これは、次のような構成になっています。</p>

<pre><code>コマンド
データ
コマンド
データ
...
</code></pre>

<p>これで効率よくデータが登録できるのですが、このようなJSONデータを別途作って上げる必要が出てきます。
結局、複数のJSONがあるのに、特殊なJSONを生成しないといけないということでプログラム書いて実行することになります。
これだと、Elasticsearchへのアクセスをプログラムで書くのとあまり大差がないかもしれません。</p>

<h2>stream2es</h2>

<p>もっとお手軽に複数のJSONを登録できないかな？と目をつけていたのが、<a href="https://github.com/elasticsearch/stream2es">stream2es</a>です。</p>

<h3>どんなもの？</h3>

<p>Clojureで作られた、Elasticsearchにデータを流し込むためのツールです。
Java 7がインストールされていれば、ダウンロードしてくれば動作せることができます。</p>

<h3>インストール</h3>

<p>公式ページに載っている方法そのままです。</p>

<pre><code class="bash">$ curl -O download.elasticsearch.org/stream2es/stream2es; chmod +x stream2es
</code></pre>

<p>実行したディレクトリにコマンドがコピーされます。
あとは、コマンドを実行すればOKです。</p>

<h3>実行</h3>

<p>データは次のような形式で<code>sample.json</code>に保存してあるとします。</p>

<pre><code class="json">{ "book_id" : 1, "title" : "ElasticSearch Server Japanese Edition", "price" : 3024, "publisher" : "KADOKAWA"}
{ "book_id" : 2, "title" : "Introduction of Apache Solr", "price" : 3888, "publisher" : "gihyo"}
</code></pre>

<p>先ほどの<code>Bulk API</code>で利用したJSONよりも、スッキリしていますね。
1行1ドキュメント1JSONです。</p>

<p>あとは、次のコマンドを実行するだけです。</p>

<pre><code class="bash">$ ./stream2es stdin --target http://localhost:9200/bookshop/books &lt; sample.json
</code></pre>

<p>ファイルをstream2esに流し込んで、stream2esが1行ずつパースして、Elasticsearchに投げ込んでくれます。</p>

<p>登録されたデータは次のようになります。
IDは自動で付与されています。</p>

<pre><code class="json">{
   "_index": "bookstore",
   "_type": "books",
   "_id": "0Hvy4IJCRkKrvGb4Dgam_w",
   "_version": 1,
   "found": true,
   "_source": {
      "book_id": 1,
      "title": "ElasticSearch Server Japanese Edition",
      "price": 3024,
      "publisher": "KADOKAWA"
   }
},
{
   "_index": "bookstore",
   "_type": "books",
   "_id": "b9M6TooFQzGYyJeix_t_WA",
   "_version": 1,
   "found": true,
   "_source": {
      "book_id": 2,
      "title": "Introduction of Apache Solr",
      "price": 3888,
      "publisher": "gihyo"
   }
}
</code></pre>

<p>せっかく、<code>book_id</code>があるんだし、<code>_id</code>をインデックスの設定に指定します。</p>

<pre><code class="bash">$ curl -XDELETE http://localhost:9200/bookshop
$ curl -XPUT http://localhost:9200/bookshop -d '
{
  "mappings": {
    "books" : {
      "_id" : {
        "path": "book_id"
      }
    }
  }
}'
</code></pre>

<p>あとは、登録すれば<code>book_id</code>が<code>_id</code>に採用されます。</p>

<pre><code class="json">{
   "_index": "bookshop",
   "_type": "books",
   "_id": "1",
   "_version": 1,
   "found": true,
   "_source": {
      "book_id": 1,
      "title": "ElasticSearch Server Japanese Edition",
      "price": 3024,
      "publisher": "KADOKAWA"
   }
},
{
   "_index": "bookshop",
   "_type": "books",
   "_id": "2",
   "_version": 1,
   "found": true,
   "_source": {
      "book_id": 2,
      "title": "Introduction of Apache Solr",
      "price": 3888,
      "publisher": "gihyo"
   }
}
</code></pre>

<h3>複数ファイル</h3>

<p>ディレクトリに複数のJSONファイルが有った場合は、次のようなコマンドでOK</p>

<pre><code class="bash">$ cat sample_data/*.json |./stream2es stdin --target http://localhost:9200/bookshop/books
</code></pre>

<p>まぁ、<code>cat</code>して流してるだけですが。。。</p>

<h3>ダメだったケース</h3>

<ul>
<li><p>JSONが複数行になっているようなデータだとエラーが出てしまいました。<br/>
（<code>jq</code>コマンドで1行に整形したりできるかなぁ？）</p></li>
<li><p>また、1行に2つのJSONが書いてある場合は、1つ目のJSONをパースしたら、そこでおしまいみたいで、その後に記述されたデータは登録されませんでした。</p></li>
</ul>


<h3>インデックスがない場合</h3>

<p>stream2esで登録するインデックスがElasticsearchに存在しない場合、stream2esがインデックスを作成してくれるのですが、
この時、シャード数などはstream2es内部に記述があるので注意が必要です。
以下がその設定です。</p>

<ul>
<li>index.number_of_shards : 2</li>
<li>index.number_of_replicas : 0</li>
<li>index.refresh_interval : 5s</li>
</ul>


<h2>課題？</h2>

<p>内部的にはおそらく、<code>Bulk</code>でデータを登録していると思うのですが、まだよくわかっていません。
Clojureが読めないので、せっかくだから、Clojureの勉強も兼ねてちょっとソースを読んでみようかなと思います。
それほど量があるわけでもないので。</p>

<p>あとは、その他にWikipediaのデータやTwitterのデータ登録、
ElasticsearchからデータをScrollで読み出しつつ、別のElasticsearchに流しこむといったこともできそうなので、そちらも試してみようかと。
他にもオプションがいくつかありそうです。</p>

<p>今回は2件ほどでしたが、大量データを流し込んだ時にどうなるか（stream2esが悲鳴を上げるのか、Elasticsearchで詰まることがあったらどうなるか）なども
気になるので、なんか適当なデータで試してみるのもいいかなぁと。
（ということで、だれか、いろいろ試してみてもらえると楽できるなぁ。）</p>
]]></content>
  </entry>
  
</feed>
