<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kibana | @johtaniの日記 2nd]]></title>
  <link href="http://blog.johtani.info/blog/categories/kibana/atom.xml" rel="self"/>
  <link href="http://blog.johtani.info/"/>
  <updated>2015-04-28T17:40:31+09:00</updated>
  <id>http://blog.johtani.info/</id>
  <author>
    <name><![CDATA[johtani]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[インデックステンプレートとLogstash]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/25/import-apache-accesslog-using-logstash-2/"/>
    <updated>2014-11-25T16:25:46+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/25/import-apache-accesslog-using-logstash-2</id>
    <content type="html"><![CDATA[<p>前回の「<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/">Logstashを利用したApacheアクセスログのインポート</a>」の続きです。
前回の記事では、Logstashの設定ファイルについて説明しました。
今回は「Elasticsearchに設定するインデックステンプレート」について説明します。</p>

<!-- more -->


<h2>テンプレートの設定</h2>

<p>Elasticsearchでは、登録するデータの特性に合わせてMappingを定義する方がデータを効率良く扱うことができる場合があります。
この場合、通常ですと、インデックス作成時にMappingを指定します。</p>

<p>ただ、今回は、<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash#output-elasticsearch">インデックス名に「年」を含める形</a>で指定してあります。
「年」はLogstashで処理したデータによって決まります。このため、あらかじめMappingを指定してインデックスを作成するのは難しいです。</p>

<p>このような場合に便利な機能として、「<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#indices-templates">インデックステンプレート</a>」があります。</p>

<h3>インデックステンプレートとは</h3>

<p>実際のテンプレートの説明に入る前に、少しだけ説明を。
インデックステンプレートとは、インデックスが作成されるタイミングで自動的に適用される設定をテンプレートとして登録できる機能のことです。
実際にテンプレートが適用されるかどうかは、インデックス名で判断されます。</p>

<p>例えば、大して重要でもなく、データ量も少ないインデックス用のテンプレートとして、シャード数が1、レプリカ数が0、&#8221;_source&#8221;を保存しない設定のテンプレートを登録する場合、
次のようになります。</p>

<pre><code class="yaml">curl -XPUT localhost:9200/_template/template_1 -d '
{
  "template" : "te*",
  "settings" : {
    "number_of_shards" : 1,
    "number_of_replicas" : 0
  },
  "mappings" : {
    "type1" : {
      "_source" : { "enabled" : false }
    }
  }
}
'
</code></pre>

<p><code>_template</code>がインデックステンプレートを登録するためのエンドポイントです。
<code>template_1</code>がこのテンプレートのIDです。削除などについては、このIDを利用します。</p>

<p>そして、重要なのは、&#8221;<code>template</code>&ldquo;の設定です。
&rdquo;<code>template</code>&ldquo;には、このテンプレートが適用されるべきインデックス名を記載します。
上記サンプルでは<code>te*</code>となっているため、<code>te</code>で始まる名前のインデックスを作成した場合にテンプレートにある設定が適用されます。</p>

<h3>今回利用するテンプレート</h3>

<p>私がJJUG CCCや第7回Elasticsearch勉強会のKibana4のデモで利用したインデックスのテンプレートは次のものになります。
&ldquo;<code>template</code>&#8220;には、<a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/#output-elasticsearch">前回の記事で紹介したoutput/elasticsearchの設定</a> に合致する<code>new_demo_access_log-*</code>を指定しています。</p>

<pre><code class="yaml">curl -XPUT localhost:9200/_template/new_access_log_for_demo -d '
{
  "template": "new_demo_access_log-*",
  "settings": {
    "number_of_shards": "2",
    "number_of_replicas": "0"
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "string_template": {
            "mapping": {
              "index": "not_analyzed",
              "type": "string"
            },
            "match_mapping_type": "string",
            "match": "*"
          }
        }
      ],
      "properties": {
        "path": {
          "type": "multi_field",
          "fields": {
            "no_analyzed": {
              "index": "not_analyzed",
              "type": "string"
            },
            "analyzed": {
              "index": "analyzed",
              "type": "string"
            }
          }
        },
        "referer": {
          "type": "multi_field",
          "fields": {
            "no_analyzed": {
              "index": "not_analyzed",
              "type": "string"
            },
            "analyzed": {
              "index": "analyzed",
              "type": "string"
            }
          }
        },
        "agent": {
          "type": "multi_field",
          "fields": {
            "no_analyzed": {
              "index": "not_analyzed",
              "type": "string"
            },
            "analyzed": {
              "index": "analyzed",
              "type": "string"
            }
          }
        },
        "geoip": {
          "type": "object",
          "properties": {
            "location": {
              "geohash": true,
              "geohash_precision": 10,
              "type": "geo_point",
              "lat_lon": true,
              "geohash_prefix": true
            }
          }
        },
        "response": {
          "copy_to": "response_int",
          "type": "string"
        },
        "bytes": {
          "type": "long"
        },
        "response_int": {
          "type": "integer"
        }
      }
    }
  }
}
'
</code></pre>

<h4>settings設定</h4>

<p>デモ用であり、手元で2台のノードを起動するということもあり、<code>number_of_shards</code>に<code>2</code>を、<code>number_of_replicas</code>に<code>0</code>を指定してあります。</p>

<h4>mappings設定</h4>

<h5>インデックスのタイプ</h5>

<p>Mappingsの指定は通常、特定の<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type">タイプ</a>を指定します。
今回のデモでは、1種類しかないのですが、タイプ名を特に意識しないために、<code>_default_</code>を使用しました。
この場合、任意のタイプに適用されることとなります。
タイプを指定してMappingの設定を行う場合は<code>_default_</code>の部分に特定のタイプ名を記入します。</p>

<pre><code class="yaml">"mappings": {
  "_default_": {
    ...
</code></pre>

<h5>ダイナミックテンプレート</h5>

<p>次は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates">ダイナミックテンプレート</a>です。
インデックステンプレートはインデックスの設定をテンプレート化しました。ダイナミックテンプレートはフィールドに対してテンプレートを設定できます。</p>

<p>以下のダイナミックテンプレートでは、<code>string</code>タイプのフィールドのデフォルト設定を変更しています。
通常、<code>string</code>タイプのフィールドは<code>analyzed</code>となりますが、<code>not_analyzed</code>に変更してあります。
詳しく検索したいフィールドの方が少ないためです。</p>

<pre><code class="yaml">...
"dynamic_templates": [
  {
    "string_template": {
      "mapping": {
        "index": "not_analyzed",
        "type": "string"
      },
      "match_mapping_type": "string",
      "match": "*"
    }
  }
],
...  
</code></pre>

<h5>multi_field指定</h5>

<p>検索もしたいし、Terms Aggregationでも利用したいフィールドについては、<code>multi_field</code>を利用して、
<code>analyzed</code>と<code>not_analyzed</code>の2種類のフィールドを用意しています。
<code>multi_field</code>設定を用いることで、1つのJSONのデータから、異なる形のフィールドを用意することが可能です。</p>

<p>今回のテンプレートでは、<code>path</code>、<code>referer</code>、<code>agent</code>に<code>multi_field</code>を指定しました。</p>

<pre><code class="yaml">...
"path": {
  "type": "multi_field",
  "fields": {
    "no_analyzed": {
      "index": "not_analyzed",
      "type": "string"
    },
    "analyzed": {
      "index": "analyzed",
      "type": "string"
    }
  }
},
...
</code></pre>

<p>例えば、上記の設定の場合、入力のJSONは<code>path</code>というデータのみですが、インデックス上には<code>path.no_analyzed</code>と
<code>path.analyzed</code>というフィールドができあがります。
実際に検索する場合は、<code>path.analyzed:検索したい文字列</code>という形で検索をすることで、いわゆる部分一致のような検索が可能です。
また、完全一致をしたい場合は<code>path.no_analyzed:検索したい文字列</code>という指定になります。
用途を考えると、<code>request</code>も指定したほうが良いかもしれません。</p>

<h5>geoip</h5>

<p><a href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash#filter-geoip">Logstashでgeoipデータ</a>を付与していました。
このgeoipのデータをKibana4で利用するために、geoデータとして登録する必要があります。</p>

<pre><code class="yaml">"geoip": {
  "type": "object",
  "properties": {
    "location": {
      "geohash": true,
      "geohash_precision": 10,
      "type": "geo_point",
      "lat_lon": true,
      "geohash_prefix": true
    }
  }
},
</code></pre>

<p>上記の設定がgeoデータの指定です。
<code>type</code>に<code>object</code>が指定してありますが、これは、geoipのデータがネストしているためです。
geoipオブジェクトのうち、緯度経度のデータは<code>location</code>に入っているため、こちらに緯度経度関係の設定を指定します。</p>

<ul>
<li><code>"type": "geo_point"</code>：<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html#mapping-geo-point-type"><code>geo_point</code></a>タイプであることを指定</li>
<li><code>"geohash": true</code>：緯度経度のデータをもとに、geohashの値もインデックス</li>
<li><code>"geohash_precision": 10</code>：geohashの精度の指定</li>
<li><code>"lat_lon": true</code>：緯度経度を個別の<code>.lat</code>、<code>.lon</code>というフィールドにもインデックス</li>
<li><code>"geohash_prefix": true</code>：該当するgeohashのみでなく、その親にあたるgeohashについてもインデックスする</li>
</ul>


<h5>response、response_int、bytes</h5>

<p>最後は、response、response_int、bytesです。</p>

<p>responseには、HTTPステータスコードが入ります。
文字列としても扱いたいですが、integerとして、Renge Aggregationなどを行いたいので、
response_intというフィールドにも値を入れています。
<code>multi_field</code>でも可能ですが、ここでは、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#copy-to"><code>copy_to</code></a>を利用しました。
<code>copy_to</code>を用いることで、異なるフィールドに値をコピーすることができます。</p>

<p>bytesについては、longで扱いたいとういう理由だけです。</p>

<pre><code class="yaml">
"response": {
  "copy_to": "response_int",
  "type": "string"
},
"bytes": {
  "type": "long"
},
"response_int": {
  "type": "integer"
}
</code></pre>

<h2>まとめ</h2>

<p>今回はデモに利用したインデックスてプレートについて説明しました。
前回の、Logstashの設定とこのインデックステンプレートを用いることで、Kibanaで解析するデータの準備ができます。
実際の操作などについては、また次回の記事で説明しようかと思います。</p>

<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logstashを利用したApacheアクセスログのインポート]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash/"/>
    <updated>2014-11-21T17:30:39+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/21/import-apache-accesslog-using-logstash</id>
    <content type="html"><![CDATA[<p>JJUG CCCや第7回Elasticsearch勉強会のKibana4のデモにアクセスログを利用しました。</p>

<p>ただ、セッションでは、どうやってElasticsearchに投入したのかという詳しい話をしていませんでした。
本記事では、データ取り込み時に利用したLogstashの設定ファイルについて説明します。</p>

<!-- more -->


<p>Logstashの設定の説明に入る前に、全体の流れを。
「ApacheアクセスログをKibana4により可視化」です。</p>

<h2>材料の準備</h2>

<p>「ApacheアクセスログをKibana4により可視化」に必要な材料は次の通りです。
（今回は起動するところまでいかないので、実際に必要なのは次回以降になります。）</p>

<ul>
<li>Java 7（u55以上を1つ）</li>
<li>Logstash 1.4.2（1つ）</li>
<li>Elasticsearch 1.4.0（1つ）</li>
<li>Kibana4 Beta2（1つ）</li>
<li>Apacheのアクセスログ（適量）</li>
</ul>


<p>Apacheのアクセスログ以外は、公式サイトからダウンロードできます。
それぞれをダウンロードして、起動できるようにしておきましょう。</p>

<p>※1台のマシン上で行う場合は、アクセスログの量を少なめにするなどの対策をとりましょう。
※今回は、1台のマシン（Mac）上で、VMなどを利用せず、それぞれ直接起動するものとします。</p>

<h2>可視化の手順と流れ</h2>

<p>可視化の流れとしては、</p>

<ol>
<li>Logstashでファイルを読み込み、各種処理（パースしたり、情報を追加したり、切り出したり）</li>
<li>Elasticsearchに保存</li>
<li>Kibanaでグラフを作ったり、検索してみたり</li>
</ol>


<p>です。</p>

<p>今回は、1のLogstashでファイルを読み込んだりする設定ファイルの説明です。</p>

<h3>Logstashの設定</h3>

<h4>Logstashの基本</h4>

<p>まずは、Logstashの設定ですが、簡単にLogstashの説明を。
Logstashは大きく3つのパーツに分かれています。</p>

<ol>
<li>input：データの入力処理</li>
<li>filter：inputで読み込んだデータに対する操作など</li>
<li>output：データの出力処理</li>
</ol>


<p>inputでデータを読み込み（複数可）、filterでデータに対して各種処理を行い、outputでデータを指定されたところに出力（複数可）します。</p>

<h4>アクセスログの読み込み設定</h4>

<p>アクセスログの読み込み処理は大まかに次のようなものとなります。</p>

<ol>
<li>アクセスログを読み込む（input/file）</li>
<li>読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</li>
<li>日付のパース（filter/date）</li>
<li>クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</li>
<li>リクエストのパスの第1階層の抽出（filter/grok）</li>
<li>ユーザエージェントのパース（filter/useragent）</li>
<li>Elasticsearchへの出力（output/elasticsearch）</li>
</ol>


<p>設定ファイルは次のようなものになります。</p>

<pre><code class="ruby">input {
  file {
    path =&gt; "/Users/johtani/demo_access_log/*/*.log"
    start_position =&gt; "beginning"
  }
}

filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    break_on_match =&gt; false
    tag_on_failure =&gt; ["_message_parse_failure"]
  }
  date {
    match =&gt; ["timestamp", "dd/MMM/YYYY:HH:mm:ss Z"]
    locale =&gt; en
  }
  geoip {
    source =&gt; ["clientip"]
  }
  grok {
    match =&gt; { "request" =&gt; "^/%{WORD:first_path}/%{GREEDYDATA}$" }
    tag_on_failure =&gt; ["_request_parse_failure"]
  }
  useragent {
    source =&gt; "agent"
    target =&gt; "useragent"
  }
}

output {
  elasticsearch {
    host =&gt; "localhost"
    index =&gt; "new_demo_access_log-%{year}"
    cluster =&gt; "demo_cluster"
    protocol =&gt; "http"
  }
}
</code></pre>

<h5>1. アクセスログを読み込む（input/file）</h5>

<p>inputの<a href="http://logstash.net/docs/1.4.2/inputs/file">fileモジュール(a)</a>を使用してアクセスログのファイルを読み込みます。
<code>path</code>でアクセスログのファイルのパスを指定します。
今回利用したアクセスログは<code>demo_access_log/2010/access20100201.log</code>といった日毎のファイルに分割されていたため、
<code>*</code>を利用してファイルのパスを指定しました。
また、今回は既存のファイルの読み込みだけのため、<code>start_position</code>に<code>beginning</code>を指定してあります。
デフォルトでは<code>end</code>が指定されるため、Logstashを起動後に追記されたログから対象になってしまうためです。
その他の設定については、公式ガイドをご覧ください。</p>

<pre><code class="ruby">input {
  file { # a
    path =&gt; "/Users/johtani/demo_access_log/*/*.log" # b
    start_position =&gt; "beginning" # c
  }
}
</code></pre>

<blockquote><p>Logstashでは、ファイルをどこまで読み込んだかという情報を保持するために、<a href="http://logstash.net/docs/1.4.2/inputs/file#sincedb_path">sincedb</a>を利用しています。
設定変更後に同じファイルを最初から読み込みたい場合などは、こちらのファイルを一旦削除するなどの対応が必要です。</p></blockquote>

<p>ちなみに、読み込んだデータは次のようなJSONになっています。</p>

<pre><code class="json">{
  "message": "読み込んだアクセスログ",
  "@version": "1",
  "@timestamp":"2014-11-21T06:16:21.644Z",
  "host":"jupiter.local",
  "path":"/Users/johtani/demo_access_log/2010/access20100201.log"}
}
</code></pre>

<p>特に指定がない場合は、<code>message</code>に読み込んだデータが入ってきます。
<code>@timestamp</code>がLogstashが読み込んだ時刻、<code>host</code>はLogstashが動作しているホスト名です。
<code>path</code>はfileモジュールが読み込んだファイルのパスを設定しています。
この後の処理で、どこの項目に対して処理を行うかといったことが重要になるので、</p>

<h5>2. 読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</h5>

<p>2.〜6.の処理は、inputで読み込んだ1アクセスログに対する処理となります。</p>

<p>ここでは、<a href="http://logstash.net/docs/1.4.2/filters/grok">grokフィルタ</a>を使用して
Apacheのアクセスログを各フィールドに分割します。
Logastashでは、簡単に使えるようにいくつかの<a href="https://github.com/elasticsearch/logstash/tree/v1.4.2/patterns">パターン</a>が用意されています。
Apacheのログのために、<a href="https://github.com/elasticsearch/logstash/blob/v1.4.2/patterns/grok-patterns#L91"><code>COMBINEDAPACHELOG</code></a>というのが用意されています。
今回はこちらを使用しています。その他にも日付などパターンが用意されているので、試してみてください。</p>

<p><code>message</code>にアクセスログが入っているので、こちらの項目に対して<code>COMBINEDAPACHELOG</code>のパターンを
<code>match</code>で適用してフィールドに抜き出します。
<code>tag_on_failure</code>は、<code>match</code>でパースに失敗した場合に、<code>tag</code>というフィールドに指定した文字列を出力する機能になります。
デフォルトだと<code>_grokparsefailure</code>が付与されますが、ここでは、どの処理で失敗したがを判別するために文字列を変更しています。</p>

<pre><code class="ruby">filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    break_on_match =&gt; false
    tag_on_failure =&gt; ["_message_parse_failure"]
  }
  ...
</code></pre>

<p><code>clientip</code>、<code>ident</code>、<code>auth</code>、<code>timestamp</code>、<code>verb</code>、<code>request</code>、<code>httpversion</code>、<code>response</code>、<code>bytes</code>、<code>referrer</code>、<code>agent</code>がgrokフィルタにより抜き出された項目です。</p>

<pre><code class="json">{
  "message":"アクセスログ",
  "@version":"1",
  "@timestamp":"2014-11-21T07:20:54.387Z",
  "host":"jupiter.local",
  "path":"/Users/johtani/demo_access_log/2010/access20100201.log",
  "clientip":"クライアントのIPアドレス",
  "ident":"-",
  "auth":"-",
  "timestamp":"01/Feb/2010:00:00:26 +0900",
  "verb":"GET",
  "request":"/images/favicon.ico",
  "httpversion":"1.1",
  "response":"200",
  "bytes":"318",
  "referrer":"\"-\"",
  "agent":"\"Mozilla/5.0 (Windows; U; Windows NT 5.1; ja; rv:1.9.1.7) Gecko/20091221 Firefox/3.5.7 (.NET CLR 3.5.30729)\""
}
</code></pre>

<h5>3. 日付のパース（filter/date）</h5>

<p>Logstashは特に指定がない場合、inputでデータを取り出した日付が<code>@timestamp</code>となります。
そして、このフィールドが特に指定がない場合は、Elasticsearchのデータの日付となり、Kibanaで利用する日付となります。</p>

<p>リアルタイムにアクセスログを読み込む場合は、読み込んだ日時でもほぼ問題はありませんが、過去データの場合はそうもいきません。
そこで、<a href="http://logstash.net/docs/1.4.2/filters/date"><code>dateフィルタ</code></a>を使用して、<code>@timestamp</code>の値を書き換えます。</p>

<pre><code class="ruby">date {
  match =&gt; ["timestamp", "dd/MMM/YYYY:HH:mm:ss Z"]
  locale =&gt; en
}
</code></pre>

<p>上記では、<code>timestamp</code>という項目に対して<code>dd/MMM/YYYY:HH:mm:ss Z</code>という日付パターンの場合に値を書き換える設定となります。
なお、日付の月の部分が<code>Feb</code>となっているため、<code>locale</code>に<code>en</code>を指定しています。Logstashが動作するマシンの<code>locale</code>が<code>ja</code>などの場合にパースに失敗するためです。</p>

<h5><a name="filter-geoip">4. クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</a></h5>

<p>どの国からのアクセスかなどを判別したいので、IPアドレスを元にgeoipを利用してより詳細な情報を付与します。
Logstashでもこの機能が用意されており、簡単に利用ができます。</p>

<pre><code class="ruby">geoip {
  source =&gt; ["clientip"]
}
</code></pre>

<p>これだけです。対象とするIPアドレスのフィールドを指定しているだけです。
<code>geoip</code>というフィールドが追加され、次のような情報が付与されます。
国名、緯度経度、タイムゾーンなどです。</p>

<pre><code class="json">{
  ...  
  "geoip": {
    "ip": "IPアドレス",
    "country_code2": "JP",
    "country_code3": "JPN",
    "country_name": "Japan",
    "continent_code": "AS",
    "latitude": 36,
    "longitude": 138,
    "timezone": "Asia/Tokyo",
    "location": [
      138,
      36
    ]
  }
  ...
}
</code></pre>

<h5>5. リクエストのパスの第1階層の抽出（filter/grok）</h5>

<p>リクエストされたURLは<code>request</code>フィールドにありますが、個別のURLだと、大まかな集計が大変です。
もちろん、クエリで処理することもできますが、Logstashで処理するついでに、第1階層のディレクトリ名を抽出しておくことで、
検索や集計を行いやすくしておきます。</p>

<pre><code class="ruby">grok {
  match =&gt; { "request" =&gt; "^/%{WORD:first_path}/%{GREEDYDATA}$" }
  tag_on_failure =&gt; ["_request_parse_failure"]
}
</code></pre>

<p>また、grokフィルタの登場です。
今回は、<code>WORD:first_path</code>という記述方法で、<code>WORD</code>パターンにマッチした文字列を<code>first_path</code>というフィールドに展開する指定をしています。</p>

<p>例えば、サイトのスクリプトなどが<code>scripts</code>というディレクトリにある場合は、<code>first_path</code>の値を利用して、
後続のフィルタでログデータを出力しないといった処理にも使えます。</p>

<h5>6. ユーザエージェントのパース（filter/useragent）</h5>

<p>Logstashではユーザエージェントの文字列から、いくつかの情報を付与するフィルタも用意されています。
<a href="http://logstash.net/docs/1.4.2/filters/useragent"><code>useragent</code>フィルタです。</a></p>

<pre><code>useragent {
  source =&gt; "agent"
  target =&gt; "useragent"
}
</code></pre>

<p><code>agent</code>というフィールドにユーザエージェントの文字列があるので、このフィールドに対してフィルタを適用します。
元の文字列も取っておきたいので、<code>useragent</code>という別のフィールドに出力するように指定してあります。</p>

<pre><code class="json">"useragent": {
  "name": "Firefox",
  "os": "Windows XP",
  "os_name": "Windows XP",
  "device": "Other",
  "major": "17",
  "minor": "0"
},
</code></pre>

<p>このように、OS名やバージョン名などが抽出できます。</p>

<h5><a name="output-elasticsearch">7. Elasticsearchへの出力（output/elasticsearch）</a></h5>

<p>最後は、<a href="http://logstash.net/docs/1.4.2/outputs/elasticsearch">Elasticsearchへのデータの出力設定</a>です。</p>

<p><code>index</code>にて、出力するindex名を指定してあります。
また、年毎のインデックス名にするために<code>%{year}</code>を利用しています。
<a href="http://logstash.net/docs/1.4.2/configuration#sprintf">sprintf format</a>です。</p>

<pre><code class="ruby">elasticsearch {
  host =&gt; "localhost"
  index =&gt; "new_demo_access_log-%{year}"
  cluster =&gt; "demo_cluster"
  protocol =&gt; "http"
}
</code></pre>

<h2>まとめ</h2>

<p>ということで、今回はアクセスログをLogstashにて読み込む時の設定について説明してきました。
次回は、実際にLogstashを起動してElasticsearchにデータを登録するところまでを説明します。</p>

<p>JJUG CCCや勉強会のデモに用いたデータは、
Elasticsearchにデータを登録する前にテンプレートも設定してありました。こちらについても、次回説明しようと思います。</p>

<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[サーバ/インフラエンジニア養成読本 ログ収集~可視化編 を手伝いました]]></title>
    <link href="http://blog.johtani.info/blog/2014/08/04/release-magazine-book-of-log-aggs-and-viz/"/>
    <updated>2014-08-04T21:54:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/08/04/release-magazine-book-of-log-aggs-and-viz</id>
    <content type="html"><![CDATA[<p>懲りずにまた、執筆してみました。みなさん「買って」から感想をいただけるとうれしいです！</p>

<iframe src="http://rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&nou=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=johtani-22&o=9&p=8&l=as1&m=amazon&f=ifr&ref=tf_til&asins=4774169838" style="width:120px;height:240px;" scrolling="no" marginwidth="0" marginheight="0" frameborder="0"></iframe>




<!-- more -->


<h2>本書について</h2>

<p>共著者の方々のブログが詳しいので、そちらを読んでもらいつつ。
実際にログを収集して解析されている方々と一緒に書かせていただくことで色々と勉強させていただいています。</p>

<h3>共著者の方々のブログ</h3>

<ul>
<li><a href="https://twitter.com/suzu_v">@suzu_v</a>さん：<a href="http://suzuken.hatenablog.jp/entry/2014/07/18/084555">サーバ/インフラエンジニア養成読本 ログ収集~可視化編 を書きました</a></li>
<li><a href="https://twitter.com/yoshi_ken">@yoshi_ken</a>さん：<a href="http://y-ken.hatenablog.com/entry/published-elasticsearch-fluentd-kibana-book">ログ収集や可視化で話題のFluentd、Elasticsearch、Kibanaを徹底解説したムック本が発売となります</a></li>
<li><a href="https://twitter.com/harukasan">@harukasan</a>さん：<a href="http://blog.harukasan.jp/entry/2014/07/18/180351">書きました: サーバ/インフラエンジニア養成読本 ログ収集~可視化編</a></li>
</ul>


<h3>どの辺を書いたの？</h3>

<p>「特集３：Elasticsearch入門」（なんか、入門ばっかりだなぁ）を書かせていただきました。
データストア入門ということで、ほんとうに簡単な他のデータストアを説明し、Elasticsearchってどんなものかを単語の説明をしつつ紹介してみました。</p>

<p>Elasticsearch自体は多くの機能を持っており、それ単体で分厚い書籍がかけるので、ログ検索に関係ありそうな部分をピックアップしてみました。
あとは、運用時に気をつける点や便利なツール（Curatorなど）の紹介をしています。</p>

<p>また、Hadoopと合わせて利用してみたい、すでにHadoopにあるデータも活用してみたいという話もありそうだということで、<a href="https://github.com/elasticsearch/elasticsearch-hadoop">elasticsearch-hadoop</a>についても簡単ですが紹介してあります。</p>

<h2>その他感想</h2>

<p>個人的に、忙しい時期<a href="http://blog.johtani.info/blog/2014/07/01/join-elasticsearch/">（参考記事）</a>だったので、あんまり力になれてないので大変申し訳なく思っています。。。
ただ、素晴らしい出来（カラーでKibanaの解説が日本語で読めたり、Fluentdの逆引きのリストがあったり、ログを貯めて可視化する意義を説明してあったり）です。</p>

<p>ぜひ、読んだ感想をいただければと！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[第2回elasticsearch勉強会を開催しました！ #elasticsearchjp]]></title>
    <link href="http://blog.johtani.info/blog/2013/11/12/elasticsearch-japan-user-meetup-no2/"/>
    <updated>2013-11-12T18:16:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/11/12/elasticsearch-japan-user-meetup-no2</id>
    <content type="html"><![CDATA[<p>第2回を開催しました！
すごい、140人くらいくらいの参加登録者（参加者は１００人ちょっと！）がいて、びっくりです。
ステキな会場を提供していただいた、<a href="http://recruit-tech.co.jp/recruitment/">リクルートテクノロジーズ</a>さん、運営していただいた方々、スピーカーの皆さん、参加者の皆さん本当にありがとうございました。
今回も素敵な看板ありがとうございます。</p>

<p><img src="/images/entries/20131112/es_signboard.jpg" width="300" title="ステキな案内板" ></p>

<p>今回もしっかり楽しめたので、次回も頑張ります！</p>

<p>今回は、<a href="https://groups.google.com/forum/#!forum/elasticsearch-jp">elasticsearch-jp</a>MLの紹介とかをできたのでよかったかなぁと。
ぜひ、活用してください！どんな質問でもいいので。</p>

<p>あと、スライドに入ってた例の本もよろしくです。</p>

<!-- more -->


<p>ということで、懇親会も盛り上がったし楽しかったです。
今後も場の提供＋自分の勉強のトリガーとして、開催していくので、ご協力お願いします！
聞きたい話など、MLや@ツイートしていただければと。</p>

<h2>elasticsearchのRouting機能：株式会社シーマーク　大谷　純　（@johtani）</h2>

<p>スライド：<a href="/images/entries/20131112/About_es_routing.pdf">Routing機能</a>※スライドはPDFです。</p>

<p>ド緊張で、大した発表ではなかったですが。。。
どちらかと言うとSolr本の紹介だったかもなぁ。スミマセン。</p>

<p>※スライドが一部文字が消えてるので、作りなおすかも。</p>

<h2>ElasticSearchを使ったBaaS基盤の開発(仮)：株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん（@pisatoshi）</h2>

<p>スライド：<a href="https://speakerdeck.com/pisatoshi/elasticsearch-trial-and-error">https://speakerdeck.com/pisatoshi/elasticsearch-trial-and-error</a></p>

<p>本日はお越しいただきありがとうございました！しかも静岡から！今後もよろしくお願い致します。</p>

<ul>
<li>EnchantMoonでシステム構成ｗ</li>
<li>0.17.0から利用されていると。（スゴイ）</li>
<li>プライマリのデータストア！ただし、登録元データはMySQLにもある。</li>
<li>階層も深く、大きめのドキュメント。</li>
<li>レプリカ１、インデックスのバックアップも取ってないと。。。</li>
<li><p>ルーティングの機能</p></li>
<li><p>DynamicMappingの問題点</p></li>
<li>マッピング定義が肥大、型がコンフリクト。。。苦労しっぱなし</li>
<li>データ登録は１台にして、１台で一気に登録してから再配置</li>
<li>実際に運用とかされてるので、いろんなノウハウがまだまだありそう！</li>
</ul>


<h2>Kibana入門：水戸祐介さん（@y_310）</h2>

<p>スライド：<a href="https://speakerdeck.com/y310/kibanaru-men">https://speakerdeck.com/y310/kibanaru-men</a></p>

<p>（やっぱりru-menになってるｗ）</p>

<p>実は、押しかけて話してもらうように説得したのでした。今後もよろしくです。</p>

<ul>
<li>COOKPADの方によるKibanaのお話。</li>
<li>Kibanaの利点とかなんで？とか。</li>
<li>画面構成の説明から</li>
<li>ダッシュボードは必ず保存して！リロードしたら悲しい思いをしてしまうので。</li>
<li>sparkline便利そうだなぁ。ほんとに、データサイエンティスト系のツールを目指してるのかな</li>
<li>一通り、ダッシュボードに配置できるパネルの説明してもらえたのですごく参考になりました！</li>
<li>Tips周りが役に立ちそう。not_analyzedは重要ですよね。</li>
</ul>


<h2>LT</h2>

<h3>「データ集計用ダッシュボードブラウザとしても使えるElasticSearch＋Kibana v3を利用する際の運用ノウハウ紹介」：株式会社リブセンス Y.Kentaro さん (@yoshi_ken) さん</h3>

<p>スライド：<a href="http://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips">http://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips</a></p>

<ul>
<li>Kibanaの紹介とかFluentdの紹介。</li>
<li>Tips満載すばらしい。</li>
<li>JDBC riverは0.90.6ではうまく動かないので、気をつけてと。</li>
</ul>


<h3>「Fluentd as a Kibana」：@repeatedly さん</h3>

<p>スライド(gist)？：<a href="https://gist.github.com/repeatedly/7427856">https://gist.github.com/repeatedly/7427856</a></p>

<p>Kibanaがfluentdの中で動くと！？</p>

<h3>「Authプラグインでアクセスコントロール」：株式会社エヌツーエスエム 菅谷信介さん (@shinsuke_sugaya)</h3>

<p>スライド：<a href="http://www.slideshare.net/shinsuke/es-auth-plugin">http://www.slideshare.net/shinsuke/es-auth-plugin</a></p>

<p>API毎？インデックスごと？にアクセス制御ができるプラグイン</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kibana3というのもありまして]]></title>
    <link href="http://blog.johtani.info/blog/2013/06/19/introduction-kibana3/"/>
    <updated>2013-06-19T23:43:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/06/19/introduction-kibana3</id>
    <content type="html"><![CDATA[<p>前回は3番煎じぐらいでしたが、今回は初記事かな？（だといいな）</p>

<p>Kibanaには、<a href="http://blog.johtani.info/blog/2013/06/10/fluent-es-kibana/">前回の記事</a>で書いたものとは別に開発中の<a href="http://three.kibana.org/">Kibana3</a>というのが存在します。</p>

<!-- more -->


<h2>Kibana3って？</h2>

<p>Kibana2はRubyで書かれていましたが、Kibana3はHTML＋JavaScriptで構成されています。
ですので、ApacheなどのWebサーバに配置することで、利用が可能となります。
ただ、HTML＋JavaScriptのため、ブラウザ上で動作するためブラウザが動作するマシンからElasticSearch（通常だと<code>http://マシン名orIPアドレス:9200/</code>とか）にアクセスできなければいけないという制限があります。</p>

<p>この条件さえクリア出来れば、Kibana3ではKibana2よりも様々なパネルが用意されていて、色々できそうなのでお勧めです。</p>

<h2>インストール</h2>

<p>ElasticSearchやログについては、前回の記事の環境を利用しました。
ですので、Kibana3のインストールのみです。（ApacheもCentOSのサーバに入っていたので。）</p>

<p>ダウンロードして、Apacheの公開ディレクトリに置いただけです。（お試し環境のため、権限とかは大目に見てください。</p>

<pre><code class="bash">$ git clone https://github.com/elasticsearch/kibana.git kibana-javascript
$ cp -R kibana-javascript /var/www/html
</code></pre>

<p>今回はApacheとElasticSearchが同一マシン（＝同一IPアドレスでアクセス可能）で動作している＋ElasticSearchへのアクセスのポートがデフォルト（9200）のため特に設定が必要ありませんでした。</p>

<p>ElasticSeachサーバとKibana3のApacheのサーバが別のサーバの場合やElasticSearchサーバのポートが異なる場合はkibana-javascript/config.jsファイルの編集が必要になります。
cloneしてすぐのconfig.jsは、以下のとおりです。</p>

<pre><code class="javascript">/*

elasticsearch:  URL to your elasticsearch server. You almost certainly don't
                want 'http://localhost:9200' here. Even if Kibana and ES are on
                the same host
kibana_index:   The default ES index to use for storing Kibana specific object
                such as stored dashboards
modules:        Panel modules to load. In the future these will be inferred
                from your initial dashboard, though if you share dashboards you
                will probably need to list them all here

If you need to configure the default dashboard, please see dashboards/default

*/
var config = new Settings(
{
  // By default this will attempt to reach ES at the same host you have
  // elasticsearch installed on. You probably want to set it to the FQDN of your
  // elasticsearch host
  elasticsearch:    "http://"+window.location.hostname+":9200",
  // elasticsearch: 'http://localhost:9200',
  kibana_index:     "kibana-int",
  modules:          ['histogram','map','pie','table','stringquery','sort',
                    'timepicker','text','fields','hits','dashcontrol',
                    'column','derivequeries','trends','bettermap'],
  }
);
</code></pre>

<p>ポート番号が異なる場合は、1つ目の「elasticsearch:」で指定されている「9200」を環境に合わせて編集するだけになります。
Kibana3とElasticSearchのホストが異なる場合は、1つ目の「elasticsearch:」の行をコメントアウトし、2つ目を有効にしてから環境に合わせたURLに修正して保存すればOKです。</p>

<p>以上で、インストールは完了します。あとは、以下のURLにアクセスするだけです。</p>

<pre><code>http://hogehoge/kibana-javascript/
</code></pre>

<h2>画面構成</h2>

<p>アクセスすると次のような画面が表示されます。</p>

<p><img src="/images/entries/20130619/kibana3+kibana2-es-index.jpeg" title="初期画面" ></p>

<p>左上に赤い帯で、「 Oops! Could not match index pattern to any ElasticSearch indices」とエラーが表示されました。</p>

<p>KibanaはElasticSearchに「logstatsh-年.月.日」という日付ごとのインデックスが存在することが前提となっています。
Kibanaに初めてアクセスした場合、「logstash-当日日付」で始まるインデックスを描画しようとします。
これは、私が前回利用したElasticSearchの環境に古いデータ（試したのが19日、データは10日のみ）しか入っていないために出たエラーです。</p>

<p>日付は「Options」というエラーが出ている付近の「Absolute」というリンクをクリックすると、特定の日付をカレンダーで指定することができるようになります。データは6/10にしか入っていないので、6/10（12時くらいから20時くらいまで）のを指定します。</p>

<p><img src="/images/entries/20130619/kibana3-selected-calendar.jpeg" title="日付指定" ></p>

<p>選択すると無事データが見えるようになりました。</p>

<p><img src="/images/entries/20130619/Kibana3-sample-include-description.jpeg" title="データ描画" ></p>

<h3>ダッシュボードの構成（初期）</h3>

<p>Kibana3では、この画面をダッシュボードというようです。
このダッシュボードは初期状態では、以下のパーツが表示されています。（子要素があとで説明するパネル名です）</p>

<ul>
<li>Options：描画対象の日付の指定やダッシュボードの保存などを行うRow

<ul>
<li>timepickerパネル：日付の指定</li>
<li>dashcontrolパネル：ダッシュボードの制御（保存とか）</li>
</ul>
</li>
<li>Query：ログ検索式を入れるところ

<ul>
<li>stringqueryパネル</li>
</ul>
</li>
<li>Graph：ヒストグラムの描画（X軸：時間、Y軸：ログ件数）

<ul>
<li>histogramパネル</li>
</ul>
</li>
<li>Events：検索にヒットしたログデータの描画領域

<ul>
<li>fieldsパネル：表示するフィールドの選択（左側。チェックを入れると右側のログ表示領域のカラムが増える）</li>
<li>tableパネル：ログデータ（右側。左側でチェックが入ったカラムだけが表示される。）</li>
</ul>
</li>
</ul>


<p>あくまで初期表示です。各パーツの設定アイコン（歯車のマーク）をクリックすると色々と設定が可能です。
また、「Events」など名称はクリック可能となっていて、クリックすると、そのパーツが折りたたまれた状態にすることも可能です。</p>

<p><img src="/images/entries/20130619/kibana3-collaped.jpeg" title="折りたたんだ状態" ></p>

<h3>ダッシュボードの設定</h3>

<p>ダッシュボードには独自のパネルを簡単に追加することができます。
ダッシュボードの構成はページの一番上にある「Logstash Search」の設定アイコンをクリックすると設定画面が開きます。</p>

<p><img src="/images/entries/20130619/kibana3-dashboad-setting.jpeg" title="ダッシュボード設定" ></p>

<p>「New row」にタイトル名を適当にいれて「Create Row」するとあたらしくパネルを追加することができるRowが追加されます。「Rows」の「Move」にある矢印でRow自体の表示場所を上下に移動することも可能です。</p>

<h3>Rowの設定</h3>

<p>追加した「Hoge」にパネルを追加する場合はHogeの上にある設定アイコンをクリックすると設定画面が開きます。</p>

<p><img src="/images/entries/20130619/kibana3-row-setting.jpeg" title="Rowの設定" ></p>

<p>ここでKibana3で用意されているパネルの追加ができます。</p>

<p><img src="/images/entries/20130619/kibana3-panel-add.jpeg" title="Panel追加ボタン" ></p>

<p>パネルを選んでボタンを押せばすぐに表示されます。</p>

<p><img src="/images/entries/20130619/kibana3-sample-panels.jpeg" title="パネルの羅列" ></p>

<p>こんな感じです。とりあえず、ポコポコと追加してみました。</p>

<p>利用できるパネルの種類は以下の様なパターンです。
適当ですが、表にしてみました。</p>

<table>
<tr><th>パネル名</th><th>概要</th></tr>
<tr><td>column</td><td>Rowの中にパネルを配置するコンテナを用意するためのパネル</td></tr>
<tr><td>dashcontrol</td><td>ダッシュボードの保存、保存したダッシュボードの表示などの操作ボタン</td></tr>
<tr><td>text</td><td>markdown形式などで記述が可能な文章を表示できるパネル</td></tr>
<tr><td>stringquery</td><td>検索クエリ入力用パネル</td></tr>
<tr><td>derivequeries</td><td>フィールドと検索式がわかれた形式の検索入力用パネル</td></tr>
<tr><td>timepicker</td><td>ログ表示の期間を指定するパネル</td></tr>
<tr><td>histogram</td><td>ログの件数のヒストグラム表示用パネル</td></tr>
<tr><td>hits</td><td>ヒット件数表示用パネル</td></tr>
<tr><td>pie</td><td>パイチャート表示用パネル</td></tr>
<tr><td>trends</td><td>指定された時間でデータの増減を%表示するパネル</td></tr>
<tr><td>sort</td><td>ソート条件指定用のプルダウン表示用パネル（変更したらtableの内容がソートされる）</td></tr>
<tr><td>table</td><td>ログデータ表示用パネル</td></tr>
<tr><td>fields</td><td>tableパネルに表示するフィールドを選択するための補助パネル</td></tr>
<tr><td>bettermap</td><td>なんか地図が出てきたパネル<br/>GeoJSONデータをゴニョゴニョ（表示かな？）できるみたい</td></tr>
<tr><td>map</td><td>なんか世界地図が出てきたパネル<br/>２文字の国コード（jaとかか？）かU.S.の州コードのデータを元に地図に色をつけるのかな？</td></tr>
</table>


<p>これらのパネルは個々に色々と設定が可能です。他にもdebug、map2など有りそうでしたがまだ使えないみたいです。</p>

<p>適当に触ってて気づいた注意点です。</p>

<ul>
<li>tableは１ダッシュボードで１つだけが良さそう。

<ul>
<li>２つあると、どちらかにしか描画されない。columnに入れるとグルーピングできたりするのかなぁ？</li>
</ul>
</li>
<li>stringquery、timepickerも１ダッシュボードで１つが良さそう。

<ul>
<li>これもtableと似たような理由です。</li>
</ul>
</li>
<li>ダッシュボード保存し忘れて泣きそうになる

<ul>
<li>JSで実装されてて、自分で色々とカスタマイズできるのですが、保存するのを忘れて泣きそうになりましたｗ</li>
<li>カスタマイズしたダッシュボードについては、ローカルに保存する以外にElasticSearchにも保存ができるみたいです。チームで共有することもできそうです。</li>
</ul>
</li>
<li>derivequeriesを表示するとグラフがカラフルに

<ul>
<li>derivequeriesを追加したらグラフが急にカラフルになりました。</li>
<li>どうもderivequeriesのFieldの部分を変更すると、そのフィールドの値を元にグラフを細分化してくれるようです。色の数の上限はderivequeriesのLength属性の数値で制御出来ます。（5だと5個まで色が出る）</li>
<li>histogramのパネルで自分でクエリを記載することも可能です。ただ、derivequeriesのフィールド変更すると書き換わっちゃいます。。。</li>
</ul>
</li>
</ul>


<p><img src="/images/entries/20130619/kibana3-multi-color-histogram-type.jpeg" title="derivequeriesを追加したらカラフルに" ></p>

<p>ヒストグラムは色々なパターンのグラフを描画できました。ラインによる描画（histo1）、総数を100%としたパーセンテージでの表示（histo2）、ライン＋点による描画（histo3）などです。</p>

<p><img src="/images/entries/20130619/kibana3-several-histogram.jpeg" title="ヒストグラムのいくつかのパターン" ></p>

<h2>感想</h2>

<p>ということで、適当にですが触ってみました。
Kibana2はApacheのアクセスログとかの表示しかできない感じがしましたが、Kibana3だといろいろなデータを描画できそうだなと。
logstash形式のインデックスを用意するのが前提になってるので、時系列データをグラフ描画するのに向いてるんでしょうか。
お手軽にグラフ化できるし、自分でダッシュボードをカスタマイズできるのは素敵です。
ただ、クエリとグラフの関係などはちょっと癖があるかもしれないので、色々と試してみないといけないかもしれないです。
（たとえば、特定のフィールドの値について「A、B、その他」みたいなグラフの描画とかをどうするかとか）</p>

<p>地図の描画は試してみたいかなぁ。</p>
]]></content>
  </entry>
  
</feed>
