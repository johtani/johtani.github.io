<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: wikipedia | @johtaniの日記 2nd]]></title>
  <link href="http://blog.johtani.info/blog/categories/wikipedia/atom.xml" rel="self"/>
  <link href="http://blog.johtani.info/"/>
  <updated>2015-06-02T17:24:46+09:00</updated>
  <id>http://blog.johtani.info/</id>
  <author>
    <name><![CDATA[johtani]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[elasticsearch-river-wikipediaの疑問点]]></title>
    <link href="http://blog.johtani.info/blog/2013/09/12/question-river-wikipedia/"/>
    <updated>2013-09-12T02:38:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/09/12/question-river-wikipedia</id>
    <content type="html"><![CDATA[<p><a href="/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch/">river-wikipediaの前々回の記事</a>で書きましたが、bulk_sizeに関連して登録件数がやけにきりが良いのが気になると書いていました。</p>

<p>で、Riverの仕組みを勉强がてら、<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia">elasticsearch-river-wikipedia</a>のソース（1.2.0）を読んでみました。</p>

<!-- more -->


<h2>Riverの作り</h2>

<p>Riverはorg.elasticsearch.river.Riverというinterfaceを実装することで作らています。
ただ、Riverがinterfaceとなっていますが、o.e.river.AbstractRiverComponentというクラスを継承して作られています。</p>

<p>AbstractRiverComponentにはRiverの名前や設定などが用意されています。
ま、ここはそれほど重要じゃないので、軽く流してと。</p>

<p>Riverの設定関連は実装したRiverクラス（ここでは、WikipediaRiverクラス）のコンストラクタで、設定値の読み取りなどの記述を記載します。
このコンストラクタが、<code>_river/hogehoge/_meta</code>をPUTした時のJSONを元にElasticSearchから呼ばれて、Riverのインスタンスが作成されます。（たぶん、<a href="https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/river/RiversService.java">このへんがその処理</a>だと思う。。。このあたりはまた今度）</p>

<p>実際のRiverの処理はWikipediaRiverクラスのstart()メソッド内部に記述されています。</p>

<pre><code class="java">    @Override
    public void start() {
        logger.info("starting wikipedia stream");
        try {
①            client.admin().indices().prepareCreate(indexName).execute().actionGet();
        } catch (Exception e) {
            if (ExceptionsHelper.unwrapCause(e) instanceof IndexAlreadyExistsException) {
                // that's fine
            } else if (ExceptionsHelper.unwrapCause(e) instanceof ClusterBlockException) {
                // ok, not recovered yet..., lets start indexing and hope we recover by the first bulk
                // TODO: a smarter logic can be to register for cluster event listener here, and only start sampling when the block is removed...
            } else {
                logger.warn("failed to create index [{}], disabling river...", e, indexName);
                return;
            }
        }
②        currentRequest = client.prepareBulk();
③        WikiXMLParser parser = WikiXMLParserFactory.getSAXParser(url);
        try {
④            parser.setPageCallback(new PageCallback());
        } catch (Exception e) {
            logger.error("failed to create parser", e);
            return;
        }
⑤        thread = EsExecutors.daemonThreadFactory(settings.globalSettings(), "wikipedia_slurper").newThread(new Parser(parser));
        thread.start();
    }
</code></pre>

<p>内部では</p>

<ol>
<li>インデックスの作成</li>
<li>バルクアップデート用クライアントの設定</li>
<li>WikiXMLのパーサの初期化</li>
<li>ページごとにキックされるコールバック処理の登録</li>
<li>デーモンスレッドの起動と起動</li>
</ol>


<p>といった処理の流れになっています。</p>

<p>で、このスレッドの起動後は、4.で用意したparser.parse()処理がグルグル回ります。</p>

<p>1ページがパースされるたびに、<code>WikipediaRiver.PageCallback</code>クラスの<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L166"><code>proess()</code>メソッド</a>が呼ばれます。
このメソッドの最後で、<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L198"><code>processBulkIfNeeded()</code>メソッド</a>が呼ばれています。ここで、実際にパースしたページをインデックスに登録する処理が実行されます。</p>

<p><a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L205">このメソッドの1行目</a>が鍵でした。
bulkSize以上の件数がバルクのリクエストに貯まった時だけ、実際にインデックスに登録する処理が実行されます。
このため、スレッドが回っている間は、bulkSize以上のデータが貯まらないと、インデックスへの登録は行われないわけです。</p>

<p>次に、このスレッドを止めるには、前々回書いたように、_riverにPUTした、Riverの設定をDELETEするしかありません。（あとは、ElasticSearchを停止するとかでしょうか。）</p>

<p>で、DELETEが実行される呼ばれるのが、<code>WikipediaRiver</code>クラスの<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java#L135"><code>close()</code>メソッド</a>です。</p>

<pre><code class="java">
    @Override
    public void close() {
        logger.info("closing wikipedia river");
        closed = true;
        if (thread != null) {
            thread.interrupt();
        }
    }
</code></pre>

<p>見ていただくと分かりますが、スレッド止めて終了です。</p>

<h3>問題点は？</h3>

<p>ということで、</p>

<ul>
<li>WikipediaのXMLを読み込んでもRiverは停止しない</li>
<li>Riverの停止を行ってもスレッドが止められるだけ。</li>
<li>bulkSize以下の件数が<code>currentRequest</code>に残っているけど、破棄される</li>
</ul>


<p>とまぁ、こんな流れになっているので、最後の端数のドキュメントがインデックスに登録されないようです。
（まだ、ちゃんと確認してないんですが、備忘録のため先に書いちゃいました。。。）</p>

<p>じゃあ、全部うまく登録するにはどうしたもんかなぁと。
いまのところ思いついたのはこんな感じです。
他にいい案があったら教えて下さい。</p>

<ul>
<li>案１：close()処理の中で、スレッド停止後に、<code>currentRequest</code>に貯まっているデータをインデックスに登録しちゃう</li>
<li>案２：bulkSize以外に、定期的（指定された時間）で登録処理を実行してしまう。</li>
</ul>


<p>簡単なのでとりあえず、案１を実装してみるかなぁと。
（さっさとコード書けよって話ですね。。。スミマセン）
その前にMLで質問ですかねぇ、英語で。</p>

<p>WikipediaのRiverをざっと眺めてみた感じですが、わかりやすい作りだなぁと。
他のRiverがどうなってるかをちゃんと見てませんが、他にもbulkSize指定をするRiverの場合は、このように件数がbulkSizeに満たない状態ではデータが登録されないといったことがあるかもしれません。</p>

<p>ElasticSearchのソースを読み始める取っ掛かりとしては面白いかと思いますので、興味ある方は読んで作ってみるといいかもしれません。（私は読んだだけですがｗ）</p>

<h2>追記（2013/09/13 21:00）</h2>

<p>MLで質問してみました。とりあえず、案1を。</p>

<p><a href="https://groups.google.com/forum/#!topic/elasticsearch/hqU-LF5aTy4">river-wikipedia does not index all pages</a></p>

<p>他のRiverでは対応してるしバグだね、Issue上げてとのことで、あげときました。
ついでにプルリクも出せばいいんでしょうが、プルリクまだやったことないヘタレです。。。</p>

<p>あと、案2についても同じトピックで質問してます。
どうやら、BulkProcessorにその機能があるよと。
<code>flushinterval</code>というプロパティがありそうです。どうやって設定して、どうやって動くのかとか見てないので、
調査してブログorLTかな。</p>

<p><a href="http://www.elasticsearch.org/guide/reference/api/bulk-udp/">bulk udp</a>にはその値を設定できそうなのがあるんだよなぁ。</p>

<h2>追記その２（2013/09/16 23:50）</h2>

<p>さっそく<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia/commit/3719ac5cd3cd5f0e4e57edaa72f5d4ca0b45ca5d">修正版がコミット(コミットログ)</a>されてました。
結構変わってます。BulkProcessorに<code>flush_interval</code>の設定をすれば、よしなにやってくれる仕組みがすでに実装されているようです。
<code>bulkSize</code>についても同様に、BulkProcessorに設定すれば良いようです。
Riverの仕組みが結構スッキリしています。
もともと実装されていた、bulkSizeごとの処理も消されています。
確かに、BulkProcessorの仕組みとして実装されている方がしっくりきますね。</p>

<p>ということで、考える暇もなくコミットされてしまいました。
こうやって質問しつつ、少しずつソースを読んでいこうかなと思ってるとこです。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[日本語Wikipediaをインデクシング（Kuromojiバージョン）]]></title>
    <link href="http://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji/"/>
    <updated>2013-09-03T01:15:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/09/03/ja-wikipedia-with-kuromoji</id>
    <content type="html"><![CDATA[<p><a href="/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch">前々回紹介した、日本語Wikipediaのデータをインデックス登録する記事</a>の続きです。</p>

<!-- more -->


<p>今回は、Kuromojiのアナライザを利用してインデックス登録してみます。</p>

<h2>余談（Proxy環境でのプラグインインストール）</h2>

<p>ElasticSearchのpluginコマンドはJavaで実装されています。（<a href="https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/plugins/PluginManager.java#L315">org.elasticsearch.plugins.PluginManager</a>）
プラグインのダウンロードには、java.net.URL.openConnection()から取得URLConnectionを使用しています。</p>

<p>ですので、pluginのインストールを行う際に、Proxy環境にある場合は以下のようにコマンドを実行します。</p>

<pre><code>./bin/plugin -DproxyPort=ポート番号 -DproxyHost=ホスト名 -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre>

<h2>elasticsearch-analysis-kuromojiのインストール</h2>

<p>WikipediaのデータをKuromojiを使って、形態素解析ベースの転置インデックスを作成していきます。
まずは、Kuromojiを利用するために、<a href="https://github.com/elasticsearch/elasticsearch-analysis-kuromoji">Analysisプラグイン</a>のインストールです。
ElasticSearchのバージョンに対応したプラグインのバージョンがあります。（プラグインのページに対応したバージョンの記載あり）
今回はElasticSearchの0.90.3を利用しているため、1.5.0をインストールします。</p>

<pre><code>./bin/plugin -i elasticsearch/elasticsearch-analysis-kuromoji/1.5.0
</code></pre>

<p>インストール後は再起動しておきます。
なお、Kuromojiを利用して、Wikipediaのデータを登録するばあい、デフォルトの設定では、ヒープが足りなくなるおそれがあります。
ElasticSearchの起動時に以下のオプションを指定して、最大ヒープサイズを2Gとしておきます。</p>

<pre><code>export ES_HEAP_SIZE=2g;./bin/elasticsearch
</code></pre>

<h2>Indexの作成（デフォルトでKuromojiのAnalyzerを利用する）</h2>

<p>Wikipediaのデータを登録する際に、Kuromojiのアナライザを利用したいのが今回の趣旨でした。
一番ラクな方法として、Wikipediaデータのインデックスの設定として、デフォルトのアナライザをKuromojiにしてしまいます。
（きちんと設計する場合は、必要に応じてフィールドごとに指定しましょう）</p>

<pre><code>curl -XPUT 'localhost:9200/ja-wikipedia-kuromoji' -d '{
    "settings": {
        "analysis": {
            "analyzer": {
                "default" : {
                    "type" : "kuromoji"
                }
            }
        }
    }
}'
</code></pre>

<p>これでkuromojiのアナライザがデフォルトで利用される形となります。
あとは、Riverを起動して登録するだけです。</p>

<h2>Riverの実行</h2>

<p>前回と一緒です。
インデックス名（<strong><em>river/&lt;インデックス名>/</em>meta</strong>）だけは、先ほど作成した「<code>ja-wikipedia-kuromoji</code>」に変更してください。</p>

<pre><code>curl -XPUT localhost:9200/_river/ja-wikipedia-kuromoji/_meta -d '
{
    "type" : "wikipedia",
    "wikipedia" : {
        "url" : "file:/home/johtani/src/jawiki-latest-pages-articles.xml"
    },
    "index" : {
        "bulk_size" : 10000
    }
}'
</code></pre>

<p>あとは、インデックスされるのを待つだけです。</p>

<h2>データ量とか</h2>

<p>5.8gbになりました。Kuromojiを利用したため、形態素解析により単語にきちんとトークないずされた結果でしょう。
Uni-gramだと、転置インデックスのボキャブラリも単語に対してヒットするドキュメント数も大きくなるため、
インデックスサイズも大きくなっているのかと。</p>

<p>検索クエリのサンプルなどはまた後日。（夜遅いので。。。）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearchにプラグインで日本語Wikipediaデータを入れてみました]]></title>
    <link href="http://blog.johtani.info/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch/"/>
    <updated>2013-08-23T12:02:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/08/23/index-wikipedia-ja-to-elasticsearch</id>
    <content type="html"><![CDATA[<p>久々のブログはElasticSearchネタです。勉強会開催する予定だったりすので、もう少し触っておきたいなと。
お手軽に検索するデータとして、よくWikipediaのデータを使っています。
ElasticSearchには<a href="https://github.com/elasticsearch/elasticsearch-river-wikipedia">elasticsearch-river-wikipedia</a>という便利なプラグインがあり、Wikipediaのデータを簡単に検索可能な状態にできます。このRiverを利用して日本語のWikipediaのデータを入れたので、メモを取っておきます。
まずは、river-wikipediaで日本語のデータをインデクシングしてみるまでの説明です。
日本語特有の設定（Kuromojiを利用したインデクシング）などはまた後日。</p>

<!-- more -->


<h2>プラグインのインストール</h2>

<p>対象とするElasticSearchは現時点で最新版の0.90.3とします。
最新版でRiver動かないなぁとつぶやいた影響かどうかはわかりませんが、2013/08/19に最新版のElasticSearchで動作するプラグインが公開されました。</p>

<p>まずはインストールです。
HPにも書いてありますが、以下のコマンドを実行すればインストールされます。</p>

<pre><code class="sh">$ ./bin/plugin -install elasticsearch/elasticsearch-river-wikipedia/1.2.0
-&gt; Installing elasticsearch/elasticsearch-river-wikipedia/1.2.0...
Trying http://download.elasticsearch.org/elasticsearch/elasticsearch-river-wikipedia/elasticsearch-river-wikipedia-1.2.0.zip...
Downloading ..........DONE
Installed river-wikipedia into /opt/elasticsearch/plugins/river-wikipedia
</code></pre>

<p>ElasticSearchが起動している場合はプラグインをインストール後、認識させるためにElasticSearchを再起動します。</p>

<h2>日本語Wikipediaのインデクシング</h2>

<p>通常は英語のWikipediaがインデクシングされますが、対象となるファイルを変更することで日本語のWikipediaもインデクシング可能です。
手元に日本語Wikipediaのダンプファイルがあるものとします。（<a href="http://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89">ダウンロードはWikipediaデータベースダウンロード</a>のページにあるpages-articles.xml.bz2のファイルです）</p>

<p>ファイルを指定してインデクシングするには、つぎのcurlコマンドを実行します。
コマンドを実行するとすぐにインデクシングが始まりますので注意が必要です。</p>

<pre><code>curl -XPUT localhost:9200/_river/ja-wikipedia/_meta -d '
{
    "type" : "wikipedia",
    "wikipedia" : {
        "url" : "file:/home/johtani/src/jawiki-latest-pages-articles.xml"
    },
    "index" : {
        "bulk_size" : 1000
    }
}'
</code></pre>

<p>ここでURLに含まれる「ja-wikipedia」がインデックス名になります。
また、JSONの&#8221;url&#8221;にはファイルの場所を指定するため、<code>file:</code>で開始するパスを指定します。
例では、bz2を解凍したファイルを指定していますが、bz2のままのファイルでもOKです。</p>

<p>上記コマンドを実行すると、<code>_river</code>というインデックスにつぎのようなエントリが増えています。
(<code>curl -XGET 'localhost:9200/_river/ja-wikipedia/_search?pretty</code>)</p>

<pre><code class="json">{
   "took": 5,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "_river",
            "_type": "ja-wikipedia",
            "_id": "_status",
            "_score": 1,
            "_source": {
               "ok": true,
               "node": {
                  "id": "gdyvwpiAR52lqUCcRhVwsg",
                  "name": "Blitzschlag, Baron Von",
                  "transport_address": "inet[/192.168.100.7:9300]"
               }
            }
         },
         {
            "_index": "_river",
            "_type": "ja-wikipedia",
            "_id": "_meta",
            "_score": 1,
            "_source": {
               "type": "wikipedia",
               "wikipedia": {
                  "url": "file:/home/johtani/src/jawiki-latest-pages-articles.xml"
               },
               "index": {
                  "bulk_size": 100
               }
            }
         }
      ]
   }
}
</code></pre>

<p><code>"_id": "_meta"</code>というエントリがさきほど登録したWikipediaのRiverに関する設定です。
<code>"_id": "_status"</code>というエントリが起動したRiverの状態になります。</p>

<h2>Riverの停止</h2>

<p>日本語Wikipediaは結構サイズが大きく、手元のAirでインデクシングするのに30分程度かかりました。（bz2圧縮されていないファイルで、何もしていない状態）</p>

<p>途中でRiverを停止したくなった場合は、以下のcurlコマンドを実行します。</p>

<pre><code>$ curl -XDELETE 'localhost:9200/_river/ja-wikipedia'
</code></pre>

<p>先ほど設定した<code>_river/ja-wikipedia</code>の情報を削除すると、エントリが削除されたのを検知してRiverが停止します。ログにはつぎのようなメッセージが表示されます。</p>

<pre><code>[2013-08-26 18:26:50,130][INFO ][cluster.metadata         ] [Blitzschlag, Baron Von] [[_river]] remove_mapping [ja-wikipedia]
[2013-08-26 18:26:50,130][INFO ][river.wikipedia          ] [Blitzschlag, Baron Von] [wikipedia][ja-wikipedia] closing wikipedia river
</code></pre>

<p>Riverを停止してもそれまでインデクシングされたデータは検索できます。
データはちょっとだけで良いという場合は、先ほどの<code>_river</code>のデータを削除してください。
（◯件だけ登録したいとかできるかは調べてないです。）</p>

<h2>サイズとかマッピングとか</h2>

<h3>サイズ</h3>

<p>インデックス前のXMLのサイズが5.7Gのとき、ElasticSearchのインデックスサイズ（Optimize後）は7.2Gとなりました。すこし古いファイルを利用しているため、最新版とはサイズが異なるかもしれません。</p>

<p>あと、データ数が、1540000件とやけにきりがいいのがちょっと気になっています。。。
bulkのサイズを10000で指定してインデックスしたので、切れてるのかなぁと。</p>

<p>ということは、データが欠落しているような気がするのでRiverの作りの問題なのか、ElasticSearchの問題なのかはちょっと調べてみないとわからないなと。</p>

<h3>マッピング</h3>

<p>出来上がったインデックスのマッピング（Solrでいうスキーマみたいなもの）は次のようになっています。</p>

<pre><code class="json">{
   "ja-wikipedia": {
      "page": {
         "properties": {
            "category": {
               "type": "string"
            },
            "disambiguation": {
               "type": "boolean"
            },
            "link": {
               "type": "string"
            },
            "redirect": {
               "type": "boolean"
            },
            "special": {
               "type": "boolean"
            },
            "stub": {
               "type": "boolean"
            },
            "text": {
               "type": "string"
            },
            "title": {
               "type": "string"
            }
         }
      }
   }
}
</code></pre>

<p>Wikipediaの各種データが上記のフィールドに入っています。
また、マッピングタイプはデフォルトで「page」というタイプになっています。</p>

<h2>検索</h2>

<p>先ほどのマッピングを元に検索すればOKです。例えばつぎのような感じです。</p>

<pre><code>curl -XPOST 'localhost:9200/ja-wikipedia/_search?pretty' -d '
{
    "size" : 3,
    "script_fields": {
       "title_only": {
          "script": "_source.title"
       }
    }, 
    "query" : {
        "query_string": {
            "default_field": "title",
            "query" : "千葉"
        }
    }
}'
</code></pre>

<p>結果はこんな感じ。</p>

<pre><code class="json">{
   "took": 51,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 8616,
      "max_score": 5.8075247,
      "hits": [
         {
            "_index": "ja-wikipedia",
            "_type": "page",
            "_id": "3582",
            "_score": 5.8075247,
            "fields": {
               "title_only": "千葉"
            }
         },
         {
            "_index": "ja-wikipedia",
            "_type": "page",
            "_id": "2352241",
            "_score": 4.94406,
            "fields": {
               "title_only": "千葉千枝子"
            }
         },
         {
            "_index": "ja-wikipedia",
            "_type": "page",
            "_id": "14020",
            "_score": 4.8754807,
            "fields": {
               "title_only": "千葉千恵巳"
            }
         }
      ]
   }
}
</code></pre>

<p>結果を見やすくするため、タイトルだけを「title_only」という表示にしています。
ただ、この検索だと、一見「千葉」できちんとヒットしているように見えますが、ElasticSearchのフィールドの定義はstring型になっています。なので、実は「千」や「葉」だけのデータもヒットしています。
マルチバイト文字は1文字ずつインデックスされてしまい、query_stringというクエリでは、フレーズ検索などができていないためです。</p>

<h2>まとめ</h2>

<p>プラグインいれて、XMLファイルがあれば、検索できるデータが出来上がるので、
暇があったら、お試しで触ってみるデータを簡単に入れてみてはどうでしょうか。</p>

<p>ただ、いくつか気になる点も。</p>

<ul>
<li>日本語が検索しにくい（string型のフィールドなのでuni-gramっぽくなっている）</li>
<li>bulk_sizeの影響で端数が登録できてない（バグ？どうなの？）</li>
</ul>


<p>ということで、ちょっと使いにくいかもなぁということで、つぎはKuromojiを利用してインデックスしてみてみようかなと。次回のエントリで書く予定です。</p>
]]></content>
  </entry>
  
</feed>
