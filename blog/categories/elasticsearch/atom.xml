<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: elasticsearch | @johtaniの日記 2nd]]></title>
  <link href="http://blog.johtani.info/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://blog.johtani.info/"/>
  <updated>2014-11-21T17:39:20+09:00</updated>
  <id>http://blog.johtani.info/</id>
  <author>
    <name><![CDATA[johtani]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Logstashを利用したApacheアクセスログのインポート]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/20/import-apache-accesslog-using-logstash/"/>
    <updated>2014-11-20T17:30:39+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/20/import-apache-accesslog-using-logstash</id>
    <content type="html"><![CDATA[<p>JJUG CCCや第7回Elasticsearch勉強会のKibana4のデモにアクセスログを利用しました。</p>

<p>ただ、セッションでは、どうやってElasticsearchに投入したのかという詳しい話をしていませんでした。
本記事では、データ取り込み時に利用したLogstashの設定ファイルについて説明します。</p>

<!-- more -->


<p>Logstashの設定の説明に入る前に、全体の流れを。
「ApacheアクセスログをKibana4により可視化」です。</p>

<h2>材料の準備</h2>

<p>「ApacheアクセスログをKibana4により可視化」に必要な材料は次の通りです。
（今回は起動するところまでいかないので、実際に必要なのは次回以降になります。）</p>

<ul>
<li>Java 7（u55以上を1つ）</li>
<li>Logstash 1.4.2（1つ）</li>
<li>Elasticsearch 1.4.0（1つ）</li>
<li>Kibana4 Beta2（1つ）</li>
<li>Apacheのアクセスログ（適量）</li>
</ul>


<p>Apacheのアクセスログ以外は、公式サイトからダウンロードできます。
それぞれをダウンロードして、起動できるようにしておきましょう。</p>

<p>※1台のマシン上で行う場合は、アクセスログの量を少なめにするなどの対策をとりましょう。
※今回は、1台のマシン（Mac）上で、VMなどを利用せず、それぞれ直接起動するものとします。</p>

<h2>可視化の手順と流れ</h2>

<p>可視化の流れとしては、</p>

<ol>
<li>Logstashでファイルを読み込み、各種処理（パースしたり、情報を追加したり、切り出したり）</li>
<li>Elasticsearchに保存</li>
<li>Kibanaでグラフを作ったり、検索してみたり</li>
</ol>


<p>です。</p>

<p>今回は、1のLogstashでファイルを読み込んだりする設定ファイルの説明です。</p>

<h3>Logstashの設定</h3>

<h4>Logstashの基本</h4>

<p>まずは、Logstashの設定ですが、簡単にLogstashの説明を。
Logstashは大きく3つのパーツに分かれています。</p>

<ol>
<li>input：データの入力処理</li>
<li>filter：inputで読み込んだデータに対する操作など</li>
<li>output：データの出力処理</li>
</ol>


<p>inputでデータを読み込み（複数可）、filterでデータに対して各種処理を行い、outputでデータを指定されたところに出力（複数可）します。</p>

<h4>アクセスログの読み込み設定</h4>

<p>アクセスログの読み込み処理は大まかに次のようなものとなります。</p>

<ol>
<li>アクセスログを読み込む（input/file）</li>
<li>読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</li>
<li>日付のパース（filter/date）</li>
<li>クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</li>
<li>リクエストのパスの第1階層の抽出（filter/grok）</li>
<li>ユーザエージェントのパース（filter/useragent）</li>
<li>Elasticsearchへの出力（output/elasticsearch）</li>
</ol>


<p>設定ファイルは次のようなものになります。</p>

<pre><code class="ruby">input {
  file {
    path =&gt; "/Users/johtani/demo_access_log/*/*.log"
    start_position =&gt; "beginning"
  }
}

filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    break_on_match =&gt; false
    tag_on_failure =&gt; ["_message_parse_failure"]
  }
  date {
    match =&gt; ["timestamp", "dd/MMM/YYYY:HH:mm:ss Z"]
    locale =&gt; en
  }
  geoip {
    source =&gt; ["clientip"]
  }
  grok {
    match =&gt; { "request" =&gt; "^/%{WORD:first_path}/%{GREEDYDATA}$" }
    tag_on_failure =&gt; ["_request_parse_failure"]
  }
  useragent {
    source =&gt; "agent"
    target =&gt; "useragent"
  }
}

output {
  elasticsearch {
    host =&gt; "localhost"
    index =&gt; "new_demo_access_log-%{year}"
    cluster =&gt; "demo_cluster"
    protocol =&gt; "http"
  }
}
</code></pre>

<h5>1. アクセスログを読み込む（input/file）</h5>

<p>inputの<a href="http://logstash.net/docs/1.4.2/inputs/file">fileモジュール(a)</a>を使用してアクセスログのファイルを読み込みます。
<code>path</code>でアクセスログのファイルのパスを指定します。
今回利用したアクセスログは<code>demo_access_log/2010/access20100201.log</code>といった日毎のファイルに分割されていたため、
<code>*</code>を利用してファイルのパスを指定しました。
また、今回は既存のファイルの読み込みだけのため、<code>start_position</code>に<code>beginning</code>を指定してあります。
デフォルトでは<code>end</code>が指定されるため、Logstashを起動後に追記されたログから対象になってしまうためです。
その他の設定については、公式ガイドをご覧ください。</p>

<pre><code class="ruby">input {
  file { # a
    path =&gt; "/Users/johtani/demo_access_log/*/*.log" # b
    start_position =&gt; "beginning" # c
  }
}
</code></pre>

<blockquote><p>Logstashでは、ファイルをどこまで読み込んだかという情報を保持するために、<a href="http://logstash.net/docs/1.4.2/inputs/file#sincedb_path">sincedb</a>を利用しています。
設定変更後に同じファイルを最初から読み込みたい場合などは、こちらのファイルを一旦削除するなどの対応が必要です。</p></blockquote>

<p>ちなみに、読み込んだデータは次のようなJSONになっています。</p>

<pre><code class="json">{
  "message": "読み込んだアクセスログ",
  "@version": "1",
  "@timestamp":"2014-11-21T06:16:21.644Z",
  "host":"jupiter.local",
  "path":"/Users/johtani/demo_access_log/2010/access20100201.log"}
}
</code></pre>

<p>特に指定がない場合は、<code>message</code>に読み込んだデータが入ってきます。
<code>@timestamp</code>がLogstashが読み込んだ時刻、<code>host</code>はLogstashが動作しているホスト名です。
<code>path</code>はfileモジュールが読み込んだファイルのパスを設定しています。
この後の処理で、どこの項目に対して処理を行うかといったことが重要になるので、</p>

<h5>2. 読み取ったアクセスログを各フィールド（IPアドレス、ユーザエージェントなど）に分割（filter/grok）</h5>

<p>2.〜6.の処理は、inputで読み込んだ1アクセスログに対する処理となります。</p>

<p>ここでは、<a href="http://logstash.net/docs/1.4.2/filters/grok">grokフィルタ</a>を使用して
Apacheのアクセスログを各フィールドに分割します。
Logastashでは、簡単に使えるようにいくつかの<a href="https://github.com/elasticsearch/logstash/tree/v1.4.2/patterns">パターン</a>が用意されています。
Apacheのログのために、<a href="https://github.com/elasticsearch/logstash/blob/v1.4.2/patterns/grok-patterns#L91"><code>COMBINEDAPACHELOG</code></a>というのが用意されています。
今回はこちらを使用しています。その他にも日付などパターンが用意されているので、試してみてください。</p>

<p><code>message</code>にアクセスログが入っているので、こちらの項目に対して<code>COMBINEDAPACHELOG</code>のパターンを
<code>match</code>で適用してフィールドに抜き出します。
<code>tag_on_failure</code>は、<code>match</code>でパースに失敗した場合に、<code>tag</code>というフィールドに指定した文字列を出力する機能になります。
デフォルトだと<code>_grokparsefailure</code>が付与されますが、ここでは、どの処理で失敗したがを判別するために文字列を変更しています。</p>

<pre><code class="ruby">filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    break_on_match =&gt; false
    tag_on_failure =&gt; ["_message_parse_failure"]
  }
  ...
</code></pre>

<p><code>clientip</code>、<code>ident</code>、<code>auth</code>、<code>timestamp</code>、<code>verb</code>、<code>request</code>、<code>httpversion</code>、<code>response</code>、<code>bytes</code>、<code>referrer</code>、<code>agent</code>がgrokフィルタにより抜き出された項目です。</p>

<pre><code class="json">{
  "message":"アクセスログ",
  "@version":"1",
  "@timestamp":"2014-11-21T07:20:54.387Z",
  "host":"jupiter.local",
  "path":"/Users/johtani/demo_access_log/2010/access20100201.log",
  "clientip":"クライアントのIPアドレス",
  "ident":"-",
  "auth":"-",
  "timestamp":"01/Feb/2010:00:00:26 +0900",
  "verb":"GET",
  "request":"/images/favicon.ico",
  "httpversion":"1.1",
  "response":"200",
  "bytes":"318",
  "referrer":"\"-\"",
  "agent":"\"Mozilla/5.0 (Windows; U; Windows NT 5.1; ja; rv:1.9.1.7) Gecko/20091221 Firefox/3.5.7 (.NET CLR 3.5.30729)\""
}
</code></pre>

<h5>3. 日付のパース（filter/date）</h5>

<p>Logstashは特に指定がない場合、inputでデータを取り出した日付が<code>@timestamp</code>となります。
そして、このフィールドが特に指定がない場合は、Elasticsearchのデータの日付となり、Kibanaで利用する日付となります。</p>

<p>リアルタイムにアクセスログを読み込む場合は、読み込んだ日時でもほぼ問題はありませんが、過去データの場合はそうもいきません。
そこで、<a href="http://logstash.net/docs/1.4.2/filters/date"><code>dateフィルタ</code></a>を使用して、<code>@timestamp</code>の値を書き換えます。</p>

<pre><code class="ruby">date {
  match =&gt; ["timestamp", "dd/MMM/YYYY:HH:mm:ss Z"]
  locale =&gt; en
}
</code></pre>

<p>上記では、<code>timestamp</code>という項目に対して<code>dd/MMM/YYYY:HH:mm:ss Z</code>という日付パターンの場合に値を書き換える設定となります。
なお、日付の月の部分が<code>Feb</code>となっているため、<code>locale</code>に<code>en</code>を指定しています。Logstashが動作するマシンの<code>locale</code>が<code>ja</code>などの場合にパースに失敗するためです。</p>

<h5>4. クライアントIPアドレスにgeoipの情報を付加（filter/geoip）</h5>

<p>どの国からのアクセスかなどを判別したいので、IPアドレスを元にgeoipを利用してより詳細な情報を付与します。
Logstashでもこの機能が用意されており、簡単に利用ができます。</p>

<pre><code class="ruby">geoip {
  source =&gt; ["clientip"]
}
</code></pre>

<p>これだけです。対象とするIPアドレスのフィールドを指定しているだけです。
<code>geoip</code>というフィールドが追加され、次のような情報が付与されます。
国名、緯度経度、タイムゾーンなどです。</p>

<pre><code class="json">{
  ...  
  "geoip": {
    "ip": "IPアドレス",
    "country_code2": "JP",
    "country_code3": "JPN",
    "country_name": "Japan",
    "continent_code": "AS",
    "latitude": 36,
    "longitude": 138,
    "timezone": "Asia/Tokyo",
    "location": [
      138,
      36
    ]
  }
  ...
}
</code></pre>

<h5>5. リクエストのパスの第1階層の抽出（filter/grok）</h5>

<p>リクエストされたURLは<code>request</code>フィールドにありますが、個別のURLだと、大まかな集計が大変です。
もちろん、クエリで処理することもできますが、Logstashで処理するついでに、第1階層のディレクトリ名を抽出しておくことで、
検索や集計を行いやすくしておきます。</p>

<pre><code class="ruby">grok {
  match =&gt; { "request" =&gt; "^/%{WORD:first_path}/%{GREEDYDATA}$" }
  tag_on_failure =&gt; ["_request_parse_failure"]
}
</code></pre>

<p>また、grokフィルタの登場です。
今回は、<code>WORD:first_path</code>という記述方法で、<code>WORD</code>パターンにマッチした文字列を<code>first_path</code>というフィールドに展開する指定をしています。</p>

<p>例えば、サイトのスクリプトなどが<code>scripts</code>というディレクトリにある場合は、<code>first_path</code>の値を利用して、
後続のフィルタでログデータを出力しないといった処理にも使えます。</p>

<h5>6. ユーザエージェントのパース（filter/useragent）</h5>

<p>Logstashではユーザエージェントの文字列から、いくつかの情報を付与するフィルタも用意されています。
<a href="http://logstash.net/docs/1.4.2/filters/useragent"><code>useragent</code>フィルタです。</a></p>

<pre><code>useragent {
  source =&gt; "agent"
  target =&gt; "useragent"
}
</code></pre>

<p><code>agent</code>というフィールドにユーザエージェントの文字列があるので、このフィールドに対してフィルタを適用します。
元の文字列も取っておきたいので、<code>useragent</code>という別のフィールドに出力するように指定してあります。</p>

<pre><code class="json">"useragent": {
  "name": "Firefox",
  "os": "Windows XP",
  "os_name": "Windows XP",
  "device": "Other",
  "major": "17",
  "minor": "0"
},
</code></pre>

<p>このように、OS名やバージョン名などが抽出できます。</p>

<h5>7. Elasticsearchへの出力（output/elasticsearch）</h5>

<p>最後は、<a href="http://logstash.net/docs/1.4.2/outputs/elasticsearch">Elasticsearchへのデータの出力設定</a>です。</p>

<p><code>index</code>にて、出力するindex名を指定してあります。
また、年毎のインデックス名にするために<code>%{year}</code>を利用しています。
<a href="http://logstash.net/docs/1.4.2/configuration#sprintf">sprintf format</a>です。</p>

<pre><code class="ruby">elasticsearch {
  host =&gt; "localhost"
  index =&gt; "new_demo_access_log-%{year}"
  cluster =&gt; "demo_cluster"
  protocol =&gt; "http"
}
</code></pre>

<h2>まとめ</h2>

<p>ということで、今回はアクセスログをLogstashにて読み込む時の設定について説明してきました。
次回は、実際にLogstashを起動してElasticsearchにデータを登録するところまでを説明します。</p>

<p>JJUG CCCや勉強会のデモに用いたデータは、
Elasticsearchにデータを登録する前にテンプレートも設定してありました。こちらについても、次回説明しようと思います。</p>

<p>不明な点、誤植などありましたら、コメント欄へお願いします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[第7回Elasticsearch勉強会を開催しました。#elasticsearchjp]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/19/hold-on-7th-elasticsearch-jp/"/>
    <updated>2014-11-19T11:19:07+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/19/hold-on-7th-elasticsearch-jp</id>
    <content type="html"><![CDATA[<p><a href="http://elasticsearch.doorkeeper.jp/events/16837">第7回Elsticsearch勉強会</a>を開催しました。
スタッフの皆さん、スピーカーの皆さん、開場提供していただいた<a href="http://recruit-tech.co.jp">リクルートテクノロジーズさん</a>、ありがとうございました！
次回もよろしくお願いします！参加していただき盛り上げていただいた参加者の皆さんもありがとうございました。</p>

<p>昨日も紹介しましたが、<a href="http://qiita.com/advent-calendar/2014/elasticsearch">Elasticsearch Advent Calendar 2014</a>を用意してみました。まだ、空きがありますので、登録お待ちしております！</p>

<!-- more -->


<p>今回は出足が好調で、早々に180人の枠を超えるという嬉しい事態でした。
最終的な参加人数は130名程度で、懇親会参加者が50名弱といったところです。</p>

<h2>「Kibana4」</h2>

<h3>Elasticsearch Inc. Jun Ohtani @johtani</h3>

<p>スライド：<a href="https://speakerdeck.com/johtani/kibana4">Kibana4</a></p>

<p>ということで、Kibana4の紹介と、Kibana4のBeta2を利用したデモを行いました。
デモの開始のところで少し環境がうまく動いてなくて手間取ってしまいましたが。。。</p>

<p>発表で1点だけ修正があります。JRubyを選択しているのがElasticsearchのライブラリを使用するためという説明をしましたが、
こちらは、Logstashに関する話でした。Kibana4は現時点では、ElasticsearchへのProxyとしての動作が主なものとなります。Rubyでも動作可能です。
bin/kibanaについてはJavaを使った起動になります。
参考：<a href="https://github.com/elasticsearch/kibana/tree/master/src/server">https://github.com/elasticsearch/kibana/tree/master/src/server</a></p>

<p>発表でも主張しましたが、ダウンロードして、Elasticsearchを用意すれば簡単に動作させることが可能です。
ぜひ、ローカルで試して見てもらえればと思います。
今回のデモのデータを入れるのに利用したLogstashの設定などについては、ブログで記事を書こうと思います。</p>

<h2>niconicoの検索を支えるElasticsearch</h2>

<h3>株式会社ドワンゴ 伊藤 祥 さん</h3>

<p>スライド：<a href="https://speakerdeck.com/shoito/niconico-elasticsearch">niconicoの検索を支えるElasticsearch</a></p>

<ul>
<li>リアルタイム検索の実現、新しい検索への対応</li>
<li>検索のアーキテクチャとか。</li>
<li>Capistranoでデプロイとかを管理</li>
<li>1.4.1が出たら、クラスタを更新予定</li>
</ul>


<p>ということで、実際に導入した話から、現在の運用の仕方、クラスタのアップグレードなど多岐にわたる内容でおもしろかったです。
遭遇した問題点とかもあったので。
Marvel便利なのでぜひ導入を検討してもらえればw</p>

<h2>Elasticsearch at CrowdWorks </h2>

<h3>株式会社クラウドワークス 九岡 佑介 さん @mumoshu</h3>

<p>スライド：<a href="http://www.slideshare.net/mumoshu/20141118-es">Elasticsearch at CrowdWorks</a></p>

<ul>
<li>会社の紹介</li>
<li>仕事が検索対象</li>
<li>検索時間が1桁減少！</li>
<li>Graceful Degradationで失敗したら、InnoDB FTSで代替：<a href="https://github.com/crowdworks/gracefully">Gracefully</a></li>
<li><a href="http://www.found.no">found.no</a>のサービスを利用</li>
<li>elasticsearch-modelの拡張を作成してOSSとして公開：<a href="https://github.com/crowdworks/elasticsearch-model-extensions">elasticsearch-model-extensions</a></li>
</ul>


<p>Gracefullyで切り替えとかは面白いなと思いました。
検索での利用の話でしたが、他のシーンでも使えそうですよね。
日本にFoundユーザがいるのも初めて知りました。
彼らの開発者ブログも質の良い情報が載っているので、参考になりますよね。</p>

<p>次は、どんなMappingで運用しているのかとか、どういった工夫をしているかといった点を詳しく聞きたいなと思いました。
またお待ちしております。</p>

<h2>1分で作るElasticsearchプラグイン</h2>

<h3>株式会社エヌツーエスエム 菅谷 信介 さん</h3>

<p>スライド：<a href="http://www.slideshare.net/shinsuke/plugins-ates7">Elasticsearchプラグインの作り方</a></p>

<ul>
<li>プラグインの作り方とか。</li>
<li>十数個のプラグインの紹介。プラグインはこちらで公開中。<a href="https://github.com/codelibs/">https://github.com/codelibs/</a></li>
<li>実際に、業務で必要なものから作成</li>
<li>まだまだ作りたいものがある</li>
</ul>


<p>コミュニティ還元できるものはPR送ってもらえるとうれしいです。
前よりは体制も増えてるので、PRも目にとまるようになってるはずです。</p>

<p>あとは、使ってみたいと思う方も多数いると思うので、ぜひ、OSSなので、貢献しましょう！
フィードバックがあるだけで、OSS活動やってるものにとってはやる気につながると思いますし。</p>

<h2>LT：GISとして活用するElasticsearch </h2>

<h3>船戸 隆さん</h3>

<p>スライド：<a href="https://speakerdeck.com/tfunato/gistositehuo-yong-suruelasticsearch">GISとして活用するElasticsearch </a></p>

<ul>
<li>java-jaからIngressの青（Registance）の勧誘に来られた方w</li>
<li>APIをハックして、情報を取得し、Kibanaで可視化</li>
<li>残念ながら、APIが変更されて見れなくなったらしい。</li>
</ul>


<p>Ingress実際にやったことはないのですが、おもしろそうでした。
発表される方の会社の採用紹介ではなく、Ingressの勧誘をされるとは想定外でしたw</p>

<p>興味のあるデータをKibanaで可視化するのも面白い例だと思うので、活用してもらえればと思います。</p>

<h2>その他、感想などのブログ</h2>

<p>適当に見つけたブログを列挙してあります。これもあるよ！などあれば、教えてください。</p>

<ul>
<li><a href="http://blog.yoslab.com/entry/2014/11/18/203159">勉強会メモ - 第7回elasticsearch勉強会</a></li>
<li><a href="http://qiita.com/t-sato/items/940ccfa9e4a668b91967">第7回elasticsearch勉強会 #elasticsearch #elasticsearchjp</a></li>
</ul>


<h2>まとめ</h2>

<p>JJUGの時とは違い、Elasticsearch勉強会ではさすがに、企業としてのElasticsearchの知名度が高かったのはありがたいことでした。
自分の発表のために始めた勉強会でもありますが、まだまだ、発表するときは緊張しますし、分かりにくいんじゃないかなぁと思うことも多々あります。
この辺がわかりにくかった、この辺をもっと知りたいなど、フィードバックをお待ちしております。</p>

<p>冒頭にも書きましたが、<a href="http://qiita.com/advent-calendar/2014/elasticsearch">Elasticsearch Advent Calendar 2014</a>の登録をお待ちしております。どんなことでも歓迎なので、Elasticsearch、Kibana、Logstashなどについて書いてもらえるとうれしいです。</p>

<p>次回ももちろん2ヶ月後くらいに行います。 スピーカー募集中ですので、コメント、メール、ツイートなど、コンタクトしていただければと思います。 よろしくお願いいたします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch 1.4.0および1.3.5リリース（日本語訳）]]></title>
    <link href="http://blog.johtani.info/blog/2014/11/06/elasticsearch-1-4-0-ja/"/>
    <updated>2014-11-06T01:30:33+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/11/06/elasticsearch-1-4-0-ja</id>
    <content type="html"><![CDATA[<p><strong>※この記事は次のブログを翻訳したものになります。</strong></p>

<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-released/">elasticsearch-1.4.0 and 1.3.5 released</a></p>

<p>本日、<strong>Lucene 4.10.2</strong>をベースにした<strong>Elasticsearch 1.4.0</strong>と、バグフィックスリリースである、<strong>Elasticsearch 1.3.5</strong>をリリースしました。
ダウンロードおよび変更リストはそれぞれ次のリンクからアクセスできます。</p>

<ul>
<li>最新ステーブルリリース：<a href="http://www.elasticsearch.org/downloads/1-4-0">Elasticsearch 1.4.0</a></li>
<li>1.3.x系バグフィックス：<a href="http://www.elasticsearch.org/downloads/1-3-5">Elasticsearch 1.3.5</a></li>
</ul>


<p>1.3ブランチに関する過去のリリースについてのブログは次のとおりです：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-4-released/">1.3.4</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-3-released/">1.3.3</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-2-released/">1.3.2</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-1-released/">1.3.1</a>, <a href="http://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/">1.3.0</a>.</p>

<!-- more -->


<p>Beta1リリースでも言及しましたが、1.4.0の主なテーマは<em>resiliency(復元性、弾力性)</em>です。
Elasticsearchをより安定し信頼性のあるものにし、メモリ管理を改善し、ディスカバリアルゴリズムを改善し、破損したデータの検知を改善しました。
Beta1リリースからのハイライトも含んでいます。</p>

<ul>
<li>Doc values (インデックス時にディスクに保存される<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/fielddata-formats.html#fielddata-formats">fielddata</a>)がヒープ利用率を激減</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-fielddata.html#request-circuit-breaker">Request circuit breaker</a>:
メモリを消費しすぎる検索リクエストの中断</li>
<li>Bloom filterの<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-update-settings.html#codec-bloom-load">デフォルト無効</a>、高速なインデキシングのためにもはや必要とされないため。</li>
<li>ノードディスカバリ、シャードリカバリの数多くのバグフィックス及び改善</li>
<li>データ破損の早期検知のためのチェックサムのさらなる利用</li>
<li>GroovyをMVELの代わりに<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-scripting.html#modules-scripting">デフォルトスクリプト言語に</a></li>
<li>CORSを<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-http.html#_settings_2">デフォルト無効</a>に。XSS攻撃防止の為。</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-shard-query-cache.html#index-modules-shard-query-cache">クエリキャッシュ</a>、変更されていないシャードからすぐにaggregation結果を返す</li>
<li>新しいAggregation：<code>filter</code>(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-filters-aggregation.html#search-aggregations-bucket-filters-aggregation">ドキュメント</a>)、<code>children</code>(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-children-aggregation.html#search-aggregations-bucket-children-aggregation">ドキュメント</a>)、<code>scripted_metric</code>(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-metrics-scripted-metric-aggregation.html#search-aggregations-metrics-scripted-metric-aggregation">ドキュメント</a>)</li>
<li>新しい<code>GET /index</code>API。インデックスのsettings、mappings、warmers、aliasesを1回のリクエストで返却(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-get-index.html#indices-get-index">ドキュメント</a>)</li>
<li>自動付与ドキュメントIDのためのFlake ID。プライマリキーの探索パフォーマンスの改善。</li>
<li>ドキュメントに変更のない更新によるドキュメントの再インデックスの防止</li>
<li><code>function_score</code>クエリの関数で<code>weight</code>パラメータによる個別の改善を可能に。(<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/query-dsl-function-score-query.html#_weight">ドキュメント</a>)</li>
</ul>


<p>詳細については<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">1.4.0.Beta1のブログ(英語)</a>(<a href="http://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/">日本語訳</a>)をご覧ください。</p>

<p>Beta1以降の1.4.0の変更の全てについては、<a href="http://www.elasticsearch.org/downloads/1-4-0">1.4.0 release notes</a>でご覧いただけます。
以下では、2つの主な変更について紹介します。</p>

<h2>HTTP Pipelining</h2>

<p>HTTP pipeliningは複数のリクエストを1回のコネクションで、関連するレスポンスを待つことなく送信することができます。
そして、レスポンスは、受け取ったリクエストと同じ順序で返却されます。
HTTP/1.1の仕様で、pipeliningのサポートが必要です。ElasticsearchはHTTP/1.1であるとしてきましたが、pipeliningはサポートしていませんでした。この問題は.NETユーザで問題を引き起こしました。</p>

<p>現在、HTTP pipeliningは公式にサポート済みで、デフォルトで利用できます。<a href="https://github.com/elasticsearch/elasticsearch/pull/8299">#8299</a>をご覧ください。</p>

<h2>Upgrade API</h2>

<p>Luceneのすべてのリリースではバグフィックスや最適化が提供されます。しかし、多くのユーザは古いバージョンのLuceneで作成されたインデックスを持っており、より最新の改善による利点を利用できないことがあります。
新しい<code>upgrade</code>APIは、あなたのインデックスすべてもしくは一部を最新のLuceneフォーマットに透過的にアップグレードできます。</p>

<p><code>GET _upgrade</code>リクエストは、インデックスのアップグレードが必要かどうかを提示し、アップグレードに必要なセグメントのサイズをリポートすることによって、どのくらいの時間が必要かの目安を提供します。
<code>POST _upgrade</code>コマンドはバックグラウンドでインデックスを最新のLuceneフォーマットに書き換えます。</p>

<p>より詳しい情報は<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-upgrade.html"><code>upgrade</code>APIドキュメント</a>をご覧ください。</p>

<h2>試してみてください。</h2>

<p>Beta1リリースを利用し、経験・体験を報告していただいたベータテスターの方々に感謝します。
1.4.0がこれまでの最高のリリースになると確信しています。
ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-0">Elasticsearch 1.4.0</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sonatypeのバージョン番号で困ったので]]></title>
    <link href="http://blog.johtani.info/blog/2014/10/15/versioning-of-sonatype/"/>
    <updated>2014-10-15T15:26:08+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/10/15/versioning-of-sonatype</id>
    <content type="html"><![CDATA[<p><a href="http://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/">Elasticsearch 1.4.0.Beta1がリリース</a>されました。</p>

<p>個人で<a href="https://github.com/johtani/elasticsearch-extended-analyze">elasticsearch-extended-analyze</a>というプラグインを開発してます。
こちらも1.4.0.Beta1に対応するべく作業をしてて、少し戸惑ったことがあったので、メモをば。</p>

<!-- more -->


<p>ここ最近はプラグインのバージョン番号をElasticsearchのバージョン番号と同じものを利用していました。
（プラグインの機能追加をサボってる？？）
その時に、<code>1.4.0.Beta1</code>という番号を指定したのですが、意味不明なエラーに悩まされてしまいまして。</p>

<p>プラグインのリリースでは、以下のコマンドを実行します。</p>

<pre><code>$ mvn release:prepare
$ mvn release:perform
</code></pre>

<p>最初のコマンド（prepare）で、パッケージングを実施し、Githubにリリースタグを打ったバージョンがpushされます。
次のコマンド（perform）で、パッケージングされたzipファイルがsonatypeのサイトに公開するためにアップロードされます。</p>

<p><code>1.4.0.Beta1</code>というバージョン文字列を利用した場合、prepareは問題なく実行できたのですが、
performで以下の様なエラーが返ってきました。</p>

<pre><code>Return code is: 401, ReasonPhrase: Unauthorized.
</code></pre>

<p>バージョン番号が<code>1.3.0</code>では特に問題はなかったのですが、、、
結局、バージョン番号を<code>1.4.0-beta1</code>に変更すると問題なくリリースが完了しました。</p>

<p>mike_neckさんと話をしていて、<a href="http://semver.org">Semantic Versioning</a>に関係しているのかなぁという話にはなったのですが、
詳しく調べていません。。。</p>

<p>そのうち調べようかなぁ。。。。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[elasticsearch 1.4.0.Beta1のリリース]]></title>
    <link href="http://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja/"/>
    <updated>2014-10-02T19:14:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2014/10/02/elasticsearch-1-4-0-beta-released-ja</id>
    <content type="html"><![CDATA[<p>※この記事は次のブログを翻訳したものになります。</p>

<p>原文：<a href="http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/">elasticsearch 1.4.0.beta1 released</a></p>

<p>本日、<em>Lucene 4.10.1</em>をベースにした、<em>Elasticsearch 1.4.0.Beta1</em>をリリースしました。
<a href="http://www.elasticsearch.org/downloads/1-4-0-Beta1">Elasticsearch 1.4.0.Beta1</a>からダウンロードできます。
また、すべての変更点に関してもこちらをご覧ください。</p>

<!-- more -->


<p>1.4.0のテーマは<em>resiliency(復元性、弾力性)</em>です。
<em>resiliency</em>とはElasticsearchをより安定し信頼性のあるものにすることを意味します。
すべての機能が正常に機能している場合は信頼することは簡単です。
予想外のことが発生した時に難しくなります：ノードでout of memoryの発生、スローGCや重いI/O、ネットワーク障害、不安定なデータの送信によるノードのパフォーマンス低下など。</p>

<p>本ベータリリースは、resiliencyの主な3つの改善を含んでいます。</p>

<ul>
<li><a href="#memory-mgmt">メモリ使用量の低下</a>によるノードの安定性向上</li>
<li>discoveryアルゴリズムの改善による<a href="#cluster-stability">クラスタの安定性</a>向上</li>
<li><a href="#checksums">チェックサム</a>の導入による破損したデータの検知</li>
</ul>


<p>分散システムは複雑です。
決して想像できないような状況をシミュレーションするために、ランダムなシナリオを作成する広範囲なテストスイートを持っています。
しかし、無数のエッジケース(特殊なケース)があることも認識しています。
1.4.0.Beta1はこれまで私たちが行ってきた改善のすべてを含んでいます。
これらの変更を実際にテストしていただき、<a href="https://github.com/elasticsearch/elasticsearch/issues">何か問題があった場合は私たちに教えてください</a>。</p>

<h2><a name="memory-mgmt">メモリ管理</a></h2>

<p>ヒープ空間は限られたリソースです。
上限を32GBとし、利用可能なRAMの50%をヒープの上限にすることを推奨します。
この上限を超えた場合、JVMは圧縮したポインタを使用することができず、GCが非常に遅くなります。
ノードの不安定性の主な原因は遅いGCです。それは、次のようなことから発生します。</p>

<ul>
<li>メモリプレッシャー</li>
<li>スワップ(参照：<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/setup-configuration.html#setup-configuration-memory">memory settings</a>)</li>
<li>非常に大きなヒープ</li>
</ul>


<p>本リリースは、メモリ管理の改善し、（結果として）ノードの安定性を改善するいくつかの変更を含んでいます。</p>

<h3>doc values</h3>

<p>メモリの利用の最も大きなものの1つは<strong>fielddata</strong>です
aggregation、ソート、スクリプトがフィールドの値に素早くアクセスするために、フィールドの値をメモリにロードして保持します。
ヒープは貴重なため、1ビットも無駄にしないためにメモリ内のデータは高度な圧縮と最適化を行っています。
これは、ヒープスペース以上のデータをもつまでは、非常によく動作します。
これは、多くのノードを追加することによって常に解決できる問題です。
しかし、CPUやI/Oが限界に達してしまうずっと前に、ヒープ空間の容量に到達します。</p>

<p>最近のリリースは、<strong>doc values</strong>によるサポートがあります。
基本的に、doc valuesはin-memory fielddataと同じ機能を提供します。
doc valuesの提供する利点は、それらが、非常に少量のヒープ空間しか使用しない点です。
doc valuesはメモリからではなく、ディスクから読み込まれます。
ディスクアクセスは遅いですが、doc valuesはカーネルのファイルシステムキャッシュの利点を得られます。
ファイルシステムキャッシュはJVMヒープとはことなり、32GBの制限による束縛がありません。
ヒープからファイルシステムキャッシュにfielddataを移行することによって、より小さなヒープを使うことができます。これは、GCがより早くなり、ノードが更に安定することを意味します。</p>

<p>本リリースより前は、doc valuesはin-memory fielddataよりもかなり遅かったです。
本リリースに含まれる変更は、パフォーマンスをかなり向上させ、in-memory fielddataとほぼ同じくらいの速度になっています。</p>

<p>in-memory fielddataの代わりにdoc valuesを利用するために必要なことは、次のように新しいフィールドをマッピングすることです。</p>

<pre><code>PUT /my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "timestamp": {
          "type":       "date",
          "doc_values": true
        }
      }
    }
  }
}
</code></pre>

<p>このマッピングで、このフィールドに対するfielddataの利用は、メモリにフィールドをロードする代わりに、自動的にディスクからdoc valuesを利用します。
<em>注意：</em>現時点で、doc valuesはanalyzedな<code>string</code>フィールドはサポートしていません。</p>

<h3>request circuit breaker</h3>

<p>fielddata circuit breakerはfielddataによって利用されるメモリの上限を制限するために追加され、OOMEの最も大きな原因の1つを防ぎました。
そして、<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-fielddata.html#request-circuit-breaker">リクエストレベルのcircuit-breaker</a>を提供するために、コンセプトを拡張しました。
これは、単一のリクエストによって使用されるメモリの上限を制限します。</p>

<h3>bloom filters</h3>

<p><a href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a> はインデキシング(前のバージョンのドキュメントが存在するかどうかのチェックのため)や、
IDによるドキュメントの検索(ドキュメントを含むセグメントがどれかを決定するため)に関する重要な性能最適化を提供しました。
しかし、もちろんそれらはコスト（メモリ）を必要とします。
現在の改善は、bloom filterの必要性を取り除きました。
現在では、Elasticsearchはまだ、インデックス時にそれらを構築します(実世界の経験がテストシナリオにそぐわない場合に備えて)。
しかし、デフォルトではメモリにはロードされません。
すべてが予定通りに運べば、将来のバージョンで完全にこれらは除去します。</p>

<h2><a name="cluster-stability">クラスタの安定性</a></h2>

<p>クラスタの安定性向上のために私たちができる最も大きなことは、ノードの安定性の向上です。
もし、ノードが安定しておりタイミングよく反応すれば、クラスタが不安定になる可能性が大いに減少します。
私たちは不完全な世界に住んでいます。- 物事は予想外にうまく行きません。クラスタはデータを失うことなくこのような状況から回復できる必要があります。</p>

<p>私たちは、<code>improve_zen</code>ブランチ上で、Elasticsearchの障害からの復旧するための能力の向上に数ヶ月費やしてきました。
まず、複雑なネットワークレベルの障害を繰り返すためのテストを追加しました。
次に、各テストのための修正を追加しました。
そこには、より多くの行うことが存在します。しかし、私たちは、<a href="https://github.com/elasticsearch/elasticsearch/issues/2488">issue #2488</a>(&ldquo;分割が交差している場合、minimum_master_nodesはsplit-brainを防げない&rdquo;)に含まれる、ユーザが経験してきた大部分の問題を私たちは解決しました。</p>

<p>私たちはクラスタのresiliencyを非常に真剣に取り組んでいます。
私たちは、Elasticsearchが何ができるか、その上で何が弱点であるかを理解してほしいと思っています。
これを考慮して、私たちは<a href="http://www.elasticsearch.org/guide/en/elasticsearch/resiliency/current/index.html">Resiliency Status Document</a>を作成しました。
このドキュメントは、私たち(または私たちユーザ)が遭遇したresiliencyの問題の、何が修正済みで、何が修正されないまま残っているかを記録します。
このドキュメントを慎重に読み、あなたのデータを保護するために適切な方法を選択してください。</p>

<h2><a name="checksums">データ破損の検知</a></h2>

<p>ネットワークをまたいだシャードリカバリのチェックサムは、圧縮ライブラリのバグを発見する助けとなりました。
それは、バージョン1.3.2で修正済みです。
それ以来、私たちはElasticsearchのいたるところにチェックサムとチェックサムの確認を追加しました。</p>

<ul>
<li>マージ中に、あるセグメント内すべてのチェックサムの確認(<a href="https://github.com/elasticsearch/elasticsearch/issues/7360">#7360</a>)</li>
<li>インデックス再オープン時に、あるセグメント内の最も小さなファイルの完全な確認と、より大きなファイルの軽量な打ち切りチェック(<a href="https://issues.apache.org/jira/browse/LUCENE-5842">LUCENE-5842</a>)</li>
<li>トランザクションログからイベントを再生するとき、各イベントはチェックサムを確認される(<a href="https://github.com/elasticsearch/elasticsearch/issues/6554">#6554</a>)</li>
<li>シャードのリカバリ中もしくは、スナップショットからのリストア中にElasticsearchはローカルファイルとリモートのコピーが同一であるか確認する必要がある。ファイルの長さとチェックサムのみを使うのは不十分であることが確認された。このため、現在はセグメントのすべてのファイルの同一性を確認(<a href="https://github.com/elasticsearch/elasticsearch/issues/7159">#7159</a>)</li>
</ul>


<h2>その他のハイライト</h2>

<p><a href="http://www.elasticsearch.org/downloads/1-4-0-Beta1">Elasticsearch 1.4.0.Beta1のchangelog</a>に本リリースの多くの機能、改善、バグフィックスについて読むことができます。
ここでは、特筆すべきいくつかの変更について述べます。</p>

<h3>groovyによるmvelの置き換え</h3>

<p>Groovyは現在、デフォルトのscripting languageです。
以前のデフォルトはMVELで、古くなってきており、サンドボックス内で実行できないという事実は、セキュリティ問題でした。
Groovyはサンドボックスであり(それは、ボックスの外へは許可が必要)、メンテナンスされており、速いです！
詳しくは<a href="http://www.elasticsearch.org/blog/scripting/">scriptingについてのブログ記事</a>をご覧ください。</p>

<h3>デフォルトでcorsはオフ</h3>

<p>Elasticsearchのデフォルト設定はクロスサイトスクリプティングに対して脆弱でした。
私たちはデフォルトで<a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS</a>をオフにすることで修正しました。
Elasticsearchにインストールされたサイトプラグインはこれまで同様に機能します。
しかし、CORSを再度オンにすることがない限り、外部のウェブサイトがリモートのクラスタにアクセスすることはできません。
ウェブサイトがあなたのクラスタにアクセス可能に制御できるように、さらに<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-http.html#_settings_2">CORS settings</a>を追加しました。
詳しくは<a href="http://www.elasticsearch.org/community/security">security page</a>をご覧ください。</p>

<h3>クエリキャッシュ</h3>

<p>新しい試験的な<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-shard-query-cache.html">shardレベルのクエリキャッシュ</a>は、静的なインデックスのアグリゲーションをほとんど即座に反応できます。
ウエブサイトのアクセスの日毎のページビュー数を見るダッシュボードを持っていると想像してみてください。
これらの数値は古いインデックスでは変更がありません。しかし、アグリゲーションはダッシュボードのリフレッシュのたびに再計算されます。
新しいクエリキャッシュを利用すると、シャードのデータが変更されない限り、アグリゲーションの結果はキャッシュから直接返却されます。
キャッシュから古い結果を決して取得することはありません。それは、常に、キャッシュされていないリクエストと同じ結果を返します。</p>

<h3>新しいaggregations</h3>

<p>3つの新しいaggregationsがあります。</p>

<ul>
<li><p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-filters-aggregation.html"><code>filters</code></a></p>

<ul>
<li>これは<code>filter</code> aggregationの拡張です。複数のバケットを定義し、バケット毎に異なるフィルタを利用できます。</li>
</ul>
</li>
<li><p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-children-aggregation.html"><code>children</code></a></p>

<ul>
<li><code>nested</code>アグリゲーションの親子版。<code>children</code> aggは親のドキュメントに属する子のドキュメントを集計できる</li>
</ul>
</li>
<li><p><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-metrics-scripted-metric-aggregation.html"><code>scripted_metric</code></a></p>

<ul>
<li>このaggregationは、データによって計算されたメトリックを完全にコントロールできます。これは、初期化フェーズ、ドキュメント収集フェーズ、shardレベル結合フェーズ、global reduceフェーズを提供します。</li>
</ul>
</li>
</ul>


<h3>get /index api</h3>

<p>以前、ある1つのインデックスのaliases、mappings、settings、warmersを取得出来ました。しかし、それらを個別にです。
<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-get-index.html"><code>get-index</code> API</a> はこれらのすべてもしくは一部を、複数もしくはひとつのインデックスに対して一緒に取得できます。
これは、既存のインデックスと同一もしくはほぼ同一であるインデックスを作成したいときに非常に役に立ちます。</p>

<h3>登録と更新</h3>

<p>ドキュメントの登録と更新にいくつかの改善があります。</p>

<ul>
<li>現在、ドキュメントIDの自動生成のために<a href="http://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang">Flake ID</a>を使用しています。これは、プライマリキー探索時に素晴らしい性能向上を提供します。</li>
<li><code>detect_noop</code>に<code>true</code>を設定すると、ドキュメントに変更を与えない更新が軽量になります。この設定を有効にすると、<code>_source</code>フィールドのコンテンツを変更する更新リクエストだけ、ドキュメントの新しいバージョンを書き込みます。</li>
<li>更新はスクリプトから完全に操作できます。以前は、スクリプトはドキュメントがすでに存在しているときだけ実行可能で、それ以外は、<code>upsert</code>ドキュメントで登録しました。<code>script_upsert</code>パラメータでスクリプトから直接ドキュメントの作成が操作できます。</li>
</ul>


<h3>function score</h3>

<p>すでに非常に便利な<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/query-dsl-function-score-query.html"><code>function_score</code>クエリ</a>が、新しく<code>weight</code>パラメータをサポートします。
これは、それぞれの指定された関数の影響をチューニングするのに使われます。
これは、人気度よりも更新日時により重みをかけたり、地理情報よりも価格により重みをかけるといったことを可能にします。
また、<code>random_score</code>機能はセグメントマージによる影響を受けません。これにより、より一貫した順序が提供されます。</p>

<h2>試してみてください。</h2>

<p>ぜひ、<a href="http://www.elasticsearch.org/downloads/1-4-0-Beta1">Elasticsearch 1.4.0.Beta1</a>をダウンロードして、試してみてください。
そして、感想をTwitter(<a href="https://twitter.com/elasticsearch">@elasticsearch</a>)などで教えて下さい。
また、問題がありましたら、<a href="https://github.com/elasticsearch/elasticsearch/issues">GitHub issues page</a>で報告をお願いします。</p>
]]></content>
  </entry>
  
</feed>
