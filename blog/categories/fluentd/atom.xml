<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: fluentd | @johtaniの日記 2nd]]></title>
  <link href="http://blog.johtani.info/blog/categories/fluentd/atom.xml" rel="self"/>
  <link href="http://blog.johtani.info/"/>
  <updated>2015-10-30T17:52:54+09:00</updated>
  <id>http://blog.johtani.info/</id>
  <author>
    <name><![CDATA[johtani]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[第2回elasticsearch勉強会を開催しました！ #elasticsearchjp]]></title>
    <link href="http://blog.johtani.info/blog/2013/11/12/elasticsearch-japan-user-meetup-no2/"/>
    <updated>2013-11-12T18:16:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/11/12/elasticsearch-japan-user-meetup-no2</id>
    <content type="html"><![CDATA[<p>第2回を開催しました！
すごい、140人くらいくらいの参加登録者（参加者は１００人ちょっと！）がいて、びっくりです。
ステキな会場を提供していただいた、<a href="http://recruit-tech.co.jp/recruitment/">リクルートテクノロジーズ</a>さん、運営していただいた方々、スピーカーの皆さん、参加者の皆さん本当にありがとうございました。
今回も素敵な看板ありがとうございます。</p>

<p><img src="/images/entries/20131112/es_signboard.jpg" width="300" title="ステキな案内板" ></p>

<p>今回もしっかり楽しめたので、次回も頑張ります！</p>

<p>今回は、<a href="https://groups.google.com/forum/#!forum/elasticsearch-jp">elasticsearch-jp</a>MLの紹介とかをできたのでよかったかなぁと。
ぜひ、活用してください！どんな質問でもいいので。</p>

<p>あと、スライドに入ってた例の本もよろしくです。</p>

<!-- more -->


<p>ということで、懇親会も盛り上がったし楽しかったです。
今後も場の提供＋自分の勉強のトリガーとして、開催していくので、ご協力お願いします！
聞きたい話など、MLや@ツイートしていただければと。</p>

<h2>elasticsearchのRouting機能：株式会社シーマーク　大谷　純　（@johtani）</h2>

<p>スライド：<a href="/images/entries/20131112/About_es_routing.pdf">Routing機能</a>※スライドはPDFです。</p>

<p>ド緊張で、大した発表ではなかったですが。。。
どちらかと言うとSolr本の紹介だったかもなぁ。スミマセン。</p>

<p>※スライドが一部文字が消えてるので、作りなおすかも。</p>

<h2>ElasticSearchを使ったBaaS基盤の開発(仮)：株式会社富士通ソフトウェアテクノロジーズ 滝田聖己さん（@pisatoshi）</h2>

<p>スライド：<a href="https://speakerdeck.com/pisatoshi/elasticsearch-trial-and-error">https://speakerdeck.com/pisatoshi/elasticsearch-trial-and-error</a></p>

<p>本日はお越しいただきありがとうございました！しかも静岡から！今後もよろしくお願い致します。</p>

<ul>
<li>EnchantMoonでシステム構成ｗ</li>
<li>0.17.0から利用されていると。（スゴイ）</li>
<li>プライマリのデータストア！ただし、登録元データはMySQLにもある。</li>
<li>階層も深く、大きめのドキュメント。</li>
<li>レプリカ１、インデックスのバックアップも取ってないと。。。</li>
<li><p>ルーティングの機能</p></li>
<li><p>DynamicMappingの問題点</p></li>
<li>マッピング定義が肥大、型がコンフリクト。。。苦労しっぱなし</li>
<li>データ登録は１台にして、１台で一気に登録してから再配置</li>
<li>実際に運用とかされてるので、いろんなノウハウがまだまだありそう！</li>
</ul>


<h2>Kibana入門：水戸祐介さん（@y_310）</h2>

<p>スライド：<a href="https://speakerdeck.com/y310/kibanaru-men">https://speakerdeck.com/y310/kibanaru-men</a></p>

<p>（やっぱりru-menになってるｗ）</p>

<p>実は、押しかけて話してもらうように説得したのでした。今後もよろしくです。</p>

<ul>
<li>COOKPADの方によるKibanaのお話。</li>
<li>Kibanaの利点とかなんで？とか。</li>
<li>画面構成の説明から</li>
<li>ダッシュボードは必ず保存して！リロードしたら悲しい思いをしてしまうので。</li>
<li>sparkline便利そうだなぁ。ほんとに、データサイエンティスト系のツールを目指してるのかな</li>
<li>一通り、ダッシュボードに配置できるパネルの説明してもらえたのですごく参考になりました！</li>
<li>Tips周りが役に立ちそう。not_analyzedは重要ですよね。</li>
</ul>


<h2>LT</h2>

<h3>「データ集計用ダッシュボードブラウザとしても使えるElasticSearch＋Kibana v3を利用する際の運用ノウハウ紹介」：株式会社リブセンス Y.Kentaro さん (@yoshi_ken) さん</h3>

<p>スライド：<a href="http://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips">http://www.slideshare.net/y-ken/elasticsearch-kibnana-fluentd-management-tips</a></p>

<ul>
<li>Kibanaの紹介とかFluentdの紹介。</li>
<li>Tips満載すばらしい。</li>
<li>JDBC riverは0.90.6ではうまく動かないので、気をつけてと。</li>
</ul>


<h3>「Fluentd as a Kibana」：@repeatedly さん</h3>

<p>スライド(gist)？：<a href="https://gist.github.com/repeatedly/7427856">https://gist.github.com/repeatedly/7427856</a></p>

<p>Kibanaがfluentdの中で動くと！？</p>

<h3>「Authプラグインでアクセスコントロール」：株式会社エヌツーエスエム 菅谷信介さん (@shinsuke_sugaya)</h3>

<p>スライド：<a href="http://www.slideshare.net/shinsuke/es-auth-plugin">http://www.slideshare.net/shinsuke/es-auth-plugin</a></p>

<p>API毎？インデックスごと？にアクセス制御ができるプラグイン</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[apache-loggen + fluentd + elasticsearch + kibana = ログ検索デモ]]></title>
    <link href="http://blog.johtani.info/blog/2013/06/10/fluent-es-kibana/"/>
    <updated>2013-06-10T23:34:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/06/10/fluent-es-kibana</id>
    <content type="html"><![CDATA[<p>もう何番煎じだ？ってくらい書かれてますが、コリもせず書いてみました。
Elasticsearch＋Kibanaの環境を作って、タムタムさんのログ生成ツールからApacheのダミーログを流しこんで入れてみました。</p>

<!-- more -->


<h2>参考URL</h2>

<ul>
<li><a href="http://memocra.blogspot.jp/2013/04/kibanakibanaelasticsearchfluentd.html">memorycraftさんのブログ</a></li>
<li><a href="http://kibana.org/">Kibana</a></li>
<li><a href="http://wwwelasticsearch.org">Elasticsearch</a></li>
<li><a href="http://fluentd.org">fluentd</a></li>
<li><a href="http://mt.orz.at/archives/2012/11/apacherubygems.html">apache-loggen</a></li>
</ul>


<h2>インストールと起動</h2>

<p>今回はCentOSへのインストールです。
基本的にはmemorycraftさんのブログの流れのままです。</p>

<h3>elasticserchのインストールと起動</h3>

<p>ダウンロードして、起動するだけ。
お試しということで、-fオプションにてコンソールにログ出力。</p>

<pre><code>curl -OL https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.1.tar.gz
tar zxvf elasticsearch-0.90.1.tar.gz
cd elasticsearch-0.90.1
./bin/elasticsearch -f
</code></pre>

<h3>Kibanaのインストールと起動</h3>

<p><code>git clone</code>してbundleインストール</p>

<pre><code>git clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git
cd Kibana
bundle install
ruby kibana.rb
</code></pre>

<p>これで、Kibana＋ESのインストール＋起動が完了。
下地が完了。</p>

<h3>td-agentのインストールと起動</h3>

<p>ログの流し込みはlogstashなのですが、fluentdのelasticsearchプラグインにて流しこむこともできます。
td.repoとしてtd-agentのリポジトリを登録してから以下を実行します。</p>

<pre><code>yum install td-agent -y
/usr/lib64/fluent/ruby/bin/fluent-gem install fluent-plugin-elasticsearch
vim　/etc/td-agent/td-agent.conf
/etc/init.d/td-agent start
</code></pre>

<p>これで、td-agentがインストール出来ました。
次は設定です。</p>

<pre><code>&lt;source&gt;
  type tail
  format apache
  path /var/log/httpd/dummy_access_log
  tag dummy.apache.access
&lt;/source&gt;

&lt;match *.apache.*&gt;
  index_name adminpack
  type_name apache
  type elasticsearch
  include_tag_key true
  tag_key @log_name
  host localhost
  port 9200
  logstash_format true
  flush_interval 10s
&lt;/match&gt;
</code></pre>

<p>以上が設定です。td-agentはtd-agentというユーザで起動されるので、/var/log/httpdディレクトリにアクセスできるかだけ確認が必要です。<br/>
いくつかの設定値について気になったので調べました。</p>

<ul>
<li>index_name：adminpackとなってるが、elasticsearchではlogstash-xxxとなってる。

<ul>
<li>これは、logstash_formatがtrueの場合は、利用されないので、指定しなくてもいい。</li>
</ul>
</li>
<li>type_name：Elasticsearchのタイプ名
  *　これはlogstash_formatを指定しても有効。ただし、Kibana側で画面からのtype指定は不可能。KibanaConfig.rbにて指定することは可能。</li>
<li>logstash_format：Kibana用にlogstashフォーマットで出力するオプション

<ul>
<li>この指定があるときは、index名が「logstash-YYYY.mm.dd」となる</li>
<li>record（ログ）に@timestampとして時刻が追加される。</li>
</ul>
</li>
<li>tag_key：include_tag_keyがtrueと指定されているため、record（ログ）にtag_keyで指定した文字列をキー、値としてtagの値（上記例だとdummy.apache.access）が付与されて登録される。</li>
</ul>


<h2>apache-loggenのインストールと起動</h2>

<p>タムタムさんが作成されたApacheのログのダミーを生成するツールです。<br/>
gem化されてるので、インストールは非常に簡単です。</p>

<pre><code>gem install apache-loggen
</code></pre>

<p>で、ログを出力します。出力先は先程設定したdummy_access_logです。</p>

<pre><code>apache-loggen --rate=10 --progress /var/log/httpd/dummy_access_log
</code></pre>

<p>秒間10アクセスログを出力してくれます。
これで、Kibanaでログが見れるようになりました。
なんて簡単なんでしょう。。。
簡単なログの検索ができてしまいました。
他の形式のログがどうなるのかとかは、また時間があれば。。。</p>

<h2>感想とか</h2>

<p>非常に簡単でした。素敵です。いくつかこうなるのかな？というのを試してみたのでメモを。</p>

<p>いくつか疑問点です。</p>

<ul>
<li>溜まったログの削除は手動？

<ul>
<li>おそらく。日付ごとにindexが出来上がっているので、削除は楽そう。「logstash-年月日」なので。</li>
</ul>
</li>
<li>認証とかかけれるの？

<ul>
<li>ログ検索は内部でするだろうから、まぁ、なくていいのかな。ログインすらないし。</li>
</ul>
</li>
<li>複数行のログとかってどーすんだろう？（JavaのExceptionとかが混ざるやつ）</li>
</ul>


<p>本格的に触るようになれば調べるかなぁ。。。</p>

<p>あと、ログが増えてきた時にどういった分割構成ができるだろう？って思って考えてみたのが以下になります。</p>

<h3>構成パターン</h3>

<p>ログを複数扱う場合は次のようなパターンがありそうかと。</p>

<h4>タグ（fluentdのタグ）で識別</h4>

<p>「@log_name」という名前＝fluentdのタグにてログを識別することで、異なるログを検索することができそうです。
タグであれば、プラグインによってはログ出力時に制御も可能だと思うので、td-agentの設定を変更したりすることもなく対応が可能かと。
ただ、ログの種別ごとにKibanaのプロセスを別にして起動したいといった用途には向いてなさそうです。</p>

<h4>type_nameによる識別</h4>

<p>ElasticSearchの機能であるtypeを利用したログの識別パターンです。<br/>
fluent-plugin-elasticsearchの設定で<code>type_name</code>を指定しました。
ここを別の名前にすることで、識別することも可能です。</p>

<p><strike>ただし、この場合はKibanaの画面から指定して検索することができません。</strike>
<strong><em>→コメント頂きました。検索条件に「_type:タイプ名」と検索することでtypeを利用した検索が可能です。</em></strong><br/>
タグ（@log_name）でも識別できるようにするなどの工夫が必要です。
その代わり、タグ識別ではできなかったKibanaのプロセスを別にして起動することは可能になります。<br/>
KibanaConfig.rbのTypeに値を設定することで、起動したKibanaが対象とするログを絞り込むことが可能です。
こうすることで例えば、apache用のKibanaとtomcat用のKibanaは別プロセスにして、ElasticSearchのクラスタは1つという構成も可能になります。</p>

<h4>ElasticSearchサーバを別立て</h4>

<p>ElasticSearchサーバをそもそも別のプロセスor別のサーバで起動し、Kibanaも別々にすればログの識別も可能です。
可能ですが、色々と管理するものが増えてめんどくさそうですね。。。</p>

<h4>インデックス名変更</h4>

<p>最後は、fluent-plugin-elasticsearchの設定で「logstash_format」をfalseにすれば、好きなindex_nameを付与できるので、
ログ種別ごとに名前を変更することで識別できます。<br/>
ただ、logstash形式でないインデックス名の場合、日付ローテーションができなかったり、Kibana内部で検索時に日付で検索対象を絞り込んで検索することで高速化するといった処理など、使えない機能が多々出てきてしまうのであまりおすすめじゃないかと。。。</p>

<p>ということで、流行りものは触っておこうということで、さわってブログ書いてみました。<br/>
開発中に立てておいて、各サーバのログを流しこんでおくなどにも利用できるかもしれないです。
アラート通知などの機能が出てくるともっと便利かもしれないです。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fluent-plugin-zoomdata作りました＋悩み事とか]]></title>
    <link href="http://blog.johtani.info/blog/2013/06/03/fluent-plugin-zoomdata-0-0-1/"/>
    <updated>2013-06-03T13:43:00+09:00</updated>
    <id>http://blog.johtani.info/blog/2013/06/03/fluent-plugin-zoomdata-0-0-1</id>
    <content type="html"><![CDATA[<p>憧れ？のfluentを使ってみました。
<a href="http://atl.recruit-tech.co.jp/blog/668/">こちらの記事</a>で紹介した<a href="http://zoomdata.com">Zoomdata</a>を最近触っているのですが、お試しにfluentdでデータ流し込むプラグインを作ってみようかなぁと。（今は、Javaでの接続も書いていて、主にそっちを使っています。）
ということで、作ってみました。<a href="https://github.com/johtani/fluent-plugin-zoomdata">fluent-plugin-zoomdata</a>。</p>

<p>基本的にはtagomoris先生の<a href="https://github.com/tagomoris/fluent-plugin-growthforecast">fluent-plugin-growthforecast</a>を参考（パクリ？）にさせてもらいました。
作っている最中もここわからんってツイートに反応していただき、大変助かりました。
私はベースがJavaの人間なので、手探り状態でRubyを書いています。そこおかしいんじゃないの？とかあればコメントもらえると嬉しいです。</p>

<!-- more -->


<h2>ZoomdataのAPI</h2>

<p>ZoomdataのAPIはこんなかんじで、JSONをHTTPSでPOSTするものになります。</p>

<script src="https://gist.github.com/wlindner/4587444.js"></script>


<p>指定する必要があるものは、以下の項目です。</p>

<ul>
<li>source：Zoomdataのデータ保存先（Zoomdataでのデータを保存する単位。）</li>
<li>user：Zoomdataのユーザ名</li>
<li>password：Zoomdataのユーザのパスワード</li>
<li>JSONデータ：Zoomdataでグラフ化するデータ</li>
</ul>


<p>sourceはデータ保存先の名称で、この単位でZoomdataはデータを保存、描画します。fluentdのタグをこれにするとわかりやすいかなぁ？と考えていったん、実装してみています。</p>

<p>で、作成したプログラムを使いつつ、Zoomdataの検証をしたかったので、つぎのような簡単なプログラムを作って動かしてみました。</p>

<h2>サンプルプログラムの構成</h2>

<p>基本的にJavaの人なので、クライアントはJavaで書いてます。
CLIプログラムで適当なJSONを作って、fluent-loggerを使って、fluentdに投げ込みます。</p>

<p><img src="/images/entries/sample_pg_zoomdata.jpg" width="500" title="&lsquo;sample program and server&rsquo;" ></p>

<p>fluentdにfluent-plugin-zoomdataを設定して、localのZoomdataサーバに対してHTTPSでJSONをPostする仕組みです。(初keynote)<br/>
利用しているライブラリなどのバージョンは次の通り</p>

<ul>
<li>td-agent.x86_64：1.1.12-0</li>
<li>fluent-logger：0.2.8</li>
<li>Zoomdata：1.0.3</li>
</ul>


<h2>バグ？</h2>

<p>で、Zoomdataにいろんなデータを流し込んでみたのですが、つぎのようなエラーが出て、エラーが出力されたあとはZoomdataにデータが流れ込まなくなってしまいました。</p>

<pre><code>2013-06-03 14:42:33 +0900 [warn]: emit transaction failed  error="SSL_connect returned=1 errno=0 state=SSLv3 read finished A: sslv3 alert handshake failure"
  2013-06-03 14:42:33 +0900 [warn]: /usr/lib64/fluent/ruby/lib/ruby/1.9.1/net/http.rb:799:in `connect'
  2013-06-03 14:42:33 +0900 [warn]: /usr/lib64/fluent/ruby/lib/ruby/1.9.1/net/http.rb:799:in `block in connect'
  2013-06-03 14:42:33 +0900 [warn]: /usr/lib64/fluent/ruby/lib/ruby/1.9.1/timeout.rb:54:in `timeout'
</code></pre>

<p><a href="https://gist.github.com/johtani/5696295">全ログはこちら</a>。</p>

<p>まだきちんと問題を調査しないでブログを書いています、すみません。</p>

<h3>現象</h3>

<p>ログが発生した時の症状です。</p>

<ul>
<li>クライアントプログラムは送信が続いており、エラーは出ない</li>
<li>td-agent.logに先ほどのエラーが出力</li>
<li>別途<code>type file</code>にて出力しているログも停止</li>
</ul>


<h3>想像</h3>

<p>とりあえず、ログを見た想像、所感です。</p>

<ul>
<li>問題の箇所はfluent-plugin-zoomdataからZoomdataサーバへのデータ送信部分</li>
<li>emit処理内部で、HTTPSでデータをPOSTする処理でエラーが起きて</li>
<li>リトライ処理とか書いてないので、emitがコケて、その後データが送信されなくなる</li>
<li>emitで例外をつかみそこねてるのがあるから止まってる？</li>
</ul>


<p>とまぁ、ちゃんと仕組みを理解しないでRubyとか書くからこうなるんですねぇ。
あとでちゃんと調べて考えて、改良してブログ書きます。</p>

<h2>悩んでいる点、今後手を入れたい点</h2>

<p>上記バグとは別に作りの点でいくつか悩んでる点も書いてみます。</p>

<h3>BufferedOutputにしてみたい</h3>

<p>fluentdのバッファリングを使って、Zoomdataが落ちていても使えるようにしたいと思っているのでBufferedOutputで書くのがいいのかなぁとか。
ちょうど<a href="http://www.slideshare.net/harukayon/fluentd-22317236">いいスライド</a>があったので、読みながらまずは中身を理解してみよう。</p>

<h3>Zoomdataのsource、userなどの扱い</h3>

<p>基本的には設定ファイルで切り替えるのが妥当かなぁと思っています。<br/>
ただ、Zoomdataのsourceやuserが増えるたびにfluentdの設定を書き換えて再起動するのかなぁと。userはしょうが無いにしても、sourceは設定じゃない所で切り替えたいなぁと。</p>

<p>で、切り替えるのにつぎの案があるかなぁと。</p>

<ol>
<li>タグで指定（今実装してるもの）</li>
<li>メッセージにメタ情報とボディ構造を設ける</li>
<li>設定をどんどん増やす（やりたくない）</li>
</ol>


<p>1と3はまぁ、いいかと。2.のパターンはどうなのかなぁと。
毎回のメッセージでヘッダ部分が送信されるのはなんだか無駄だなぁというのが否めないので悩ましいところです。1、2の両方対応できるように作るのもありか。</p>

<pre><code class="json">{
    "header": {
        "source": "source_name", 
        "user": "userid",
        "password": "userid",
    },
    "body": {
        "label": "label1",
        "count": 1
    }
}
</code></pre>

<p>ということで、fluent触って遊ぶの楽しいですね。Rubyの勉強にもなりそうだし。
ちょっとずつ頑張ってみようかなぁと。
まぁ、まだ私以外にニーズは無さそうなプラグインですが。</p>
]]></content>
  </entry>
  
</feed>
